title,author,link,text
Blaming individuals for obesity may be altogether wrong | Aeon Essays,,https://aeon.co/essays/blaming-individuals-for-obesity-may-be-altogether-wrong,"Years ago, after a plane trip spent reading Fyodor Dostoyevsky’s Notes from the Underground and Weight Watchers magazine, Woody Allen melded the two experiences into a single essay. ‘I am fat,’ it began. ‘I am disgustingly fat. I am the fattest human I know. I have nothing but excess poundage all over my body. My fingers are fat. My wrists are fat. My eyes are fat. (Can you imagine fat eyes?).’ It was 1968, when most of the world’s people were more or less ‘height-weight proportional’ and millions of the rest were starving. Weight Watchers was a new organisation for an exotic new problem. The notion that being fat could spur Russian-novel anguish was good for a laugh.
That, as we used to say during my Californian adolescence, was then. Now, 1968’s joke has become 2013’s truism. For the first time in human history, overweight people outnumber the underfed, and obesity is widespread in wealthy and poor nations alike. The diseases that obesity makes more likely — diabetes, heart ailments, strokes, kidney failure — are rising fast across the world, and the World Health Organisation predicts that they will be the leading causes of death in all countries, even the poorest, within a couple of years. What’s more, the long-term illnesses of the overweight are far more expensive to treat than the infections and accidents for which modern health systems were designed. Obesity threatens individuals with long twilight years of sickness, and health-care systems with bankruptcy.
And so the authorities tell us, ever more loudly, that we are fat — disgustingly, world-threateningly fat. We must take ourselves in hand and address our weakness. After all, it’s obvious who is to blame for this frightening global blanket of lipids: it’s us, choosing over and over again, billions of times a day, to eat too much and exercise too little. What else could it be? If you’re overweight, it must be because you are not saying no to sweets and fast food and fried potatoes. It’s because you take elevators and cars and golf carts where your forebears nobly strained their thighs and calves. How could you do this to yourself, and to society?
Moral panic about the depravity of the heavy has seeped into many aspects of life, confusing even the erudite. Earlier this month, for example, the American evolutionary psychologist Geoffrey Miller expressed the zeitgeist in this tweet: ‘Dear obese PhD applicants: if you don’t have the willpower to stop eating carbs, you won’t have the willpower to do a dissertation. #truth.’ Businesses are moving to profit on the supposed weaknesses of their customers. Meanwhile, governments no longer presume that their citizens know what they are doing when they take up a menu or a shopping cart. Yesterday’s fringe notions are becoming today’s rules for living — such as New York City’s recent attempt to ban large-size cups for sugary soft drinks, or Denmark’s short-lived tax surcharge on foods that contain more than 2.3 per cent saturated fat, or Samoa Air’s 2013 ticket policy, in which a passenger’s fare is based on his weight because: ‘You are the master of your air ‘fair’, you decide how much (or how little) your ticket will cost.’
Several governments now sponsor jauntily named pro-exercise programmes such as Let’s Move! (US), Change4Life (UK) and actionsanté (Switzerland). Less chummy approaches are spreading, too. Since 2008, Japanese law requires companies to measure and report the waist circumference of all employees between the ages of 40 and 74 so that, among other things, anyone over the recommended girth can receive an email of admonition and advice.
Hand-in-glove with the authorities that promote self-scrutiny are the businesses that sell it, in the form of weight-loss foods, medicines, services, surgeries and new technologies. A Hong Kong company named Hapilabs offers an electronic fork that tracks how many bites you take per minute in order to prevent hasty eating: shovel food in too fast and it vibrates to alert you. A report by the consulting firm McKinsey & Co predicted in May 2012 that ‘health and wellness’ would soon become a trillion-dollar global industry. ‘Obesity is expensive in terms of health-care costs,’ it said before adding, with a consultantly chuckle, ‘dealing with it is also a big, fat market.’
And so we appear to have a public consensus that excess body weight (defined as a Body Mass Index of 25 or above) and obesity (BMI of 30 or above) are consequences of individual choice. It is undoubtedly true that societies are spending vast amounts of time and money on this idea. It is also true that the masters of the universe in business and government seem attracted to it, perhaps because stern self-discipline is how many of them attained their status. What we don’t know is whether the theory is actually correct.
Higher levels of female obesity correlated with higher levels of gender inequality in each nation
Of course, that’s not the impression you will get from the admonishments of public-health agencies and wellness businesses. They are quick to assure us that ‘science says’ obesity is caused by individual choices about food and exercise. As the Mayor of New York, Michael Bloomberg, recently put it, defending his proposed ban on large cups for sugary drinks: ‘If you want to lose weight, don’t eat. This is not medicine, it’s thermodynamics. If you take in more than you use, you store it.’ (Got that? It’s not complicated medicine , it’s simple physics , the most sciencey science of all.)
Yet the scientists who study the biochemistry of fat and the epidemiologists who track weight trends are not nearly as unanimous as Bloomberg makes out. In fact, many researchers believe that personal gluttony and laziness cannot be the entire explanation for humanity’s global weight gain. Which means, of course, that they think at least some of the official focus on personal conduct is a waste of time and money. As Richard L Atkinson, Emeritus Professor of Medicine and Nutritional Sciences at the University of Wisconsin and editor of the International Journal of Obesity , put it in 2005: ‘The previous belief of many lay people and health professionals that obesity is simply the result of a lack of willpower and an inability to discipline eating habits is no longer defensible.’
C onsider, for example, this troublesome fact, reported in 2010 by the biostatistician David B Allison and his co-authors at the University of Alabama in Birmingham: over the past 20 years or more, as the American people were getting fatter, so were America’s marmosets. As were laboratory macaques, chimpanzees, vervet monkeys and mice, as well as domestic dogs, domestic cats, and domestic and feral rats from both rural and urban areas. In fact, the researchers examined records on those eight species and found that average weight for every one had increased. The marmosets gained an average of nine per cent per decade. Lab mice gained about 11 per cent per decade. Chimps, for some reason, are doing especially badly: their average body weight had risen 35 per cent per decade. Allison, who had been hearing about an unexplained rise in the average weight of lab animals, was nonetheless surprised by the consistency across so many species. ‘Virtually in every population of animals we looked at, that met our criteria, there was the same upward trend,’ he told me.
It isn’t hard to imagine that people who are eating more themselves are giving more to their spoiled pets, or leaving sweeter, fattier garbage for street cats and rodents. But such results don’t explain why the weight gain is also occurring in species that human beings don’t pamper, such as animals in labs, whose diets are strictly controlled. In fact, lab animals’ lives are so precisely watched and measured that the researchers can rule out accidental human influence: records show those creatures gained weight over decades without any significant change in their diet or activities. Obviously, if animals are getting heavier along with us, it can’t just be that they’re eating more Snickers bars and driving to work most days. On the contrary, the trend suggests some widely shared cause, beyond the control of individuals, which is contributing to obesity across many species.
Such a global hidden factor (or factors) might help to explain why most people gain weight gradually, over decades, in seeming contradiction of Bloomberg’s thermodynamics. This slow increase in fat stores would suggest that they are eating only a tiny bit more each month than they use in fuel. But if that were so, as Jonathan C K Wells, professor of child nutrition at University College London, has pointed out, it would be easy to lose weight. One recent model estimated that eating a mere 30 calories a day more than you use is enough to lead to serious weight gain. Given what each person consumes in a day (1,500 to 2,000 calories in poorer nations; 2,500 to 4,000 in wealthy ones), 30 calories is a trivial amount: by my calculations, that’s just two or three peanut M&Ms. If eliminating that little from the daily diet were enough to prevent weight gain, then people should have no trouble losing a few pounds. Instead, as we know, they find it extremely hard.
Many other aspects of the worldwide weight gain are also difficult to square with the ‘it’s-just-thermodynamics’ model. In rich nations, obesity is more prevalent in people with less money, education and status. Even in some poor countries, according to a survey published last year in the International Journal of Obesity, increases in weight over time have been concentrated among the least well-off. And the extra weight is unevenly distributed among the sexes, too. In a study published in the Social Science and Medicine journal last year, Wells and his co-authors found that, in a sample that spanned 68 nations, for every two obese men there were three obese women. Moreover, the researchers found that higher levels of female obesity correlated with higher levels of gender inequality in each nation. Why, if body weight is a matter of individual decisions about what to eat, should it be affected by differences in wealth or by relations between the sexes?
Chemicals ingested on Tuesday might promote more fat retention on Wednesday
To make sense of all this, the purely thermodynamic model must appeal to complicated indirect effects. The story might go like this: being poor is stressful, and stress makes you eat, and the cheapest food available is the stuff with a lot of ‘empty calories’, therefore poorer people are fatter than the better-off. These wheels-within-wheels are required because the mantra of the thermodynamic model is that ‘a calorie is a calorie is a calorie’: who you are and what you eat are irrelevant to whether you will add fat to your frame. The badness of a ‘bad’ food such as a Cheeto is that it makes calorie intake easier than it would be with broccoli or an apple.
Yet a number of researchers have come to believe, as Wells himself wrote earlier this year in the European Journal of Clinical Nutrition , that ‘all calories are not equal’. The problem with diets that are heavy in meat, fat or sugar is not solely that they pack a lot of calories into food; it is that they alter the biochemistry of fat storage and fat expenditure, tilting the body’s system in favour of fat storage. Wells notes, for example, that sugar, trans-fats and alcohol have all been linked to changes in ‘insulin signalling’, which affects how the body processes carbohydrates. This might sound like a merely technical distinction. In fact, it’s a paradigm shift: if the problem isn’t the number of calories but rather biochemical influences on the body’s fat-making and fat-storage processes, then sheer quantity of food or drink are not the all-controlling determinants of weight gain. If candy’s chemistry tilts you toward fat, then the fact that you eat it at all may be as important as the amount of it you consume.
More importantly, ‘things that alter the body’s fat metabolism’ is a much wider category than food. Sleeplessness and stress, for instance, have been linked to disturbances in the effects of leptin, the hormone that tells the brain that the body has had enough to eat. What other factors might be at work? Viruses, bacteria and industrial chemicals have all entered the sights of obesity research. So have such aspects of modern life as electric light, heat and air conditioning. All of these have been proposed, with some evidence, as direct causes of weight gain: the line of reasoning is not that stress causes you to eat more, but rather that it causes you to gain weight by directly altering the activities of your cells. If some or all of these factors are indeed contributing to the worldwide fattening trend, then the thermodynamic model is wrong.
We are, of course, surrounded by industrial chemicals. According to Frederick vom Saal, professor of biological sciences at the University of Missouri, an organic compound called bisphenol-A (or BPA) that is used in many household plastics has the property of altering fat regulation in lab animals. And a recent study by Leonardo Trasande and colleagues at the New York University School of Medicine with a sample size of 2,838 American children and teens found that, for the majority, those with the highest levels of BPA in their urine were five times more likely to be obese than were those with the lowest levels.
BPA has been used so widely — in everything from children’s sippy cups to the aluminium in fizzy drink cans — that almost all residents of developed nations have traces of it in their pee. This is not to say that BPA is unique. In any developed or developing nation there are many compounds in the food chain that seem, at the very least, to be worth studying as possible ‘obesogens’ helping to tip the body’s metabolism towards obesity. For example, a study by the Environmental Working Group of the umbilical cords of 10 babies born in US hospitals in 2004 found 287 different industrial chemicals in their blood. Beatrice Golomb, professor of medicine at the University of California, San Diego, has proposed a long list of candidates — all chemicals that, she has written, disrupt the normal process of energy storage and use in cells. Her suspects include heavy metals in the food supply, chemicals in sunscreens, cleaning products, detergents, cosmetics and the fire retardants that infuse bedclothes and pyjamas.
Chemicals and metals might promote obesity in the short term by altering the way that energy is made and stored within cells, or by changing the signals in the fat-storage process so that the body makes more fat cells, or larger fat cells. They could also affect the hormones that spur or tamp down the appetite. In other words, chemicals ingested on Tuesday might promote more fat retention on Wednesday.
It’s also possible that chemical disrupters could affect people’s body chemistry on longer timescales — starting, for instance, before their birth. Contrary to its popular image of serene imperturbability, a developing foetus is in fact acutely sensitive to the environment into which it will be born, and a key source of information about that environment is the nutrition it gets via the umbilical cord. As David J P Barker, professor of clinical epidemiology of the University of Southampton, noted some 20 years ago, where mothers have gone hungry, their offspring are at a greater risk of obesity. The prenatal environment, Barker argued, tunes the children’s metabolism for a life of scarcity, preparing them to store fat whenever they can, to get them through periods of want. If those spells of scarcity never materialise, the child’s proneness to fat storage ceases to be an advantage. The 40,000 babies gestated during Holland’s ‘Hunger Winter’ of 1944-1945 grew up to have more obesity, more diabetes and more heart trouble than their compatriots who developed without the influence of war-induced starvation.
It’s possible that widespread electrification is promoting obesity by making humans eat at night, when our ancestors were asleep
Just to double down on the complexity of the question, a number of researchers also think that industrial compounds might be affecting these signals. For example, Bruce Blumberg, professor of developmental and cell biology at the University of California, Irvine, has found that pregnant mice exposed to organotins (tin-based chemical compounds that are used in a wide variety of industries) will have heavier offspring than mice in the same lab who were not so exposed. In other words, the chemicals might be changing the signal that the developing foetus uses to set its metabolism. More disturbingly, there is evidence that this ‘foetal programming’ could last more than one generation. A good predictor of your birth weight, for instance, is your mother’s weight at her birth.
L urking behind these prime suspects, there are the fugitive possibilities — what David Allison and another band of co-authors recently called the ‘roads less travelled’ of obesity research. For example, consider the increased control civilisation gives people over the temperature of their surroundings. There is a ‘thermoneutral zone’ in which a human body can maintain its normal internal temperature without expending energy. Outside this zone, when it’s hot enough to make you sweat or cold enough to make you shiver, the body has to expend energy to maintain homeostasis. Temperatures above and below the neutral zone have been shown to cause both humans and animals to burn fat, and hotter conditions also have an indirect effect: they make people eat less. A restaurant on a warm day whose air conditioning breaks down will see a sharp decline in sales (yes, someone did a study). Perhaps we are getting fatter in part because our heaters and air conditioners are keeping us in the thermoneutral zone.
And what about light? A study by Laura Fonken and colleagues at the Ohio State University in Columbus, published in 2010 in the Proceedings of the National Academy of Sciences , reported that mice exposed to extra light (experiencing either no dark at all or a sort of semidarkness instead of total night) put on nearly 50 per cent more weight than mice fed the same diet who lived on a normal night-day cycle of alternating light and dark. This effect might be due to the constant light robbing the rodents of their natural cues about when to eat. Wild mice eat at night, but night-deprived mice might have been eating during the day, at the ‘wrong’ time physiologically. It’s possible that widespread electrification is promoting obesity by making humans eat at night, when our ancestors were asleep.
There is also the possibility that obesity could quite literally be contagious. A virus called Ad-36, known for causing eye and respiratory infections in people, also has the curious property of causing weight gain in chickens, rats, mice and monkeys. Of course, it would be unethical to test for this effect on humans, but it is now known that antibodies to the virus are found in a much higher percentage of obese people than in people of normal weight. A research review by Tomohide Yamada and colleagues at the University of Tokyo in Japan, published last year in the journal PLoS One , found that people who had been infected with Ad-36 had significantly higher BMI than those who hadn’t.
As with viruses, so with bacteria. Experiments by Lee Kaplan and colleagues at Massachusetts General Hospital in Boston earlier this year found that bacteria from mice that have lost weight will, when placed in other mice, apparently cause those mice to lose weight, too. And a study in humans by Ruchi Mathur and colleagues at the Cedars-Sinai Medical Center in Los Angeles, published in the Journal of Clinical Endocrinology and Metabolism earlier this year, found that those who were overweight were more likely than others to have elevated populations of a gut microorganisms called Methanobrevibacter smithii. The researchers speculated that these organisms might in fact be especially good at digesting food, yielding up more nutrients and thus contributing to weight gain.
The researcher who first posited a viral connection in 1992 — he had noticed that the chickens in India that were dead of an adenovirus infection were plump instead of gaunt — was Nikhil Dhurandhar, now a professor at the Pennington Biomedical Research Centre in Louisiana. He has proposed a catchy term for the spread of excess weight via bugs and viruses: ‘infectobesity’.
N o one has claimed, or should claim, that any of these ‘roads less taken’ is the one true cause of obesity, to drive out the false idol of individual choice. Neither should we imagine that the existence of alternative theories means that governments can stop trying to forestall a major public-health menace. These theories are important for a different reason. Their very existence — the fact that they are plausible, with some supporting evidence and suggestions for further research — gives the lie to the notion that obesity is a closed question, on which science has pronounced its final word. It might be that every one of the ‘roads less travelled’ contributes to global obesity; it might be that some do in some places and not in others. The openness of the issue makes it clear that obesity isn’t a simple school physics experiment.
We are increasingly understanding that attributing obesity to personal responsibility is very simplistic
This is the theme of perhaps the most epic of the alternative theories of obesity, put forward by Jonathan C K Wells. As I understand his view, obesity is like poverty, or financial booms and busts, or war — a large-scale development that no one deliberately intends, but which emerges out of the millions of separate acts that together make human history. His model suggests that the best Russian novelist to invoke when thinking about obesity isn’t Dostoyevsky, with his self-punishing anguish, but Leo Tolstoy, with his vast perspective on the forces of history.
In Wells’s theory, the claim that individual choice drives worldwide weight gain is an illusion — like the illusion that individuals can captain their fates independent of history. In reality, Tolstoy wrote at the end of War and Peace (1869) , we are moved by social forces we do not perceive, just as the Earth moves through space, driven by physical forces we do not feel. Such is the tenor of Wells’s explanation for modern obesity. Its root cause, he proposed last year in the American Journal of Human Biology, is nothing less than the history of capitalism.
I will paraphrase Wells’s intricate argument (the only one I’ve ever read that references both receptor pathways for leptin and data on the size of the Indian economy in the 18th century). It is a saga spanning many generations. Let’s start with a poor farmer growing food crops in a poor country in Africa or Asia. In a capitalistic quest for new markets and cheap materials and labour, Europeans take control of the economy in the late 18th or early 19th century. With taxes, fees and sometimes violent repression, their new system strongly ‘encourages’ the farmer and his neighbours to stop growing their own food and start cultivating some more marketable commodity instead – coffee for export, perhaps. Now that they aren’t growing food, the farmers must buy it. But since everyone is out to maximise profit, those who purchase the coffee crop strive to pay as little as possible, and so the farmers go hungry. Years later, when the farmer’s children go to work in factories, they confront the same logic: they too are paid as little as possible for their labour. By changing the farming system, capitalism first removes traditional protections against starvation, and then pushes many previously self-sufficient people into an economic niche where they aren’t paid enough to eat well.
Eighty years later, the farmer’s descendants have risen out of the ranks of the poor and joined the fast-growing ranks of the world’s 21st-century middle-class consumers, thanks to globalisation and outsourcing. Capitalism welcomes them: these descendants are now prime targets to live the obesogenic life (the chemicals, the stress, the air conditioning, the elevators-instead-of-stairs) and to buy the kinds of foods and beverages that are ‘metabolic disturbers’.
But that’s not the worst of it. As I’ve mentioned, the human body’s response to its nutrition can last a lifetime, and even be passed on to the next generation. If you or your parents – or their parents – were undernourished, you’re more likely to become obese in a food-rich environment. Moreover, obese people, when they have children, pass on changes in metabolism that can predispose the next generation to obesity as well. Like the children of underfed people, the children of the over fed have their metabolism set in ways that tend to promote obesity. This means that a past of under nutrition, combined with a present of over nutrition, is an obesity trap.
Wells memorably calls this double-bind the ‘metabolic ghetto’, and you can’t escape it just by turning poor people into middle-class consumers: that turn to prosperity is precisely what triggers the trap. ‘Obesity,’ he writes, ‘like undernutrition, is thus fundamentally a state of malnutrition, in each case promoted by powerful profit-led manipulations of the global supply and quality of food.’
The trap is deeper than that, however. The ‘unifying logic of capitalism’, Wells continues, requires that food companies seek immediate profit and long-term success, and their optimal strategy for that involves encouraging people to choose foods that are most profitable to produce and sell — ‘both at the behavioural level, through advertising, price manipulations and restriction of choice, and at the physiological level through the enhancement of addictive properties of foods’ (by which he means those sugars and fats that make ‘metabolic disturber’ foods so habit-forming). In short, Wells told me via email, ‘We need to understand that we have not yet grasped how to address this situation, but we are increasingly understanding that attributing obesity to personal responsibility is very simplistic.’ Rather than harping on personal responsibility so much, Wells believes, we should be looking at the global economic system, seeking to reform it so that it promotes access to nutritious food for everyone. That is, admittedly, a tall order. But the argument is worth considering, if only as a bracing critique of our individual-responsibility ideology of fatness.
W hat are we onlookers — non-activists, non-scientists — to make of these scientific debates? One possible response, of course, is to decide that no obesity policy is possible, because ‘science is undecided’. But this is a moron’s answer: science is never completely decided; it is always in a state of change and self-questioning, and it offers no final answers. There is never a moment in science when all doubts are gone and all questions settled, which is why ‘wait for settled science’ is an argument advanced by industries that want no interference with their status quo.
Making policy, as the British politician Wayland Young once said, is ‘the art of taking good decisions on insufficient evidence’. Faced with signs of a massive public-health crisis in the making, governments are right to seek to do something , using the best information that science can render, in the full knowledge that science will have different information to offer in 10 or 20 years.
The issue, rather, is whether the government policies and corporate business plans are in fact doing their best with the evidence they already have. Does the science justify assuming that obesity is a simple matter of individuals letting themselves eat too much? To the extent that it is, policies such as Japan’s mandatory waist-measuring and products like the Hapifork will be effective. If, on the other hand, there is more to obesity than simple thermodynamics, some of the billions spent on individual-centred policies and products may be being wasted. Time, in that case, to try some alternative policies based on alternative theories, and see how they fare.
Today’s priests of obesity prevention proclaim with confidence and authority that they have the answer. So did Bruno Bettelheim in the 1950s, when he blamed autism on mothers with cold personalities. So, for that matter, did the clerics of 18th-century Lisbon, who blamed earthquakes on people’s sinful ways. History is not kind to authorities whose mistaken dogmas cause unnecessary suffering and pointless effort, while ignoring the real causes of trouble. And the history of the obesity era has yet to be written."
The idea that sperm race to the egg is just another macho myth | Aeon Essays,,https://aeon.co/essays/the-idea-that-sperm-race-to-the-egg-is-just-another-macho-myth,"Before science was able to shed light on human reproduction, most people thought new life arose through spontaneous generation from non-living matter. That changed a smidgen in the middle of the 17th century, when natural philosophers were able (barely) to see the female ovum, or egg, with the naked eye. They theorised that all life was spawned at the moment of divine creation; one person existed inside the other within a woman’s eggs, like Russian nesting dolls. This view of reproduction, called preformation , suited the ruling class well. ‘By putting lineages inside each other,’ notes the Portuguese developmental biologist and writer Clara Pinto-Correia in The Ovary of Eve (1997), ‘preformation could function as a “politically correct” antidemocratic doctrine, implicitly legitimising the dynastic system – and of course, the leading natural philosophers of the Scientific Revolution certainly were not servants.’
One might think that, as science progressed, it would crush the Russian-doll theory through its lucid biological lens. But that’s not precisely what occurred – instead, when the microscope finally enabled researchers to see not just eggs but sperm, the preformation theory morphed into a new, even more patriarchal political conceit: now, held philosophers and some students of reproduction, the egg was merely a passive receptacle waiting for vigorous sperm to arrive to trigger development. And sperm? The head of each contained a tiny preformed human being – a homunculus, to be exact. The Dutch mathematician and physicist Nicolaas Hartsoeker, inventor of the screw-barrel microscope, drew his image of the homunculus when sperm became visible for the first time in 1695. He did not actually see a homunculus in the sperm head, Hartsoeker conceded at the time, but he convinced himself that it was there.
More powerful microscopes eventually relegated the homunculus to the dustbin of history – but in some ways not much has changed. Most notably, the legacy of the homunculus survives in the stubbornly persistent notion of the egg as a passive participant in fertilisation, awaiting the active sperm to swim through a hailstorm of challenges to perpetuate life. It’s understandable – though unfortunate – that a lay public might adopt these erroneous, sexist paradigms and metaphors. But biologists and physicians are guilty as well.
It was in the relatively recent year of 1991, long after much of the real science had been set in stone, that the American anthropologist Emily Martin, now at New York University, described what she called a ‘scientific fairy tale’ – a picture of egg and sperm that suggests that ‘female biological processes are less worthy than their male counter-parts’ and that ‘women are less worthy than men’. The ovary, for instance, is depicted with a limited stock of starter eggs depleted over a lifetime whereas the testes are said to produce new sperm throughout life. Human egg production is commonly described as ‘wasteful’ because, from 300,000 egg starter cells present at puberty, only 400 mature eggs will ever be released; yet that adjective is rarely used to describe a man’s lifetime production of more than 2 trillion sperm. Whether in the popular or scientific press, human mating is commonly portrayed as a gigantic marathon swimming event in which the fastest, fittest sperm wins the prize of fertilising the egg. If this narrative was just a prejudicial holdover from our sexist past – an offensive male fantasy based on incorrect science – that would be bad enough, but continued buy-in to biased information impedes crucial fertility treatments for men and women alike.
T o grasp how we got here, a tour through history can help. Scientific understanding of sex cells and the process of human conception is a comparatively recent development. An egg, the largest cell in a human body, is barely visible to the naked eye, and about as big as the period ending this sentence. So the smallest human body cell, a sperm, is utterly invisible for the unaided eye.
Sperm were unknown to science until 1677, when the Dutch amateur scientist Antonie van Leeuwenhoek first observed human sperm under a microscope. Around the same time, it was realised that the human ovary produced eggs, although it was not until 1827 that the German biologist Karl Ernst von Baer first reported actual observations of human and other mammalian eggs.
After van Leeuwenhoek’s discovery of sperm, it took another century before anyone realised that they were needed to fertilise eggs. That revelation came in the 1760s, when the Italian priest and natural scientist Lazzaro Spallanzani, experimenting on male frogs wearing tight-fitting taffeta pants, demonstrated that eggs would not develop into tadpoles unless sperm was shed into the surrounding water. Bizarrely, until Spallanzani announced his findings, it was widely thought – even by van Leeuwenhoek for some years – that sperm were tiny parasites living in human semen. It was only in 1876 that the German zoologist Oscar Hertwig demonstrated the fusion of sperm and egg in sea urchins.
Eventually, powerful microscopes revealed that an average human ejaculate, with a volume of about half a teaspoon, contains some 250 million sperm. But a key question remains unanswered: ‘Why so many?’ In fact, studies show that pregnancy rates tend to decline once a man’s ejaculate contains less than 100 million sperm.
Clearly, then, almost half the sperm in an average human ejaculate are needed for normal fertility. A favoured explanation for this is sperm competition , stemming from that macho-male notion of sperm racing to fertilise – often with the added contention that more than one male might be involved. As in a lottery, the more tickets you buy, the likelier you are to win. Natural selection, the thinking goes, drives sperm numbers sky-high in a kind of arms race for the fertilisation prize.
Striking examples of sperm competition do indeed abound in the animal kingdom. Our closest relatives, the chimpanzees, live in social units containing several adult males that regularly engage in promiscuous mating; females in turn are mated by multiple males. Numerous features, such as conspicuously large testes, reflect a particularly high level of sperm production in such mammal species. In addition to large testes, they have fast sperm production, high sperm counts, large sperm midpieces (containing numerous energy-generating mitochondria for propulsion), notably muscular sperm-conducting ducts, large seminal vesicles and prostate glands, and high counts of white blood cells (to neutralise sexually transmitted pathogens). The vesicles and the prostate gland together produce seminal fluid, which can coagulate to form a plug in the vagina, temporarily blocking access by other males.
Popular opinion and even many scientists perpetuate the same sperm scenario for humans, but evidence points in a different direction. In fact, despite various lurid claims to the contrary, there’s no convincing evidence that men are biologically adapted for sperm competition. The story of sperm abundance in promiscuously mating chimpanzees contrasts with what we see in various other primates, including humans. Many primates live in groups with just a single breeding male, lack direct competition and have notably small testes. In all relevant comparisons, humans emerge as akin to primates living in single-male groups – including the typical nuclear family. Walnut-sized human testes are just a third of the size of chimpanzee testes, which are about as large chickens’ eggs. Moreover, while chimpanzee ejaculate contains remarkably few physically abnormal sperm, human semen contains a large proportion of duds. Quality controls on human ejaculate have seemingly been relaxed in the absence of direct sperm competition.
Sperm passage is more like a challenging military obstacle course than a standard swimming race
For species not regularly exposed to direct sperm competition, the only promising alternative explanation for high sperm counts concerns genetic variation. In a couple of rarely cited papers published more than four decades ago, the biologist Jack Cohen at the University of Birmingham in the UK noted an association between sperm counts and the generation of chromosome copies during sperm production. During meiosis , the special type of cell division that produces sex cells, pairs of chromosomes exchange chunks of material through crossing over. What Cohen found is that, across species, sperm counts increase in tandem with the number of crossovers during their production. Crossing over increases variation, the essential raw material for natural selection. Think of sperm production as a kind of lottery in which enough tickets (sperm) are printed to match available numbers (different genetic combinations).
Other findings fly in the face of the popular scenario, too. For instance, most mammalian sperm do not in fact swim up the entire female tract but are passively transported part or most of the way by pumping and wafting motions of the womb and oviducts. Astoundingly, sperm of smaller mammals tend to be longer on average than sperm of larger mammals – a mouse sperm is longer than the sperm of a whale. But even if these were equivalent in size, swimming up to an egg becomes more of a stretch the larger a species gets. Indeed, it might be feasible for a mouse sperm to swim all the way up to the egg – but it is quite impossible for an even smaller blue whale sperm to swim 100 times further up the female tract unaided. Convincing evidence has instead revealed that human sperm are passively transported over considerable distances while travelling through the womb and up the oviducts. So much for Olympic-style racing sperm!
In fact, of the 250 million sperm in the average human ejaculate, only a few hundred actually end up at the fertilisation site high up in the oviduct. Sperm passage up the female tract is more like an extremely challenging military obstacle course than a standard sprint-style swimming race. Sperm numbers are progressively whittled down as they migrate up the female tract, so that less than one in a million from the original ejaculate will surround the egg at the time of fertilisation. Any sperm with physical abnormalities are progressively eliminated along the way, but survivors surrounding the egg are a random sample of intact sperm.
Many sperm do not even make it into the neck of the womb (cervix). Acid conditions in the vagina are hostile and sperm do not survive there for long. Passing through the cervix, many sperm that escape the vagina become ensnared in mucus. Any with physical deformities are trapped. Moreover, hundreds of thousands of sperm migrate into side-channels, called crypts , where they can be stored for several days. Relatively few sperm travel directly though the womb cavity, and numbers are further reduced during entry into the oviduct. Once in the oviduct, sperm are temporarily bound to the inner surface, and only some are released and allowed to approach the egg.
P ushing the notion that the fertilising sperm is some kind of Olympic champion has obscured the fact that an ejaculate can contain too many sperm. If sperm surround the egg in excessive numbers, the danger of fertilisation by more than one ( polyspermy ) arises with catastrophic results. Polyspermy occasionally occurs in humans, especially when fathers have very high sperm counts. In the commonest outcome in which two sperm fertilise an egg, cells of the resulting embryo contain 69 chromosomes instead of the usual 46. This is always fatal, usually resulting in miscarriage. Although some individuals survive as far as birth, they always expire shortly afterwards. Because polyspermy typically has a fatal outcome, evolution has evidently led to a series of obstacles in the female reproductive tract that strictly limit the number of sperm allowed to surround an egg.
Polyspermy has practical implications for assisted reproduction in cases of compromised fertility or infertility. For instance, the original standard procedure of introducing semen into the vagina for artificial insemination has been replaced by direct injection into the womb (intrauterine insemination, or IUI). Directly introducing semen into the womb bypasses the reduction of sperm numbers that normally occurs in the cervix, where mucus weeds out physically abnormal sperm. Analyses of clinical data have revealed that depositing 20 million sperm in the womb (less than a 10th of the number in the average ejaculate) is enough to achieve a routine pregnancy rate.
Sperm numbers become even more important when it comes to in vitro fertilisation (IVF), with direct exposure of an egg to sperm in a glass vessel. This bypasses every single one of the natural filters between the vagina and the egg. In the early development of IVF, the general tendency was to use far too many sperm. This reflected the understandable aim of maximising fertilisation success, but it ignored natural processes. High sperm numbers between 50,000 and 0.5 million increasingly depressed the success rate. Optimal fertilisation rates were achieved with only 25,000 sperm around an egg. Both IUI and IVF potentially increase the risk of polyspermy and the likelihood of miscarriage.
Human fertilisation is a gigantic lottery with 250 million tickets: for healthy sperm, it is the luck of the draw
The possibility of polyspermy casts new light on the evolution of sperm counts. Discussions of sperm competition generally focus exclusively on maximising sperm counts, but – as is common in biology – some kind of trade-off is involved. Whereas natural selection can lead to increased sperm production if males are in direct competition, it will also favour mechanisms in the female tract that constrain numbers of sperm around the egg. In promiscuously mating primates, such as chimpanzees, increased oviduct length in females offsets increased sperm production by males. This presumably limits the numbers of sperm approaching the egg. It also shows that the female’s role in fertilisation is by no means as passive as is often assumed.
The entrenched idea that ‘the best sperm wins’ has elicited various suggestions that some kind of selection occurs, but it is difficult to imagine how this could possibly happen. The DNA in a sperm head is tightly bound and virtually crystalline, so how could its properties be detected from outside? Experiments on mice indicate, for instance, that there is no selection according to whether a sperm contains a male-determining Y-chromosome or a female-determining X-chromosome. It seems far more likely that human fertilisation is a gigantic lottery with 250 million tickets, in which – for healthy sperm – successful fertilisation is essentially the luck of the draw.
Other puzzling features of sperm also await explanation. It has long been known, for instance, that human semen contains a large proportion of structurally abnormal sperm with obvious defects such as double tails or tiny heads. The ‘kamikaze sperm’ hypothesis proposed that these dud sperm in fact serve different functions in competition, such as blocking or even killing sperm from other men. However, this has since been effectively discredited .
The entrenched notion that human sperm, once ejaculated, engage in a frantic race to reach the egg has completely overshadowed the real story of reproduction, including evidence that many sperm do not dash towards the egg but are instead stored for many days before proceeding. It was long accepted as established fact that human sperm survive for only two days in a woman’s genital tract. However, from the mid-1970s on, mounting evidence revealed that human sperm can survive intact for at least five days. An extended period of sperm survival is now widely accepted, and it could be as long as 10 days or more.
Other myths abound. Much has been written about mucus produced by the human cervix. In so-called ‘natural’ methods of birth control, the consistency of mucus exuding from the cervix has been used as a key indicator. Close to ovulation, cervical mucus is thin and has a watery, slippery texture. But precious little has been reported regarding the association between mucus and storage of sperm in the cervix. It has been clearly established that sperm are stored in the crypts from which the mucus flows. But our knowledge of the process involved is regrettably restricted to a single study reported in 1980 by the gynaecologist Vaclav Insler and colleagues of Tel Aviv University in Israel.
In this study, 25 women bravely volunteered to be artificially inseminated on the day before scheduled surgical removal of the womb (hysterectomy). Then, Insler and his team microscopically examined sperm stored in the crypts in serial sections of the cervix. Within two hours after insemination, sperm colonised the entire length of the cervix. Crypt size was very variable, and sperm were stored mainly in the larger ones. Insler and colleagues calculated the number of crypts containing sperm and sperm density per crypt. In some women, up to 200,000 sperm were stored in the cervical crypts.
Insler and colleagues also reported that live sperm had actually been found in cervical mucus up to the ninth day after insemination. Summarising available evidence, they suggested that after insemination the cervix serves as a sperm reservoir from which viable sperm are gradually released to make their way up the oviduct. This dramatic finding has been widely cited yet largely ignored, and there has never been a follow-up study.
Mutations accumulate four times faster in sperm than in eggs, so semen from old men is risk-laden
In his textbook Conception in the Human Female (1980) – more than 1,000 pages in length – Sir Robert Edwards, a recipient of the 2010 Nobel prize for the development of IVF, mentioned cervical crypts in a single sentence. Since then, many other authors have mentioned sperm storage in those cervical crypts equally briefly. Yet storage of sperm, with gradual release, has major implications for human reproduction. Crucially, the widespread notion of a restricted ‘fertile window’ in the menstrual cycle depends on the long-accepted wisdom that sperm survive only two days after insemination. Sperm survival perhaps for 10 days or more radically erodes the basis for so-called ‘natural’ methods of birth control through avoidance of conception. Sperm storage is also directly relevant to attempts to treat infertility.
Another dangerous misconception is the myth that men retain full fertility into old age, starkly contrasting with the abrupt cessation of fertility seen in women at menopause. Abundant evidence shows that, in men, sperm numbers and quality decline with increasing age. Moreover, it has recently emerged that mutations accumulate about four times faster in sperm than in eggs, so semen from old men is actually risk-laden.
Much has been written about the fact that in industrialised societies age at first birth is increasing in women, accompanied by slowly mounting reproductive problems. A proposed solution is the highly invasive and very expensive procedure of ‘fertility preservation’ in which eggs are harvested from young women for use later in life. However, increasing reproductive problems with ageing men, notably more rapid accumulation of sperm mutations, have passed largely unmentioned. One very effective and far less expensive and invasive way of reducing reproductive problems for ageing couples would surely be to store semen samples from young men to be used later in life. This is just one of the benefits to be gained from less sexism and more reliable knowledge in the realm of human reproduction.
Nowadays, the story of Hartsoeker’s homunculus might seem veiled in the mist of time, mentioned only as an entertaining illustration of blunders in the early exploration of human sex cells. But its influence, along with the macho-male bias that spawned it, has lived on in subtler form among the cultural stereotypes that influence the questions we ask about reproductive biology."
Elon Musk puts his case for a multi-planet civilisation | Aeon Essays,,https://aeon.co/essays/elon-musk-puts-his-case-for-a-multi-planet-civilisation,"‘Fuck Earth!’ Elon Musk said to me, laughing. ‘Who cares about Earth?’ We were sitting in his cubicle, in the front corner of a large open-plan office at SpaceX headquarters in Los Angeles. It was a sunny afternoon, a Thursday, one of three designated weekdays Musk spends at SpaceX. Musk was laughing because he was joking: he cares a great deal about Earth. When he is not here at SpaceX, he is running an electric car company. But this is his manner. On television Musk can seem solemn, but in person he tells jokes. He giggles. He says things that surprise you.
When I arrived, Musk was at his computer, powering through a stream of single-line email replies. I took a seat and glanced around at his workspace. There was a black leather couch and a large desk, empty but for a few wine bottles and awards. The windows looked out to a sunbaked parking lot. The vibe was ordinary, utilitarian, even boring. After a few minutes passed, I began to worry that Musk had forgotten about me, but then suddenly, and somewhat theatrically, he wheeled around, scooted his chair over, and extended his hand. ‘I’m Elon,’ he said.
It was a nice gesture, but in the year 2014 Elon Musk doesn’t need much of an introduction. Not since Steve Jobs has an American technologist captured the cultural imagination like Musk. There are tumblrs and subreddits devoted to him. He is the inspiration for Robert Downey Jr’s Iron Man. His life story has already become a legend. There is the alienated childhood in South Africa, the video game he invented at 12, his migration to the US in the mid-1990s. Then the quick rise, beginning when Musk sold his software company Zip2 for $300 million at the age of 28, and continuing three years later, when he dealt PayPal to eBay for $1.5 billion. And finally, the double down, when Musk decided idle hedonism wasn’t for him, and instead sank his fortune into a pair of unusually ambitious startups. With Tesla he would replace the world’s cars with electric vehicles, and with SpaceX he would colonise Mars. Automobile manufacturing and aerospace are mature industries, dominated by corporate behemoths with plush lobbying budgets and factories in all the right congressional districts. No matter. Musk would transform both, simultaneously, and he would do it within the space of a single generation.
Musk announced these plans shortly after the bursting of the first internet bubble, when many tech millionaires were regarded as mere lottery winners. People snickered. They called him a dilettante. But in 2010, he took Tesla public and became a billionaire many times over. SpaceX is still privately held, but it too is now worth billions, and Musk owns two-thirds of it outright. SpaceX makes its rockets from scratch at its Los Angeles factory, and it sells rides on them cheap, which is why its launch manifest is booked out for years. The company specialises in small satellite launches, and cargo runs to the space station, but it is now moving into the more mythic business of human spaceflight. In September, NASA selected SpaceX, along with Boeing, to become the first private company to launch astronauts to the International Space Station (ISS). Musk is on an epic run. But he keeps pushing his luck. In every interview, there is an outlandish new claim, a seeming impossibility, to which he attaches a tangible date. He is always giving you new reasons to doubt him.
I had come to SpaceX to talk to Musk about his vision for the future of space exploration, and I opened our conversation by asking him an old question: why do we spend so much money in space, when Earth is rife with misery, human and otherwise? It might seem like an unfair question. Musk is a private businessman, not a publicly funded space agency. But he is also a special case. His biggest customer is NASA and, more importantly, Musk is someone who says he wants to influence the future of humanity. He will tell you so at the slightest prompting, without so much as flinching at the grandiosity of it, or the track record of people who have used this language in the past. Musk enjoys making money, of course, and he seems to relish the billionaire lifestyle, but he is more than just a capitalist. Whatever else might be said about him, Musk has staked his fortune on businesses that address fundamental human concerns. And so I wondered, why space?
Musk did not give me the usual reasons. He did not claim that we need space to inspire people. He did not sell space as an R & D lab, a font for spin-off technologies like astronaut food and wilderness blankets. He did not say that space is the ultimate testing ground for the human intellect. Instead, he said that going to Mars is as urgent and crucial as lifting billions out of poverty, or eradicating deadly disease.
‘I think there is a strong humanitarian argument for making life multi-planetary,’ he told me, ‘in order to safeguard the existence of humanity in the event that something catastrophic were to happen, in which case being poor or having a disease would be irrelevant, because humanity would be extinct. It would be like, “Good news, the problems of poverty and disease have been solved, but the bad news is there aren’t any humans left.”’
Musk has been pushing this line – Mars colonisation as extinction insurance – for more than a decade now, but not without pushback. ‘It’s funny,’ he told me. ‘Not everyone loves humanity. Either explicitly or implicitly, some people seem to think that humans are a blight on the Earth’s surface. They say things like, “Nature is so wonderful; things are always better in the countryside where there are no people around.” They imply that humanity and civilisation are less good than their absence. But I’m not in that school,’ he said. ‘I think we have a duty to maintain the light of consciousness, to make sure it continues into the future.’
P eople have been likening light to consciousness since the days of Plato and his cave because, like light, consciousness illuminates. It makes the world manifest. It is, in the formulation of the great Carl Sagan, the Universe knowing itself. But the metaphor is not perfect. Unlike light, whose photons permeate the entire cosmos, human-grade consciousness appears to be rare in our Universe. It appears to be something akin to a single candle flame, flickering weakly in a vast and drafty void.
Musk told me he often thinks about the mysterious absence of intelligent life in the observable Universe. Humans have yet to undertake an exhaustive, or even vigorous, search for extraterrestrial intelligence, of course. But we have gone a great deal further than a casual glance skyward. For more than 50 years, we have trained radio telescopes on nearby stars, hoping to detect an electromagnetic signal, a beacon beamed across the abyss. We have searched for sentry probes in our solar system, and we have examined local stars for evidence of alien engineering. Soon, we will begin looking for synthetic pollutants in the atmospheres of distant planets, and asteroid belts with missing metals, which might suggest mining activity.
The failure of these searches is mysterious, because human intelligence should not be special. Ever since the age of Copernicus, we have been told that we occupy a uniform Universe, a weblike structure stretching for tens of billions of light years, its every strand studded with starry discs, rich with planets and moons made from the same material as us. If nature obeys identical laws everywhere, then surely these vast reaches contain many cauldrons where energy is stirred into water and rock, until the three mix magically into life. And surely some of these places nurture those first fragile cells, until they evolve into intelligent creatures that band together to form civilisations, with the foresight and staying power to build starships.
‘At our current rate of technological growth, humanity is on a path to be godlike in its capabilities,’ Musk told me. ‘You could bicycle to Alpha Centauri in a few hundred thousand years, and that’s nothing on an evolutionary scale. If an advanced civilisation existed at any place in this galaxy, at any point in the past 13.8 billion years, why isn’t it everywhere? Even if it moved slowly, it would only need something like .01 per cent of the Universe’s lifespan to be everywhere. So why isn’t it?’
‘If you look at our current technology level, something strange has to happen to civilisations, and I mean strange in a bad way’
Life’s early emergence on Earth, only half a billion years after the planet coalesced and cooled, suggests that microbes will arise wherever Earthlike conditions obtain. But even if every rocky planet were slick with unicellular slime, it wouldn’t follow that intelligent life is ubiquitous. Evolution is endlessly inventive, but it seems to feel its way toward certain features, like wings and eyes, which evolved independently on several branches of life’s tree. So far, technological intelligence has sprouted only from one twig. It’s possible that we are merely the first in a great wave of species that will take up tool-making and language. But it’s also possible that intelligence just isn’t one of natural selection’s preferred modules. We might think of ourselves as nature’s pinnacle, the inevitable endpoint of evolution, but beings like us could be too rare to ever encounter one another. Or we could be the ultimate cosmic outliers, lone minds in a Universe that stretches to infinity.
Musk has a more sinister theory. ‘The absence of any noticeable life may be an argument in favour of us being in a simulation,’ he told me. ‘Like when you’re playing an adventure game, and you can see the stars in the background, but you can’t ever get there. If it’s not a simulation, then maybe we’re in a lab and there’s some advanced alien civilisation that’s just watching how we develop, out of curiosity, like mould in a petri dish.’ Musk flipped through a few more possibilities, each packing a deeper existential chill than the last, until finally he came around to the import of it all. ‘If you look at our current technology level, something strange has to happen to civilisations, and I mean strange in a bad way,’ he said. ‘And it could be that there are a whole lot of dead, one-planet civilisations.’
I t is true that no civilisation can last long in this Universe if it stays confined to a single planet. The science of stellar evolution is complex, but we know that our mighty star, the ball of fusing hydrogen that anchors Earth and powers all of its life, will one day grow so large that its outer atmosphere will singe and sterilise our planet, and maybe even engulf it. This event is usually pegged for 5-10 billion years from now, and it tends to mark Armageddon in secular eschatologies. But our biosphere has little chance of surviving until then.
Five hundred million years from now, the Sun won’t be much larger than it is today but it will be swollen enough to start scorching the food chain. By then, Earth’s continents will have fused into a single landmass, a new Pangaea. As the Sun dilates, it will pour more and more radiation into the atmosphere, widening the daily swing between hot and cold. The supercontinent’s outer shell will suffer expansions and contractions of increasing violence. Its rocks will become brittle, and its silicates will begin to erode at unprecedented rates, taking carbon dioxide with them, down to the seafloor and into the deep crust. Eventually, the atmosphere will become so carbon-poor that trees will be unable to perform photosynthesis. The planet will be shorn of its forests, but a few plants will make a valiant last stand, until the brightening Sun kills them off, too, along with every animal that depends on them, which is to say every animal on Earth.
In a billion years, the oceans will have boiled away altogether, leaving empty trenches that are deeper than Everest is tall. Earth will become a new Venus, a hothouse planet where even the hardiest microbes cannot survive. And this is the optimistic scenario, for it assumes our biosphere will die of old age, and not something more sudden and stroke-like. After all, a billion years is a long time, long enough to make probabilistic space for all kinds of catastrophes, including those that have no precedent in human memory.
Of all the natural disasters that appear in our histories, the most severe are the floods, tales of global deluge inspired by the glacial melt at the end of the last Ice Age. There are a few stray glimmers of cosmic disasters, as in Plato’s Timaeus , when he tells the story of Phaeton, the son of the Sun god, who could not drive his father’s fiery chariot across the sky, and so crashed it into the Earth, burning the planet’s surface to a crisp. Plato writes:
A remarkable piece of ancient wisdom, but on the whole, human culture is too fresh an invention to have preserved the scarier stuff we find in the geological record. We have no tales of mile-wide asteroid strikes, or super volcanoes, or the deep freezes that occasionally turn our blue planet white. The biosphere has bounced back from each of these shocks, but not before sacrificing terrifying percentages of its species. And even its most remarkable feats of resilience are cold comfort, for the future might subject Earth to entirely novel experiences.
Some in the space exploration community, including no less a figure than Freeman Dyson, say that human spaceflight is folly in the short term
A billion years will give us four more orbits of the Milky Way galaxy, any one of which could bring us into collision with another star, or a supernova shockwave, or the incinerating beam of a gamma ray burst. We could swing into the path of a rogue planet, one of the billions that roam our galaxy darkly, like cosmic wrecking balls. Planet Earth could be edging up to the end of an unusually fortunate run.
If human beings are to survive these catastrophes, both the black swans and the certainties, we will need to do what life has always done: move in the service of survival. We will need to develop new capabilities, as our aquatic forebears once evolved air-gulping lungs, and bony fins for crude locomotion, struggling their way onto land. We will need to harness the spirit that moved our own species to trek into new continents, so that our recent ancestors could trickle out to islands and archipelagos, before crossing whole oceans, on their way to the very ends of this Earth. We will need to set out for new planets and eventually, new stars. But need we make haste?
Some in the space exploration community, including no less a figure than the physicist Freeman Dyson, say that human spaceflight is folly in the short term. We humans are still in our technological infancy, after all, only a million years removed from the first control of fire. We have progressed quickly, from those first campfire sparks to the explosions we bottle in tall cylinders, to power our way out of Earth’s gravity well. But not everyone who sits atop our rockets returns safely. To seed a colony on another planet, we need astronaut safety to scale up. Perhaps we should park human missions for now, and explore space through the instruments of our cosmic drones, like the Voyager probe that recently slipped from the Solar System, to send us its impressions of interstellar space. We can resume human spaceflight later this century, or next, after we have reaped the full fruits of our current technological age. For all we know, revolutions in energy, artificial intelligence and materials science could be imminent. Any one of them would make human spaceflight a much easier affair.
‘There is an argument you often hear in space circles,’ I said to Musk, ‘where people say the focus on human space travel in the near-term is entirely misplaced – ’
‘What focus? There isn’t one, you know,’ he said, cutting me off.
‘But to the extent you’re advocating for one,’ I said, ‘there is an argument that says until we ramp up technologically, we’re better off sending probes because, as you know, the presence of a single human being on a spacecraft makes the engineering exponentially more difficult.’
‘Well, we are sending probes,’ Musk told me. ‘And they are very expensive probes, by the way. They aren’t exactly bargain-basement. The last RC car we sent to Mars cost more than $3 billion. That’s a hell of a droid. For that kind of money, we should be able to send a lot of people to Mars.’
T here is a story Musk likes to tell, part of the founding myth of SpaceX, about how he stayed up late one night searching NASA’s website for information about a crewed mission to Mars. This was back in 2001, when the space shuttles were still flying, their launches providing a steady drumbeat of spectacle, just enough to convince the casual observer that human spaceflight wasn’t in serious decline. Today, it is impossible to sustain that delusion.
The idea that humans would one day venture into the sky is as old as mythology, but it wasn’t until the scientific revolution, when the telescope made the sky legible, that it began to seem like a realistic objective. In 1610, the astronomer Johannes Kepler wrote, in a letter to Galileo:
After the hot air balloon and airplane were invented, a few visionaries moved on to planning for space colonisation itself. But it wasn’t until the Space Race, the extraordinary period of progress that began with Sputnik in 1957 and ended with the first Moon landing in 1969, that the idea of cosmic manifest destiny moved from the fringe to the mainstream. In the ensuing decades, it would inspire whole literatures and subcultures, becoming, in the process, one of the dominant secular narratives of the human future. But reality has not kept up.
It has been three years since NASA, the world’s best-funded space agency, fired a human being into orbit. Americans who wish to fly to the ISS must now ride on Russian rockets, launched from Kazakhstan, at the pleasure of Vladimir Putin. Even the successful trips are, in their own way, evidence of decline, because the space station sits a thousand times closer to Earth than the Moon. Watching NASA astronauts visit it is about as thrilling as watching Columbus sail to Ibiza. But that’s as good as it’s going to get for a while. The agency’s next generation rocket isn’t due until 2018, and its first iteration will barely best the Saturn V, the pyrotechnic beast that powered the Apollo missions. American presidents occasionally make bold, Kennedy-like pronouncements about sending humans to Mars. But as Musk discovered more than a decade ago, there are no real missions planned, and even optimists say it will be 2030 at the earliest.
It wasn’t supposed to be like this. Only a few decades ago, it seemed as though we were entering a new epoch of exploration, one that would shame the seafarers of the High Renaissance. We would begin by mastering lower Earth orbit, so that visits to space were safe and routine. Then we’d go to the Moon and build a permanent base there, a way station that would let us leap to the planets, each in quick succession, as though they were lily pads on a pond, and not massive moving worlds spaced by hundreds of millions of miles. We’d start with Mars and then shoot through the asteroid belt to Jupiter and its ocean-harbouring moons. We’d drink in Saturn’s sublimity, its slanted rings and golden hue, and then head for the outer giants, and the icy rubble at the Solar System’s edge. The Sun would look small out there, and the stars beckoning. We would spread through the Milky Way’s safe zone, the doughnut of gas and fire, billions of stars strong, that surrounds our galaxy’s violent core, and then we’d press out into intergalactic space. We’d use wormholes or warp drives, or some other vaguely sketched physics, to pretend away the millions of light years that separate us from Andromeda and the glittering web beyond it, whose glimpsable regions alone contain hundreds of billions of galaxies.
When Musk realized there were no missions to Mars on the books, he figured Americans had lost interest in space exploration. Two years later, the public response to the Columbia shuttle disaster convinced him otherwise. ‘It was in every newspaper, every magazine, every news station, even those that had nothing to do with space,’ he told me. ‘And yeah, seven people died and that was awful, but seven people die all the time, and nobody pays any attention to it. It’s obvious that space is deeply ingrained in the American psyche.’ Musk now sees the Space Race as a transient Cold War phenomenon, a technological pissing match fuelled by unsustainable public spending. ‘The Soviets were crowing after Sputnik, about how they had better technology than we did, and so therefore communism is better,’ he told me. ‘And so we set a really tough target and said we would beat them there, and money was no object. But once the ideological battle was won, the impetus went away, and money very quickly became an object.’
NASA’s share of the US federal budget peaked at 4.4 per cent in 1966, but a decade later it was less than 1 per cent, where it has remained ever since. The funding cut forced NASA to shutter the Saturn V production lines, along with the final three Moon landings, and a mission to Mars slated for the late 1980s. That’s why the agency’s website looked so barren when Musk visited it in 2001.
Aghast at this backsliding, and still thinking it a failure of will, Musk began planning a Mars mission of his own. He wanted to send a greenhouse to Mars, filled with plants that would become, in the course of their long journeying, the most distant travellers of all multicellular life. Images of lush, leafy organisms living on the red planet would move people, he figured, just as images of the Earth rising, sunlike, on the lunar plain had moved previous generations. With a little luck, the sentiment would translate into political will for a larger NASA budget.
When Musk went to price the mission with US launch companies, he was told transport would cost $60-80 million. Reeling, he tried to buy a refurbished Russian intercontinental ballistic missile to do the job, but his dealer kept raising the price on him. Finally, he’d had enough. Instead of hunting around for a cheaper supplier, Musk founded his own rocket company. His friends thought he was crazy, and tried to intervene, but he would not be talked down. Musk identifies strongly as an engineer. That’s why he usually takes a title like chief technical officer at the companies he runs, in addition to chief executive officer. He had been reading stacks of books about rockets. He wanted to try building his own.
Great migrations are often a matter of timing, of waiting for a strait to freeze, a sea to part, or a planet to draw near
Six years later, it all looked like folly. It was 2008, a year Musk describes as the worst of his life. Tesla was on the verge of bankruptcy. Lehman had just imploded, making capital hard to come by. Musk was freshly divorced and borrowing cash from friends to pay living expenses. And SpaceX was a flameout, in the most literal sense. Musk had spent $100 million on the company and its new rocket, the Falcon 1. But its first three launches had all detonated before reaching orbit. The fourth was due to lift off in early Fall of that year, and if it too blew apart in the atmosphere, SpaceX would likely have numbered among the casualties. Aerospace journalists were drafting its obituary already. Musk needed a break, badly. And he got it, in the form of a fully intact Falcon 1, riding a clean column of flame out of the atmosphere and into the history books, as the first privately funded, liquid-fuelled rocket to reach orbit.
SpaceX nabbed a $1.6 billion contract with NASA in the aftermath of that launch, and Musk used the money to expand rapidly. In the years since, he has reeled off 15 straight launches without a major failure, including the first private cargo flights to the ISS. Last year, he signed a 20-year lease on launch pad 39A, the hallowed stretch of Cape Canaveral concrete that absorbed the fire of Apollo’s rockets. Earlier this year, he bought a tract of land near Brownsville, Texas, where he plans to build a dedicated spaceport for SpaceX. ‘It took us ages to get all the approvals,’ he told me. ‘There were a million federal agencies that needed to sign off, and the final call went to the National Historic Landmark Association, because the last battle of the Civil War was fought a few miles away from our site, and visitors might be able to see the tip of our rocket from there. We were like, “ Really ? Have you seen what it’s like around there? Nobody visits that place.”’
Musk isn’t shy about touting the speed of his progress. Indeed, he has an Ali-like appetite for needling the competition. A Bloomberg TV interviewer once asked him about one of Tesla’s competitors and he laughed in response. ‘Why do you laugh?’ she said. ‘Have you seen their car?’ he replied, incredulously. This same streak of showmanship surfaced when Musk and I discussed the aerospace industry. ‘There have been a number of space startups,’ he told me. ‘But they have all failed, or their success was irrelevant.’
But SpaceX does have competitors, both industry giants and scrappy startups alike. The company has just spent three years in a dogfight to become the first commercial space outfit to launch US astronauts to the space station. The awarding of this contract became more urgent in March, after the US sanctioned Russia for rolling tanks into Crimea. A week later, Russia’s Deputy Prime Minister Dmitry Rogozin quipped: ‘After analysing the sanctions against our space industry, I suggest the US deliver its astronauts to the ISS with a trampoline.’
SpaceX was an early favourite to win the contract, but it was never a lock. Critics have hammered the company for delaying launches, and in August it suffered a poorly timed mishap, when one of its test rockets blew up shortly after lift-off. In the end, NASA split the contract between Boeing and SpaceX, giving each six launches. Musk said that he would move into human missions, win or lose, but his progress would have been slowed considerably. The contract is only for short hops to lower Earth orbit, but it will give Musk the chance to demonstrate that he can do human spaceflight better than anyone else. And it will give him the money and reputation he needs to work up to a more extraordinary feat of engineering, one that has not been attempted in more than four decades: the safe transport of human beings to a new world.
G reat migrations are often a matter of timing, of waiting for a strait to freeze, a sea to part, or a planet to draw near. The distance between Earth and Mars fluctuates widely as the two worlds whirl around in their orbits. At its furthest, Mars is a thousand times further than the Moon. But every 26 months they align, when the faster moving Earth swings into position between Mars and the Sun. When this alignment occurs where their orbits are tightest, Mars can come within 36 million miles, only 150 times further than the Moon. The next such window is only four years away, too soon to send a crewed ship. But in the mid-2030s, Mars will once again burn bright and orange in our sky, and by then Musk might be ready to send his first flurry of missions, to seed a citylike colony that he expects to be up and running by 2040.
‘SpaceX is only 12 years old now,’ he told me. ‘Between now and 2040, the company’s lifespan will have tripled. If we have linear improvement in technology, as opposed to logarithmic, then we should have a significant base on Mars, perhaps with thousands or tens of thousands of people.’
Musk told me this first group of settlers will need to pay their own way. ‘There needs to be an intersection of the set of people who wish to go, and the set of people who can afford to go,’ he said. ‘And that intersection of sets has to be enough to establish a self-sustaining civilisation. My rough guess is that for a half-million dollars, there are enough people that could afford to go and would want to go. But it’s not going to be a vacation jaunt. It’s going to be saving up all your money and selling all your stuff, like when people moved to the early American colonies.’
Even at that price, a one-way trip to Mars could be a tough sell. It would be fascinating to experience a deep space mission, to see the Earth receding behind you, to feel that you were afloat between worlds, to walk a strange desert under an alien sky. But one of the stars in that sky would be Earth, and one night, you might look up at it, through a telescope. At first, it might look like a blurry sapphire sphere, but as your eyes adjusted, you might be able to make out its oceans and continents. You might begin to long for its mountains and rivers, its flowers and trees, the astonishing array of life forms that roam its rainforests and seas. You might see a network of light sparkling on its dark side, and realise that its nodes were cities, where millions of lives are coming into collision. You might think of your family and friends, and the billions of other people you left behind, any one of which you could one day come to love.
The austerity of life on Mars might nurture these longings into regret, or even psychosis. From afar, the Martian desert evokes sweltering landscapes like the Sahara or the American West, but its climate is colder than the interior of Antarctica. Mars used to be wrapped in a thick blanket of atmosphere, but something in the depths of time blew it away, and the patchy remains are too thin to hold in heat or pressure. If you were to stroll onto its surface without a spacesuit, your eyes and skin would peel away like sheets of burning paper, and your blood would turn to steam, killing you within 30 seconds. Even in a suit you’d be vulnerable to cosmic radiation, and dust storms that occasionally coat the entire Martian globe, in clouds of skin-burning particulates, small enough to penetrate the tightest of seams. Never again would you feel the sun and wind on your skin, unmediated. Indeed, you would probably be living underground at first, in a windowless cave, only this time there would be no wild horses to sketch on the ceiling.
‘Even at a million people you’re assuming an incredible amount of productivity per person, because you would need to recreate the entire industrial base on Mars’
It is possible that Mars could one day be terraformed into an Earthly paradise, but not anytime soon. Even on our planet, whose natural systems we have studied for centuries, the weather is too complex to predict, and geoengineering is a frontier technology. We know we could tweak the Earth’s thermostat, by sending a silvery mist of aerosols into the stratosphere, to reflect away sunlight. But no one knows how to manufacture an entire atmosphere. On Mars, the best we can expect is a crude habitat, erected by robots. And even if they could build us a Four Seasons, near a glacier or easily mined ore, videoconferencing with Earth won’t be among the amenities. Messaging between the two planets will always be too delayed for any real-time give and take.
Cabin fever might set in quickly on Mars, and it might be contagious. Quarters would be tight. Governments would be fragile. Reinforcements would be seven months away. Colonies might descend into civil war, anarchy or even cannibalism, given the potential for scarcity. US colonies from Roanoke to Jamestown suffered similar social breakdowns, in environments that were Edenic by comparison. Some individuals might be able to endure these conditions for decades, or longer, but Musk told me he would need a million people to form a sustainable, genetically diverse civilisation.
‘Even at a million, you’re really assuming an incredible amount of productivity per person, because you would need to recreate the entire industrial base on Mars,’ he said. ‘You would need to mine and refine all of these different materials, in a much more difficult environment than Earth. There would be no trees growing. There would be no oxygen or nitrogen that are just there. No oil.’
I asked Musk how quickly a Mars colony could grow to a million people. ‘Excluding organic growth, if you could take 100 people at a time, you would need 10,000 trips to get to a million people,’ he said. ‘But you would also need a lot of cargo to support those people. In fact, your cargo to person ratio is going to be quite high. It would probably be 10 cargo trips for every human trip, so more like 100,000 trips. And we’re talking 100,000 trips of a giant spaceship.’
Musk told me all this could happen within a century. He is rumoured to have a design in mind for this giant spaceship, a concept vehicle he calls the Mars Colonial Transporter. But designing the ship is the easy part. The real challenge will be driving costs down far enough to launch whole fleets of them. Musk has an answer for that, too. He says he is working on a reusable rocket, one that can descend smoothly back to Earth after launch, and be ready to lift off again in an hour.
‘Rockets are the only form of transportation on Earth where the vehicle is built anew for each journey,’ he says. ‘What if you had to build a new plane for every flight?’ Musk’s progress on reusable rockets has been slow, but one of his prototypes has already flown a thousand metres into the air, before touching down softly again. He told me full reusability would reduce mission costs by two orders of magnitude, to tens of dollars per pound of weight. That’s the price that would convert Earth’s launch pads into machine guns, capable of firing streams of spacecraft at deep space destinations such as Mars. That’s the price that would launch his 100,000 ships.
A ll it takes is a glance over your shoulder, to the alien world of 1914, to remind yourself how much can happen in a century. But a million people on Mars sounds like a techno-futurist fantasy, one that would make Ray Kurzweil blush. And yet, the very existence of SpaceX is fantasy. After talking with Musk, I took a stroll through his cathedral-like rocket factory. I wandered the rows of chromed-out rocket engines, all agleam under blue neon. I saw white tubes as huge as stretched-out grain silos, with technicians crawling all over them, their ant-farm to-and-fro orchestrated from above, by managers in glass cube offices. Mix in the cleanroom jumpsuits and the EDM soundtrack, and the place felt something like Santa’s workshop as re-imagined by James Cameron. And to think: 12 years ago, this whole thrumming hive, this assembly line for spaceships , did not even exist, except as a hazy notion, a few electrified synapses in Musk’s overactive imagination.
Who am I to say what SpaceX will accomplish in a century’s time? For all I know Musk will be hailed as a visionary by then, a man of action without parallel in the annals of spaceflight. But there are darker scenarios, too. Musk could push the envelope, and see his first mission to Mars end in tragedy. Travel to Mars could prove elusive, like cold fusion. It might be one of those feats of technology that is always 25 years away. Musk could come to be seen as a cultural artifact, a personification of our post-Apollo hangover. An Icarus.
I asked Musk if he’d made peace with the possibility that his project could still be in its infancy, when death or infirmity forces him to pass the baton. ‘That’s what I expect will be the case,’ he said. ‘Make peace with it, of course. I’ve thought about that quite a lot. I’m trying to construct a world that maximises the probability that SpaceX continues its mission without me,’ he said. I nodded toward a cluster of frames on his wall, portraits of his five sons. ‘Will you give it to them?’ He told me he had planned to give it to an institution, or several, but now he thinks that a family influence might be stabilising. ‘I just don’t want it to be controlled by some private equity firm that would milk it for near-term revenue,’ he said. ‘That would be terrible.’
‘We need to be laser-focused on becoming a multi-planet civilisation. That’s the next step’
This fear, that the sacred mission of SpaceX could be compromised, resurfaced when I asked Musk if he would one day go to Mars himself. ‘I’d like to go, but if there is a high risk of death, I wouldn’t want to put the company in jeopardy,’ he told me. ‘I only want to go when I could be confident that my death wouldn’t result in the primary mission of the company falling away.’ It’s possible to read Musk as a Noah figure, a man obsessed with building a great vessel, one that will safeguard humankind against global catastrophe. But he seems to see himself as a Moses, someone who makes it possible to pass through the wilderness – the ‘empty wastes,’ as Kepler put it to Galileo – but never sets foot in the Promised Land.
Before I left SpaceX, I wanted to know how far Musk thought human exploration would go. When a man tells you that a million people will live on Mars within a century, you want to know his limits, if only for credibility’s sake. ‘Do you think we will go to the stars?’ I asked him.
‘Wow,’ he said. ‘It’s pretty hard to get to another star system. Alpha Centauri is four light years away, so if you go at 10 per cent of the speed of light, it’s going to take you 40 years, and that’s assuming you can instantly reach that speed, which isn’t going to be the case. You have to accelerate. You have to build up to 20 or 30 per cent and then slow down, assuming you want to stay at Alpha Centauri and not go zipping past.’ To accentuate this last point, Musk made a high-pitched zooming noise, like kids make when playing with toy spaceships.
I pressed him about star travel a bit more, but he stayed tight. ‘It’s just hard,’ he said. ‘With current life spans, you need generational ships. You need antimatter drives, because that’s the most mass-efficient. It’s doable, but it’s super slow.’
‘So you’re skeptical,’ I said. He cracked then, but only a little.
‘I’m not saying I’m skeptical of the stars,’ he said. ‘I just wonder what humanity will even look like when we try to do that. If we can establish a Mars colony, we can almost certainly colonise the whole Solar System, because we’ll have created a strong economic forcing function for the improvement of space travel. We’ll go to the moons of Jupiter, at least some of the outer ones for sure, and probably Titan on Saturn, and the asteroids. Once we have that forcing function, and an Earth-to-Mars economy, we’ll cover the whole Solar System. But the key is that we have to make the Mars thing work. If we’re going to have any chance of sending stuff to other star systems, we need to be laser-focused on becoming a multi-planet civilisation. That’s the next step.’
You can see why NASA has given Musk a shot at human spaceflight. He makes a great rocket but, more than that, he has the old vision in him. He is a revivalist, for those of us who still buy into cosmic manifest destiny. And he can preach. He says we are doomed if we stay here. He says we will suffer fire and brimstone, and even extinction. He says we should go with him, to that darkest and most treacherous of shores. He promises a miracle."
How bad experiences in childhood lead to adult illness | Aeon Essays,,https://aeon.co/essays/how-bad-experiences-in-childhood-lead-to-adult-illness,"If you saw Laura walking down the New York City street where she lives today, you’d see a well-dressed 46-year-old woman with auburn hair and green eyes, who exudes a sense of ‘I matter here.’ She looks entirely in charge of her life, but behind Laura’s confident demeanour lies a history of trauma: a bipolar mother who vacillated between braiding her daughter’s hair and peppering her with insults, and a father who moved out-of-state with his wife-to-be when Laura was 15 years old.
She recalls a family trip to the Grand Canyon when she was 10. In a photo taken that day, Laura and her parents sit on a bench, sporting tourist whites. ‘Anyone looking at us would have assumed that we were a normal, loving family.’ But as they put on fake smiles for the camera, Laura’s mother suddenly pinched her daughter’s midriff and told her to stop ‘staring off into space’. A second pinch: ‘No wonder you’re turning into a butterball, you ate so much cheesecake last night you’re hanging over your shorts!’ If you look hard at Laura’s face in the photograph, you can see that she’s not squinting at the Arizona sun, but holding back tears.
After her father left the family, he sent cards and money, but called less and less. Meanwhile, her mother’s untreated bipolar disorder worsened. Sometimes, Laura says: ‘My mom would go on a vitriolic diatribe about my dad until spittle foamed on her chin. I’d stand there, trying not to hear her as she went on and on, my whole body shaking inside.’ Laura never invited friends over, for fear they’d find out her secret: her mom ‘wasn’t like other moms’.
Some 30 years later, Laura says: ‘In many ways, no matter where I go or what I do, I’m still in my mother’s house.’ Today, ‘If a car swerves into my lane, a grocery store clerk is rude, my husband and I argue, or my boss calls me in to talk over a problem, I feel something flip over inside. It’s like there’s a match standing inside too near a flame, and with the smallest breeze, it ignites.’
To see Laura, you’d never know that she is ‘always shaking a little, only invisibly, deep down in my cells’.
Her sense that something is wrong inside is mirrored by her physical health. During a routine exam, Laura’s doctor discovered that Laura was suffering from dilated cardiomyopathy and would require a cardioverter defibrillator to keep her heart pumping. The two-inch scar from her surgery only hints at the more severe scars she hides from her childhood.
F or as long as John can remember, he says, his parents’ marriage was deeply troubled, as was his relationship with his father. ‘I consider myself to have been raised by my mom and her mom. I longed to feel a deeper connection with my dad, but it just wasn’t there. He couldn’t extend himself in that way.’ John’s poor relationship with his father was due, in large part, to his father’s reactivity and need for control. For instance, if John’s father said that the capital of New York was New York City, there was just no use telling him that it was Albany.
As John got older, it seemed wrong to him that his father ‘was constantly pointing out all the mistakes that my brother and I made, without acknowledging any of his own’. His father relentlessly criticised his mother, who was ‘kinder and more confident’. Aged 12, John began to interject himself into the fights between his parents. He remembers one Christmas Eve, when he found his father with his hands around his mother’s neck and had to separate them. ‘I was always trying to be the adult between them,’ John says.
John is now a boyish 40, with warm hazel eyes and a wide, affable grin. But beneath his easy, open demeanour, he struggles with an array of chronic illnesses. By the time he was 33, his blood pressure was shockingly high; he began to experience bouts of stabbing stomach pain and diarrhoea and often had blood in his stool; he struggled from headaches almost daily. By 34, he’d developed chronic fatigue, and was so wiped out that he sometimes struggled to make it through an entire workday.
John’s relationships, like his body, were never completely healthy. He ended a year‑long romance with a woman he deeply loved because he felt riddled with anxiety around her normal, ‘happy family’. He just didn’t know how to fit in. ‘She wanted to help,’ he says, ‘but instead of telling her how insecure I was around her, I told her I wasn’t in love with her.’ Bleeding from his inflamed intestines, exhausted by chronic fatigue, debilitated and distracted by pounding headaches, often struggling with work, and unable to feel comfortable in a relationship, John was stuck in a universe of pain and solitude, and he couldn’t get out.
L aura’s and John’s life stories illustrate the physical price we can pay, as adults, for trauma that took place 10, 20, even 30 years ago. New findings in neuroscience, psychology and immunology tell us that the adversity we face during childhood has farther-reaching consequences than we might ever have imagined. Today, in labs across the country, neuroscientists are peering into the once-inscrutable brain-body connection, and breaking down, on a biochemical level, exactly how the stress we experience during childhood and adolescence catches up with us when we are adults, altering our bodies, our cells, and even our DNA.
Emotional stress in adult life affects us on a physical level in quantifiable, life-altering ways. We all know that when we are stressed, chemicals and hormones can flush our body and increase levels of inflammation. That’s why stressful events in adult life are correlated with the likelihood of getting a cold or having a heart attack.
But when children or teens face adversity and especially unpredictable stressors, they are left with deeper, longer‑lasting scars. When the young brain is thrust into stressful situations over and over again without warning, and stress hormones are repeatedly ramped up, small chemical markers, known as methyl groups, adhere to specific genes that regulate the activity of stress‑hormone receptors in the brain. These epigenetic changes hamper the body’s ability to turn off the stress response. In ideal circumstances, a child learns to respond to stress, and recover from it, learning resilience. But kids who’ve faced chronic, unpredictable stress undergo biological changes that cause their inflammatory stress response to stay activated.
Joan Kaufman, director of the Child and Adolescent Research and Education (CARE) programme at the Yale School of Medicine, recently analysed DNA in the saliva of happy, healthy children, and of children who had been taken from abusive or neglectful parents. The children who’d experienced chronic childhood stress showed epigenetic changes in almost 3,000 sites on their DNA, and on all 23 chromosomes – altering how appropriately they would be able to respond to and rebound from future stressors.
kids who’ve had early adversity have a drip of fight-or-flight hormones turned on every day – it’s as if there is no off switch
Likewise, Seth Pollak, professor of psychology and director of the Child Emotion Research Laboratory at the University of Wisconsin at Madison, uncovered startling genetic changes in children with a history of adversity and trauma. Pollak identified damage to a gene responsible for calming the stress response. This particular gene wasn’t working properly; the kids’ bodies weren’t able to reign in their heightened stress response. ‘A crucial set of brakes are off,’ says Pollak.
Imagine for a moment that your body receives its stress hormones and chemicals through an IV drip that’s turned on high when needed and, when the crisis passes, it’s switched off again. You might think of kids whose brains have undergone epigenetic changes because of early adversity as having an inflammation-promoting drip of fight-or-flight hormones turned on every day – it’s as if there is no off switch.
Experiencing stress in childhood changes your set point of wellbeing for decades to come. In people such as Laura and John, the endocrine and immune systems are churning out a damaging and inflammatory cocktail of stress neurochemicals in response to even small stressors – an unexpected bill, a disagreement with their spouse, a car that swerves in front of them on the highway, a creak on the staircase – for the rest of their lives. They might find themselves overreacting to, and less able to recover from, the inevitable stressors of life. They’re always responding. And all the while, they’re unwittingly marinating in inflammatory chemicals, which sets the stage for full-throttle disease down the road, in the form of autoimmune disease, heart disease, cancer, fibromyalgia, chronic fatigue, fibroid tumours, irritable bowel syndrome, ulcers, migraines and asthma.
S cientists first came to understand the relationship between early chronic stress and later adult disease through the work of a dedicated physician in San Diego and a determined epidemiologist from the Centers for Disease Control and Prevention (CDC) in Atlanta. Together, during the 1980s and ’90s – the years when Laura and John were growing up – these two researchers began a paradigm-shifting public-health investigation known as the Adverse Childhood Experiences (ACE) Study.
In 1985, Vincent J Felitti, chief of a revolutionary preventive care initiative at the Kaiser Permanente Medical Care programme in San Diego, noticed a startling pattern in adult patients at an obesity clinic. A significant number were, with the support of Felitti and his nurses, successfully losing hundreds of pounds a year, a remarkable feat, only to withdraw from the programme despite weight-loss success. Felitti, determined to get to the bottom of the attrition rate, conducted face-to-face interviews with 286 patients. It turned out there was a common denominator. Many confided that they had suffered some sort of trauma, often sexual abuse, in their childhoods. To these patients, eating was a solution, not a problem: it soothed the anxiety and depression they had harboured for decades; their weight served as a shield against undesired attention, and they didn’t want to let it go.
Felitti’s interviews gave him a new way of looking at human health and wellbeing that other physicians just weren’t seeing. He presented his findings at a national obesity conference, arguing that ‘our intractable public health problems’ had root causes hidden ‘by shame, by secrecy, and by social taboos against exploring certain areas of life experience’. Felitti’s peers were quick to blast him. One even stood up in the audience and accused Felitti of offering ‘excuses’ for patients’ ‘failed lives’. Felitti, however, remained unfazed; he felt sure that he had stumbled upon a piece of information that would hold enormous import for the field of medicine.
After a colleague who attended that same conference suggested that he design a study with thousands of patients who suffered from a wide variety of diseases, not just obesity, Felitti joined forces with Robert Anda, a medical epidemiologist at the CDC who had, at the time, been researching the relationship between coronary heart disease and depression. Felitti and Anda took advantage of Kaiser Permanente’s vast patient cohort to set up a national epidemiology laboratory. Of the 26,000 patients they invited to take part in their study, more than 17,000 agreed.
Anda and Felitti surveyed these 17,000 individuals on about 10 types of adversity, or adverse childhood experiences (ACEs), probing into patients’ childhood and adolescent histories. Questions included: ‘Was a biological parent ever lost to you through divorce, abandonment or other reason?’; ‘Did a parent or other adult in the household often swear at you, insult you, put you down or humiliate you?’; and ‘Was a household member depressed or mentally ill?’ Other questions looked at types of family dysfunction that included growing up with a parent who was an alcoholic or addicted to other substances; being physically or emotionally neglected; being sexually or physically abused; witnessing domestic violence; having a family member who was sent to prison; feeling that there was no one to provide protection; and feeling that one’s family didn’t look out for each other. For each category to which a patient responded ‘yes’, one point would be added to her ACE score, so an ACE score of 2 would indicate that she had suffered two adverse childhood experiences.
To be clear, the patients Felitti and Anda surveyed were not troubled or disadvantaged; the average patient was 57, and three-quarters had attended college. These were ‘successful’ men and women, mostly white, middle-class, with stable jobs and health benefits. Felitti and Anda expected their number of ‘yes’ answers to be fairly low.
The correlation between having a difficult childhood and facing illness as an adult offered a whole new lens through which we could view human health and disease
When the results came in, Felitti and Anda were shocked: 64 per cent of participants answered ‘yes’ to having encountered at least one category of early adversity, and 87 per cent of those patients also had additional adverse childhood experiences; 40 per cent had suffered two or more ACEs; 12.5 per cent had an ACE score greater than or equal to 4.
Felitti and Anda wanted to find out whether there was a correlation between the number of adverse childhood experiences an individual had faced, and the number and severity of illnesses and disorders she developed as an adult. The correlation proved so powerful that Anda was not only ‘stunned’, but deeply moved.
‘I wept,’ he says. ‘I saw how much people had suffered, and I wept.’
Felitti, too, was deeply affected. ‘Our findings exceeded anything we had conceived. The correlation between having a difficult childhood and facing illness as an adult offered a whole new lens through which we could view human health and disease.’
Here, says Felitti, ‘was the missing piece as to what was causing so much of our unspoken suffering as human beings’.
T he number of adverse childhood experiences a patient had suffered could by and large predict the amount of medical care she would require in adulthood: the higher the ACE score, the higher the number of doctor’s appointments she’d had in the past year, and the more unexplained physical symptoms she’d reported.
People with an ACE score of 4 were twice as likely to be diagnosed with cancer than people who hadn’t faced any form of childhood adversity. For each point an individual had, her chance of being hospitalised with an autoimmune disease in adulthood rose 20 per cent. Someone with an ACE score of 4 was 460 per cent more likely to face depression than someone with a score of 0.
An ACE score of 6 or higher shortened an individual’s lifespan by almost 20 years.
Researchers wondered if those who encountered childhood adversity were also more likely to smoke, drink and overeat as a sort of coping strategy, and while that was sometimes the case, unhealthy habits didn’t wholly account for the correlation Felitti and Anda saw between adverse childhood experiences and later illness. For instance, those with ACE scores greater than or equal to 7 who didn’t drink or smoke, weren’t overweight or diabetic, and didn’t have high cholesterol still had a 360 per cent higher risk of heart disease than those with ACE scores of 0.
‘Time,’ says Felitti, ‘does not heal all wounds. One does not “just get over” something – not even 50 years later.’ Instead, he says: ‘Time conceals. And human beings convert traumatic emotional experiences in childhood into organic disease later in life.’
Often, these illnesses can be chronic and lifelong. Autoimmune disease. Heart disease. Chronic bowel disorders. Migraines. Persistent depression. Even today, doctors puzzle over these very conditions: why are they so prevalent; why are some patients more prone to them than others; and why are they so difficult to treat?
The more research that’s done, the more granular details emerge about the profound link between adverse experiences and adult disease. Scientists at Duke University in North Carolina, the University of California, San Francisco, and Brown University in Rhode Island have shown that childhood adversity damages us on a cellular level in ways that prematurely age our cells and affect our longevity. Adults who faced early life stress show greater erosion in what are known as telomeres – protective caps that sit on the ends of DNA strands to keep the DNA healthy and intact. As telomeres erode, we’re more likely to develop disease, and we age faster; as our telomeres age and expire, our cells expire and so, eventually, do we.
Researchers have also seen a correlation between specific types of adverse childhood experiences and a range of diseases. For instance, children whose parents die, or who face emotional or physical abuse, or experience childhood neglect, or witness marital discord between their parents are more likely to develop cardiovascular disease, lung disease, diabetes, headaches, multiple sclerosis and lupus as adults. Facing difficult circumstances in childhood increases six-fold your chances of having myalgic encephalomyelitis (chronic fatigue syndrome) as an adult. Kids who lose a parent have triple the risk of depression in their lifetimes. Children whose parents divorce are twice as likely to suffer a stroke later down the line.
Laura and John’s stories illustrate that the past can tick away inside us for decades like a silent time bomb, until it sets off a cellular message that lets us know the body does not forget its history.
Something that happened to you when you were five or 15 can land you in the hospital 30 years later
John’s ACE score would be a 3: a parent often put him down; he witnessed his mother being harmed; and, clearly, his father suffered from an undiagnosed behaviour health disorder, perhaps narcissism or depression, or both.
Laura had an ACE score of 4.
Laura and John are hardly alone. Two-thirds of American adults are carrying wounds from childhood quietly into adulthood, with little or no idea of the extent to which these wounds affect their daily health and wellbeing. Something that happened to you when you were five or 15 can land you in the hospital 30 years later, whether that something was headline news, or happened quietly, without anyone else knowing it, in the living room of your childhood home.
The adversity a child faces doesn’t have to be severe abuse in order to create deep biophysical changes that can lead to chronic health conditions in adulthood.
‘Our findings showed that the 10 different types of adversity we examined were almost equal in their damage,’ says Felitti. He and Anda found that no single ACE significantly trumped another. This was true even though some types, such as being sexually abused, are far worse in that society regards them as particularly shameful, and others, such as physical abuse, are more overt in their violence.
This makes sense if you think about how the stress response functions on an optimal level. You meet a bear in the woods, and your body floods with adrenaline and cortisol so that you can quickly decide whether to run in the opposite direction or stay and try to frighten the bear. After you deal with the crisis, you recover, your stress hormones abate, and you go home with a great story. For Laura and John, though, that feeling that the bear is still out there, somewhere, circling in the woods, stalking, and might strike again any day, anytime – that feeling never disappears.
There are a lot of bears out there. Chronic parental discord; enduring low-dose humiliation or blame and shame; chronic teasing; the quiet divorce between two secretly seething parents; a parent’s premature exit from a child’s life; the emotional scars of growing up with a hypercritical, unsteady, narcissistic, bipolar, alcoholic, addicted or depressed parent; physical or emotional abuse or neglect: these happen in all too many families. Although the details of individual adverse experiences differ from one home to another and from one neighbourhood to another, they are all precursors to the same organic chemical changes deep in the gray matter of the developing brain.
E very few decades, a groundbreaking psychosocial ‘theory of everything’ helps us to develop a new understanding of why we are the way we are – and how we got that way. In the early 20th century, the psychoanalyst Sigmund Freud transformed the landscape of psychology when he argued that the unconscious rules much of our waking life and dreams. Jungian theory taught, among other ideas, that we tend toward introversion or extroversion, which led the American educationalist Katharine Cook Briggs and her daughter Isabel Briggs Myers to develop a personality indicator. More recently, neuroscientists discovered that age ‘zero to three’ was a critical synaptic window for brain development, giving birth to Head Start and other preschool programmes. The correlation between childhood trauma, brain architecture and adult wellbeing is the newest, and perhaps our most important, psychobiological theory of everything.
Today’s research on adverse childhood experiences revolutionises how we see ourselves, our understanding of how we came to be the way we are, why we love the way we do, how we can better nurture our children, and how we can work to realise our potential.
To date, more than 1,500 studies founded on Felitti and Anda’s hallmark ACE research show that both physical and emotional suffering are rooted in the complex workings of the immune system, the body’s master operating control centre – and what happens to the brain during childhood sets the programming for how our immune systems will respond for the rest of our lives.
The unifying principle of this new theory of everything is this: your emotional biography becomes your physical biology, and together, they write much of the script for how you will live your life. Put another way: your early stories script your biology and your biology scripts the way your life will play out.
Unlike previous theories of everything, though, this one has been mind-bogglingly slow to change how we do medicine, according to Felitti. ‘Very few internists or medical schools are interested in embracing the added responsibility that this understanding imposes on them.’
With the ACE research now available, we might hope that physicians will begin to see patients as a holistic sum of their experiences and embrace the understanding that a stressor from long ago can be a health-risk time bomb that has exploded. Such a medical paradigm, which sees adverse childhood experiences as one of many key factors that can play a role in disease, could save many patients years in the healing process.
But seeing that connection takes a little time. It means asking patients to fill out the ACE questionnaire and delving into that patient’s history for insight into sources of both physical and emotional pain. As health-care budgets have become stretched, physicians spend less time interacting one-on-one with patients in their exam rooms; the average physician schedules patients back-to-back at 15-minute intervals.
Still, the cost of not intervening is far greater – not only in the loss of human health and wellbeing, but also in additional healthcare. According to the CDC, the total lifetime cost of child maltreatment in the US is $124 billion each year. The lifetime healthcare cost for each individual who experiences childhood maltreatment is estimated at $210,012 – comparable to other costly health conditions, such as having a stroke, which has a lifetime estimated cost of $159,846 per person, or type-2 diabetes, which is estimated to cost between $181,000 and $253,000.
Further hindering change is the fact that adult physical medicine and psychological medicine remain in separate silos. Utilising ACE research requires breaking down these long-standing divisions in healthcare between what is ‘physical’ and what is ‘mental’ or ‘emotional,’ and that’s hard to achieve. Physicians have been well-trained to deal only with what they can touch with their hands, see with their eyes, or view with microscopes or scans.
Just as physical wounds and bruises heal, just as we can regain our muscle tone, we can recover function in underconnected areas of the brain
However, now that we have scientific evidence that the brain is genetically modified by childhood experience, we can no longer draw that line in the sand. With hundreds of studies showing that childhood adversity hurts our mental and physical health, putting us at greater risk for learning disorders, cardiovascular disease, autoimmune disease, depression, obesity, suicide, substance abuse, failed relationships, violence, poor parenting and early death, we just can’t afford to make such distinctions.
Science tells us that biology does not have to be destiny. ACEs can last a lifetime, but they don’t have to. Just as physical wounds and bruises heal, just as we can regain our muscle tone, we can recover function in underconnected areas of the brain. If anything, that’s the most important take-away from ACE research: the brain and body are never static; they are always in the process of becoming and changing.
Even if we have been set on high-reactive mode for decades or a lifetime, we can still dial it down. We can respond to life’s inevitable stressors more appropriately and shift away from an overactive inflammatory response. We can become neurobiologically resilient. We can turn bad epigenetics into good epigenetics and rescue ourselves. We have the capacity, within ourselves, to create better health. We might call this brave undertaking ‘the neurobiology of awakening’.
Today, scientists recognise a range of promising approaches to help create new neurons (known as neurogenesis), make new synaptic connections between those neurons (known as synaptogenesis), promote new patterns of thoughts and reactions, bring underconnected areas of the brain back online – and reset our stress response so that we decrease the inflammation that makes us ill.
You can find ways to start right where you are, no matter how deep your scars or how long ago they occurred. Many mind-body therapies not only help you to calm your thoughts and increase your emotional and physical wellbeing, but research suggests that they have the potential to reverse, on a biological level, the harmful impact of childhood adversity.
Recent studies indicate that individuals who practice mindfulness meditation and mindfulness-based stress reduction (MBSR) show an increase in gray matter in parts of the brain associated with managing stress, and experience shifts in genes that regulate their stress response and their levels of inflammatory hormones. Other research suggests that a process known as neurofeedback can help to regrow connections in the brain that were lost to adverse childhood experiences.
Meditation, mindfulness, neurofeedback, cognitive therapy, EMDR (eye movement desensitisation and reprocessing) therapy: these promising new avenues to healing can be part of any patient’s recovery plan, if only healthcare practitioners would begin to treat the whole patient – past, present and future, without making distinctions between physical and mental health – and encourage patients to explore all the treatment options available to them. The more we learn about the toxic impact of early stress, the better equipped we are to counter its effects, and help to uncover new strategies and modalities to come back to who it is we really are, and who it was we might have been had we not encountered childhood adversity in the first place.
This is an adapted and reprinted extract from ‘Childhood Disrupted: How Your Biography Becomes Your Biology, and How You Can Heal’ (Atria), by Donna Jackson Nakazawa. Copyright © Donna Jackson Nakazawa, 2015."
Sugar is a toxic agent that creates conditions for disease | Aeon Essays,,https://aeon.co/essays/sugar-is-a-toxic-agent-that-creates-conditions-for-disease,"‘Virtually zero.’ That’s a reasonable estimate of the probability that public health authorities in the foreseeable future will successfully curb the worldwide epidemics of obesity and diabetes, at least according to Margaret Chan, the director general of the World Health Organization (WHO) – a person who should know. Virtually zero is the likelihood, Chan said at the National Academy of Medicine’s annual meeting in October, that she and her many colleagues worldwide will successfully prevent ‘a bad situation’ from ‘getting much worse’. That Chan also described these epidemics as a ‘slow-motion disaster’ suggests the critical nature of the problem: ‘population-wide’ explosions in the prevalence of obesity along with increases in the occurrence of diabetes that frankly strain the imagination: a disease that leads to blindness, kidney failure, amputation, heart disease and premature death, and that was virtually non-existent in hospital inpatient records from the mid-19th century, now afflicts one in 11 Americans; in some populations, as many as one in two adults are diabetic.
In the midst of such a public health crisis, the obvious question to ask is why. Many reasons can be imagined for any public health failure, but we have no precedents for a failure of this magnitude. As such, the simplest explanation is that we’re not targeting the right agent of disease; that our understanding of the aetiology of both obesity and diabetes is somehow flawed, perhaps tragically so.
Researchers in harder sciences have a name for such situations: ‘pathological science’, defined by the Nobel Laureate chemist Irving Langmuir in 1953 as ‘the science of things that aren’t so’. Where experimental investigation is prohibitively expensive or impossible to do, mistaken assumptions, misconceived paradigms and pathological science can survive indefinitely. Whether this is the case with the current epidemics is an all-too-regrettable possibility: perhaps we’ve simply misconceived the reality of the link between diet, lifestyle and the related disorders of obesity and diabetes? As the Oxford scholar Robert Burton suggested in The Anatomy of Melancholy (1621) , in cases in which the cures are ‘imperfect, lame, and to no purpose’ it’s quite possible that the causes are misunderstood.
The history of obesity and nutrition research suggests that this is indeed what has happened. In the decades leading up to the Second World War, German and Austrian clinical investigators had concluded that common obesity was clearly caused by a hormonal disturbance; starting in the 1960s, other research would link that disturbance to the sugar in our diets. But the German and Austrian thinking evaporated with the war, and the possibility that sugar was to blame never took hold, dismissed by a nutrition community who, by the 1970s, became fixated on dietary fat as the trigger of our chronic diseases. Now, with an explosion of the epidemic and compelling new research, it’s time to reconsider both our causal thinking on obesity and diabetes, and the possibility that sugar is playing the critical role.
When researchers and public health authorities today discuss their failure to curb the rising tide of obesity and diabetes, they offer the explanation that these disorders are ‘multifactorial and complex’, implying that failure is somehow understandable. But this obscures the reality that prescriptions to prevent and treat the two depend almost entirely on two simple causal concepts, neither one of which is necessarily correct.
The first assumption equates obesity and Type 2 diabetes (the common form of the disease, formerly known as ‘adult-onset’ until it began appearing in children as well). Because obesity and Type 2 diabetes are so closely associated in both individuals and populations, the assumption is that it’s the obesity – or at least the accumulation of excess fat – that causes the diabetes. By this logic, whatever causes obesity is ultimately the cause of the diabetes as well.
The second assumption then strives to explain ‘the fundamental cause’ of the obesity itself: an energy imbalance between calories consumed on one hand, and calories expended on the other hand.
This thinking, espoused by the WHO and virtually every other medical authority, is a paradigm in the true Kuhnian sense of the word. Researchers and public health authorities describe obesity as a disorder of ‘energy balance’. This conception underlies virtually all aspects of obesity research from prevention through treatment, and, by association, diabetes. As such, it has also shaped how we think about the role of what is now, finally, considered a prime suspect – refined or ‘added’ sugars, and specifically, sucrose (table sugar) and high-fructose corn syrup.
T he WHO and other health organisations have recently taken to arguing that sugar and particularly sugary beverages should be taxed heavily or regulated. But they do so not because they say sugar causes disease – using the same definition of causality that we use when we say cigarettes cause lung cancer – but, rather, because, from their perspective, sugar represents ‘empty calories’ that we eat in excess. By this thinking, we still get fatter because we eat too much or exercise too little. The solution is to eat in moderation, and consume sugar in moderation or balance it with more physical activity.
The energy balance paradigm implies that the only way in which foods influence our body fat is through their energy content, or calories – that is, through the energy that we absorb without excreting, and so make available to be oxidised or stored. This is the only variable that matters. It’s the implication of the phrase ‘a calorie is a calorie’, which, by the 1960s, had become a mantra of nutrition and obesity researchers, evoked invariably to support the dogma that only calories count when it comes to understanding and treating human obesity.
This logic has been the lifeblood of the sugar industry. If sugar was uniquely toxic, in that it possessed some special property that made us respond to it by accumulating fat or becoming diabetic, then government health agencies would have to regulate it. If all sugar does is add calories to the diet, just as any other food does, then it is, in effect, benign. When the sugar industry embarked in 1956 on a nationwide advertising offensive to knock down reports that sugar is ‘fattening’, it did so on the seemingly sound scientific basis that ‘[s]ugar is neither a “reducing food” nor a “fattening food”’, as the industry advertisements explained. ‘There are no such things. All foods supply calories and there is no difference between the calories that come from sugar or steak or grapefruit or ice cream.’
Thinking of obesity as an energy-balance disorder is as meaningless as calling poverty a money-balance problem
Even 60 years later, in 2015, when The New York Times reported that academic researchers were doing the bidding of Coca-Cola by taking its money to fund a Global Energy Balance Network and ‘shift blame for obesity away from bad diets’, this was still the logic invoked in sugar’s defence: if you believe that obesity is caused by a mere caloric surplus, then the solution to the epidemic is not necessarily to avoid Coca-Cola, but to either consume it (and everything else) in moderation or to burn off the excess calories with physical activity. For the sugar industry and the purveyors, such as Coca-Cola, of sugar-rich foods and beverages, this remarkably resilient, century-old conception of why some of us get fat (or are born fat) and others don’t (or aren’t) has been the gift that keeps on giving.
So here’s another way to frame what is now the imperative question: is the energy-balance hypothesis of obesity correct? Is it the right paradigm to understand the disorder? The competing hypothesis has existed for over a century: in this paradigm, obesity is not an energy-balance disorder but a disorder of excess fat accumulation and so, clearly, a hormonal and metabolic disorder – the result of an ‘endocrine disturbance’, as it was phrased in the 1930s by Eugene Du Bois, then the leading American authority on metabolism. By this logic, the foods we eat influence fat accumulation not because of their caloric content but because of their macronutrient content, the proteins, fats and carbohydrates they contain. This paradigm attends to how organisms (humans, of course, in particular) orchestrate the careful ‘partitioning’ of the macronutrient fuels they consume, determining whether they will be burned for energy or stored or used to rebuild tissues and organs. It proposes that dysregulation of this exquisitely-evolved, finely-tuned homeostatic system (a system that is biologically balanced) is the necessary component to explain both the excessive storage of calories of fat – obesity – and the diabetes that accompanies it.
This alternate hypothesis implies that sugar has unique effects in the human body leading directly to both diabetes and obesity, independent of the calories consumed. By this way of thinking, refined sugars are indeed toxic, albeit over the course of years or decades. We get fat and diabetic not because we eat too much of them – although that is implied tautologically merely by the terms ‘overconsumption’ and ‘overeating’ – but because they have unique physiological, metabolic and hormonal effects that directly trigger these disorders. If all this is right, then thinking of obesity as an energy-balance disorder is as meaningless as calling poverty a money-balance problem (caused, of course, by earning too little or spending too much, or both). By conceiving of obesity as a problem caused by the behaviours of excessive consumption and physical inactivity, researchers not only took a physiological defect – the excess accumulation of fat, often to a massive extent – and turned it into a behavioural problem. But they made a critical error, one that has grown over the course of decades into an idea that seems too big to fail.
U nderstanding how this happened requires we attend to history. The modern era of nutrition science dates to the late 1860s, when German researchers pioneered the use of room-sized devices called calorimeters. These allowed them to measure the energy expended by human or animal subjects under different conditions of diet and activity. For the next half a century, effectively all nutrition research was directed toward studying energy balance (the energy content of foods and the energy expended or excreted by those who ate it) and the protein, vitamins, minerals and fibre necessary for health and wellbeing. This was a function of the research tools available at the time, and it has remained the foundation of nutrition wisdom ever since.
Today, when nutritionists say that sugar consists of ‘empty calories’, they’re defining it in the terms of this century-old research and the tools available to researchers of that era. When obesity researchers blame obesity on the imbalance of energy consumed to expended, they’re doing the same. Both are assuming that the science that came after, including the emergence of entire disciplines of medicine, is irrelevant.
The idea of obesity as an energy-balance disorder emerged directly from what was considered one of the great triumphs in nutrition in the late 19th-century: the confirmation that the laws of thermodynamics – conservation of energy, specifically – applied not just to inanimate matter but to living organisms and humans. In line with this research, nutritionists embraced calories and energy as the currency of their discipline, and physicians, speculating as to the cause of obesity, naturally did the same. By the early 1900s, the German diabetes specialist Carl von Noorden was proposing that ‘the ingestion of a quantity of food greater than that required by the body, leads to an accumulation of fat, and to obesity, should the disproportion be continued over a considerable period’.
In the 1920s, Von Noorden’s ideas were taken up in the United States by Louis Newburgh, a physician at the University of Michigan, who espoused what he considered an indisputable truth: ‘All obese persons are alike in one fundamental respect – they literally overeat.’ By assuming that this overeating must be the cause of obesity, Newburgh proceeded to blame the disorder on some combination of a ‘perverted appetite’ (excessive energy consumption) and a ‘lessened outflow of energy’ (insufficient expenditure). To explain why obese individuals failed to respond to this imbalance by either eating less or exercising more – both, after all, should be under conscious control – Newburgh also suggested that the overeating and/or under-expending were often compounded by ‘various human weaknesses such as overindulgence and ignorance’, thus blaming the victim and beginning the process that would turn obesity research in the 1960s into a subdiscipline of psychology and behavioural science.
This logic is still with us today. By 1939, Newburgh’s biography at the University of Michigan was already crediting him with the discovery that ‘the whole problem of weight lies in regulation of the inflow and outflow of calories’ and for having ‘undermined conclusively the generally held theory that obesity is the result of some fundamental fault’.
The existence of a fundamental fault, however, could not be dismissed so lightly, as German and Austrian investigators were still arguing at the time. They had concluded that obesity could be explained only by such a fault, a hormonal or regulatory defect. Worth noting is that the German and Austrian research communities had pioneered all the fields of science relevant to understanding obesity – including nutrition, metabolism, endocrinology and genetics. They dominated medical science, just as they did physics and chemistry through the Second World War. This was an era during which the lingua franca of science – medical or otherwise – was German, and when individuals serious about pursuing science travelled to Germany and Austria to learn from, if not mentor with, these authorities.
Coincident with von Noorden’s suggestion that obesity was an energy-balance disorder, his contemporary, Gustav von Bergmann, who would become the leading German authority on internal medicine, argued that it was clearly not. Von Bergmann pointed out that the overconsumption of energy that von Noorden was blaming as the cause of obesity – more energy in than out – was merely a description of what happened when the mass of any system increased, not an explanation at all.
The purpose of a hypothesis in science, quite simply, is to offer an explanation for what we observe, either in nature or the laboratory. How many of these observations can the hypothesis explain or predict in a simple and straightforward way? Yet the energy-balance conception fails to explain anything: it cannot explain why calories of fat are trapped in fat tissue rather than oxidised for fuel, nor such simple observations as the genetic basis of obesity (identical twins, after all, are identical not just in their facial features, height and colouring, but in body type too) or why fat accumulates differently in men and women.
With obesity, ‘a sort of anarchy exists, the adipose tissue does not fit into the precisely regulated management of the whole organism’
By von Bergmann’s logic, obesity was clearly not a problem of energy balance, but of fat trapping (just as global warming is not an energy-balance problem, but an energy-trapping one). The question that had to be answered is why this trapping occurs. Any viable hypothesis of obesity had to explain why the fat tissue of the obese is so avid in hoarding calories as fat, rather than allowing that fat to be metabolised and provide energy for the body.
By 1930, Julius Bauer of the University of Vienna – the ‘noted Vienna authority on internal diseases’, as The New York Times called him – had taken up von Bergmann’s ideas, arguing that obesity had to result from a dysregulation of the biological factors that normally work to keep fat accumulation under check. Bauer argued that fat cells are clearly being driven by these factors to hoard excessive calories as fat, and this in turn would deprive the rest of the body of the energy it needed to thrive. In this hormonal/regulatory conception, excessive fat-accumulation causes hunger and physical inactivity, not the other way around.
Bauer likened the fat tissue of an obese person to that of ‘a malignant tumour or … the foetus, the uterus or the breasts of a pregnant woman’, all with independent agendas, causing them to take up calories of fuel from the circulation and hoard them or put them to localised use, regardless of how much the person might be eating or exercising. With obesity, wrote Bauer, ‘a sort of anarchy exists, the adipose tissue lives for itself and does not fit into the precisely regulated management of the whole organism’.
By 1938, Russell Wilder, head of the department of medicine at the Mayo Clinic, was writing that this German/Austrian hypothesis ‘deserves attentive consideration’, and that ‘the effect after meals of withdrawing from the circulation even a little more fat than usual might well account both for the delayed sense of satiety and for the frequently abnormal taste for carbohydrate encountered in obese persons … A slight tendency in this direction would have a profound effect in the course of time.’
In 1940, when Hugo Rony, an endocrinologist at Northwestern University in Chicago, published the first academic treatise written on obesity in the US, he asserted that the hormonal/regulatory hypothesis was ‘more or less fully accepted’ by the European authorities.
A nd then it vanished. The German and Austrian medical-research community evaporated with the rise of Hitler, and the nexus of medical science shifted from Germany and Austria to the US, a nation not devastated by the war; the lingua franca of medical science shifted as well from German to English. With those shifts, arguably the best thinking of the era in medical science would no longer be read, nor would it be referenced. The conception of obesity as a hormonal regulatory disorder faded out of fashion.
In the post-war era of nutrition and obesity research, Newburgh’s energy balance conception was fixed as the obesity paradigm, not because it answered any meaningful questions about obesity and how, why and when we accumulate excess fat, but because it was a US conception at a time when young American physicians, many with little scientific training, came to dominate the field.
Embrace of the energy-balance paradigm and, with it, the death of the hormonal/regulatory hypothesis, can be seen clearly in the citation records. In 1941, Bauer published what would be his second and last English-language article on obesity: a 27-page review in Archives of Internal Medicine entitled ‘Obesity: Its Pathogenesis, Etiology and Treatment’. (By then, he had fled to the US and was living unaffiliated in Los Angeles). He spent the first third of the article critiquing, point by point, Newburgh’s ‘energy theory of obesity’, and the remainder discussing the ‘biologic theory’, and the evidence for why obesity had to be a hormonal/regulatory disorder. In 1942, Newburgh countered with a 64-page review in the same journal, refuting the biologic hypothesis and insisting that obesity is ‘invariably the result of a disproportion between the inflow and the outflow of energy’. In 1944, Newburgh published a second review, this one in Physiological Reviews , again insisting that von Bergmann and Bauer’s ideas had been refuted.
By 1959, Bauer’s article had been referenced only 10 times and would not be cited again in the indexed medical literature for another half a century. Meanwhile, Newburgh’s two articles on obesity as an energy-balance disorder would continue to be cited through to the end of the 1970s – accumulating 69 and 64 citations by that time respectively, enormous numbers for that era.
Despite its almost-universal acceptance, the energy theory remained at loggerheads with much of the science. For instance, animal models of obesity – the first of which was discussed in the literature in the late 1930s – consistently refuted Newburgh’s arguments and supported Bauer’s. Obese animals would frequently manifest what Newburgh might have described as a perverted appetite (technically, hyperphagia ): as they grew fatter they would be exceedingly hungry and consume great amounts of food. But they would invariably get obese, or at least significantly fatter, even when they didn’t eat any more, or weren’t allowed to eat any more than control animals, often littermates, that remained lean. Some of these animals would remain excessively fat even as they were being starved.
Insulin partitions how we use the fuels we consume: it directs fat cells to store fat
Whatever the defect or fundamental fault that caused these animals to accumulate excessive fat, a perverted appetite (ie, overeating) could be ruled out. The defect had to be working either to cause the fat cells to hoard calories as fat, or to suppress the animals’ ability to burn fatty acids for fuel. Or both.
Not until the 1960s, though, would researchers elucidate the basic mechanisms of fat accumulation. To do so required invention of a technology that allowed researchers to accurately measure the level of hormones circulating in the bloodstream. This was the work of Rosalyn Yalow, a medical physicist, and Solomon Berson, a physician. When Yalow was awarded the Nobel Prize for the work in 1977 (by then, Berson was not alive to share it), the Nobel Foundation described it aptly as bringing about ‘a revolution in biological and medical research’. Those interested in obesity could now finally answer the questions on which the pre-war European clinicians could only speculate: what hormones regulate the storage of fat in fat cells and its use for fuel by the rest of the body?
Answers began coming with the very first publications out of Yalow and Berson’s laboratory and were swiftly confirmed. As it turns out, virtually all hormones work to mobilise fatty acids from fat cells so that they can then be used for fuel. The one dominant exception to this fuel-mobilisation signalling is insulin, which partitions how we use the fuels we consume: in particular, it directs fat cells to store fat, while facilitating the uptake and oxidation of glucose (blood sugar) by muscle and organ cells. In other words, when insulin is secreted – primarily in response to the carbohydrates in our diet – it directs our cells to burn carbohydrate as fuel and store fat. And so, the one biological factor necessary to mobilise fat from storage and have it used for fuel, as Yalow and Berson suggested in 1965, is ‘the negative stimulus of insulin deficiency’. Put simply, when insulin levels in circulation are elevated, we store fat and use glucose for fuel; as insulin levels drop, fat is mobilized and we burn it instead.
Yalow and Berson themselves described insulin as a ‘lipogenic’, or fat-forming hormone. This lipogenic signal must be turned off, or at least muted significantly, for the fat cells to release their stored fat and for the body to metabolise it for energy. While obesity researchers like to say that the sine qua non of a weight reduction diet is calorie-restriction, this alternative, biologically-based hypothesis would say that the sine qua non is lowering insulin. The more we consume carbohydrates, though, and particularly sugar, the higher i our insulin levels will be.
T he potential role of insulin in obesity was illuminated further by a second revelation from Yalow and Berson’s early research: both Type 2 diabetics and the obese tend to have elevated levels of blood sugar and abnormally high levels of circulating insulin. This implies that the cells of their muscles and organs are resistant to the insulin circulating in their blood, an observation that was also quickly and widely confirmed. By the mid-1960s, both physicians and researchers were realising that Type 2 diabetes was not a disease of insulin deficiency – as Type 1 diabetes is – at least not at first, but one of insulin resistance. But if insulin is a fat-forming hormone and Type 2 diabetes is a disorder of insulin resistance, it then follows that high circulating levels of insulin in the blood, rather than insulin deficiency, could be the cause of the disease and obesity as well.
Perhaps the obese get that way not because they eat too much or exercise too little, but because they have elevated levels of insulin or their fat tissue is excessively sensitive to the insulin they secrete. Perhaps the relationship between obesity and Type 2 diabetes is not one of cause and effect, as doctors have said for years.
Berson and Yalow saw it another way: ‘We generally accept that obesity predisposes to diabetes; but does not mild diabetes predispose to obesity?’ the team wrote in 1965. ‘Since insulin is a most potent lipogenic agent, chronic [elevated insulin] would favour the accumulation of body fat.’
If Yalow and Berson’s speculation were to be true, and it certainly made sense from a biological perspective, then obesity could clearly be a hormonal/regulatory defect and Bauer and von Bergmann would have been right. Embracing this conclusion, though, depended on explaining why we become insulin-resistant. By rejecting a hormonal hypothesis of obesity two decades earlier, obesity researchers had predetermined how they would answer the question: by assuming that insulin resistance was caused by obesity, and insisting that obesity itself was caused merely by taking in more calories than expended. And that’s what they did.
The problem, as ever, appears to have been cognitive dissonance: Yalow and Berson’s revelations led both directly and indirectly to the notion that diets restricted in carbohydrates – and restricted in sugar most of all – would be uniquely effective in slimming the obese. By the mid-1960s, these carbohydrate-restricted diets, typically high in fat, were becoming fashionable, promoted by working physicians often in the form of hugely successful diet books.
Academic nutritionists led by Fred Stare and Jean Mayer of Harvard denounced these diets as dangerous fads because of their high fat content and perhaps, in Stare’s case, because of funding from the sugar and grain industries. They suggested that the physician-authors were trying to con the obese with the fraudulent argument that they could become lean without doing the hard work of curbing their perverted appetites.
This battle played out through the mid-1970s, with the academic nutritionists and obesity researchers on one side, and the physicians-turned-diet-book-authors on the other. The obesity researchers began the 1960s believing that obesity was indeed an eating disorder – Newburgh’s perverted appetite. The ongoing revolution in endocrinology, spurred by Yalow and Berson’s groundbreaking invention, failed to convince them otherwise.
The cognitive dissonance created by the biological revelations of the role of insulin in fat storage can still be seen in textbooks today. These books – for instance, the Lehninger Principles of Biochemistry, now in its sixth edition – have discussions of the regulation of fat accumulation in fat cells, in which the process is said to be driven by ‘high blood glucose elicit[ing] the release of insulin’, which favours fat storage ‘while inhibiting fatty acid mobilisation in adipose tissue’. And yet they also have sections on human obesity that state dogmatically that it is ‘the result of taking in more calories in the diet than are expended by the body’s energy-consuming activities’. Both exist side by side in the same books. Both cannot be true. The unjustifiable implication is that the mechanism determining whether or not our fat cells accumulate excessive fat are somehow different from those determining whether we become fat ourselves, despite our excess fat accumulation being merely a summation of all the excess fat stored in those cells.
Focusing on the problems of eating too much and exercising too little, health authorities have failed to target the correct causes
A far more parsimonious hypothesis is that the same thing that makes our fat cells fat makes us fat: ‘high blood glucose’ and concomitant elevated levels of insulin and the insulin resistance itself, both caused by the carbohydrate content of our diets. Insulin is secreted in response to rising blood sugar, and rising blood sugar is a response to a carbohydrate-rich meal. Sugar is implicated, in particular, because its chemical structure includes a large proportion of the carbohydrate fructose, and fructose is preferentially metabolised in the liver. As such, it is a prime suspect for the fat accumulation in liver cells that is hypothesised to be the trigger of insulin resistance itself.
If we accept von Bergmann and Bauer’s thinking that obesity is a hormonal/regulatory disorder and combine it with the revelations of the 1960s about the hormonal regulation of fat accumulation and the insulin resistance that is associated with obesity and diabetes, then the result is a very simple hypothesis that explains not just obesity but also the current epidemics and our failures to curb them. The sugars and refined grains that make up such a high proportion of the foods we consume in modern Westernised diets trigger the dysregulation of a homeostatic system that has evolved to depend on insulin to regulate both fat accumulation and blood sugar. Hence, the same dietary factors – sugars and refined grains – trigger both obesity and diabetes. By focusing on the problems of eating too much and exercising too little, public health authorities have simply failed to target the correct causes.
Scientific understanding is always driven by the tools available to do the research. These tools dictate the questions that can be asked, and the answers that can be obtained – and that, in turn, tends to shape causal hypotheses and paradigms. Ideally, when new technology comes along and new questions can be asked, then new answers can be obtained, and paradigms can shift. But this requires that the research community be open to new evidence and new ways of thinking. In nutrition and obesity research, particularly at critical times in the evolution of the science, this was simply not the case. With the epidemics of obesity and diabetes having long ago passed into crisis level, isn’t it time we finally considered seriously the possibility that our prescriptions and approaches to prevention and treatment of these diseases are simply wrong, based on incorrect paradigms and a century of misguided science?
Adapted from ‘The Case Against Sugar’ by Gary Taubes. Copyright © 2016 by Penguin Random House. Adapted by permission of Alfred A Knopf, a division of Penguin Random House LLC. All rights reserved. No part of this piece may be reproduced or reprinted without permission in writing from the publisher."
Has progress in science and technology come to a halt? | Aeon Essays,,https://aeon.co/essays/has-progress-in-science-and-technology-come-to-a-halt,"We live in a golden age of technological, medical, scientific and social progress. Look at our computers! Look at our phones! Twenty years ago, the internet was a creaky machine for geeks. Now we can’t imagine life without it. We are on the verge of medical breakthroughs that would have seemed like magic only half a century ago: cloned organs, stem-cell therapies to repair our very DNA. Even now, life expectancy in some rich countries is improving by five hours a day. A day ! Surely immortality, or something very like it, is just around the corner.
The notion that our 21st-century world is one of accelerating advances is so dominant that it seems churlish to challenge it. Almost every week we read about ‘new hopes’ for cancer sufferers, developments in the lab that might lead to new cures, talk of a new era of space tourism and super-jets that can fly round the world in a few hours. Yet a moment’s thought tells us that this vision of unparalleled innovation can’t be right, that many of these breathless reports of progress are in fact mere hype, speculation – even fantasy.
Yet there once was an age when speculation matched reality. It spluttered to a halt more than 40 years ago. Most of what has happened since has been merely incremental improvements upon what came before. That true age of innovation – I’ll call it the Golden Quarter – ran from approximately 1945 to 1971. Just about everything that defines the modern world either came about, or had its seeds sown, during this time. The Pill. Electronics. Computers and the birth of the internet. Nuclear power. Television. Antibiotics. Space travel. Civil rights.
There is more. Feminism. Teenagers. The Green Revolution in agriculture. Decolonisation. Popular music. Mass aviation. The birth of the gay rights movement. Cheap, reliable and safe automobiles. High-speed trains. We put a man on the Moon, sent a probe to Mars, beat smallpox and discovered the double-spiral key of life. The Golden Quarter was a unique period of less than a single human generation, a time when innovation appeared to be running on a mix of dragster fuel and dilithium crystals.
Today, progress is defined almost entirely by consumer-driven, often banal improvements in information technology. The US economist Tyler Cowen, in his essay The Great Stagnation (2011), argues that, in the US at least, a technological plateau has been reached. Sure, our phones are great, but that’s not the same as being able to fly across the Atlantic in eight hours or eliminating smallpox. As the US technologist Peter Thiel once put it: ‘We wanted flying cars, we got 140 characters.’
Economists describe this extraordinary period in terms of increases in wealth. After the Second World War came a quarter-century boom; GDP-per-head in the US and Europe rocketed. New industrial powerhouses arose from the ashes of Japan. Germany experienced its Wirtschaftswunder . Even the Communist world got richer. This growth has been attributed to massive postwar government stimulus plus a happy nexus of low fuel prices, population growth and high Cold War military spending.
But alongside this was that extraordinary burst of human ingenuity and societal change. This is commented upon less often, perhaps because it is so obvious, or maybe it is seen as a simple consequence of the economics. We saw the biggest advances in science and technology: if you were a biologist, physicist or materials scientist, there was no better time to be working. But we also saw a shift in social attitudes every bit as profound. In even the most enlightened societies before 1945, attitudes to race, sexuality and women’s rights were what we would now consider antediluvian. By 1971, those old prejudices were on the back foot. Simply put, the world had changed.
B ut surely progress today is real? Well, take a look around. Look up and the airliners you see are basically updated versions of the ones flying in the 1960s – slightly quieter Tristars with better avionics. In 1971, a regular airliner took eight hours to fly from London to New York; it still does. And in 1971, there was one airliner that could do the trip in three hours. Now, Concorde is dead. Our cars are faster, safer and use less fuel than they did in 1971, but there has been no paradigm shift.
And yes, we are living longer, but this has disappointingly little to do with any recent breakthroughs. Since 1970, the US Federal Government has spent more than $100 billion in what President Richard Nixon dubbed the ‘War on Cancer’. Far more has been spent globally, with most wealthy nations boasting well-funded cancer‑research bodies. Despite these billions of investment, this war has been a spectacular failure. In the US, the death rates for all kinds of cancer dropped by only 5 per cent in the period 1950-2005, according to the National Center for Health Statistics. Even if you strip out confounding variables such as age (more people are living long enough to get cancer) and better diagnosis, the blunt fact is that, with most kinds of cancer, your chances in 2014 are not much better than they were in 1974. In many cases, your treatment will be pretty much the same.
After the dizzying breakthroughs of the 20th century, physics seems to have ground to a halt
For the past 20 years, as a science writer, I have covered such extraordinary medical advances as gene therapy, cloned replacement organs, stem-cell therapy, life-extension technologies, the promised spin-offs from genomics and tailored medicine. None of these new treatments is yet routinely available. The paralyzed still cannot walk, the blind still cannot see. The human genome was decoded (one post-Golden Quarter triumph) nearly 15 years ago and we’re still waiting to see the benefits that, at the time, were confidently asserted to be ‘a decade away’. We still have no real idea how to treat chronic addiction or dementia. The recent history of psychiatric medicine is, according to one eminent British psychiatrist I spoke to, ‘the history of ever-better placebos’. And most recent advances in longevity have come about by the simple expedient of getting people to give up smoking, eat better, and take drugs to control blood pressure.
There has been no new Green Revolution. We still drive steel cars powered by burning petroleum spirit or, worse, diesel. There has been no new materials revolution since the Golden Quarter’s advances in plastics, semi-conductors, new alloys and composite materials. After the dizzying breakthroughs of the early to mid-20th century, physics seems (Higgs boson aside) to have ground to a halt. String Theory is apparently our best hope of reconciling Albert Einstein with the Quantum world, but as yet, no one has any idea if it is even testable. And nobody has been to the Moon for 42 years.
Why has progress stopped? Why, for that matter, did it start when it did, in the dying embers of the Second World War?
O ne explanation is that the Golden Age was the simple result of economic growth and technological spinoffs from the Second World War. It is certainly true that the war sped the development of several weaponisable technologies and medical advances. The Apollo space programme probably could not have happened when it did without the aerospace engineer Wernher Von Braun and the V-2 ballistic missile. But penicillin, the jet engine and even the nuclear bomb were on the drawing board before the first shots were fired. They would have happened anyway.
Conflict spurs innovation, and the Cold War played its part – we would never have got to the Moon without it. But someone has to pay for everything. The economic boom came to an end in the 1970s with the collapse of the 1944 Bretton Woods trading agreements and the oil shocks. So did the great age of innovation. Case closed, you might say.
And yet, something doesn’t quite fit. The 1970s recession was temporary: we came out of it soon enough. What’s more, in terms of Gross World Product, the world is between two and three times richer now than it was then. There is more than enough money for a new Apollo, a new Concorde and a new Green Revolution. So if rapid economic growth drove innovation in the 1950s and ’60s, why has it not done so since?
In The Great Stagnation , Cowen argues that progress ground to a halt because the ‘low-hanging fruit’ had been plucked off. These fruits include the cultivation of unused land, mass education, and the capitalisation by technologists of the scientific breakthroughs made in the 19th century. It is possible that the advances we saw in the period 1945-1970 were similarly quick wins, and that further progress is much harder. Going from the prop-airliners of the 1930s to the jets of the 1960s was, perhaps, just easier than going from today’s aircraft to something much better.
But history suggests that this explanation is fanciful. During periods of technological and scientific expansion, it has often seemed that a plateau has been reached, only for a new discovery to shatter old paradigms completely. The most famous example was when, in 1900, Lord Kelvin declared physics to be more or less over, just a few years before Einstein proved him comprehensively wrong. As late as the turn of the 20th century, it was still unclear how powered, heavier-than-air aircraft would develop, with several competing theories left floundering in the wake of the Wright brothers’ triumph (which no one saw coming).
Lack of money, then, is not the reason that innovation has stalled. What we do with our money might be, however. Capitalism was once the great engine of progress. It was capitalism in the 18th and 19th centuries that built roads and railways, steam engines and telegraphs (another golden era). Capital drove the industrial revolution.
Now, wealth is concentrated in the hands of a tiny elite. A report by Credit Suisse this October found that the richest 1 per cent of humans own half the world’s assets. That has consequences. Firstly, there is a lot more for the hyper-rich to spend their money on today than there was in the golden age of philanthropy in the 19th century. The superyachts, fast cars, private jets and other gewgaws of Planet Rich simply did not exist when people such as Andrew Carnegie walked the earth and, though they are no doubt nice to have, these fripperies don’t much advance the frontiers of knowledge. Furthermore, as the French economist Thomas Piketty pointed out in Capital (2014), money now begets money more than at any time in recent history. When wealth accumulates so spectacularly by doing nothing, there is less impetus to invest in genuine innovation.
the new ideal is to render your own products obsolete as fast as possible
During the Golden Quarter, inequality in the world’s economic powerhouses was, remarkably, declining. In the UK, that trend levelled off a few years later, to reach a historic low point in 1977. Is it possible that there could be some relationship between equality and innovation? Here’s a sketch of how that might work.
As success comes to be defined by the amount of money one can generate in the very short term, progress is in turn defined not by making things better, but by rendering them obsolete as rapidly as possible so that the next iteration of phones, cars or operating systems can be sold to a willing market.
In particular, when share prices are almost entirely dependent on growth (as opposed to market share or profit), built-in obsolescence becomes an important driver of ‘innovation’. Half a century ago, makers of telephones, TVs and cars prospered by building products that their buyers knew (or at least believed) would last for many years. No one sells a smartphone on that basis today; the new ideal is to render your own products obsolete as fast as possible. Thus the purpose of the iPhone 6 is not to be better than the iPhone 5, but to make aspirational people buy a new iPhone (and feel better for doing so). In a very unequal society, aspiration becomes a powerful force. This is new, and the paradoxical result is that true innovation, as opposed to its marketing proxy, is stymied. In the 1960s, venture capital was willing to take risks, particularly in the emerging electronic technologies. Now it is more conservative, funding start-ups that offer incremental improvements on what has gone before.
But there is more to it than inequality and the failure of capital.
During the Golden Quarter, we saw a boom in public spending on research and innovation. The taxpayers of Europe, the US and elsewhere replaced the great 19th‑century venture capitalists. And so we find that nearly all the advances of this period came either from tax-funded universities or from popular movements. The first electronic computers came not from the labs of IBM but from the universities of Manchester and Pennsylvania. (Even the 19th-century analytical engine of Charles Babbage was directly funded by the British government.) The early internet came out of the University of California, not Bell or Xerox. Later on, the world wide web arose not from Apple or Microsoft but from CERN, a wholly public institution. In short, the great advances in medicine, materials, aviation and spaceflight were nearly all pump-primed by public investment. But since the 1970s, an assumption has been made that the private sector is the best place to innovate.
The story of the past four decades might seem to cast doubt on that belief. And yet we cannot pin the stagnation of ingenuity on a decline in public funding. Tax spending on research and development has, in general, increased in real and relative terms in most industrialised nations even since the end of the Golden Quarter. There must be another reason why this increased investment is not paying more dividends.
C ould it be that the missing part of the jigsaw is our attitude towards risk? Nothing ventured, nothing gained, as the saying goes. Many of the achievements of the Golden Quarter just wouldn’t be attempted now. The assault on smallpox, spearheaded by a worldwide vaccination campaign, probably killed several thousand people, though it saved tens of millions more. In the 1960s, new medicines were rushed to market. Not all of them worked and a few (thalidomide) had disastrous consequences. But the overall result was a medical boom that brought huge benefits to millions. Today, this is impossible.
The time for a new drug candidate to gain approval in the US rose from less than eight years in the 1960s to nearly 13 years by the 1990s. Many promising new treatments now take 20 years or more to reach the market. In 2011, several medical charities and research institutes in the UK accused EU-driven clinical regulations of ‘stifling medical advances’. It would not be an exaggeration to say that people are dying in the cause of making medicine safer.
Risk-aversion has become a potent weapon in the war against progress on other fronts. In 1992, the Swiss genetic engineer Ingo Potrykus developed a variety of rice in which the grain, rather than the leaves, contain a large concentration of Vitamin A. Deficiency in this vitamin causes blindness and death among hundreds of thousands every year in the developing world. And yet, thanks to a well-funded fear-mongering campaign by anti-GM fundamentalists, the world has not seen the benefits of this invention.
Apollo couldn’t happen today, not because we don’t want to go to the Moon, but because the risk would be unacceptable
In the energy sector, civilian nuclear technology was hobbled by a series of mega-profile ‘disasters’, including Three Mile Island (which killed no one) and Chernobyl (which killed only dozens). These incidents caused a global hiatus into research that could, by now, have given us safe, cheap and low-carbon energy. The climate change crisis, which might kill millions, is one of the prices we are paying for 40 years of risk-aversion.
Apollo almost certainly couldn’t happen today. That’s not because people aren’t interested in going to the Moon any more, but because the risk – calculated at a couple-of-per-cent chance of astronauts dying – would be unacceptable. Boeing took a huge risk when it developed the 747, an extraordinary 1960s machine that went from drawing board to flight in under five years. Its modern equivalent, the Airbus A380 (only slightly larger and slightly slower), first flew in 2005 – 15 years after the project go-ahead. Scientists and technologists were generally celebrated 50 years ago, when people remembered what the world was like before penicillin, vaccination, modern dentistry, affordable cars and TV. Now, we are distrustful and suspicious – we have forgotten just how dreadful the world was pre-Golden Quarter.
we could be in a world where Alzheimer’s was treatable, clean nuclear power had ended the threat of climate change, and cancer was on the back foot
Risk played its part, too, in the massive postwar shift in social attitudes. People, often the young, were prepared to take huge, physical risks to right the wrongs of the pre-war world. The early civil rights and anti-war protestors faced tear gas or worse. In the 1960s, feminists faced social ridicule, media approbation and violent hostility. Now, mirroring the incremental changes seen in technology, social progress all too often finds itself down the blind alleyways of political correctness. Student bodies used to be hotbeds of dissent, even revolution; today’s hyper-conformist youth is more interested in the policing of language and stifling debate when it counters the prevailing wisdom. Forty years ago a burgeoning media allowed dissent to flower. Today’s very different social media seems, despite democratic appearances, to be enforcing a climate of timidity and encouraging groupthink.
D oes any of this really matter? So what if the white heat of technological progress is cooling off a bit? The world is, in general, far safer, healthier, wealthier and nicer than it has ever been. The recent past was grim; the distant past disgusting. As Steven Pinker and others have argued, levels of violence in most human societies had been declining since well before the Golden Quarter and have continued to decline since.
We are living longer. Civil rights have become so entrenched that gay marriage is being legalised across the world and any old-style racist thinking is met with widespread revulsion. The world is better in 2014 than it was in 1971.
And yes, we have seen some impressive technological advances. The modern internet is a wonder, more impressive in many ways than Apollo. We might have lost Concorde but you can fly across the Atlantic for a couple of days’ wages – remarkable. Sci-fi visions of the future often had improbable spacecraft and flying cars but, even in Blade Runner ’s Los Angeles of 2019, Rick Deckard had to use a payphone to call Rachael.
But it could have been so much better. If the pace of change had continued, we could be living in a world where Alzheimer’s was treatable, where clean nuclear power had ended the threat of climate change, where the brilliance of genetics was used to bring the benefits of cheap and healthy food to the bottom billion, and where cancer really was on the back foot. Forget colonies on the Moon; if the Golden Quarter had become the Golden Century, the battery in your magic smartphone might even last more than a day."
"How many dimensions are there, and what do they do to reality? | Aeon Essays",,https://aeon.co/essays/how-many-dimensions-are-there-and-what-do-they-do-to-reality,"Writing away at my desk, I reach my hand up to turn on a lamp, and down to open a drawer to take out a pen. Extending my arm forward , I brush my fingers against a small, strange figurine given to me by my sister as a good-luck charm, while reaching behind I can pat the black cat snuggling into my back. Right leads to the research notes for my article, left to my pile of ‘must-do’ items (bills and correspondence). Up, down, forward, back, right, left: I pilot myself in a personal cosmos of three-dimensional space, the axes of this world invisibly pressed upon me by the rectilinear structure of my office, defined, like most Western architecture, by three conjoining right angles.
Our architecture, our education and our dictionaries tell us that space is three-dimensional. The OED defines it as ‘a continuous area or expanse which is free, available or unoccupied … The dimensions of height, depth and width, within which all things exist and move.’ In the 18th century, Immanuel Kant argued that three-dimensional Euclidean space is an a priori necessity and, saturated as we are now in computer-generated imagery and video games, we are constantly subjected to representations of a seemingly axiomatic Cartesian grid. From the perspective of the 21st century, this seems almost self-evident.
Yet the notion that we inhabit a space with any mathematical structure is a radical innovation of Western culture, necessitating an overthrow of long-held beliefs about the nature of reality. Although the birth of modern science is often discussed as a transition to a mechanistic account of nature, arguably more important – and certainly more enduring – is the transformation it entrained in our conception of space as a geometrical construct.
Over the past century, the quest to describe the geometry of space has become a major project in theoretical physics, with experts from Albert Einstein onwards attempting to explain all the fundamental forces of nature as byproducts of the shape of space itself. While on the local level we are trained to think of space as having three dimensions, general relativity paints a picture of a four-dimensional universe, and string theory says it has 10 dimensions – or 11 if you take an extended version known as M-Theory. There are variations of the theory in 26 dimensions, and recently pure mathematicians have been electrified by a version describing spaces of 24 dimensions. But what are these ‘dimensions’? And what does it mean to talk about a 10-dimensional space of being?
In order to come to the modern mathematical mode of thinking about space, one first has to conceive of it as some kind of arena that matter might occupy. At the very least, ‘space’ has to be thought of as something extended . Obvious though this might seem to us, such an idea was anathema to Aristotle, whose concepts about the physical world dominated Western thinking in late antiquity and the Middle Ages.
Strictly speaking, Aristotelian physics didn’t include a theory of space , only a concept of place . Think of a cup sitting on a table. For Aristotle, the cup is surrounded by air, itself a substance. In his world picture, there is no such thing as empty space, there are only boundaries between one kind of substance, the cup, and another, the air. Or the table. For Aristotle, ‘space’ (if you want to call it that), was merely the infinitesimally thin boundary between the cup and what surrounds it. Without extension, space wasn’t something anything else could be in .
Centuries before Aristotle, Leucippus and Democritus had posited a theory of reality that invoked an inherently spatialised way of seeing – an ‘atomistic’ vision, whereby the material world is composed of minuscule particles (or atoms ) moving through a void. But Aristotle rejected atomism, claiming that the very concept of a void was logically incoherent. By definition, he said, ‘nothing’ cannot be . Overcoming Aristotle’s objection to the void, and thus to the concept of extended space, would be a project of centuries. Not until Galileo and Descartes made extended space one of the cornerstones of modern physics in the early 17th century does this innovative vision come into its own. For both thinkers, as the American philosopher Edwin Burtt put it in 1924, ‘physical space was assumed to be identical with the realm of geometry’ – that is, the three-dimensional Euclidean geometry we are now taught in school.
L ong before physicists embraced the Euclidean vision, painters had been pioneering a geometrical conception of space, and it is to them that we owe this remarkable leap in our conceptual framework. During the late Middle Ages, under a newly emerging influence deriving from Plato and Pythagoras, Aristotle’s prime intellectual rivals, a view began to percolate in Europe that God had created the world according to the laws of Euclidean geometry. Hence, if artists wished to portray it truly, they should emulate the Creator in their representational strategies. From the 14th to the 16th centuries, artists such as Giotto, Paolo Uccello and Piero della Francesca developed the techniques of what came to be known as perspective – a style originally termed ‘geometric figuring’.­ By consciously exploring geometric principles, these painters gradually learned how to construct images of objects in three-dimensional space. In the process, they reprogrammed European minds to see space in a Euclidean fashion.
The historian Samuel Edgerton recounts this remarkable segue into modern science in The Heritage of Giotto’s Geometry (1991), noting how the overthrow of Aristotelian thinking about space was achieved in part as a long, slow byproduct of people standing in front of perspectival paintings and feeling, viscerally, as if they were ‘looking through’ to three-dimensional worlds on the other side of the wall. What is so extraordinary here is that, while philosophers and proto-scientists were cautiously challenging Aristotelian precepts about space, artists cut a radical swathe through this intellectual territory by appealing to the senses. In a very literal fashion, perspectival representation was a form of virtual reality that, like today’s VR games, aimed to give viewers the illusion that they had been transported into geometrically coherent and psychologically convincing other worlds.
The structure of the ‘real’ went from a philosophical and theological question to a geometrical proposition
The illusionary Euclidean space of perspectival representation that gradually imprinted itself on European consciousness was embraced by Descartes and Galileo as the space of the real world. Worth adding here is that Galileo himself was trained in perspective. His ability to represent depth was a critical feature in his groundbreaking drawings of the Moon, which depicted mountains and valleys and implied that the Moon was as solidly material as the Earth.
By adopting the space of perspectival imagery, Galileo could show how objects such as cannonballs moved according to mathematical laws. The space itself was an abstraction – a featureless, inert, untouchable, un-sensable void, whose only knowable property was its Euclidean form. By the end of the 17th century, Isaac Newton had expanded this Galilean vision to encompass the universe at large, which now became a potentially infinite three-dimensional vacuum – a vast, quality-less, emptiness extending forever in all directions. The structure of the ‘real’ had thus been transformed from a philosophical and theological question into a geometrical proposition.
W here painters had used mathematical tools to develop new ways of making images, now, at the dawn of the ‘scientific revolution’, Descartes discovered a way to make images of mathematical relations in and of themselves. In the process, he formalised the concept of a dimension, and injected into our consciousness not only a new way of seeing the world but a new tool for doing science.
Almost everyone today recognises the fruits of Descartes’s genius in the image of the Cartesian plane – a rectangular grid marked with an x and y axis, and a coordinate system .
By definition, the Cartesian plane is a two-dimensional space because we need two coordinates to identify any point within it. Descartes discovered that with this framework he could link geometric shapes and equations. Thus, a circle with a radius of 1 can be described by the equation x 2 + y 2 =1.
A vast array of figures that we can draw on this plane can be described by equations, and such ‘analytic’ or ‘Cartesian’ geometry would soon become the basis for the calculus developed by Newton and G W Leibniz to further physicists’ analysis of motion. One way to understand calculus is as the study of curves; so, for instance, it enables us to formally define where a curve is steepest, or where it reaches a local maximum or minimum. When applied to the study of motion, calculus gives us a way to analyse and predict where, for instance, an object thrown into the air will reach a maximum height, or when a ball rolling down a curved slope will reach a specific speed. Since its invention, calculus has become a vital tool for almost every branch of science.
Considering the previous diagram, it’s easy to see how we can add a third axis. Thus with an x, y and z axis, we can describe the surface of a sphere – as in the skin of a beach ball. Here the equation (for a sphere with a radius of 1 ) becomes: x 2 + y 2 + z 2 = 1
With three axes, we can describe forms in three-dimensional space. And again, every point is uniquely identified by three coordinates: it’s the necessary condition of three-ness that makes the space three -dimensional.
But why stop there? What if I add a fourth dimension? Let’s call it ‘p’. Now I can write an equation for something I claim is a sphere sitting in four-dimensional space: x 2 + y 2 + z 2 + p 2 = 1. I can’t draw this object for you, yet mathematically the addition of another dimension is a legitimate move. ‘Legitimate’ meaning there’s nothing logically inconsistent about doing so – there’s no reason I can’t.
A ‘dimension’ becomes a purely symbolic concept not necessarily linked to the material world at all
And I can keep on going, adding more dimensions. So I define a sphere in five-dimensional space with five coordinate axes (x, y, z, p, q) giving us the equation: x 2 + y 2 + z 2 + p 2 + q 2 = 1. And one in six-dimensions: x 2 + y 2 + z 2 + p 2 + q 2 + r 2 = 1, and so on.
Although I might not be able to visualise higher-dimensional spheres, I can describe them symbolically, and one way of understanding the history of mathematics is as an unfolding realisation about what seemingly sensible things we can transcend. This is what Charles Dodgson, aka Lewis Carroll, was getting at when, in Through the Looking Glass, and What Alice Found There (1871), he had the White Queen assert her ability to believe ‘six impossible things before breakfast’.
Mathematically, I can describe a sphere in any number of dimensions I choose. All I have to do is keep adding new coordinate axes, what mathematicians call ‘degrees of freedom’. Conventionally, they are named x 1 , x 2 , x 3 , x 4 , x 5 , x 6 et cetera . Just as any point on a Cartesian plane can be described by two (x, y) coordinates, so any point in a 17-dimensional space can be described by set of 17 coordinates (x 1 , x 2 , x 3 , x 4 , x 5 , x 6 … x 15 , x 16 , x 17 ). Surfaces like the spheres above, in such multidimensional spaces, are generically known as manifolds .
From the perspective of mathematics, a ‘dimension’ is nothing more than another coordinate axis (another degree of freedom), which ultimately becomes a purely symbolic concept not necessarily linked at all to the material world. In the 1860s, the pioneering logician Augustus De Morgan, whose work influenced Lewis Carroll, summed up the increasingly abstract view of this field by noting that mathematics is purely ‘the science of symbols’, and as such doesn’t have to relate to anything other than itself. Mathematics, in a sense, is logic let loose in the field of the imagination.
U nlike mathematicians, who are at liberty to play in the field of ideas, physics is bound to nature, and at least in principle, is allied with material things. Yet all this raises a liberating possibility, for if mathematics allows for more than three dimensions, and we think mathematics is useful for describing the world, how do we know that physical space is limited to three? Although Galileo, Newton and Kant had taken length, breadth and height to be axiomatic, might there not be more dimensions to our world?
Again, the idea of a universe with more than three dimensions was injected into public consciousness through an artistic medium, in this case literary speculation, most famously in the mathematician Edwin A Abbott’s Flatland (1884). This enchanting social satire tells the story of a humble Square living on a plane, who is one day visited by a three-dimensional being, Lord Sphere, who propels him into the magnificent world of Solids. In this volumetric paradise, Square beholds a three-dimensional version of himself, the Cube, and begins to dream of pushing on to a fourth, fifth and sixth dimension. Why not a hypercube? And a hyper-hypercube, he wonders?
Sadly, back in Flatland, Square is deemed a lunatic, and locked in an insane asylum. One of the virtues of the story, unlike some of the more saccharine animations and adaptations it has inspired, is its recognition of the dangers entailed in flaunting social convention. While Square is arguing for other dimensions of space, he is also making a case for other dimensions of being – he’s a mathematical queer.
In the late 19th and early 20th centuries, a raft of authors (H G Wells, the mathematician and sci-fi writer Charles Hinton, who coined the word ‘tesseract’ for the 4D cube), artists (Salvador Dalí) and mystical thinkers (P D Ouspensky), explored ideas about the fourth dimension and what it might mean for humans to encounter it.
Then in 1905, an unknown physicist named Albert Einstein published a paper describing the real world as a four-dimensional setting. In his ‘special theory of relativity’, time was added to the three classical dimensions of space. In the mathematical formalism of relativity, all four dimensions are bound together, and the term spacetime entered our lexicon. This assemblage was by no means arbitrary. Einstein found that, by going down this path, a powerful mathematical apparatus came into being that transcended Newton’s physics and enabled him to predict the behaviour of electrically charged particles. Only in a 4D model of the world can electromagnetism be fully and accurately described.
Relativity was a great deal more than another literary game, especially once Einstein extended it from the ‘special’ to the ‘general’ theory. Now multidimensional space became imbued with deep physical meaning.
In Newton’s world picture, matter moves through space in time under the influence of natural forces, particularly gravity. Space, time, matter and force are distinct categories of reality. With special relativity, Einstein demonstrated that space and time were unified, thus reducing the fundamental physical categories from four to three: spacetime, matter and force. General relativity takes a further step by enfolding the force of gravity into the structure of spacetime itself. Seen from a 4D perspective, gravity is just an artifact of the shape of space.
To comprehend this remarkable situation, let’s imagine for the moment its two-dimensional analogue. Think of a trampoline, and imagine we draw on its surface a Cartesian grid. Now put a bowling ball onto the grid. Around it, the surface will stretch and warp so some points become further away from each other. We’ve disturbed the inherent measure of distance within the space, making it uneven. General relativity says that this warping is what a heavy object, such as the Sun, does to spacetime, and the aberration from Cartesian perfection of the space itself gives rise to the phenomenon we experience as gravity.
Whereas in Newton’s physics, gravity comes out of nowhere, in Einstein’s it arises naturally from the inherent geometry of a four-dimensional manifold; in places where the manifold stretches most, or deviates most from Cartesian regularity, gravity feels stronger. This is sometimes referred to as ‘rubber-sheet physics’. Here, the vast cosmic force holding planets in orbit around stars, and stars in orbit around galaxies, is nothing more than a side-effect of warped space. Gravity is literally geometry in action.
If moving into four dimensions helps to explain gravity, then might thinking in five dimensions have any scientific advantage? Why not give it a go? a young Polish mathematician named Theodor Kaluza asked in 1919, thinking that if Einstein had absorbed gravity into spacetime, then perhaps a further dimension might similarly account for the force of electromagnetism as an artifact of spacetime’s geometry. So Kaluza added another dimension to Einstein’s equations, and to his delight found that in five dimensions both forces fell out nicely as artifacts of the geometric model.
You’re an ant running on a long, thin hose, without ever being aware of the tiny circle-dimension underfoot
The mathematics fit like magic, but the problem in this case was that the additional dimension didn’t seem to correlate with any particular physical quality. In general relativity, the fourth dimension was time ; in Kaluza’s theory, it wasn’t anything you could point to, see, or feel: it was just there in the mathematics. Even Einstein balked at such an ethereal innovation. What is it? he asked. Where is it?
In 1926, the Swedish physicist Oskar Klein answered this question in a way that reads like something straight out of Wonderland. Imagine, he said, you are an ant living on a long, very thin length of hose. You could run along the hose backward and forward without ever being aware of the tiny circle-dimension under your feet. Only your ant-physicists with their powerful ant-microscopes can see this tiny dimension. According to Klein, every point in our four-dimensional spacetime has a little extra circle of space like this that’s too tiny for us to see. Since it is many orders of magnitude smaller than an atom, it’s no wonder we’ve missed it so far. Only physicists with super-powerful particle accelerators can hope to see down to such a minuscule scale.
Once physicists got over their initial shock, they became enchanted by Klein’s idea, and during the 1940s the theory was elaborated in great mathematical detail and set into a quantum context. Unfortunately, the infinitesimal scale of the new dimension made it impossible to imagine how it could be experimentally verified. Klein calculated that the diameter of the tiny circle was just 10 -30 cm. By comparison, the diameter of a hydrogen atom is 10 -8 cm, so we’re talking about something more than 20 orders of magnitude smaller than the smallest atom. Even today, we’re nowhere close to being able to see such a minute scale. And so the idea faded out of fashion.
Kaluza, however, was not a man easily deterred. He believed in his fifth dimension, and he believed in the power of mathematical theory, so he decided to conduct an experiment of his own. He settled on the subject of swimming. Kaluza could not swim, so he read all he could about the theory of swimming, and when he felt he’d absorbed aquatic exercise in principle, he escorted his family to the seaside and hurled himself into the waves, where lo and behold he could swim. In Kaluza’s mind, the swimming experiment upheld the validity of theory and, though he did not live to see the triumph of his beloved fifth dimension, in the 1960s string theorists resurrected the idea of higher-dimensional space.
B y the 1960s, physicists had discovered two additional forces of nature, both operating at the subatomic scale. Called the weak nuclear force and the strong nuclear force, they are responsible for some types of radioactivity and for holding quarks together to form the protons and neutrons that make up atomic nuclei. In the late 1960s, as physicists began to explore the new subject of string theory (which posits that particles are like minuscule rubber bands vibrating in space), Kaluza’s and Klein’s ideas bubbled back into awareness, and theorists gradually began to wonder if the two subatomic forces could also be described in terms of spacetime geometry.
It turns out that in order to encompass both of these two forces, we have to add another five dimensions to our mathematical description. There’s no a priori reason it should be five; and, again, none of these additional dimensions relates directly to our sensory experience. They are just there in the mathematics. So this gets us to the 10 dimensions of string theory. Here there are the four large-scale dimensions of spacetime (described by general relativity), plus an extra six ‘compact’ dimensions (one for electromagnetism and five for the nuclear forces), all curled up in some fiendishly complex, scrunched-up, geometric structure.
A great deal of effort is being expended by physicists and mathematicians to understand all the possible shapes that this miniature space might take, and which, if any, of the many alternatives is realised in the real world. Technically, these forms are known as Calabi-Yau manifolds, and they can exist in any even number of higher dimensions. Exotic, elaborate creatures, these extraordinary forms constitute an abstract taxonomy in multidimensional space; a 2D slice through them (about the best we can do in visualising what they look like) brings to mind the crystalline structures of viruses; they almost look alive .
A 2D slice through a Calabi-Yau manifold. Courtesy Wikipedia
There are many versions of string-theory equations describing 10-dimensional space, but in the 1990s the mathematician Edward Witten, at the Institute for Advanced Study in Princeton (Einstein’s old haunt), showed that things could be somewhat simplified if we took an 11-dimensional perspective. He called his new theory M-Theory, and enigmatically declined to say what the ‘M’ stood for. Usually it is said to be ‘membrane’, but ‘matrix’, ‘master’, ‘mystery’ and ‘monster’ have also been proposed.
Ours might be just one of many co-existing universes, each a separate 4D bubble in a wider arena of 5D space
So far, we have no evidence for any of these additional dimensions – we are still in the land of swimming physicists dreaming of a miniature landscape we cannot yet access – but string theory has turned out to have powerful implications for mathematics itself. Recently, developments in a version of the theory that has 24 dimensions has shown unexpected interconnections between several major branches of mathematics, which means that, even if string theory doesn’t pan out in physics, it will have proven a richly rewarding source of purely theoretical insight . In mathematics, 24-dimensional space is rather special – magical things happen there, such as the ability to pack spheres together in a particularly elegant way – though it’s unlikely that the real world has 24 dimensions. For the world we love and live in, most string theorists believe that 10 or 11 dimensions will prove sufficient.
There is one final development in string theory that warrants attention. In 1999, Lisa Randall (the first woman to get tenure at Harvard as a theoretical physicist) and Raman Sundrum (an Indian-American particle theorist) proposed that there might be an additional dimension on the cosmological scale, the scale described by general relativity. According to their ‘brane’ theory – ‘brane’ being short for ‘membrane’ – what we normally call our Universe might be embedded in a vastly bigger five-dimensional space, a kind of super-universe. Within this super-space, ours might be just one of a whole array of co-existing universes, each a separate 4D bubble within a wider arena of 5D space.
It is hard to know if we’ll ever be able to confirm Randall and Sundrum’s theory. However analogies have been drawn between this idea and the dawn of modern astronomy. Europeans 500 years ago found it impossible to imagine other physical ‘worlds’ beyond our own, yet now we know that the Universe is populated by billions of other planets orbiting around billions of other stars. Who knows, one day our descendants could find evidence for billions of other universes, each with their own unique spacetime equations.
T he project of understanding the geometrical structure of space is one of the signature achievements of science, but it might be that physicists have reached the end of this road. For it turns out that, in a sense, Aristotle was right – there are indeed logical problems with the notion of extended space. For all the extraordinary successes of relativity, we know that its description of space cannot be the final one because at the quantum level it breaks down. For the past half-century, physicists have been trying without success to unite their understanding of space at the cosmological scale with what they observe at the quantum scale, and increasingly it seems that such a synthesis could require radical new physics.
After Einstein developed general relativity, he spent much of the rest of his life trying to ‘build all of the laws of nature out of the dynamics of space and time, reducing physics to pure geometry’, as Robbert Dijkgraaf, director of the Institute for Advanced Study at Princeton, put it recently. ‘For [Einstein], space-time was the natural “ground-level” in the infinite hierarchy of scientific objects.’ Like Newton’s world picture, Einstein’s makes space the primary grounding of being, the arena in which all things happen. Yet at very tiny scales, where quantum properties dominate, the laws of physics reveal that space, as we are used to thinking about it, might not exist.
A view is emerging among some theoretical physicists that space might in fact be an emergent phenomenon created by something more fundamental, in much the same way that temperature emerges as a macroscopic property resulting from the motion of molecules. As Dijkgraaf put it: ‘The present point of view thinks of space-time not as a starting point, but as an end point, as a natural structure that emerges out of the complexity of quantum information.’
A leading proponent of new ways of thinking about space is the cosmologist Sean Carroll at Caltech, who recently said that classical space isn’t ‘a fundamental part of reality’s architecture’, and argued that we are wrong to assign such special status to its four or 10 or 11 dimensions. Where Dijkgraaf makes an analogy with temperature, Carroll invites us to consider ‘wetness’, an emergent phenomenon of lots of water molecules coming together. No individual water molecule is wet, only when you get a bunch of them together does wetness come into being as a quality. So, he says, space emerges from more basic things at the quantum level.
Carroll writes that, from a quantum perspective, the Universe ‘evolves in a mathematical realm with more than 10 (10^100) dimensions’ – that’s 10 followed by a googol of zeroes, or 10,000 trillion trillion trillion trillion trillion trillion trillion trillion zeroes. It’s hard to conceive of this almost impossibly vast number, which dwarfs into insignificance the number of particles in the known Universe. Yet every one of them is a separate dimension in a mathematical space described by quantum equations; every one a new ‘degree of freedom’ that the Universe has at its disposal.
Even Descartes might have been stunned by where his vision has taken us, and what dazzling complexity has come to be contained in the simple word ‘dimension’.
This Essay was made possible through the support of a grant to Aeon magazine from Templeton Religion Trust. The opinions expressed in this publication are those of the author(s) and do not necessarily reflect the views of Templeton Religion Trust.
Funders to Aeon Magazine are not involved in editorial decision-making, including commissioning or content approval."
Why pregnancy is a biological war between mother and baby | Aeon Essays,,https://aeon.co/essays/why-pregnancy-is-a-biological-war-between-mother-and-baby,"What sight could be more moving than a mother nursing her baby? What better icon could one find for love, intimacy and boundless giving? There’s a reason why the Madonna and Child became one of the world’s great religious symbols.
To see this spirit of maternal generosity carried to its logical extreme, consider Diaea ergandros , a species of Australian spider. All summer long, the mother fattens herself on insects so that when winter comes her little ones may suckle the blood from her leg joints. As they drink, she weakens, until the babies swarm over her, inject her with venom and devour her like any other prey.
You might suppose such ruthlessness to be unheard-of among mammalian children. You would be wrong. It isn’t that our babies are less ruthless than Diaea ergandros , but that our mothers are less generous. The mammal mother works hard to stop her children from taking more than she is willing to give. The children fight back with manipulation, blackmail and violence. Their ferocity is nowhere more evident than in the womb.
This fact sits uncomfortably with some enduring cultural ideas about motherhood. Even today, it is common to hear doctors talking about the uterine lining as the ‘optimal environment’ for nurturing the embryo. But physiology has long cast doubt on this romantic view.
The cells of the human endometrium are tightly aligned, creating a fortress-like wall around the inside of the uterus. That barrier is packed with lethal immune cells. As far back as 1903, researchers observed embryos ‘invading’ and ‘digesting’ their way into the uterine lining. In 1914, R W Johnstone described the implantation zone as ‘the ﬁghting line where the conﬂict between the maternal cells and the invading trophoderm takes place’. It was a battlefield ‘strewn with… the dead on both sides’.
When scientists tried to gestate mice outside the womb, they expected the embryos to wither, deprived of the surface that had evolved to nurture them. To their shock they found instead that – implanted in the brain, testis or eye of a mouse – the embryo went wild. Placental cells rampaged through surrounding tissues, slaughtering everything in their path as they hunted for arteries to sate their thirst for nutrients. It’s no accident that many of the same genes active in embryonic development have been implicated in cancer. Pregnancy is a lot more like war than we might care to admit.
S o if it’s a fight, what started it? The original bone of contention is this: you and your nearest relatives are not genetically identical. In the nature of things, this means that you are in competition. And because you live in the same environment, your closest relations are actually your most immediate rivals.
It was Robert Trivers, in the 1970s, who first dared to explore the sinister implications of this reality in a series of influential papers. The following decade, a part-time graduate student named David Haig was musing over Trivers’s ideas when he realised that the nurturing behaviour of mammal mothers creates a particularly excellent opportunity for exploitation.
It is in your mother’s genetic interests, Haig understood, to provide equally for all her children. But your father might never have another child with her. This makes her other children your direct competitors, and also gives your father’s genes a reason to game the system. His genome would evolve to manipulate your mother into providing more resources for you. In turn, her genes would manoeuvre to provide you with fewer resources. The situation becomes a tug-of-war. Some genes fall silent, while others become more active, counterbalancing them.
Even with the help of modern medicine, pregnancy still kills about 800 women every day worldwide
That insight led Haig to found the theory of genomic imprinting , which explains how certain genes are expressed differently depending on whether they come from your father or your mother. Armed with this theory, we can see how conflicts of genetic interest between parents play out within the genomes of their offspring.
Because both parental genomes drive each other to keep ramping up their production of powerful hormones, should one gene fail, the result can be disastrous for both mother and infant. Normal development can proceed only as long as both parental genotypes are correctly balanced against one another. Just as in a tug-of-war, if one party drops its end, both fall over. This is one reason why mammals cannot reproduce asexually, and why cloning them is so difficult: mammalian development requires the intricate co-ordination of paternal and maternal genomes. A single misstep can ruin everything.
Diaea ergandros, the ultimate mother, doesn’t have to worry about this, of course. She will never have more than one brood, so there is no need for her to restrain her offspring. But most mammal mothers breed more than once, and often with different males. This fact alone ensures that the paternal and maternal genomes work against one another. You can see the tragic consequences of this hidden war throughout the class Mammalia. Yet there is one species where it ascends to really mind-boggling heights of bloodiness.
Yours.
F or most mammals, despite the underlying conflict, life goes on almost as normal during pregnancy. They flee from predators, capture prey, build homes and defend territories – all while gestating. Even birth is pretty safe: they might grimace or sweat a bit during labour, but that’s usually the worst of it. There are exceptions. Hyena mothers, for example, give birth through an impractical penis-like structure, and about 18 per cent of them die during their first delivery. But even for them, pregnancy itself is rarely perilous.
If we look at primates, however, it’s a different story. Primate embryos can sometimes implant in the Fallopian tube instead of the womb. When that happens, they tunnel ferociously towards the richest nutrient source they can find; the result is often a bloodbath. And among the great apes, things look even dicier. Here we start to see perhaps the most sinister complication of pregnancy: preeclampsia, a mysterious condition characterised by high blood pressure and protein discharge in the urine. Preeclampsia is responsible for around 12 per cent of human maternal deaths worldwide. But it’s very much just the start of our problems.
The mother is a despot: she provides only what she chooses
A list of the reproductive ills that afflict our species might start with placental abruption, hyperemesis gravidarum, gestational diabetes, cholestasis and miscarriage, and carry on from there. In all, about 15 per cent of women suffer life-threatening complications during each pregnancy. Without medical assistance, more than 40 per cent of hunter-gatherer women never reach menopause. Even with the help of modern medicine, pregnancy still kills about 800 women every day worldwide.
So, we have a bit of a mystery here. The basic genetic conflict that makes the womb such a battle zone crops up across innumerable species: all it takes for war to break out is for mothers to have multiple offspring by different fathers. But this is quite a common reproductive arrangement in nature, and as we saw, it doesn’t cause other mammals so many problems. How did we humans get so unlucky? And does it have anything to do with our other extraordinary feature – our unparalleled brain development?
I n most mammals, the mother’s blood supply remains safely isolated from the foetus. It passes its nutrients to the foetus through a filter, which the mother controls. The mother is a despot: she provides only what she chooses, which makes her largely invulnerable to paternal manipulation during pregnancy.
In primates and mice, it’s a different story. Cells from the invading placenta digest their way through the endometrial surface, puncturing the mother’s arteries, swarming inside and remodelling them to suit the foetus. Outside of pregnancy, these arteries are tiny, twisty things spiralling through depths of the uterine wall. The invading placental cells paralyse the vessels so they cannot contract, then pump them full of growth hormones, widening them tenfold to capture more maternal blood. These foetal cells are so invasive that colonies of them often persist in the mother for the rest of her life, having migrated to her liver, brain and other organs. There’s something they rarely tell you about motherhood: it turns women into genetic chimeras.
Perhaps this enormous blood supply explains why primates have brains five to ten times larger than the average mammal. Metabolically speaking, brains are extremely expensive organs, and most of their growth occurs before birth. How else is the fetus to fund such extravagance?
Is this unfettered access to maternal blood the key to the extraordinary brain development we see in young primates?
Given the invasive nature of pregnancy, it’s perhaps not surprising that the primate womb has evolved to be wary of committing to it. Mammals whose placentae don’t breach the walls of the womb can simply abort or reabsorb unwanted foetuses at any stage of pregnancy. For primates, any such manoeuvre runs the risk of haemorrhage, as the placenta rips away from the mother’s enlarged and paralysed arterial system. And that, in a sentence, is why miscarriages are so dangerous.
It’s also why primates make every effort to test their embryos before they allow them to implant. The embryo is walled out by the tight-packed cells of the endometrium, while an intimate hormonal dialogue takes place. This conversation is, in Haig’s words, a ‘job interview’. Should the embryo fail to convince its mother that it is a perfectly normal, healthy individual, it will be summarily expelled.
How does an embryo convince its mother that it is healthy? By honestly displaying its vigour and lust for life, which is to say, by striving with all its strength to implant. And how does the mother test the embryo? By making the embryo’s task incredibly difficult. Just as the placenta has evolved to be aggressive and invasive, the endometrium has evolved to be tough and hostile. For humans, the result is that half of all human pregnancies fail, most at the implantation stage, so early that the mother may not even realise she was pregnant.
Embryonic development becomes a trial of strength. And this leads to another peculiarity of the primate reproductive system – menstruation. We have it for the simple reason that it’s not such an easy matter to dispose of an embryo that is battling to survive. The tissues of the endometrium are partially insulated from the mother’s bloodstream, protecting her circulatory system from invasion by a placenta she has not yet decided to accept. But that means her own hormonal signals can struggle to be heard inside the womb. So, rather than risk corruption of the endometrial tissue and ongoing conflict with an embryo, what does the mother do? She just sloughs off the whole endometrium after each ovulation. This way, even the most aggressive embryo has to have her agreement before it can get comfortable. In the absence of continual, active hormonal signalling from a healthy embryo, the entire system auto-destructs. Around 30 per cent of pregnancies end this way.
I said that the mother struggles to pass hormonal signals into the womb. The thing is, once the embryo implants, it gets full access to her tissues. This asymmetry means two things. Firstly, the mother can no longer control the nutrient supply she offers the foetus – not without reducing the nutrient supply to her own tissues. Is this unfettered access to maternal blood the key to the extraordinary brain development we see in young primates? Fascinatingly, the intensity of the invasion does seem to correlate with brain development. Great apes, the largest-brained primates, seem to experience deeper and more extensive invasion of the maternal arteries than other primates. In humans – the largest-brained ape of all – placental cells invade the maternal bloodstream earlier even than in other great apes, allowing the foetus unprecedented access to oxygen and nutrients during early development. This would be one of evolution’s little ironies: after all, if it wasn’t for the cognitive and social capacities granted by our big brains, many more of us would die from the rigours of our brutal reproductive cycle. One can imagine how the two traits might have arisen in tandem. But the connection remains speculative. Uteri rarely fossilise, so the details of placental evolution are lost to us.
The second major consequence of the foetus’s direct access to maternal nutrients is that the foetus can also release its own hormones into the mother’s bloodstream, and thus manipulate her. And so it does. The mother counters with manipulations of her own, of course. But there is a strong imbalance: while the foetus freely injects its products into the mother’s blood, the mother is granted no such access to foetal circulation. She is walled out by placental membranes, and so her responses are limited to defensively regulating hormones within her own body.
As the pregnancy continues, the foetus escalates its hormone production, sending signals designed to increase the mother’s blood sugar and blood pressure and thus its own resource supply. In particular, the foetus increases its production of a hormone that prompts the mother’s brain to release cortisol, the primary stress hormone. Cortisol suppresses her immune system, stopping it from attacking the foetus. More importantly, it increases her blood pressure, so that more blood pumps past the placenta and consequently more nutrients are available to the foetus.
The mother doesn’t take this foetal manipulation lying down. In fact, she pre-emptively reduces her blood sugar levels. She also releases a protein that binds to the foetal hormone, rendering it ineffective. So then the foetus further increases its production. By eight months, the foetus spends an estimated 25 per cent of its daily protein intake on manufacturing these hormonal messages to its mother. And how does the mother reply? She increases her own hormonal production, countering the embryo’s hormones with her own that decrease her blood pressure and sugar. Through all this manipulation and mutual reprisal, most of the time the foetus ultimately gets about the right amount of blood, and about the right amount of sugar, allowing it to grow fat and healthy in time for birth. This is the living instantiation of Haig’s tug-of-war between maternal and paternal genomes. As long as each side holds its end up, nobody gets hurt.
B ut what happens when things go wrong? Since the turn of the millennium, the Human Genome Project has provided a wealth of data, most of which remains incomprehensible to us. Yet by looking for signs of genomic imprinting – that is, genes that are expressed differently depending on whether they are inherited from the father or the mother – researchers have been able to pin down the genetic causes of numerous diseases of pregnancy and childhoods. Genomic imprinting, and the maternal-fetal battle behind it, have been shown to account for gestational diabetes, Prader-Willi Syndrome, Angelman Syndrome, childhood obesity and several cancers. Researchers suspect that it may also underlie devastating psychiatric conditions such as schizophrenia, bipolar disorder and autism. In 2000, Ian Morison and colleagues compiled a database of more than 40 imprinted genes. That number had doubled by 2005; by 2010, it had nearly doubled again. Identifying genetic mechanisms does not in itself provide a cure for these complex diseases, but it is a vital step towards one.
Preeclampsia, perhaps the most mysterious disease of pregnancy, turns out to be a particularly good example of the way in which the evolutionary, genetic and medical pictures are all lining up. More than two decades ago, Haig suggested that it resulted from a breakdown in communication between mother and foetus. In 1998, Jenny Graves expanded on this idea, suggesting that it could be explained by failure of imprinting on a maternally inherited gene. It’s only in the past few years, however, that we’ve pieced together how this process occurs.
This story shows how, with the help of evolutionary theory, we are at last starting to make sense of the grim, tangled mess that is human development
So, picture the foetus tunnelling towards the mother’s bloodstream. All else being equal, the arterial expansion of early pregnancy would cause the mother’s blood pressure to drop. Foetal hormones counter this effect by raising her blood pressure.
Several hormones are involved when the maternal arteries expand during early pregnancy. If these chemicals get out of balance, those arteries can fail to expand, starving the foetus of oxygen. If that happens, the foetus sometimes resorts to more extreme measures. It releases toxins that damage and constrict the mother’s blood vessels, driving up blood pressure. This risks kidney and liver damage, if not stroke: the symptoms of preeclampsia.
In 2009, researchers showed that the maternally inherited gene H19 is strongly associated with the disease. This was just as Jenny Graves predicted. H19 is known to be crucial to early growth of the placenta. Changes in several other maternally inherited genes, and some paternally inherited ones, are also suspected of being involved. There’s a lot that has yet to be discovered, but this story shows how, with the help of evolutionary theory, we are at last starting to make sense of the grim, tangled mess that is human development.
Our huge brains and our traumatic gestation seem intimately connected; at the very least, they are both extraordinary features of humanity. Did the ancients guess this connection when they crafted their mythologies? Perhaps the story of Eve, cursed with the sorrows of pregnancy when she ‘ate the fruit of the tree of knowledge’, was once just an intuitive explanation for the cruelty that nature saw fit to visit on our species. Be that as it may, if we want to reduce the danger and suffering of pregnancy, the only way out is through. We need more knowledge – lots of it."
Getting down and medieval: the sex lives of the Middle Ages | Aeon Essays,,https://aeon.co/essays/getting-down-and-medieval-the-sex-lives-of-the-middle-ages,"In the popular imagination, the history of sex is a straightforward one. For centuries, the people of the Christian West lived in a state of sexual repression, straitjacketed by an overwhelming fear of sin, combined with a complete lack of knowledge about their own bodies. Those who fell short of the high moral standards that church, state and society demanded of them faced ostracism and punishment. Then in the mid-20th century things changed forever when, in Philip Larkin’s oft-quoted words, ‘Sexual intercourse began in 1963 … between the end of the Chatterley ban and the Beatles’ first LP.’
In reality, the history of human sexuality is far more interesting and wild. Many prevailing presumptions about the sex lives of our medieval ancestors are rooted in the erroneous belief that they lived in an unsophisticated age of religious fanaticism and medical ignorance. While Christian ideals indeed influenced medieval attitudes to sex, they were rather more complex than contemporary prejudices suggest. Christian beliefs interacted with medieval medical theories to help shape some surprising and sophisticated ideas about sex, and a wide variety of different sexual practices, long before the sexual revolution.
The case of the French cleric Arnaud de Verniolle illustrates the sophistication of medieval sexuality. One day in the early 14th century, when Arnaud was a student, he had sex with a prostitute. Several years later, he confessed this lapse to the Inquisition, explaining that:
Arnaud’s tale is not unusual. Many medieval men found themselves with undesirable symptoms after a brothel visit, and attributed their plight to their sexual behaviour. Among the various medical miracles attributed to St Thomas Becket, for example, was the cure of Odo de Beaumont, who became leprous immediately after a late-12th-century visit to a prostitute. Much has been made of the medieval tendency to interpret disease as a product of sexual sin. Too much. In fact, the medieval tendency to see disease as sexual sin was not solely based on moral judgments – there were also strong medical elements.
C oncerns about the sexual transmission of disease via prostitutes were often addressed in an entirely rational manner. Sometimes, for example, local authorities took preventative action: a set of regulations from 15th-century Southwark banished women with a ‘burning sickness’ (probably gonorrhoea) from the local stews (brothels). Moreover, the concerns of the people of Southwark were rooted in medical theory. The Prose Salernitan Questions , a 13th-century medical text, explained how a woman might be left unharmed after having intercourse with a leper, but her next lover would contract the disease: the coldness of the female complexion meant that the leper’s semen would remain in the woman’s uterus, where it would turn to putrid vapour. When the penis of the healthy man came into contact with this vapour, the heat of his body would ensure that it was absorbed through his open pores. Sores would soon appear on his genitals, before spreading around his body. Placed within the context of contemporary medical ideas, Arnaud’s fears over his tryst with a prostitute made perfect sense.
Fortunately for Arnaud, and many others, it was often possible to treat sexually transmitted leprosy. The 14th-century English physician John of Gaddesden suggested several protective measures that a man should take after having sexual relations with a woman he believed to be leprous. He should cleanse his penis as soon as possible, either with his own urine or with vinegar and water. Then he should undergo intensive bleeding by a phlebotomist, followed by a three-month course of purgation, ointments and medication.
He treated this unfortunate individual by cutting away the dead flesh with a blade, then applying quicklime
If such prophylactic measures failed, then the patient might need one of the many remedies for swollen, itchy or pustulent genitals found in medical treatises and recipe collections. The 12th-century medical compendium Trotula noted that there are men ‘who suffer swelling of the virile member, having there and under the prepuce many holes, and they suffer lesions’. Such a man should use a poultice to reduce the swelling. Then, ‘we wash the ulcerous or wounded neck of the prepuce with warm water, and sprinkle on it powder of Greek pitch and dry rot of wood or of worms and rose and root of mullein and bilberry’.
Such preparations were undoubtedly unpleasant, but the surgical remedies recommended by the 14th-century English surgeon John of Arderne were downright brutal. In one of his recorded cases, ‘the man’s yard began to swell after coitus, due to the falling of his own sperm, whereof he suffered great grievousness of burning and aching as men do when they are so hurt’. Arderne treated this unfortunate individual by cutting away the dead flesh with a blade, then applying quicklime – a process that must have been extremely painful, but apparently produced a cure.
Both Trotula and Arderne describe symptoms that suggest a sexually transmitted disease, and Arderne directly links sexual intercourse with his patient’s symptoms. However, neither author explicitly identifies their remedies as cures for diseases transmitted by sexual contact. The man with the swollen yard might well have been viewed by his contemporaries as a victim not of infection, but of overindulgence.
M edieval physicians saw too much sex as a real medical concern. Conventional wisdom held that several noblemen died of sexual excess. John of Gaunt, the 14th-century first duke of Lancaster, allegedly ‘died of putrefaction of his genitals and body, caused by the frequenting of women, for he was a great fornicator’. Today, his symptoms would suggest venereal disease, but his contemporaries would probably have seen parallels with the case of Ralph, count of Vermandois. This 12th-century French nobleman had recently married his third wife when he fell seriously ill. During his convalescence, he was advised by his physician that he must abstain from intercourse, but disregarded this warning. When the doctor detected from Ralph’s urine that he had done so, he advised him to set his house in order, for he would be dead within three days – a prognosis that proved to be accurate.
According to medieval understandings of the body, based on the system of the four humours (blood, phlegm, black bile and yellow bile), these men’s behaviour presented problems. The humours system derived from the idea that health was based on an equilibrium of the humours, and illness the product of imbalance. Humours were balanced, and good health maintained, through the expulsion of various bodily fluids, including semen. Regular sexual intercourse was thus part of a healthy life for most men, but moderation was key. Too much sex would leave the body depleted; in the most serious cases it could have fatal consequences, as Count Ralph found to his cost.
On the other hand, medieval medical authority held that too little sex presented a medical problem: celibacy was potentially detrimental to health, particularly for young men. Long-term celibacy meant the retention of excess semen, which would affect the heart, which in turn could damage other parts of the body. The celibate might experience symptoms including headaches, anxiety, weight loss and, in the most serious cases, death. Although celibacy was highly valued as a spiritual virtue in medieval society, in medical terms the celibate was as much at risk as the debauchee.
King Louis VIII of France, for example, insisted on remaining faithful to his wife while fighting in the Albigensian Crusade of 1209-29. Conventional opinion attributed his death to the resulting celibacy, making him the most famous victim of death by celibacy. According to the 12th-century Norman poet Ambroise, abstinence claimed many victims:
For most crusaders, sexual abstinence was (at most) a temporary inconvenience, to be endured only until they returned home and were reunited with their wives. But for medieval Europe’s many priests, celibacy was a lifelong state, and this could leave them facing a difficult choice. Thomas Becket’s doctor urged him to give up celibacy for the sake of his health, telling him that the celibate life was incompatible with his age and complexion, but the saint disregarded the physician’s advice. Becket lived for many years after this (and ultimately died a martyr at the hands of an assassin), but other bishops were less fortunate. An unnamed 12th-century archdeacon of Louvain, having struggled to remain celibate for a long time, was promoted against his will to the bishopric of the same city. For a month, he abstained from all sexual activity, but soon his genitals swelled up and he became seriously ill. His family and friends urged him to secretly ‘take a woman to himself’, but he was determined to resist temptation. Within days, he was dead.
Routine bleedings were used to balance the monks’ humours, minimising involuntary emissions of semen
Non-saintly celibates who faced the challenge of celibacy tended to favour the obvious cure. Maurice, an 11th-century bishop of London, was rumoured to have been advised by his doctors to ‘look to the health of his body by the emission of humours’, and to have prolonged his life by breaking his vow of celibacy. Others, hoping never to face this predicament, adopted behaviours (informed by medical theory) believed to protect the health of a celibate man by promoting alternative forms of excretion.
Humours-based medical theory held that all bodily fluids were processed forms of blood, and that their common origins rendered them interchangeable. Consequently, regular phlebotomy was deemed necessary for celibate men: routine bleedings were widely used in medieval monasteries to balance the monks’ humours and thus minimise the risk of involuntary emissions of semen. Weeping (for example, the lachrymose prayers favoured by pious individuals) could also serve as an alternative to sexual intercourse, with the blood that would have been converted into semen instead producing tears . Exercise and bathing, both of which produced sweat, were also useful for those who wished to practise long-term abstinence.
As well as taking measures to encourage the excretion of superfluities, a celibate man needed to be careful about what he put into his body. Diet thus directly related to sexual health. The problem was threefold. Firstly, the proximity of the genitals to the stomach meant that the former would be warmed by the food or wine contained in the latter, providing the heat that defined the male body, and was necessary for the production of semen. Secondly, semen was thought to be the product of completely digested food, with nourishing foods such as meat and eggs especially conducive to its production. Finally, certain windy foodstuffs (including beans) produced an excess of flatulence, which in turn produced an erection. Taken together, these factors made overindulgence at the table a real problem for priests. Numerous medieval writers told tales of monks who ate too well, and consequently experienced a violent desire for sex, along with almost continuous emissions of semen.
On the other hand, knowledge is power, and religious men could use fasting as a practical strategy to protect themselves from the health risks posed by clerical celibacy. A man who wished to avoid sex while maintaining his physical wellbeing would be well advised to fast regularly, and to eat a diet that consisted primarily of the cold foods and drinks that ‘impede, repress and thicken semen and extinguish lust’. Salted fish, vegetables in vinegar, and cold water were thought to be particularly suitable foods for monks.
In addition, some medical writers recommended anaphrodisiacs for men who wished to avoid sexual intercourse. The 11th-century physician Constantine the African recommended rue, a strong and bitter tea made from an evergreen shrub. Drinking rue, he wrote, would ‘dry out the sperm and kill the desire for intercourse’. Two centuries later, Peter of Spain (the only practising physician ever to become pope) was also recommending rue; alternatively, one could drink juice of water-lilies for 40 days. Maino de Maineri (a 14th-century physician whose employers included two bishops) included advice on anaphrodisiacs in his Regimen Sanitatis : a man who wished to repress lust should make use of ‘things which are cold’, such as lentils and lentil water cooled with cauliflower seeds, and water-lily and lettuce seeds and lettuce water, strongly vinegary, and also seeds of purslane. Being both celibate and healthy was difficult but, for those willing to live a life in which one’s chief pleasures were prayer and vegetable water, it was not impossible.
A lthough the most famous cases of death by celibacy relate to male clerics, women were, in their own way, equally vulnerable to this medical problem. According to contemporary medical theory, both sexes produced seed that was necessary for conception – and just like semen, the female seed needed to be expelled from the body during regular sexual intercourse. In a woman who was not sexually active, the seed would be retained within her body; as it built up, it would cause suffocation of the womb. The symptoms of this condition included fainting and shortness of breath, and in the most serious cases it could be fatal. For women, as for men, the best way to avoid death by celibacy was to get married and have regular, Church-sanctioned intercourse with one’s spouse. If this was not possible, there were a range of useful remedies, including restricted diets and vinegar suppositories. Some physicians, however, recommended a rather startling alternative: masturbation.
Unsurprisingly, the medieval Church took a rather dim view of this practice: most medieval penitentials (handbooks for confessors) identified masturbation as a sin, and imposed heavy penances for it – typically around 30 days of fasting, but sometimes as much as two years. On the other hand, masturbation was usually placed towards the bottom of the hierarchy of sexual sins, and confessors were permitted to make some allowance for those (including unmarried youths) who lacked another outlet for their desires. This caveat reflects the Church’s awareness of contemporary medical teachings: it was impossible to ignore the fact that medical authorities from Galen onwards had recommended masturbation as a form of preventative medicine for both men and women.
Later medieval physicians were rarely as explicit as Galen and other ancients. Late medieval medical books rarely mentioned male masturbation. For women lacking regular sexual relations, they offered a variety of treatments, including, stimulation of the genitals (either by the patient or by a medical professional). Such treatments were particularly suitable for women who were suffering from suffocation of the womb. If such a woman could not marry (for example, because she was a nun), and if her life was in genuine danger, then genital massage might be the only solution, and could even be performed without sin. The 14th-century English physician John of Gaddesden thought that such a woman should try to cure her condition through exercise, foreign travel and medication. But ‘if she has a fainting fit, the midwife should insert a finger covered with oil of lily, laurel or spikenard into her womb, and move it vigorously about’.
Having ‘desired’ a woman 70 times before matins, the monk died. His autopsy revealed a brain shrunk to the size of a pomegranate, and eyes that had been destroyed
Other medical writers, including clerics, echoed Gaddesden’s teachings. The 13th-century Dominican friar Albertus Magnus wrote extensively about human health. He argued that certain women needed to ‘use their fingers or other instruments until their channels are opened and by the heat of the friction and coition the humour comes out, and with it the heat’. Albertus thought that such a course of action would not only solve women’s health problems, but also lessen their desire for sexual intercourse, since ‘their groins are cooled off and they are made more chaste’. The view that female masturbation could prevent less socially acceptable forms of female sexual activity helped some medieval medical experts countenance it.
As with sexual intercourse, masturbation was to be enjoyed in moderation. Albertus told of a lustful monk who came to an unfortunate end: having ‘desired’ a beautiful woman 70 times before matins, the monk died. His autopsy revealed that his brain had shrunk to the size of a pomegranate, while his eyes had been destroyed. The manner of his death reflected one of the terrible realities of medieval life: sin was just one of many dangers associated with sex.
Long before syphilis arrived in Europe in the late 15th century, sexual health merited widespread concern. Prostitutes and their clients were thought to be at risk of contracting leprosy, a fearsome possibility for Arnaud and many others. But contagious disease was not the only problem. Arnaud vowed that he would never sleep with another woman, but he didn’t simply give up sex. Instead, he admitted that ‘in order to keep this oath, I began to abuse little boys’.
This solution was as distasteful then as now. It also reflected the widespread belief that sexual activity of some kind was medically necessary for most adults, and echoed fears that clerical celibacy would force priests into the same vice. When it came to sex, medieval people faced a dilemma: how to preserve the vital bodily equilibrium without exposing themselves to either disease or sin? The decline of humoural medicine and changes in religious belief have removed some of the anxieties faced by Arnaud and medieval people. But not everything has changed. Discourses about sex still revolve around conflicting demands of health, social pressures and personal inclination. As it was in the Middle Ages, sex in the 21st century remains both a pleasure and a problem.
If you enjoyed this Essay, Katherine Harvey went on to have success with her book The Fires of Lust: Sex in the Middle Ages . Of the connection between her book and this Essay, Katherine said: ‘“The Salacious Middle Ages” was my first serious foray into the subject of medieval sex, which I’d become interested in when researching medieval bishops who supposedly died of celibacy. But it grew into an entire book, “The Fires of Lust” (2021), which was commissioned by Reaktion after my editor there read this Aeon essay.’ You can also read more from Katherine Harvey on Aeon here ."
The terror and the bliss of sleep paralysis | Aeon Essays,,https://aeon.co/essays/the-terror-and-the-bliss-of-sleep-paralysis,"Here I am, lying in bed. If you walk in now, you’ll think I’m sleeping. But I see you. Although my eyelids look shut, they are fluttering slightly. They are the only parts of me that I can move. I am fully conscious but I cannot shout out to you: my body is completely frozen.
Everybody is paralysed during rapid eye movement (REM) sleep, the stage of sleep where dreaming occurs. If we weren’t paralysed, we would act out our dreams, endangering ourselves and our sleeping partners. But sometimes, especially when sleep patterns are disrupted or we get exhausted, things go awry: REM extends into waking consciousness, our bodies become immobile and our alert brains fuse with the imagery of dreams. The phenomenon of waking up during REM, completely unable to move, is called sleep paralysis.
The experience can be terrifying. Trapped in your paralysed body, you might sense the presence of a malevolent intruder in the room or a pressure on your chest, squeezing the breath out of your lungs. Hallucinations can jangle the senses: there are ominous voices, supernatural entities, strange lights. You feel as if you are being touched or dragged, bed covers seem to be snatched from you, and you are helpless to grab them back.
I have experienced the frightening imagery of sleep paralysis since childhood, but only later did I understand that my dark journey was not unique – I share it with at least 6 per cent of people worldwide, and it has been reported for thousands of years as encounters with sexual demons, beasts, and ghosts. These reports differ by culture – but the texture and the biology is the same. From Newfoundland come tales of the Old Hag, a hideous witch who pins down sleepers by sitting on their chests. Japanese folklore gives us kanashibari , the fate of the unfortunate or cursed who have been magically tied up in their sleep by evil spirits. In Old Norse, the Mara is a malevolent spirit who straddles the body of the sleeper as if riding a horse, then tries to strangle them; mara is the origin of the English word ‘nightmare’. UFO abduction stories and alien encounters likely emerge from sleep paralysis, too.
Ever since I was a teen, I have seen shadow figures in the corner of my bedroom, and awoken to find strange entities – grinning vampires or silent watchers – by my bed. I’ve felt my hand grasped, my chest crushed by the weight of a strange beast; my body twisting and spinning in space. I’ve heard buzzing, ringing, whooshing and nasty names whispered in my ear. If the radio or TV were on, I could hear the programmes clearly and, after paralysis released me, I could report them back. If someone walked into the room, or the doorbell rang, or a dog barked, or (as happened on one occasion) there was a power outage, I was fully aware. I tried to shout out, to pull at my eyelids, desperate to snap out of it, but I could not budge.
With this ghoulish treasure trove to draw upon, sleep paralysis has naturally spawned some very scary stories and films. But as a writer and filmmaker as well as a long-time percipient, I have another story to tell. Beyond the sheer terror, sleep paralysis can open a doorway to thrilling, extraordinary, and quite enjoyable altered states. One is the lucid dream state, in which you can consciously manipulate your dreams, traversing incredible landscapes and interacting with creatures conjured in your mind. Another is the out-of-body experience – the waking sensation of separating from your physical body and floating, spinning and flying through your surroundings; often, you’d look down to see yourself lying below.
T he biological underpinnings of sleep paralysis have become less mysterious in recent years. The psychologist Kazuhiko Fukuda at Edogawa University in Japan explains the likely involvement of the amygdala, a brain region that signals fear from threats in the environment and triggers our primal ‘fight or flight’ reactions. Waking up paralysed constitutes an environmental threat, yet we cannot react. The amygdala is in hyperdrive, and REM physiology has invaded our consciousness. We are left stuck in a state of overwhelming terror, leaving us dreaming awake and set upon by our deepest fears.
In 2012, University of Toronto neuroscientists Patricia Brooks and John Peever reported the physiological process behind the altered state: GABA A and GABA B , the receptors that regulate the body’s muscle tone, combine with glycine, an amino acid, to switch off motor-neurone activity in our voluntary muscles during REM sleep. Normally, they switch our motor-neurone activity back on before we wake up. But, sometimes, we wake up during REM, and the GABA and glycine keep hold of us – the scary result is dreaming awake.
I could float up to my bedroom ceiling or into the living room or out through the solid front door
One of the most probing explorations of this state, and the one that helped free me from the terror, comes from Jorge Conesa-Sevilla, a neurocognitive psychologist and shamanic artist based in Oregon who regularly experiences sleep paralysis himself. In his book Wrestling with Ghosts (2004), he takes a refreshing approach to the subject, couching sleep paralysis in scientific terms, without denying his personal, exploratory approach.
Conesa-Sevilla taught me that people who experience sleep paralysis have a unique advantage in dreaming lucidly – they can use their altered state as a launch pad for full-blown dream control. It makes sense: both lucid dreams and sleep paralysis are ‘blended states’, according to the psychologist James Cheyne of the University of Waterloo in Canada – but these states are distinct . ‘ Lucid dreaming seems to consist of waking awareness intruding into dreams and sleep paralysis of dream imagery intruding into waking consciousness.’
Conesa-Sevilla has developed specific, highly honed techniques to help us move from one blended state to the other. Like many others who regularly experience sleep paralysis, I had naturally slipped into lucid dreams on occasion, but I did not understand what they were, or that I could initiate this switch. Wrestling with Ghosts explained how to do this, but most importantly, it made me understand that sleep paralysis was not a curse; it could be a gift.
Conesa-Sevilla’s system, called Sleep Paralysis Signalling (SPS), is used to acknowledge and exploit your self-awareness in order to transition from one altered state to the other: from terror to bliss. It includes focusing on particular parts of your body, imagining that you are spinning, and using meditation, controlled breathing and relaxation for managing the fear of the paralysed state. Tapping SPS, I can wilfully go from waking to the dream state, retaining just enough consciousness to influence the action within.
T o switch from sleep paralysis into lucid dreaming is no mean feat; it is hard to keep a cool head with a ghost sitting on top of you. I can rarely pinpoint the moment that terror becomes lucidity but, when it does, I am launched into the vast landscapes and vivid colours of my lucid dreams.
I often return to the same places, worlds that I have created. There is a city with a complex network of streets, elaborate houses, an underground system, a harbour and swimming pools. The whites, blues, yellows and greens are far more intense than any I have seen in waking life. And there are great natural landscapes: a coastline with high cliffs and forests. I know my way around. I could draw a map of these worlds. I can choose where to go and I can walk or fly. I populate these landscapes with people; be they familiar or fantastical, living or departed, I talk to them. I am fully conscious during these dreams.
My lucid dreams are often accompanied by sensations of flying, floating or leaping across the landscape. But sometimes I have another experience, similar in that it is characterised by flying and floating sensations, yet distinct. During a lucid dream I am ‘intact’ and moving around a dreamscape, whereas during these other experiences, I seem to physically twist or stand up and ‘out of myself’ and into my immediate surroundings. This sensation feels as real to me as it would if I were to stand up now – and it is experienced as fully alert consciousness. I now understand this to be a form of out-of-body experience, or OBE.
With hindsight, I realise I’ve been having OBEs for some time. In one childhood memory, I’m lying both ‘on’ and ‘under’ my bed at the same time. Later, I willed the experience out of terror during the sleep paralysis itself. If I yell, but make no sound, I thought, if I feel, but nothing is touching me, if I move my arms, but they are still, then my paralysed body is, somehow, receiving sensations of movement from my brain. What would happen if I consciously willed this phenomenal body to twist out of my paralysed body? And I found that, in my mind at least, I could.
At first there were loud noises, buzzing and whooshing. At times it felt as if my brain was being sucked out of the top of my head, or that my whole body was being pulled backwards at high speed. I would panic and fight it, but each time I became a little braver. I would ride out the scary sounds and sensations, and find that they gave way to a pleasant feeling of being completely separate from my body.
I could see my bedroom, but in altered form. The plain wooden door had beautiful paintings on it; the trees in the garden were a different species or larger than normal. At times I seemed to be dragging myself around; at others I was light and moved with ease.
During these OBEs, I wondered what would happen if I tried to push my body through my closed bedroom door, and I found that I could. I started to play with these sensations, to float up to my bedroom ceiling or into the living room or out through the solid front door. I enjoyed the feeling of spinning around my house and garden.
My lucid dreams and OBEs are delightful because I can consciously heighten my experience, and a little terror can be exhilarating
I understand the mind and body to be a complex biological and chemical entity, intertwined, yet my hallucinogenic nights suggested otherwise. What was this ‘self’ that appeared to get free? I was unnerved by a friend who suggested I stop leaving my body lest a ‘lost soul’ inhabit it while I was gone, blocking my return. But my fears were eased by talks with the experts. The neuroscience fascinated me, and set me free.
Our sense of ‘self’ as embodied, moving around space relative to gravity, comes from sensual input: spatial position and balance, touch and movement, and visual cues. These ‘vestibular’ sensations, coming from diverse neural networks in different parts of the brain, are brought together and processed at the junction of the temporal and parietal lobes (or our temporal-parietal junction), a region just above and behind the ears. When we are awake, our temporal-parietal junction is highly active, and it processes information efficiently and coherently. During dream sleep, however, vestibular sensations come not from the external environment but from within the brain itself.
During REM, vestibular sensations might be the source of those lovely flying dreams. But they can also be felt during sleep paralysis – and can be used to propel us to OBE. The jet fuel here is dissonance: from REM, we get vestibular sensations produced by the brain and from waking we get vestibular input from the outside world; both land in the temporal parietal junction. During sleep paralysis, Cheyne explains, the brain tries to reconcile the conflict ‘between movement and non-movement, between simultaneously floating above, and lying on, one’s bed’. He thinks we resolve the conflict by ‘a splitting of the phenomenal self and the physical body, sometimes referred to as an out-of-body experience’.
The notion is supported by Olaf Blanke, professor of neurology at the University Hospital of Geneva, whose studies over the past decade have helped to transform the field. Blanke has shown when he stimulates the brain with electrical current, it generates its own vestibular sensation and transports it to the temporal-parietal junction, recreating the pattern that occurs during REM. As a result, the sense of self as embodied is lost and the individual reports feeling separate and detached. Of special note is the position of the temporal-parietal junction just above and behind the ears, perhaps an explanation for the sense of a presence that seems to lurk behind or just out of sight – the very entities so many of us describe as part of the sleep-paralysis experience.
Some might think that these neurological explanations for sleep paralysis, lucid dreams and OBEs are impediments to the experience but, to me, they enrich it. My lucid dreams and OBEs are even more delightful because I can consciously heighten my experience, and a little terror can be exhilarating. To this day, I’m frightened when I wake up paralysed. After all, my amygdala is screaming FEAR! FEAR! FEAR! But, with my new-found understanding, I can overcome the terror and take advantage of being awake to explore these altered states. The transition from state to state can be slippery but, the more I understand what is happening in my brain, the more control I have and the more enjoyable the experience becomes.
Here I am again, lying in bed. If you walk in now, you’ll think I’m sleeping. But I am not: I am conscious and I am flying, bounding across landscapes coloured by dreams.
If you ever wake up unable to move, try not to panic. Remind yourself that you stand at the threshold of a fantastical world, a strange hinterland, an exhilarating space in which you are awake, but have a REM toy box at your disposal."
Time is not an illusion. It’s an object with physical size | Aeon Essays,,https://aeon.co/essays/time-is-not-an-illusion-its-an-object-with-physical-size,"A timeless universe is hard to imagine, but not because time is a technically complex or philosophically elusive concept. There is a more structural reason: imagining timelessness requires time to pass. Even when you try to imagine its absence, you sense it moving as your thoughts shift, your heart pumps blood to your brain, and images, sounds and smells move around you. The thing that is time never seems to stop. You may even feel woven into its ever-moving fabric as you experience the Universe coming together and apart. But is that how time really works?
According to Albert Einstein, our experience of the past, present and future is nothing more than ‘a stubbornly persistent illusion’. According to Isaac Newton, time is nothing more than backdrop, outside of life. And according to the laws of thermodynamics, time is nothing more than entropy and heat. In the history of modern physics, there has never been a widely accepted theory in which a moving, directional sense of time is fundamental. Many of our most basic descriptions of nature – from the laws of movement to the properties of molecules and matter – seem to exist in a universe where time doesn’t really pass. However, recent research across a variety of fields suggests that the movement of time might be more important than most physicists had once assumed.
A new form of physics called assembly theory suggests that a moving, directional sense of time is real and fundamental. It suggests that the complex objects in our Universe that have been made by life, including microbes, computers and cities, do not exist outside of time: they are impossible without the movement of time. From this perspective, the passing of time is not only intrinsic to the evolution of life or our experience of the Universe. It is also the ever-moving material fabric of the Universe itself. Time is an object. It has a physical size, like space. And it can be measured at a molecular level in laboratories.
The unification of time and space radically changed the trajectory of physics in the 20th century. It opened new possibilities for how we think about reality. What could the unification of time and matter do in our century? What happens when time is an object?
F or Newton, time was fixed. In his laws of motion and gravity, which describe how objects change their position in space, time is an absolute backdrop. Newtonian time passes, but never changes. And it’s a view of time that endures in modern physics – even in the wave functions of quantum mechanics time is a backdrop , not a fundamental feature. For Einstein, however, time was not absolute. It was relative to each observer. He described our experience of time passing as ‘a stubbornly persistent illusion’. Einsteinian time is what is measured by the ticking of clocks; space is measured by the ticks on rulers that record distances. By studying the relative motions of ticking clocks and ticks on rulers, Einstein was able to combine the concepts of how we measure both space and time into a unified structure we now call ‘spacetime’. In this structure, space is infinite and all points exist at once. But time, as Einstein described it, also has this property, which means that all times – past, present and future – are equally real. The result is sometimes called a ‘block universe’, which contains everything that has and will happen in space and time. Today, most physicists support the notion of the block universe.
But the block universe was cracked before it even arrived. In the early 1800s, nearly a century before Einstein developed the concept of spacetime, Nicolas Léonard Sadi Carnot and other physicists were already questioning the notion that time was either a backdrop or an illusion. These questions would continue into the 19th century as physicists such as Ludwig Boltzmann also began to turn their minds to the problems that came with a new kind of technology: the engine.
Though engines could be mechanically reproduced, physicists didn’t know exactly how they functioned. Newtonian mechanics were reversible; engines were not. Newton’s solar system ran equally well moving forward or backward in time. However, if you drove a car and it ran out of fuel, you could not run the engine in reverse, take back the heat that was generated, and unburn the fuel. Physicists at the time suspected that engines must be adhering to certain laws, even if those laws were unknown. What they found was that engines do not function unless time passes and has a direction. By exploiting differences in temperature, engines drive the movement of heat from warm parts to cold parts. As time moves forward, the temperature difference diminishes and less ‘work’ can be done. This is the essence of the second law of thermodynamics (also known as the law of entropy) that was proposed by Carnot and later explained statistically by Boltzmann. The law describes the way that less useful ‘work’ can be done by an engine over time. You must occasionally refuel your car, and entropy must always increase.
Do we really live in a universe that has no need for time as a fundamental feature?
This makes sense in the context of engines or other complex objects, but it is not helpful when dealing with a single particle. It is meaningless to talk about the temperature of a single particle because temperature is a way of quantifying the average kinetic energy of many particles. In the laws of thermodynamics, the flow and directionality of time are considered an emergent property rather than a backdrop or an illusion – a property associated with the behaviour of large numbers of objects. While thermodynamic theory introduced how time should have a directionality to its passage, this property was not fundamental. In physics, ‘fundamental’ properties are reserved for those properties that cannot be described in other terms. The arrow of time in thermodynamics is therefore considered ‘emergent’ because it can be explained in terms of more fundamental concepts, such as entropy and heat.
Charles Darwin, working between the steam engine era of Carnot and the emergence of Einstein’s block universe, was among the first to clearly see how life must exist in time. In the final sentence from On the Origin of Species (1859), he eloquently captured this perspective: ‘[W]hilst this planet has gone cycling on according to the fixed law of gravity, from so simple a beginning endless forms most beautiful and most wonderful have been and are being evolved.’ The arrival of Darwin’s ‘endless forms’ can be explained only in a universe where time exists and has a clear directionality.
During the past several billion years, life has evolved from single-celled organisms to complex multicellular organisms. It has evolved from simple societies to teeming cities, and now a planet potentially capable of reproducing its life on other worlds. These things take time to come into existence because they can emerge only through the processes of selection and evolution.
We think Darwin’s insight does not go deep enough. Evolution accurately describes changes observed across different forms of life, but it does much more than this: it is the only physical process in our Universe that can generate the objects we associate with life. This includes bacteria, cats and trees, but also things like rockets, mobile phones and cities. None of these objects fluctuates into existence spontaneously, despite what popular accounts of modern physics may claim can happen. These objects are not random flukes. Instead, they all require a ‘memory’ of the past to be made in the present. They must be produced over time – a time that continually moves forward. And yet, according to Newton, Einstein, Carnot, Boltzmann and others, time is either nonexistent or merely emergent.
T he times of physics and of evolution are incompatible. But this has not always been obvious because physics and evolution deal with different kinds of objects. Physics, particularly quantum mechanics, deals with simple and elementary objects: quarks, leptons and force carrier particles of the Standard Model. Because these objects are considered simple, they do not require ‘memory’ for the Universe to make them (assuming sufficient energy and resources are available). Think of ‘memory’ as a way to describe the recording of actions or processes that are needed to build a given object. When we get to the disciplines that engage with evolution, such as chemistry and biology, we find objects that are too complex to be produced in abundance instantaneously (even when energy and materials are available). They require memory, accumulated over time, to be produced. As Darwin understood, some objects can come into existence only through evolution and the selection of certain ‘recordings’ from memory to make them.
This incompatibility creates a set of problems that can be solved only by making a radical departure from the current ways that physics approaches time – especially if we want to explain life. While current theories of quantum mechanics can explain certain features of molecules, such as their stability, they cannot explain the existence of DNA, proteins, RNA, or other large and complex molecules. Likewise, the second law of thermodynamics is said to give rise to the arrow of time and explanations of how organisms convert energy, but it does not explain the directionality of time, in which endless forms are built over evolutionary timescales with no final equilibrium or heat-death for the biosphere in sight. Quantum mechanics and thermodynamics are necessary to explain some features of life, but they are not sufficient.
These and other problems led us to develop a new way of thinking about the physics of time, which we have called assembly theory. It describes how much memory must exist for a molecule or combination of molecules – the objects that life is made from – to come into existence. In assembly theory, this memory is measured across time as a feature of a molecule by focusing on the minimum memory required for that molecule (or molecules) to come into existence. Assembly theory quantifies selection by making time a property of objects that could have emerged only via evolution.
We began developing this new physics by considering how life emerges through chemical changes. The chemistry of life operates combinatorially as atoms bond to form molecules, and the possible combinations grow with each additional bond. These combinations are made from approximately 92 naturally occurring elements, which chemists estimate can be combined to build as many as 10 60 different molecules – 1 followed by 60 zeroes. To become useful, each individual combination would need to be replicated billions of times – think of how many molecules are required to make even a single cell, let alone an insect or a person. Making copies of any complex object takes time because each step required to assemble it involves a search across the vastness of combinatorial space to select which molecules will take physical shape.
Combinatorial spaces seem to show up when life exists
Consider the macromolecular proteins that living things use as catalysts within cells. These proteins are made from smaller molecular building blocks called amino acids, which combine to form long chains typically between 50 and 2,000 amino acids long. If every possible 100-amino-acid-long protein was assembled from the 20 most common amino acids that form proteins, the result would not just fill our Universe but 10 23 universes.
Photo by Donna Enriquez/Flickr
The space of all possible molecules is hard to fathom. As an analogy, consider the combinations you can build with a given set of Lego bricks. If the set contained only two bricks, the number of combinations would be small. However, if the set contained thousands of pieces, like the 5,923-piece Lego model of the Taj Mahal, the number of possible combinations would be astronomical. If you specifically needed to build the Taj Mahal according to the instructions, the space of possibilities would be limited, but if you could build any Lego object with those 5,923 pieces, there would be a combinatorial explosion of possible structures that could be built – the possibilities grow exponentially with each additional block you add. If you connected two Lego structures you had already built every second, you would not be able to exhaust all possible objects of the size of the Lego Taj Mahal set within the age of the Universe. In fact, any space built combinatorially from even a few simple building blocks will have this property. This includes all possible cell-like objects built from chemistry, all possible organisms built from different cell-types, all possible languages built from words or utterances, and all possible computer programs built from all possible instruction sets. The pattern here is that combinatorial spaces seem to show up when life exists. That is, life is evident when the space of possibilities is so large that the Universe must select only some of that space to exist. Assembly theory is meant to formalise this idea. In assembly theory, objects are built combinatorially from other objects and, just as you might use a ruler to measure how big a given object is spatially, assembly theory provides a measure – called the ‘assembly index’ – to measure how big an object is in time.
The Lego Taj Mahal set is equivalent to a complex molecule in this analogy. Reproducing a specific object, like a Lego set, in a way that isn’t random requires selection within the space of all possible objects. That is, at each stage of construction, specific objects or sets of objects must be selected from the vast number of possible combinations that could be built. Alongside selection, ‘memory’ is also required: information is needed in the objects that exist to assemble the specific new object, which is implemented as a sequence of steps that can be completed in finite time, like the instructions required to build the Lego Taj Mahal. More complex objects require more memory to come into existence.
In assembly theory, objects grow in their complexity over time through the process of selection. As objects become more complex, their unique parts will increase, which means local memory must also increase. This ‘local memory’ is the causal chain of events in how the object is first ‘discovered’ by selection and then created in multiple copies. For example, in research into the origin of life, chemists study how molecules come together to become living organisms. For a chemical system to spontaneously emerge as ‘life’, it must self-replicate by forming, or catalysing, self-sustaining networks of chemical reactions. But how does the chemical system ‘know’ which combinations to make? We can see ‘local memory’ in action in these networks of molecules that have ‘learned’ to chemically bind together in certain ways. As the memory requirements increase, the probability that an object was produced by chance drops to zero because the number of alternative combinations that weren’t selected is just too high. An object, whether it’s a Lego Taj Mahal or a network of molecules, can be produced and reproduced only with memory and a construction process. But memory is not everywhere, it’s local in space and time. This means an object can be produced only where there is local memory that can guide the selection of which parts go where, and when.
In assembly theory, ‘selection’ refers to what has emerged in the space of possible combinations. It is formally described through an object’s copy number and complexity. Copy number or concentration is a concept used in chemistry and molecular biology that refers to how many copies of a molecule are present in a given volume of space. In assembly theory, complexity is as significant as the copy number. A highly complex molecule that exists only as a single copy is not important. What is of interest to assembly theory are complex molecules with a high copy number, which is an indication that the molecule has been produced by evolution. This complexity measurement is also known as an object’s ‘assembly index’. This value is related to the amount of physical memory required to store the information to direct the assembly of an object and set a directionality in time from the simple to the complex. And, while the memory must exist in the environment to bring the object into existence, in assembly theory the memory is also an intrinsic physical feature of the object. In fact, it is the object.
L ife is stacks of objects building other objects that build other objects – it’s objects building objects, all the way down. Some objects emerged only relatively recently, such as synthetic ‘forever chemicals’ made from organofluorine chemical compounds. Others emerged billions of years ago, such as photosynthesising plant cells. Different objects have different depths in time. And this depth is directly related to both an object’s assembly index and copy number, which we can combine into a number: a quantity called ‘Assembly’, or A. The higher the Assembly number, the deeper an object is in time.
To measure assembly in a laboratory, we chemically analyse an object to count how many copies of a given molecule it contains. We then infer the object’s complexity, known as its molecular assembly index, by counting the number of parts it contains. These molecular parts, like the amino acids in a protein string, are often inferred by determining an object’s molecular assembly index – a theoretical assembly number. But we are not inferring theoretically. We are ‘counting’ the molecular components of an object using three visualising techniques: mass spectrometry, infrared and nuclear magnetic resonance (NMR) spectroscopy. Remarkably, the number of components we’ve counted in molecules maps to their theoretical assembly numbers. This means we can measure an object’s assembly index directly with standard lab equipment.
A high Assembly number – a high assembly index and a high copy number – indicates that it can be reliably made by something in its environment. This could be a cell that constructs high-Assembly molecules like proteins, or a chemist that makes molecules with an even higher Assembly value, such as the anti-cancer drug Taxol (paclitaxel). Complex objects with high copy numbers did not come into existence randomly but are the result of a process of evolution or selection. They are not formed by a series of chance encounters, but by selection in time . More specifically, a certain depth in time.
It’s like throwing the 5,923 Lego Taj Mahal pieces in the air and expecting them to come together spontaneously
This is a difficult concept. Even chemists find this idea hard to grasp since it is easy to imagine that ‘complex’ molecules form by chance interactions with their environment. However, in the laboratory, chance interactions often lead to the production of ‘tar’ rather than high-Assembly objects. Tar is a chemist’s worst nightmare, a messy mixture of molecules that cannot be individually identified. It is found frequently in origin-of-life experiments. In the US chemist Stanley Miller’s ‘prebiotic soup’ experiment in 1953, the amino acids that formed at first turned into a mess of unidentifiable black gloop if the experiment was run too long (and no selection was imposed by the researchers to stop chemical changes taking place). The problem in these experiments is that the combinatorial space of possible molecules is so vast for high-Assembly objects that no specific molecules are produced in high abundance. ‘Tar’ is the result.
It’s like throwing the 5,923 pieces from the Lego Taj Mahal set in the air and expecting them to come together, spontaneously, exactly as the instructions specify. Now imagine taking the pieces from 100 boxes of the same Lego set, throwing them into the air, and expecting 100 copies of the exact same building. The probabilities are incredibly low and might be zero, if assembly theory is on the right track. It is as likely as a smashed egg spontaneously reforming.
But what about complex objects that occur naturally without selection or evolution? What about snowflakes , minerals and complex storm systems? Unlike objects generated by evolution and selection, these do not need to be explained through their ‘depth in time’. Though individually complex, they do not have a high Assembly value because they form randomly and require no memory to be produced. They have a low copy number because they never exist in identical copies. No two snowflakes are alike, and the same goes for minerals and storm systems.
A ssembly theory not only changes how we think about time, but how we define life itself. By applying this approach to molecular systems, it should be possible to measure if a molecule was produced by an evolutionary process. That means we can determine which molecules could have been made only by a living process, even if that process involves chemistries different to those on Earth. In this way, assembly theory can function as a universal life-detection system that works by measuring the assembly indexes and copy numbers of molecules in living or non-living samples.
In our laboratory experiments , we found that only living samples produce high-Assembly molecules. Our teams and collaborators have reproduced this finding using an analytical technique called mass spectrometry, in which molecules from a sample are ‘weighed’ in an electromagnetic field and then smashed into pieces using energy. Smashing a molecule to bits allows us to measure its assembly index by counting the number of unique parts it contains. Through this, we can work out how many steps were required to produce a molecular object and then quantify its depth in time with standard laboratory equipment.
To verify our theory that high-Assembly objects can be generated only by life, the next step involved testing living and non-living samples. Our teams have been able to take samples of molecules from across the solar system, including diverse living, fossilised and abiotic systems on Earth. These solid samples of stone, bone, flesh and other forms of matter were dissolved in a solvent and then analysed with a high-resolution mass spectrometer that can identify the structure and properties of molecules. We found that only living systems produce abundant molecules with an assembly index above an experimentally determined value of 15 steps. The cut-off between 13 and 15 is sharp, meaning that molecules made by random processes cannot get beyond 13 steps. We think this is indicative of a phase transition where the physics of evolution and selection must take over from other forms of physics to explain how a molecule was formed.
These experiments verify that only objects with a sufficiently high Assembly number – highly complex and copied molecules – seem to be found in life. What is even more exciting is that we can find this information without knowing anything else about the molecule present. Assembly theory can determine whether molecules from anywhere in the Universe were derived from evolution or not, even if we don’t know what chemistry is being used.
The possibility of detecting living systems elsewhere in the galaxy is exciting, but more exciting for us is the possibility of a new kind of physics, and a new explanation of life. As an empirical measure of objects uniquely producible by evolution, Assembly unlocks a more general theory of life. If the theory holds, its most radical philosophical implication is that time exists as a material property of the complex objects created by evolution. That is, just as Einstein radicalised our notion of time by unifying it with space, assembly theory points to a radically new conception of time by unifying it with matter.
Assembly theory explains evolved objects, such as complex molecules, biospheres, and computers
It is radical because, as we noted, time has never been fundamental in the history of physics. Newton and some quantum physicists view it as a backdrop. Einstein thought it was an illusion. And, in the work of those studying thermodynamics, it’s understood as merely an emergent property. Assembly theory treats time as fundamental and material: time is the stuff out of which things in the Universe are made. Objects created by selection and evolution can be formed only through the passing of time. But don’t think about this time like the measured ticking of a clock or a sequence of calendar years. Time is a physical attribute. Think about it in terms of Assembly, a measurable intrinsic property of a molecule’s depth or size in time.
This idea is radical because it also allows physics to explain evolutionary change. Physics has traditionally studied objects that the Universe can spontaneously assemble, such as elementary particles or planets. Assembly theory, on the other hand, explains evolved objects, such as complex molecules, biospheres, and computers. These complex objects exist only along lineages where information has been acquired specific to their construction.
If we follow those lineages back, beyond the origin of life on Earth to the origin of the Universe, it would be logical to suggest that the ‘memory’ of the Universe was lower in the past. This means that the Universe’s ability to generate high-Assembly objects is fundamentally limited by its size in time. Just as a semi-trailer truck will not fit inside a standard home garage, some objects are too large in time to come into existence in intervals that are smaller than their assembly index. For complex objects like computers to exist in our Universe, many other objects needed to form first: stars, heavy elements, life, tools, technology, and the abstraction of computing. This takes time and is critically path-dependent due to the causal contingency of each innovation made. The early Universe may not have been capable of computation as we know it, simply because not enough history existed yet. Time had to pass and be materially instantiated through the selection of the computer’s constituent objects. The same goes for Lego structures, large language models, new pharmaceutical drugs, the ‘technosphere’, or any other complex object.
The consequences of objects having an intrinsic material depth in time is far reaching. In the block universe, everything is treated as static and existing all at once. This means that objects cannot be ordered by their depth in time, and selection and evolution cannot be used to explain why some objects exist and not others. Re-conceptualising time as a physical dimension of complex matter, and setting a directionality for time could help us solve such questions. Making time material through assembly theory unifies several perplexing philosophical concepts related to life in one measurable framework. At the heart of this theory is the assembly index, which measures the complexity of an object. It is a quantifiable way of describing the evolutionary concept of selection by showing how many alternatives were excluded to yield a given object. Each step in the assembly process of an object requires information, memory, to specify what should and shouldn’t be added or changed. In building the Lego Taj Mahal, for example, we must take a specific sequence of steps, each directing us toward the final building. Each misstep is an error, and if we make too many errors we cannot build a recognisable structure. Copying an object requires information about the steps that were previously needed to produce similar objects.
This makes assembly theory a causal theory of physics, because the underlying structure of an assembly space – the full range of required combinations – orders things in a chain of causation. Each step relies on a previously selected step, and each object relies on a previously selected object. If we removed any steps in an assembly pathway, the final object would not be produced. Buzzwords often associated with the physics of life, such as ‘theory’, ‘information’, ‘memory’, ‘causation’ and ‘selection’, are material because objects themselves encode the rules to help construct other ‘complex’ objects. This could be the case in mutual catalysis where objects reciprocally make each other. Thus, in assembly theory, time is essentially the same thing as information, memory, causation and selection. They are all made physical because we assume they are features of the objects described in the theory, not the laws of how these objects behave. Assembly theory reintroduces an expanding, moving sense of time to physics by showing how its passing is the stuff complex objects are made of: the size of the future increases with complexity.
T his new conception of time might solve many open problems in fundamental physics. The first and foremost is the debate between determinism and contingency. Einstein famously said that God ‘does not play dice’, and many physicists are still forced to conclude that determinism holds, and our future is closed. But the idea that the initial conditions of the Universe, or any process, determine the future has always been a problem. In assembly theory, the future is determined, but not until it happens. If what exists now determines the future, and what exists now is larger and more information-rich than it was in the past, then the possible futures also grow larger as objects become more complex. This is because there is more history existing in the present from which to assemble novel future states. Treating time as a material property of the objects it creates allows novelty to be generated in the future.
Novelty is critical for our understanding of life as a physical phenomenon. Our biosphere is an object that is at least 3.5 billion years old by the measure of clock time (Assembly is a different measure of time). But how did life get started? What allowed living systems to develop intelligence and consciousness? Traditional physics suggests that life ‘emerged’. The concept of emergence captures how new structures seem to appear at higher levels of spatial organisation that could not be predicted from lower levels. Examples include the wetness of water, which is not predicted from individual water molecules, or the way that living cells are made from individual non-living atoms. However, the objects traditional physics considers emergent become fundamental in assembly theory. From this perspective, an object’s ‘emergent-ness’ – how far it departs from a physicist’s expectations of elementary building blocks – depends on how deep it lies in time. This points us toward the origins of life, but we can also travel in the other direction.
If we are on the right track, assembly theory suggests time is fundamental. It suggests change is not measured by clocks but is encoded in chains of events that produce complex molecules with different depths in time. Assembled from local memory in the vastness of combinatorial space, these objects record the past, act in the present, and determine the future. This means the Universe is expanding in time, not space – or perhaps space emerges from time, as many current proposals from quantum gravity suggest. Though the Universe may be entirely deterministic, its expansion in time implies that the future cannot be fully predicted, even in principle. The future of the Universe is more open-ended than we could have predicted.
Time may be an ever-moving fabric through which we experience things coming together and apart. But the fabric does more than move – it expands. When time is an object, the future is the size of the Universe.
Published in association with the Santa Fe Institute, an Aeon Strategic Partner."
How economists rode maths to become our era’s astrologers | Aeon Essays,,https://aeon.co/essays/how-economists-rode-maths-to-become-our-era-s-astrologers,"Since the 2008 financial crisis, colleges and universities have faced increased pressure to identify essential disciplines, and cut the rest. In 2009, Washington State University announced it would eliminate the department of theatre and dance, the department of community and rural sociology, and the German major – the same year that the University of Louisiana at Lafayette ended its philosophy major. In 2012, Emory University in Atlanta did away with the visual arts department and its journalism programme. The cutbacks aren’t restricted to the humanities: in 2011, the state of Texas announced it would eliminate nearly half of its public undergraduate physics programmes. Even when there’s no downsizing, faculty salaries have been frozen and departmental budgets have shrunk.
But despite the funding crunch, it’s a bull market for academic economists. According to a 2015 sociological study in the Journal of Economic Perspectives , the median salary of economics teachers in 2012 increased to $103,000 – nearly $30,000 more than sociologists. For the top 10 per cent of economists, that figure jumps to $160,000, higher than the next most lucrative academic discipline – engineering. These figures, stress the study’s authors, do not include other sources of income such as consulting fees for banks and hedge funds, which, as many learned from the documentary Inside Job (2010), are often substantial. (Ben Bernanke, a former academic economist and ex-chairman of the Federal Reserve, earns $200,000-$400,000 for a single appearance.)
Unlike engineers and chemists, economists cannot point to concrete objects – cell phones, plastic – to justify the high valuation of their discipline. Nor, in the case of financial economics and macroeconomics, can they point to the predictive power of their theories. Hedge funds employ cutting-edge economists who command princely fees, but routinely underperform index funds. Eight years ago, Warren Buffet made a 10-year, $1 million bet that a portfolio of hedge funds would lose to the S&P 500, and it looks like he’s going to collect. In 1998, a fund that boasted two Nobel Laureates as advisors collapsed, nearly causing a global financial crisis.
The failure of the field to predict the 2008 crisis has also been well-documented. In 2003, for example, only five years before the Great Recession, the Nobel Laureate Robert E Lucas Jr told the American Economic Association that ‘macroeconomics […] has succeeded: its central problem of depression prevention has been solved’. Short-term predictions fair little better – in April 2014, for instance, a survey of 67 economists yielded 100 per cent consensus: interest rates would rise over the next six months. Instead, they fell. A lot.
Nonetheless, surveys indicate that economists see their discipline as ‘the most scientific of the social sciences’. What is the basis of this collective faith, shared by universities, presidents and billionaires? Shouldn’t successful and powerful people be the first to spot the exaggerated worth of a discipline, and the least likely to pay for it?
In the hypothetical worlds of rational markets, where much of economic theory is set, perhaps. But real-world history tells a different story, of mathematical models masquerading as science and a public eager to buy them, mistaking elegant equations for empirical accuracy.
A s an extreme example, take the extraordinary success of Evangeline Adams, a turn-of-the-20th-century astrologer whose clients included the president of Prudential Insurance, two presidents of the New York Stock Exchange, the steel magnate Charles M Schwab, and the banker J P Morgan. To understand why titans of finance would consult Adams about the market, it is essential to recall that astrology used to be a technical discipline, requiring reams of astronomical data and mastery of specialised mathematical formulas. ‘An astrologer’ is, in fact, the Oxford English Dictionary ’s second definition of ‘mathematician’. For centuries, mapping stars was the job of mathematicians, a job motivated and funded by the widespread belief that star-maps were good guides to earthly affairs. The best astrology required the best astronomy, and the best astronomy was done by mathematicians – exactly the kind of person whose authority might appeal to bankers and financiers.
In fact, when Adams was arrested in 1914 for violating a New York law against astrology, it was mathematics that eventually exonerated her. During the trial, her lawyer Clark L Jordan emphasised mathematics in order to distinguish his client’s practice from superstition, calling astrology ‘a mathematical or exact science’. Adams herself demonstrated this ‘scientific’ method by reading the astrological chart of the judge’s son. The judge was impressed: the plaintiff, he observed, went through a ‘mathematical process to get at her conclusions… I am satisfied that the element of fraud… is absent here.’
Romer compares debates among economists to those between 16th-century advocates of heliocentrism and geocentrism
The enchanting force of mathematics blinded the judge – and Adams’s prestigious clients – to the fact that astrology relies upon a highly unscientific premise, that the position of stars predicts personality traits and human affairs such as the economy. It is this enchanting force that explains the enduring popularity of financial astrology, even today. The historian Caley Horan at the Massachusetts Institute of Technology described to me how computing technology made financial astrology explode in the 1970s and ’80s. ‘Within the world of finance, there’s always a superstitious, quasi-spiritual trend to find meaning in markets,’ said Horan. ‘Technical analysts at big banks, they’re trying to find patterns in past market behaviour, so it’s not a leap for them to go to astrology.’ In 2000, USA Today quoted Robin Griffiths, the chief technical analyst at HSBC, the world’s third largest bank, saying that ‘most astrology stuff doesn’t check out, but some of it does’.
Ultimately, the problem isn’t with worshipping models of the stars, but rather with uncritical worship of the language used to model them, and nowhere is this more prevalent than in economics. The economist Paul Romer at New York University has recently begun calling attention to an issue he dubs ‘mathiness’ – first in the paper ‘Mathiness in the Theory of Economic Growth’ (2015) and then in a series of blog posts. Romer believes that macroeconomics, plagued by mathiness, is failing to progress as a true science should, and compares debates among economists to those between 16th-century advocates of heliocentrism and geocentrism. Mathematics, he acknowledges, can help economists to clarify their thinking and reasoning. But the ubiquity of mathematical theory in economics also has serious downsides: it creates a high barrier to entry for those who want to participate in the professional dialogue, and makes checking someone’s work excessively laborious. Worst of all, it imbues economic theory with unearned empirical authority.
‘I’ve come to the position that there should be a stronger bias against the use of math,’ Romer explained to me. ‘If somebody came and said: “Look, I have this Earth-changing insight about economics, but the only way I can express it is by making use of the quirks of the Latin language”, we’d say go to hell, unless they could convince us it was really essential. The burden of proof is on them.’
Right now, however, there is widespread bias in favour of using mathematics. The success of math-heavy disciplines such as physics and chemistry has granted mathematical formulas with decisive authoritative force. Lord Kelvin, the 19th-century mathematical physicist, expressed this quantitative obsession:
The trouble with Kelvin’s statement is that measurement and mathematics do not guarantee the status of science – they guarantee only the semblance of science. When the presumptions or conclusions of a scientific theory are absurd or simply false, the theory ought to be questioned and, eventually, rejected. The discipline of economics, however, is presently so blinkered by the talismanic authority of mathematics that theories go overvalued and unchecked.
Romer is not the first to elaborate the mathiness critique. In 1886, an article in Science accused economics of misusing the language of the physical sciences to conceal ‘emptiness behind a breastwork of mathematical formulas’. More recently, Deirdre N McCloskey’s The Rhetoric of Economics (1998) and Robert H Nelson’s Economics as Religion (2001) both argued that mathematics in economic theory serves, in McCloskey’s words, primarily to deliver the message ‘Look at how very scientific I am.’
After the Great Recession, the failure of economic science to protect our economy was once again impossible to ignore. In 2009, the Nobel Laureate Paul Krugman tried to explain it in The New York Times with a version of the mathiness diagnosis. ‘As I see it,’ he wrote, ‘the economics profession went astray because economists, as a group, mistook beauty, clad in impressive-looking mathematics, for truth.’ Krugman named economists’ ‘desire… to show off their mathematical prowess’ as the ‘central cause of the profession’s failure’.
The mathiness critique isn’t limited to macroeconomics. In 2014, the Stanford financial economist Paul Pfleiderer published the paper ‘Chameleons: The Misuse of Theoretical Models in Finance and Economics’, which helped to inspire Romer’s understanding of mathiness. Pfleiderer called attention to the prevalence of ‘chameleons’ – economic models ‘with dubious connections to the real world’ that substitute ‘mathematical elegance’ for empirical accuracy. Like Romer, Pfleiderer wants economists to be transparent about this sleight of hand. ‘Modelling,’ he told me, ‘is now elevated to the point where things have validity just because you can come up with a model.’
The notion that an entire culture – not just a few eccentric financiers – could be bewitched by empty, extravagant theories might seem absurd. How could all those people, all that math, be mistaken? This was my own feeling as I began investigating mathiness and the shaky foundations of modern economic science. Yet, as a scholar of Chinese religion, it struck me that I’d seen this kind of mistake before, in ancient Chinese attitudes towards the astral sciences. Back then, governments invested incredible amounts of money in mathematical models of the stars. To evaluate those models, government officials had to rely on a small cadre of experts who actually understood the mathematics – experts riven by ideological differences, who couldn’t even agree on how to test their models. And, of course, despite collective faith that these models would improve the fate of the Chinese people, they did not.
A stral Science in Early Imperial China , a forthcoming book by the historian Daniel P Morgan, shows that in ancient China, as in the Western world, the most valuable type of mathematics was devoted to the realm of divinity – to the sky, in their case (and to the market, in ours). Just as astrology and mathematics were once synonymous in the West, the Chinese spoke of li , the science of calendrics, which early dictionaries also glossed as ‘calculation’, ‘numbers’ and ‘order’. Li models, like macroeconomic theories, were considered essential to good governance. In the classic Book of Documents , the legendary sage king Yao transfers the throne to his successor with mention of a single duty: ‘Yao said: “Oh thou, Shun! The li numbers of heaven rest in thy person.”’
China’s oldest mathematical text invokes astronomy and divine kingship in its very title – The Arithmetical Classic of the Gnomon of the Zhou . The title’s inclusion of ‘Zhou’ recalls the mythic Eden of the Western Zhou dynasty (1045–771 BCE), implying that paradise on Earth can be realised through proper calculation. The book’s introduction to the Pythagorean theorem asserts that ‘the methods used by Yu the Great in governing the world were derived from these numbers’. It was an unquestioned article of faith: the mathematical patterns that govern the stars also govern the world. Faith in a divine, invisible hand, made visible by mathematics. No wonder that a newly discovered text fragment from 200 BCE extolls the virtues of mathematics over the humanities. In it, a student asks his teacher whether he should spend more time learning speech or numbers. His teacher replies: ‘If my good sir cannot fathom both at once, then abandon speech and fathom numbers, [for] numbers can speak, [but] speech cannot number.’
Modern governments, universities and businesses underwrite the production of economic theory with huge amounts of capital. The same was true for li production in ancient China. The emperor – the ‘Son of Heaven’ – spent astronomical sums refining mathematical models of the stars. Take the armillary sphere, such as the two-metre cage of graduated bronze rings in Nanjing, made to represent the celestial sphere and used to visualise data in three-dimensions. As Morgan emphasises, the sphere was literally made of money. Bronze being the basis of the currency, governments were smelting cash by the metric ton to pour it into li . A divine, mathematical world-engine, built of cash, sanctifying the powers that be.
The enormous investment in li depended on a huge assumption: that good government, successful rituals and agricultural productivity all depended upon the accuracy of li . But there were, in fact, no practical advantages to the continued refinement of li models. The calendar rounded off decimal points such that the difference between two models, hotly contested in theory, didn’t matter to the final product. The work of selecting auspicious days for imperial ceremonies thus benefited only in appearance from mathematical rigour. And of course the comets, plagues and earthquakes that these ceremonies promised to avert kept on coming. Farmers, for their part, went about business as usual. Occasional governmental efforts to scientifically micromanage farm life in different climes using li ended in famine and mass migration.
Like many economic models today, li models were less important to practical affairs than their creators (and consumers) thought them to be. And, like today, only a few people could understand them. In 101 BCE, Emperor Wudi tasked high-level bureaucrats – including the Great Director of the Stars – with creating a new li that would glorify the beginning of his path to immortality. The bureaucrats refused the task because ‘they couldn’t do the math’, and recommended the emperor outsource it to experts.
The equivalent in economic theory might be to grant a model high points for success in predicting short-term markets, while failing to deduct for missing the Great Recession
The debates of these ancient li experts bear a striking resemblance to those of present-day economists. In 223 CE, a petition was submitted to the emperor asking him to approve tests of a new li model developed by the assistant director of the astronomical office, a man named Han Yi.
At the time of the petition, Han Yi’s model, and its competitor, the so-called Supernal Icon, had already been subjected to three years of ‘reference’, ‘comparison’ and ‘exchange’. Still, no one could agree which one was better. Nor, for that matter, was there any agreement on how they should be tested.
In the end, a live trial involving the prediction of eclipses and heliacal risings was used to settle the debate. With the benefit of hindsight, we can see this trial was seriously flawed. The helical rising (first visibility) of planets depends on non-mathematical factors such as eyesight and atmospheric conditions. That’s not to mention the scoring of the trial, which was modelled on archery competitions. Archers scored points for proximity to the bullseye, with no consideration for overall accuracy. The equivalent in economic theory might be to grant a model high points for success in predicting short-term markets, while failing to deduct for missing the Great Recession.
None of this is to say that li models were useless or inherently unscientific. For the most part, li experts were genuine mathematical virtuosos who valued the integrity of their discipline. Despite being based on inaccurate assumptions – that the Earth was at the centre of the cosmos – their models really did work to predict celestial motions. Imperfect though the live trial might have been, it indicates that superior predictive power was a theory’s most important virtue. All of this is consistent with real science, and Chinese astronomy progressed as a science, until it reached the limits imposed by its assumptions.
However, there was no science to the belief that accurate li would improve the outcome of rituals, agriculture or government policy. No science to the Hall of Light, a temple for the emperor built on the model of a magic square. There, by numeric ritual gesture, the Son of Heaven was thought to channel the invisible order of heaven for the prosperity of man. This was quasi-theology, the belief that heavenly patterns – mathematical patterns – could be used to model every event in the natural world, in politics, even the body. Macro- and microcosm were scaled reflections of one another, yin and yang in a unifying, salvific mathematical vision. The expensive gadgets, the personnel, the bureaucracy, the debates, the competition – all of this testified to the divinely authoritative power of mathematics. The result, then as now, was overvaluation of mathematical models based on unscientific exaggerations of their utility.
I n ancient China it would have been unfair to blame li experts for the pseudoscientific exploitation of their theories. These men had no way to evaluate the scientific merits of assumptions and theories – ‘science’, in a formalised, post-Enlightenment sense, didn’t really exist. But today it is possible to distinguish, albeit roughly, science from pseudoscience, astronomy from astrology. Hypothetical theories, whether those of economists or conspiracists, aren’t inherently pseudoscientific. Conspiracy theories can be diverting – even instructive – flights of fancy. They become pseudoscience only when promoted from fiction to fact without sufficient evidence.
Romer believes that fellow economists know the truth about their discipline, but don’t want to admit it. ‘If you get people to lower their shield, they’ll tell you it’s a big game they’re playing,’ he told me. ‘They’ll say: “Paul, you may be right, but this makes us look really bad, and it’s going to make it hard for us to recruit young people.”’
Demanding more honesty seems reasonable, but it presumes that economists understand the tenuous relationship between mathematical models and scientific legitimacy. In fact, many assume the connection is obvious – just as in ancient China, the connection between li and the world was taken for granted. When reflecting in 1999 on what makes economics more scientific than the other social sciences, the Harvard economist Richard B Freeman explained that economics ‘attracts stronger students than [political science or sociology], and our courses are more mathematically demanding’. In Lives of the Laureates (2004), Robert E Lucas Jr writes rhapsodically about the importance of mathematics: ‘Economic theory is mathematical analysis. Everything else is just pictures and talk.’ Lucas’s veneration of mathematics leads him to adopt a method that can only be described as a subversion of empirical science:
Even for those who agree with Romer, conflict of interest still poses a problem. Why would skeptical astronomers question the emperor’s faith in their models? In a phone conversation, Daniel Hausman, a philosopher of economics at the University of Wisconsin, put it bluntly: ‘If you reject the power of theory, you demote economists from their thrones. They don’t want to become like sociologists.’
George F DeMartino, an economist and an ethicist at the University of Denver, frames the issue in economic terms. ‘The interest of the profession is in pursuing its analysis in a language that’s inaccessible to laypeople and even some economists,’ he explained to me. ‘What we’ve done is monopolise this kind of expertise, and we of all people know how that gives us power.’
Every economist I interviewed agreed that conflicts of interest were highly problematic for the scientific integrity of their field – but only tenured ones were willing to go on the record. ‘In economics and finance, if I’m trying to decide whether I’m going to write something favourable or unfavourable to bankers, well, if it’s favourable that might get me a dinner in Manhattan with movers and shakers,’ Pfleiderer said to me. ‘I’ve written articles that wouldn’t curry favour with bankers but I did that when I had tenure.’
when mathematical theory is the ultimate arbiter of truth, it becomes difficult to see the difference between science and pseudoscience
Then there’s the additional problem of sunk-cost bias. If you’ve invested in an armillary sphere, it’s painful to admit that it doesn’t perform as advertised. When confronted with their profession’s lack of predictive accuracy, some economists find it difficult to admit the truth. Easier, instead, to double down, like the economist John H Cochrane at the University of Chicago. The problem isn’t too much mathematics, he writes in response to Krugman’s 2009 post-Great-Recession mea culpa for the field, but rather ‘that we don’t have enough math’. Astrology doesn’t work, sure, but only because the armillary sphere isn’t big enough and the equations aren’t good enough.
If overhauling economics depended solely on economists, then mathiness, conflict of interest and sunk-cost bias could easily prove insurmountable. Fortunately, non-experts also participate in the market for economic theory. If people remain enchanted by PhDs and Nobel Prizes awarded for the production of complicated mathematical theories, those theories will remain valuable. If they become disenchanted, the value will drop.
Economists who rationalise their discipline’s value can be convincing, especially with prestige and mathiness on their side. But there’s no reason to keep believing them. The pejorative verb ‘rationalise’ itself warns of mathiness, reminding us that we often deceive each other by making prior convictions, biases and ideological positions look ‘rational’, a word that confuses truth with mathematical reasoning. To be rational is, simply, to think in ratios, like the ratios that govern the geometry of the stars. Yet when mathematical theory is the ultimate arbiter of truth, it becomes difficult to see the difference between science and pseudoscience. The result is people like the judge in Evangeline Adams’s trial, or the Son of Heaven in ancient China, who trust the mathematical exactitude of theories without considering their performance – that is, who confuse math with science, rationality with reality.
There is no longer any excuse for making the same mistake with economic theory. For more than a century, the public has been warned, and the way forward is clear. It’s time to stop wasting our money and recognise the high priests for what they really are: gifted social scientists who excel at producing mathematical explanations of economies, but who fail, like astrologers before them, at prophecy."
Microchimerism: how pregnancy changes the mother’s very DNA | Aeon Essays,,https://aeon.co/essays/microchimerism-how-pregnancy-changes-the-mothers-very-dna,"When Lee Nelson first began researching autoimmune disorders in the 1980s, the prevailing assumption was that conditions such as arthritis and lupus tend to show up more commonly in women because they are linked to female sex hormones. But to Nelson, a rheumatologist at the Fred Hutchinson Cancer Research Center in Seattle, this explanation did not make sense. If hormones were the culprit, one would expect these afflictions to peak during a woman’s prime reproductive years, when instead they typically appear later in life.
One day in 1994, a colleague specialising in prenatal diagnosis called her up to say that a blood sample from a female technician in his lab was found to contain male DNA a full year after the birth of her son. ‘It set off a light bulb,’ Nelson told me. ‘I wondered what the consequences might be of harbouring these lingering cells.’ Since the developing foetus is genetically half-foreign to the mother, Nelson set out to investigate whether it could be that pregnancy poses a long-term challenge to women’s health.
Evidence that cells travel from the developing foetus into the mother dates back to 1893, when the German pathologist Georg Schmorl found signs of these genetic remnants in women who had died of pregnancy-induced hypertensive disorder. Autopsies revealed ‘giant’ and ‘very particular’ cells in the lungs, which he theorised had been transported as foreign bodies, originating in the placenta. While Schmorl speculated that this sort of cellular transfer also took place during healthy pregnancies, it was not until more than a century later that researchers realised that these migrant cells, crossing from the foetus to the mother, could survive indefinitely.
Within weeks of conception, cells from both mother and foetus traffic back and forth across the placenta, resulting in one becoming a part of the other. During pregnancy, as much as 10 per cent of the free-floating DNA in the mother’s bloodstream comes from the foetus, and while these numbers drop precipitously after birth, some cells remain. Children, in turn, carry a population of cells acquired from their mothers that can persist well into adulthood, and in the case of females might inform the health of their own offspring. And the foetus need not come to full term to leave its lasting imprint on the mother: a woman who had a miscarriage or terminated a pregnancy will still harbour foetal cells. With each successive conception, the mother’s reservoir of foreign material grows deeper and more complex, with further opportunities to transfer cells from older siblings to younger children, or even across multiple generations.
Far from drifting at random, human and animal studies have found foetal origin cells in the mother’s bloodstream, skin and all major organs, even showing up as part of the beating heart. This passage means that women carry at least three unique cell populations in their bodies – their own, their mother’s, and their child’s – creating what biologists term a microchimera , named for the Greek fire-breathing monster with the head of a lion, the body of a goat, and the tail of a serpent.
Microchimerism is not unique to pregnancy. Researchers realised in the 1990s that it also occurs during organ transplantation, where the genetic match between donor and recipient determines whether the body accepts or rejects the grafted tissue, or if it triggers disease. The body’s default tendency to reject foreign material begs the question of how, and why, microchimeric cells picked up during pregnancy linger on indefinitely. No one fully understands why these ‘interlopers’, as Nelson calls them, are tolerated for decades. One explanation is that they are stem or stem-like cells that are absorbed into the different features of the body’s internal landscape, able to bypass immune defences because they are half-identical to the mother’s own cell population. Another is that pregnancy itself changes the immune identity of the mother, altering the composition of what some researchers have dubbed the ‘microchiome’, making her more tolerant of foreign cells.
The phenomenon, believed to have developed in mammals some 93 million years ago, is common to placental mammals to this day. Its persistence and reach was made astonishingly clear in 2012, when Nelson and colleagues analysed brain samples drawn from dozens of deceased women, ranging in age from 32 to 101. They found that the majority contained male DNA, presumably picked up from past pregnancies. And some of these Y chromosome cells had apparently been there for decades: the oldest subject was 94, meaning that male DNA that transferred during gestation would have persisted for more than half a century.
Most of the research focuses on the Y chromosome as a marker for foetal microchimerism. This does not mean that sons, rather than daughters, uniquely affect their mother’s bodies, but rather reflects an ease of measurement: the Y chromosome stands out among a woman’s XX genes. And there is nothing to suggest that the presence of male cells in women’s brains wields a particular influence. Nonetheless, the findings gesture toward an array of questions about what it means for one individual to play host to the cellular material of another, prompting scientists to look into whether this phenomenon affects physical health or influences behaviour, or even carries metaphysical consequences. The Western self is a bounded, autonomous entity, defined in no small part by its presumed distinction from the other. But this unfolding field of research, advanced by Nelson and others, suggests that we humans are not oppositional but constituent beings, made of many. Nelson, who is fond of referencing the poet Walt Whitman’s multitudes , says we need a ‘new paradigm of the biological self’.
O ne of the most cherished images in the West – if not in the world – is the mother and child. Gazes intermingled, bonded as if one, they are suspended in serene togetherness. This cooing placidity presents a scene of utter naturalness, of womanhood fulfilled, of tender destiny. In 1884, the physician John Harvey Kellogg urged women – at a time when childbirth was a leading cause of women’s death – to opt for the ‘slight inconveniences of normal pregnancy and physiological childbirth rather than the dismal comfort of a childless old age’. In spite of the acute health risks that gestation and delivery entailed for Western women well into the 20th century, pregnancy was commonly depicted as the ultimate form of cooperation – mothers sharing their bodies to the point of sacrifice for the sake of kin and species.
This vision utterly obscures the fraught evolutionary journey that delivers the babe in arms, and the screaming, nerve-jangled moments that surround it. Increasingly, pregnancy has come under scrutiny for its profound paradoxes. It is at once essential and unrivalled in its perils. As it engenders life, it also results in staggeringly high rates of death and disease. Scientists are starting to look to microchimerism for clues as to why pregnancy is both life-giving and a singular source of risk.
On one side of the spectrum, foetal microchimeric cells have been implicated in autoimmune disorders, certain cancers and pre-eclampsia , a potentially fatal condition characterised by high blood-pressure during the latter half of pregnancy. But another body of research has found that foetal cells can protect the mother. They appear to congregate at wound sites, including Caesarean incisions, to speed up healing. They participate in angiogenesis, the creation of new blood vessels. A recent survey of the immunological implications of microchimerism in Nature Reviews by researchers at the Cincinnati Children’s Hospital asserts that these cells ‘are not accidental “souvenirs” of pregnancy, but are purposefully retained within mothers and their offspring to promote genetic fitness by improving the outcome of future pregnancies’. The researchers suggest that microchimeric cells boost the mother’s tolerance of successive pregnancies, representing an ‘altruistic act of first children’ to support the success of their genetically similar siblings. And they are associated with decreased risk of Alzheimer’s, lower risk of some cancer , and improved immune surveillance – that is, the body’s ability to recognise and stave off pathogens. According to Nelson, having a different set of genes provides ‘a different looking glass for detecting a pre-malignant cell’.
Although foetal cells might contribute to certain autoimmune disorders, they could also benefit women with rheumatoid arthritis. While doctors have been aware since the early 20th century that arthritic pain tends to recede with pregnancy, Nelson and her colleagues wondered whether there is an immunological reason why it tends to re-emerge later. They found that higher levels of microchimerism were associated with a lessening of symptoms, and that giving birth offered a long-term protective benefit. ‘It really looks vaccine-like,’ Nelson said, noting that pregnancy provides temporary protection against rheumatoid arthritis that, much like a vaccine, diminishes over time. ‘Protection starts about a year after birth, and then gradually attenuates after about 15 years.’
‘There is definitely an association between the presence of foetal cells and improved disease status’
Foetal microchimeric cells might even extend longevity and help to explain why women tend to live longer than men. In a 2012 study of nearly 300 elderly Danish women, the first to explicitly link microchimerism and survival, researchers found that the presence of microchimeric cells, as indicated by the presence of the Y chromosome, reduced women’s mortality for all causes by 60 per cent, largely because of a significantly reduced risk of death from cancer. Although the researchers looked only at male microchimerism (because there are no easy targets to distinguish cells between mothers and daughters), they maintain that female foetuses would have the same impact on longevity: 85 per cent of women who possessed these cells lived to age 80, as compared with 67 per cent who did not. While there are no clear answers to explain how microchimeric cells might lead to longer lifespans, researchers speculate that it could be associated with greater immune surveillance and improved repair of damaged tissue. However, the jury is out as to whether the presence of foetal cells in tissues is a sign of repair or of developing disease.
To Kirby Johnson, professor of paediatrics at Tufts University in Boston, the evidence favours a protective role. Like the Nelson lab, Johnson and his colleagues were also investigating autoimmune diseases. However, they reasoned that, if foetal cells were causing disease, then they should be found in greater concentration in affected tissue. ‘But what we found was that it really didn’t matter if you were looking at women with a particular autoimmune disorder or who were perfectly healthy – we were finding male DNA everywhere we looked,’ Johnson said. ‘That observation of ubiquity – of presence everywhere – didn’t match up to the hypothesis that these cells cause disease.’
While that finding was revelatory for Johnson, the bigger moment came during a study in 2001 on the role of microchimeric cells in disease of the thyroid, a hormone-secreting gland located in the neck. Analyses of samples taken from women who had their thyroids removed showed ‘perfectly intact thyroid follicles from male cells. These were not sad, scattered cells like you’d expect’ but strikingly healthy. Johnson recalled: ‘Finding male cells that had assumed the structure of functional tissue made us say, wait a second, it doesn’t look like it’s causing disease. It looks like they’re actually coming to the rescue and participating in repair.’
Not long after, a mother with severe hepatitis C and a history of intravenous drug use checked into a Boston clinic. Hepatitis C is a disease of the liver, and when Johnson and colleagues looked at a biopsy of the organ, they found a high number of male cells. Moreover, these cells appeared to be functioning as healthy liver tissue. Although the woman declined further treatment for her disease, she participated in tests confirming that the cells had indeed come from her son. When she came in at a later date to provide blood samples, Johnson and his research team were astounded to discover that she was free of the disease. ‘We can’t with absolute certainty say, foetal cells cured her hepatitis,’ Johnson told me. ‘But we can say, there is definitely an association between the presence of foetal cells and improved disease status.’
F or hundreds of millions of years, microchimerism has been a part of mammalian reproduction. From a survival-of-the-fittest perspective, it would make sense that microchimerism might preserve the health of mother and child, helping her survive childbirth and beyond as her offspring make their slow way to independence. However, current evolutionary thinking suggests that the interests of parents and their kin might be at odds – in the womb, as well as in the world. Because mother and the foetus are not genetically identical, they might be engaged in a tug of war over resources. In addition, the mother’s goals, presumably being the successful reproduction and rearing of multiple children, might be at odds with the evolutionary aims of the individual foetus: its own, solitary survival and eventual reproduction.
The geneticist Amy Boddy of the University of California, Santa Barbara, says that microchimerism presents a paradoxical picture of conflict and cooperation, and foetal cells might well play a host of roles, from helpful partners to hostile adversaries. These tensions are thought to originate with the creation of the placenta. Trophoblasts, cells that form the outer layer of the early embryo, attach and burrow into the uterine lining, establishing pregnancy and initiating the process of directing blood, oxygen and nutrients from the mother to the developing foetus. Boddy suggests that microchimeric cells act like a ‘placenta beyond the womb’, directing resources to the baby throughout gestation and after birth.
Conflict ensues: on the one hand, mothers and babies have a shared investment in mutual survival; on the other, the foetus is a demanding, voracious presence, actively trying to draw resources to itself, while the mother places limits on just how much she is willing to give.
In other words, on an unconscious level, the mother might be engaged in a struggle with the foetus over just how much she can provide without harm to herself. Microchimerism extends this silent chemical conversation into the months and years after birth, where, theorists propose, foetal cells can play an important role in ‘manipulating’ the breasts to lactate, the body to increase its temperature, and the mind to become more attached to this new wailing and growing human.
The idea that the womb might not be an enclave of rosy communion took hold in the work of the American evolutionary biologist Robert Trivers. An original and often unorthodox figure, Trivers was the creator of seminal theories – such as parental investment, altruism and parent-offspring conflict – that are now mainstays of evolutionary psychology. Where others embraced the veneer of presumed harmony, Trivers saw roiling conflicts hidden from view, whether in the womb or in romantic partnership. He made the case that familial struggles are rooted in ‘conflict between the biology of the parent and the biology of the child’. Tensions arise, he suggests, because a mother wants to make sure all her children have an equal chance at survival and procreation, whereas a child privileges its own survival and wants to commandeer the mother’s resources for itself.
The foetus has been depicted as a manipulative entity, conniving to direct the mother to its own advantage
The evolutionary biologist David Haig at Harvard University elaborated on this idea through the concept of genomic imprinting. For most genes, the foetus inherits two working copies, one from the mother and one from the father. However, with imprinted genes, one of the copies is silenced, leading to genes that are differently expressed depending on whether they are inherited from the mother or father. Haig suggests that genetically determined behaviours that benefit the paternal line might be favoured by natural selection when a gene is transmitted by the sperm. And conversely, behaviour that benefits that maternal side might be favoured when a gene is transmitted by the egg.
Haig extends the battle in the womb to the mother and father, whose evolutionary agendas differ on just how much the mother should give to the foetus, and how much the foetus should take. He theorises that genes of paternal origin are likely to promote increased demands for maternal resources. Moreover, Haig suggests that a given man will not necessarily reproduce with one woman, but rather increase his own reproductive success by having children with multiple partners. As a result, he is, evolutionarily speaking, more invested in the health of his offspring, whose fitness benefits from extracting as much from the mother as possible, than he is in her long-term wellbeing.
Haig has been influential in depicting the foetus as a manipulative entity, conniving to direct the mother to its own advantage. Lactation might be evidence of this subtle control at work, and result from foetal cells that are commonly found in breast tissue signalling to the mother’s body to make milk. Haig also speculates that birth timing might owe to the silent influence of older siblings – which he describes as ‘the colonisation of maternal bodies by offspring cells’ – pushing the mother’s body to delay subsequent pregnancies. While there is nothing overtly harmful to the mother about lactation or delayed birth timing, by Haig’s rendering they are evidence of the parasitic control that a foetus wields over its mother, and the developing child’s efforts to claim the largest share of a presumably scant pie. He argues that microchimeric cells can extend inter-birth intervals beyond the mother’s optimum time frame, and cites as evidence a 2010 study showing that male births are more likely to be followed by multiple miscarriages .
Haig is quick to point out that these antagonisms are not an expression of feuding spouses, squabbling families or ongoing culture wars, but rather are playing out unconsciously through ‘genetic politics’. Nonetheless, there is a ready slippage between the interpretation of social behaviour and analyses of biological activity, and current research is ripe with hyperbole and bellicose metaphors.
If I am both my children and my mother, does that change who I am and the way I behave in the world?
An ‘evolutionary arms race’ is what Oliver Griffith, a postdoctoral associate at Yale University, calls pregnancy. He elaborates that mothers ‘marshal their best defensive tactics’ against offspring’s ‘strategies to steal resources’.
Harvey Kliman, a reproductive scientist at the Yale School of Medicine, makes the case that the placenta, which he proposes is controlled by the genes of the father, is at odds with the evolutionary aims of the mother. While the father’s goal is to make ‘the biggest placenta and the biggest baby possible’, the mother’s objective is to place limits on this growth so that she can survive childbirth. Kliman was part of a group that investigated the role of a protein dubbed PP13 in pre-eclampsia. During gestation, trophoblasts work to expand the mother’s arteries to bring blood flow and nutrients to the foetus. In an analysis of placentas from terminated pregnancies, the group found that PP13 was largely absent around these arteries, but that it was concentrated near the veins. They concluded that PP13 acts as a diversion, luring the mother’s immune cells to the veins and away from the placental expansion – Kliman uses the term ‘invasion’ – into the arteries. As Kliman put it to The New York Times in 2011: ‘Let’s say we’re planning to rob a bank, but before we rob the bank we blow up a grocery store a few blocks away so the police are distracted. That’s what we think this is.’
But as alluringly action-packed as these analogies are, they remain wholly speculative. And indeed, theories of conflict in and beyond the womb are just that. As the biologist Stephen Stearns at Yale has remarked : ‘the annals of research journals are littered with the corpses of beautiful ideas that were killed by facts’. At present, there is no definitive proof that the microchimeric activity, commonly described as conflict, combat or colonisation, reveals one entity pitted against the other. The assumption that the solitary organism strictly pursues goals of survival and genetic self-interest favours a parsimonious view of the individual: homo economicus operating in an environment of scarcity, in eternal competition with a nameless other.
The self emerging from microchimeric research appears to be of a different order: porous, unbounded, rendered constituently. Nelson suggests that each human being is not so much an isolated island as a dynamic ecosystem. And if this is the case, the question follows as to how this state of collectivity changes our conscious and unconscious motivations. If I am both my children and my mother, if I carry traces of my sibling and remnants of pregnancies that never resulted in birth, does that change who I am and the way I behave in the world? If we are to take to heart Whitman’s multitudes , we encounter an I composed of shared identity, collective affiliations and motivations that emerge not from a mean and solitary struggle, but a group investment in greater survival."
Innovation is overvalued. Maintenance often matters more | Aeon Essays,,https://aeon.co/essays/innovation-is-overvalued-maintenance-often-matters-more,"Innovation is a dominant ideology of our era, embraced in America by Silicon Valley, Wall Street, and the Washington DC political elite. As the pursuit of innovation has inspired technologists and capitalists, it has also provoked critics who suspect that the peddlers of innovation radically overvalue innovation. What happens after innovation, they argue, is more important. Maintenance and repair, the building of infrastructures, the mundane labour that goes into sustaining functioning and efficient infrastructures, simply has more impact on people’s daily lives than the vast majority of technological innovations.
The fates of nations on opposing sides of the Iron Curtain illustrate good reasons that led to the rise of innovation as a buzzword and organising concept. Over the course of the 20th century, open societies that celebrated diversity, novelty, and progress performed better than closed societies that defended uniformity and order.
In the late 1960s in the face of the Vietnam War, environmental degradation, the Kennedy and King assassinations, and other social and technological disappointments, it grew more difficult for many to have faith in moral and social progress. To take the place of progress, ‘innovation’, a smaller, and morally neutral, concept arose. Innovation provided a way to celebrate the accomplishments of a high-tech age without expecting too much from them in the way of moral and social improvement.
Before the dreams of the New Left had been dashed by massacres at My Lai and Altamont, economists had already turned to technology to explain the economic growth and high standards of living in capitalist democracies. Beginning in the late 1950s, the prominent economists Robert Solow and Kenneth Arrow found that traditional explanations – changes in education and capital, for example – could not account for significant portions of growth. They hypothesised that technological change was the hidden X factor. Their finding fit hand-in-glove with all of the technical marvels that had come out of the Second World War, the Cold War, the post-Sputnik craze for science and technology, and the post-war vision of a material abundance.
Robert Gordon’s important new book, The Rise and Fall of American Growth , offers the most comprehensive history of this golden age in the US economy. As Gordon explains, between 1870 and 1940, the United States experienced an unprecedented – and probably unrepeatable – period of economic growth. That century saw a host of new technologies and new industries produced, including the electrical, chemical, telephone, automobile, radio, television, petroleum, gas and electronics. Demand for a wealth of new home equipment and kitchen appliances, that typically made life easier and more bearable, drove the growth. After the Second World War, Americans treated new consumer technologies as proxies for societal progress – most famously, in the ‘Kitchen Debate’ of 1959 between the US vice-president Richard Nixon and the Soviet premier Nikita Kruschev. Critics wondered if Nixon was wise to point to modern appliances such as blenders and dishwashers as the emblems of American superiority.
Nevertheless, growth was strongly tied to continued social improvement. As older industries matured and declined, ‘new industries associated with new technologies’ would have to rise to take their place.
Y et, this need for booming new industries became problematic as the United States headed into the troubled times of the 1970s and early 1980s. Whole economic sectors, the auto industry, for example, hit the skids. A new term – ‘innovation policy’ – arose, designed to spur economic growth by fostering technological change, particularly in the face of international economic competition from Japan. Silicon Valley, a term that had just emerged in the late 1970s, became the exemplar of innovation during this time.
By the early 1980s, books casting Silicon Valley as a land of almost magical technological ingenuity had begun to hit the market. Innovation policy turned to focus more and more on ‘regional innovation systems’ and ‘innovation clusters’. Everywhere was potentially the next Silicon Valley of X. This theme of locality reached its apotheosis in Richard Florida’s 2002 book, The Rise of the Creative Class , which argued that regions succeeded by becoming the kinds of places that granola-crunching, mountain-bike-riding, computer-coding creative types wanted to live in. The book used the word ‘innovation’ more than 90 times and heavily idealised Silicon Valley.
During the 1990s, scholars and pop audiences also rediscovered the work of Joseph Schumpeter. Schumpeter was an Austrian economist who championed innovation and its partner term, entrepreneurship. Schumpeter pictured economic growth and change in capitalism as a ‘gale of creative destruction’, in which new technologies and business practices outmoded or totally destroyed old ones. Neo-Schumpeterian thought sometimes led to a mountain of dubious scholarship and magical thinking, most notably, Clayton M Christensen’s 1997 tome, The Innovator’s Dilemma: The Revolutionary Book that Will Change the Way You Do Business . Now mostly discredited, Christensen’s work exerted tremendous influence, with its emphasis on ‘disruptive’ technologies that undermined whole industries to make fortunes.
At the turn of the millennium, in the world of business and technology, innovation had transformed into an erotic fetish. Armies of young tech wizards aspired to become disrupters. The ambition to disrupt in pursuit of innovation transcended politics, enlisting liberals and conservatives alike. Conservative politicians could gut government and cut taxes in the name of spurring entrepreneurship, while liberals could create new programmes aimed at fostering research. The idea was vague enough to do nearly anything in its name without feeling the slightest conflict, just as long as you repeated the mantra: INNOVATION!! ENTREPRENEURSHIP!!
A professional innovation consultant advised his clients to ban the word at their companies. He said it was just a ‘word to hide the lack of substance’
A few years later, however, one could detect tremors of dissent. In a biting essay titled ‘Innovation is the New Black’, Michael Bierut, writing in Design Observer in 2005, lamented the ‘mania for innovation, or at least for endlessly repeating the word “innovation”’. Soon, even business publications began to raise the question of inherent worth. In 2006, The Economist noted that Chinese officials had made innovation into a ‘national buzzword’, even as it smugly reported that China’s educational system ‘stresses conformity and does little to foster independent thinking’, and that the Communist Party’s new catchphrases ‘mostly end up fizzling out in puddles of rhetoric’. Later that year, Businessweek warned: ‘Innovation is in grave danger of becoming the latest overused buzzword. We’re doing our part at Businessweek .’ Again in Businessweek , on the last day of 2008, the design critic Bruce Nussbaum returned to the theme, declaring that innovation ‘died in 2008, killed off by overuse, misuse, narrowness, incrementalism and failure to evolve… In the end, “Innovation” proved to be weak as both a tactic and strategy in the face of economic and social turmoil.’
In 2012, even the Wall Street Journal got into innovation-bashing act, noting ‘the Term Has Begun to Lose Meaning’. At the time, it counted ‘more than 250 books with “innovation” in the title… published in the last three months’ . A professional innovation consultant it interviewed advised his clients to ban the word at their companies. He said it was just a ‘word to hide the lack of substance’.
E vidence has emerged that regions of intense innovation also have systemic problems with inequality. In 2013, protests erupted in San Francisco over the gentrification and social stratification symbolised by Google buses and other private commuter buses. These shuttles brought high-tech employees from hip, pricey urban homes to their lush suburban campuses, without exposing them to the inconvenience of public transportation or to the vast populations of the poor and homeless who also call Silicon Valley their home. The dramatic, unnecessary suffering exposed by such juxtapositions of economic inequality seems to be a feature, not a bug of highly innovative regions.
The trajectory of ‘innovation’ from core, valued practice to slogan of dystopian societies, is not entirely surprising, at a certain level. There is a formulaic feel: a term gains popularity because it resonates with the zeitgeist, reaches buzzword status, then suffers from overexposure and cooptation. Right now, the formula has brought society to a question: after ‘innovation’ has been exposed as hucksterism, is there a better way to characterise relationships between society and technology?
There are three basic ways to answer that question. First, it is crucial to understand that technology is not innovation. Innovation is only a small piece of what happens with technology. This preoccupation with novelty is unfortunate because it fails to account for technologies in widespread use, and it obscures how many of the things around us are quite old. In his book, Shock of the Old (2007), the historian David Edgerton examines technology-in-use. He finds that common objects, like the electric fan and many parts of the automobile, have been virtually unchanged for a century or more. When we take this broader perspective, we can tell different stories with drastically different geographical, chronological, and sociological emphases. The stalest innovation stories focus on well-to-do white guys sitting in garages in a small region of California, but human beings in the Global South live with technologies too. Which ones? Where do they come from? How are they produced, used, repaired? Yes, novel objects preoccupy the privileged, and can generate huge profits. But the most remarkable tales of cunning, effort, and care that people direct toward technologies exist far beyond the same old anecdotes about invention and innovation.
Second, by dropping innovation, we can recognise the essential role of basic infrastructures. ‘Infrastructure’ is a most unglamorous term, the type of word that would have vanished from our lexicon long ago if it didn’t point to something of immense social importance. Remarkably, in 2015 ‘infrastructure’ came to the fore of conversations in many walks of American life. In the wake of a fatal Amtrak crash near Philadelphia, President Obama wrestled with Congress to pass an infrastructure bill that Republicans had been blocking, but finally approved in December 2015. ‘Infrastructure’ also became the focus of scholarly communities in history and anthropology, even appearing 78 times on the programme of the annual meeting of the American Anthropological Association. Artists, journalists, and even comedians joined the fray, most memorably with John Oliver’s hilarious sketch starring Edward Norton and Steve Buscemi in a trailer for an imaginary blockbuster on the dullest of subjects. By early 2016, the New York Review of Books brought the ‘earnest and passive word’ to the attention of its readers, with a depressing essay titled ‘A Country Breaking Down’.
Despite recurring fantasies about the end of work, the central fact of our industrial civilisation is labour , most of which falls far outside the realm of innovation
The best of these conversations about infrastructure move away from narrow technical matters to engage deeper moral implications. Infrastructure failures – train crashes, bridge failures, urban flooding, and so on – are manifestations of and allegories for America’s dysfunctional political system, its frayed social safety net, and its enduring fascination with flashy, shiny, trivial things. But, especially in some corners of the academic world, a focus on the material structures of everyday life can take a bizarre turn, as exemplified in work that grants ‘agency’ to material things or wraps commodity fetishism in the language of high cultural theory, slick marketing, and design. For example, Bloomsbury’s ‘Object Lessons’ series features biographies of and philosophical reflections on human-built things, like the golf ball. What a shame it would be if American society matured to the point where the shallowness of the innovation concept became clear, but the most prominent response was an equally superficial fascination with golf balls, refrigerators, and remote controls.
Third, focusing on infrastructure or on old, existing things rather than novel ones reminds us of the absolute centrality of the work that goes into keeping the entire world going. Despite recurring fantasies about the end of work or the automation of everything, the central fact of our industrial civilisation is labour , and most of this work falls far outside the realm of innovation. Inventors and innovators are a small slice – perhaps somewhere around one per cent – of this workforce. If gadgets are to be profitable, corporations need people to manufacture, sell, and distribute them. Another important facet of technological labour comes when people actually use a product. In some cases, the image of the ‘user’ could be an individual like you, sitting at your computer, but in other cases, end users are institutions – companies, governments, or universities that struggle to make technologies work in ways that their inventors and makers never envisioned.
T he most unappreciated and undervalued forms of technological labour are also the most ordinary: those who repair and maintain technologies that already exist, that were ‘innovated’ long ago. This shift in emphasis involves focusing on the constant processes of entropy and un-doing – which the media scholar Steven Jackson calls ‘broken world thinking’ – and the work we do to slow or halt them, rather than on the introduction of novel things. In recent years, scholars have produced a number of studies of people who do this kind of work. For example, the science studies researcher Lilly Irani has examined the work low-wage labourers do to scrub digital information for the web, including Indian workers who check advertisements to ‘filter out porn, alcohol, and violence’. Why not extend this style of analysis to think more clearly about subjects such as ‘cybersecurity’? The need for coders and programmers in the cybersecurity field is obvious, but it should be equally obvious that fundamental vulnerabilities in our cyber-infrastructures are protected by the guards who work graveyard shifts and staff who repair fences and ID card-readers.
We can think of labour that goes into maintenance and repair as the work of the maintainers , those individuals whose work keeps ordinary existence going rather than introducing novel things. Brief reflection demonstrates that the vast majority of human labour, from laundry and trash removal to janitorial work and food preparation, is of this type: upkeep. This realisation has significant implications for gender relations in and around technology. Feminist theorists have long argued that obsessions with technological novelty obscures all of the labour, including housework, that women, disproportionately, do to keep life on track. Domestic labour has huge financial ramifications but largely falls outside economic accounting, like Gross Domestic Product. In her classic 1983 book, More Work for Mother , Ruth Schwartz Cowan examined home technologies – such as washing machines and vacuum cleaners – and how they fit into women’s ceaseless labour of domestic upkeep. One of her more famous findings was that new housekeeping technologies, which promised to save labour, literally created more work for mother as cleanliness standards rose, leaving women perpetually unable to keep up.
There is no point in keeping the practice of hero-worship that merely changes the cast of heroes without confronting the deeper problems
Nixon, wrong about so many things, also was wrong to point to household appliances as self-evident indicators of American progress. Ironically, Cowan’s work first met with scepticism among male scholars working in the history of technology, whose focus was a male pantheon of inventors: Bell, Morse, Edison, Tesla, Diesel, Shockley, and so on. A renewed focus on maintenance and repair also has implications beyond the gender politics that More Work for Mother brought to light. When they set innovation-obsession to the side, scholars can confront various kinds of low-wage labour performed by many African-Americans, Latinos, and other racial and ethnic minorities. From this perspective, recent struggles over increasing the minimum wage, including for fast food workers, can be seen as arguments for the dignity of being a maintainer.
We organised a conference to bring the work of the maintainers into clearer focus. More than 40 scholars answered a call for papers asking, ‘What is at stake if we move scholarship away from innovation and toward maintenance?’ Historians, social scientists, economists, business scholars, artists, and activists responded. They all want to talk about technology outside of innovation’s shadow.
One important topic of conversation is the danger of moving too triumphantly from innovation to maintenance. There is no point in keeping the practice of hero-worship that merely changes the cast of heroes without confronting some of the deeper problems underlying the innovation obsession. One of the most significant problems is the male-dominated culture of technology, manifest in recent embarrassments such as the flagrant misogyny in the ‘#GamerGate’ row a couple of years ago, as well as the persistent pay gap between men and women doing the same work.
There is an urgent need to reckon more squarely and honestly with our machines and ourselves. Ultimately, emphasising maintenance involves moving from buzzwords to values, and from means to ends. In formal economic terms, ‘innovation’ involves the diffusion of new things and practices. The term is completely agnostic about whether these things and practices are good. Crack cocaine, for example, was a highly innovative product in the 1980s, which involved a great deal of entrepreneurship (called ‘dealing’) and generated lots of revenue. Innovation! Entrepreneurship! Perhaps this point is cynical, but it draws our attention to a perverse reality: contemporary discourse treats innovation as a positive value in itself, when it is not.
Entire societies have come to talk about innovation as if it were an inherently desirable value, like love, fraternity, courage, beauty, dignity, or responsibility. Innovation-speak worships at the altar of change, but it rarely asks who benefits, to what end? A focus on maintenance provides opportunities to ask questions about what we really want out of technologies. What do we really care about? What kind of society do we want to live in? Will this help get us there? We must shift from means, including the technologies that underpin our everyday actions, to ends, including the many kinds of social beneficence and improvement that technology can offer. Our increasingly unequal and fearful world would be grateful."
The selfish gene is a great meme. Too bad it’s so wrong | Aeon Essays,,https://aeon.co/essays/the-selfish-gene-is-a-great-meme-too-bad-it-s-so-wrong,"A couple of years ago, at a massive conference of neuroscientists — 35,000 attendees, scores of sessions going at any given time — I wandered into a talk that I thought would be about consciousness but proved (wrong room) to be about grasshoppers and locusts. At the front of the room, a bug-obsessed neuroscientist named Steve Rogers was describing these two creatures — one elegant, modest, and well-mannered, the other a soccer hooligan.
The grasshopper, he noted, sports long legs and wings, walks low and slow, and dines discreetly in solitude. The locust scurries hurriedly and hoggishly on short, crooked legs and joins hungrily with others to form swarms that darken the sky and descend to chew the farmer’s fields bare.
Related, yes, just as grasshoppers and crickets are. But even someone as insect-ignorant as I could see that the hopper and the locust were radically different animals — different species, doubtless, possibly different genera. So I was quite amazed when Rogers told us that grasshopper and locust are in fact the same species, even the same animal, and that, as Jekyll is Hyde, one can morph into the other at alarmingly short notice.
Not all grasshopper species, he explained (there are some 11,000), possess this morphing power; some always remain grasshoppers. But every locust was, and technically still is, a grasshopper — not a different species or subspecies, but a sort of hopper gone mad. If faced with clues that food might be scarce, such as hunger or crowding, certain grasshopper species can transform within days or even hours from their solitudinous hopper states to become part of a maniacally social locust scourge. They can also return quickly to their original form.
In the most infamous species, Schistocerca gregaria, the desert locust of Africa, the Middle East and Asia, these phase changes (as this morphing process is called) occur when crowding spurs a temporary spike in serotonin levels, which causes changes in gene expression so widespread and powerful they alter not just the hopper’s behaviour but its appearance and form. Legs and wings shrink. Subtle camo colouring turns conspicuously garish. The brain grows to manage the animal’s newly complicated social world, which includes the fact that, if a locust moves too slowly amid its million cousins, the cousins directly behind might eat it.
How does this happen? Does something happen to their genes? Yes, but — and here was the point of Rogers’s talk — their genes don’t actually change. That is, they don’t mutate or in any way alter the genetic sequence or DNA. Nothing gets rewritten. Instead, this bug’s DNA — the genetic book with millions of letters that form the instructions for building and operating a grasshopper — gets reread so that the very same book becomes the instructions for operating a locust. Even as one animal becomes the other, as Jekyll becomes Hyde, its genome stays unchanged. Same genome, same individual, but, I think we can all agree, quite a different beast.
Why?
T ransforming the hopper is gene expression — a change in how the hopper’s genes are ‘expressed’, or read out. Gene expression is what makes a gene meaningful, and it’s vital for distinguishing one species from another. We humans, for instance, share more than half our genomes with flatworms; about 60 per cent with fruit flies and chickens; 80 per cent with cows; and 99 per cent with chimps. Those genetic distinctions aren’t enough to create all our differences from those animals — what biologists call our particular phenotype, which is essentially the recognisable thing a genotype builds. This means that we are human, rather than wormlike, flylike, chickenlike, feline, bovine, or excessively simian, less because we carry different genes from those other species than because our cells read differently our remarkably similar genomes as we develop from zygote to adult. The writing varies — but hardly as much as the reading.
This raises a question: if merely reading a genome differently can change organisms so wildly, why bother rewriting the genome to evolve? How vital, really, are actual changes in the genetic code? Do we always need DNA changes to adapt to new environments? Are there other ways to get the job done? Is the importance of the gene as the driver of evolution being overplayed?
You’ve probably noticed that these questions are not gracing the cover of Time or haunting Oprah , Letterman , or even TED talks. Yet for more than two decades they have been stirring a heated argument among geneticists and other evolutionary theorists. As evidence of the power of rapid gene expression and other complex genomic dynamics mounts, these questions might (or might not, for pesky reasons we’ll get to) begin to change not only mainstream evolutionary theory but our more everyday understanding of evolution.
Twenty years ago, phase changes such as those that turn grasshopper to locust were relatively unknown, and, outside of botany anyway, rarely viewed as changes in gene expression. Now, notes Mary Jane West-Eberhard, a wasp researcher at the Smithsonian Tropical Research Institute in Panama, sharp phenotype changes due to gene expression are ‘everywhere’. They show up in gene-expression studies of plants, microbes, fish, wasps, bees, birds, and even people. The genome is continually surprising biologists with how fast and fluidly it can change gene expression — and thus phenotype.
These discoveries closely follow the recognition, during the 1980s, that gene-expression changes during very early development — such as in embryos or sprouting plant seeds — help to create differences between species. At around the same time, genome sequencing began to reveal the startling overlaps mentioned above between the genomes of starkly different creatures. (To repeat: you are 80 per cent cow.)
Shapeshifter: the locust. Photo by Ocean/Corbis
Gregory Wray, a biologist at Duke University in North Carolina who studies fruit flies, sees this flexibility of genomic interpretation as a short path to adaptive flexibility. When one game plan written in the book can’t provide enough flexibility, fast changes in gene expression — a change in the book’s reading — can provide another plan that better matches the prevailing environment.
‘Different groups of animals succeed for different reasons,’ says Wray. ‘Primates, including humans, have succeeded because they’re especially flexible. You could even say flexibility is the essence of being a primate.’
According to Wray, West-Eberhard and many others, this recognition of gene expression’s power, along with other dynamics and processes unanticipated by mainstream genetic theory through the middle of last century, requires that we rethink and expand the way we view genes and evolution. For a century, the primary account of evolution has emphasised the gene’s role as architect: a gene (or gene variant) creates a trait that either proves advantageous or not, and is thus selected for, changing a species for the better, or not. Thus, a genetic blueprint creates traits and drives evolution.
This gene-centric view, as it is known, is the one you learnt in high school. It’s the one you hear or read of in almost every popular account of how genes create traits and drive evolution. It comes from Gregor Mendel and the work he did with peas in the 1860s. Since then, and especially over the past 50 years, this notion has assumed the weight, solidity, and rootedness of an immovable object.
But a number of biologists argue that we need to replace this gene-centric view with one that more heavily emphasises the role of more fluid, environmentally dependent factors such as gene expression and intra-genome complexity — that we need to see the gene less as an architect and more as a member of a collaborative remodelling and maintenance crew.
‘We have a more complicated understanding of football than we do genetics and evolution. Nobody thinks just the quarterback wins the game’
They ask for something like the rejection a century ago of the Victorian-era ‘Great Man’ model of history. This revolt among historians recast leaders not as masters of history, as Tolstoy put it, but as servants. Thus the Russian Revolution exploded not because Marx and Lenin were so clever, but because fed-up peasants created an impatience and an agenda that Marx articulated and Lenin ultimately hijacked. Likewise, D-Day succeeded not because Eisenhower was brilliant but because US and British soldiers repeatedly improvised their way out of disastrously fluid situations. Wray, West-Eberhard and company want to depose genes likewise. They want to cast genes not as the instigators of change, but as agents that institutionalise change rising from more dispersed and fluid forces.
This matters like hell to people like West-Eberhard and Wray. Need it concern the rest of us?
It should. We are rapidly entering a genomic age. A couple of years ago, for instance, I became one of what is now almost a half-million 23andMe customers, paying the genetic-profiling company to identify hundreds of genetic variants that I carry. I now know ‘genes of interest’ that reveal my ancestry and help determine my health. Do I know how to make sense of them? Do they even make sense? Sometimes; sometimes not. They tell me, for instance, that I’m slightly more likely than most to develop Alzheimer’s disease, which allows me to manage my health accordingly. But those genes also tell me I should expect to be short and bald, when in fact I’m 6’3” with a good head of hair.
Soon, it will be practical to buy my entire genome. Will it tell me more? Will it make sense? Millions of people will face this puzzle. Along with our doctors, we’ll draw on this information to decide everything from what drugs to take to whether to have kids, including kids a few days past conception — a true make-or-break decision.
Yet we enter this genomic age with a view of genetics that, were we to apply it, say, to basketball, would reduce that complicated team sport to a game of one-on-one. A view like that can be worse than no view. It tempts you to think you understand the game when you don’t. We need something more complex.
‘And it’s not as if people can’t handle things more complex,’ says Wray. ‘Educated people handle ideas more complex than this all the time. We have a more complicated understanding of football than we do genetics and evolution. Nobody thinks just the quarterback wins the game.
‘We’re stuck in an outmoded way of thinking that should have fallen long ago.’
T his outmoded thinking grew from seeds planted 150 years ago by Gregor Mendel, the monk who studied peas. Mendel spent seven years breeding peas in a five-acre monastery garden in the town of Brno, now part of the Czech Republic. He crossed plants bearing wrinkled peas with those bearing smooth peas, producing 29,000 plants altogether. When he was done and he had run the numbers, he had exposed the gene.
This was the Holy Shit! moment that launched genetics’ Holy Shit! century
Mendel didn’t expose the physical gene, of course (that would come a century later), but the conceptual gene. And this conceptual gene, revealed in the tables and calculations of this math-friendly monk, seemed an agent of mathematical neatness. Mendel’s thousands of crossings showed that the traits he studied — smooth skin versus wrinkled, for instance, or purple flower versus white — appeared or disappeared in consistent ratios dictated by clear mathematical formulas. Inheritance appeared to work like algebra. Anything so math-friendly had to be driven by discrete integers.
It was beautiful work. Yet when Mendel first published his findings in 1866, just seven years after Charles Darwin’s On the Origin of Species, no one noticed. Starting in 1900, however, biologists rediscovering his work began to see that these units of heredity he’d discovered — dubbed genes in 1909 — filled a crucial gap in Darwin’s theory of evolution. This recognition was the Holy Shit! moment that launched genetics’ Holy Shit! century. It seemed to explain everything. And it saved Darwin.
Darwin had legitimised evolution by proposing for it a viable mechanism — natural selection, in which organisms with the most favourable traits survive and multiply at higher rates than do others. But he could not explain what created or altered traits.
Mendel could. Genes created traits, and both would spread through a population if a gene created a trait that survived selection.
That much was clear by 1935. Naturally, some kinks remained, but more math-friendly biologists soon straightened those out. This took most of the middle part of the 20th century. Biologists now call this decades-long project the modern evolutionary synthesis. And it was all about maths.
The first vital calculations were run in the 1930s, when Ronald Fisher, J B S Haldane and Sewall Wright, two Brits and an American working more or less separately, worked out how Mendel’s rather binary genetic model could create not just binary differences such as smooth versus wrinkled peas but the gradual evolutionary change of the sort that Darwin described. Fisher, Haldane and Wright, working the complicated maths of how multiple genes interacted through time in a large population, showed that significant evolutionary change often revealed itself as many small changes yielded a large effect, just as a series of small nested equations within a long algebra equation could.
The second kink was tougher. If organisms prospered by out-competing others, why did humans and some other animals help one another? This might seem a non-mathy problem. Yet in the 1960s, British biologist William Hamilton and American geneticist George Price, who was working in London at the time, solved it too with maths, devising formulas quantifying precisely how altruism could be selected for. Some animals act generously, they explained, because doing so can aid others, such as their children, parents, siblings, cousins, grandchildren, or tribal mates, who share or might share some of their genes. The closer the kin, the kinder the behaviour. Thus, as Haldane once allegedly quipped, ‘I would lay down my life for two brothers or eight cousins.’
Thus maths reconciled Mendel and Darwin and made modern genetics and evolutionary theory a coherent whole. Watson and Crick’s 1953 discovery of the structure of DNA simply iced the cake: now we knew the structure that performed the maths.
Finally, also in the 1960s, Hamilton and American George Williams upped the ante on the gene’s primacy. With fancy maths, they argued that we should view any organism, including any human, as merely a sort of courier for genes and their traits. This flipped the usual thinking. It made the gene vital and the organism expendable. Our genes did not exist for us. We existed for them. We served only to carry these chemical codes forward through time, like those messengers in old sword-and-sandal war movies who run non-stop for days to deliver data and then drop dead. A radical idea. Yet it merely extended the logic of kin selection, in which any gene-courier — say, a mom watching her children’s canoe overturn — would risk her life to let her kin carry forth her DNA.
This notion of the gene as the unit selected, and the organism as a kludged-up cart for carrying it through time, placed the gene smack at the centre of things. It granted the gene something like agency.
At first, not even many academics paid this any heed. This might be partly because people resist seeing themselves as donkey carts. Another reason was that neither Hamilton nor Williams were masterly communicators.
But 15 years after Hamilton and Williams kited this idea, it was embraced and polished into gleaming form by one of the best communicators science has ever produced: the biologist Richard Dawkins. In his magnificent book The Selfish Gene (1976), Dawkins gathered all the threads of the modern synthesis — Mendel, Fisher, Haldane, Wright, Watson, Crick, Hamilton, and Williams — into a single shimmering magic carpet.
These days, Dawkins makes the news so often for things like pointing out that a single college in Cambridge has won more Nobel Prizes than the entire Muslim world, that some might wonder how he ever became so celebrated. The Selfish Gene is how. To read The Selfish Gene is to be amazed, entertained, transported. For instance, when Dawkins describes how life might have begun — how a randomly generated strand of chemicals pulled from the ether could happen to become a ‘replicator’, a little machine that starts to build other strands like itself, and then generates organisms to carry it — he creates one of the most thrilling stretches of explanatory writing ever penned. It’s breathtaking.
Dawkins reveals the gene as not just the centre of the cell but the centre of all life, agency, and behaviour
Dawkins assembles genetics’ dry materials and abstract maths into a rich but orderly landscape through which he guides you with grace, charm, urbanity, and humour. He replicates in prose the process he describes. He gives agency to chemical chains, logic to confounding behaviour. He takes an impossibly complex idea and makes it almost impossible to misunderstand. He reveals the gene as not just the centre of the cell but the centre of all life, agency, and behaviour. By the time you’ve finished his book, or well before that, Dawkins has made of the tiny gene — this replicator, this strip of chemicals little more than an abstraction — a huge, relentlessly turning gearwheel of steel, its teeth driving smaller cogs to make all of life happen.
It’s a gorgeous story. Along with its beauty and other advantageous traits, it is amenable to maths and, at its core, wonderfully simple. It has inspired countless biologists and geneticists to plumb the gene’s wonders and do brilliant work. Unfortunately, say Wray, West-Eberhard and many others, the selfish-gene story is so focused on the gene’s singular role in natural selection that in an age when it’s ever more clear that evolution works in ways far more clever and complex than we realise, the selfish-gene model increasingly impoverishes both scientific and popular views of genetics and evolution. As both conceptual framework and metaphor, the selfish-gene has helped us see the gene as it revealed itself over the 20th century. But as a new age and new tools reveal a more complicated genome, the selfish-gene is blinding us.
F or over two decades, Wray, West-Eberhard and other evolutionary theorists — such as Massimo Pigliucci, professor of philosophy at the City University of New York; Eva Jablonka, a geneticist and historian of science at Tel Aviv University, London; Stuart Kauffman, professor of biochemistry and mathematics at the University of Vermont; Stuart A Newman, professor of cell biology and anatomy at the New York Medical College; and the late Stephen Jay Gould, to name a few — have been calling for an ‘extended modern synthesis’ to replace the gene-centric view of evolution with something richer. They do so even though they agree with most of what Dawkins says a gene does. They agree, in essence, that the gene is a big cog, but would argue that the biggest cog doesn’t necessarily always drive the other cogs. In many cases, the other cogs drive the gene. The gene, in short, just happens to be the biggest, most obvious part of the trait-making inheritance and evolutionary machine. But not the driver.
Another way to put it: Mendel stumbled over the wrong chunk of gold.
Mendel ran experiments that happened to reveal strong single-gene dynamics whose effects — flower colour, skin texture — can seem far more significant than they really are. Many plant experiments since then, for instance, have shown that environmental factors such as temperature changes can spur gene-expression changes that alter a plant far more than Mendel’s gene variants do. As with grasshoppers, a new environment can quickly turn a plant into something almost unrecognisable from its original form. If Mendel had owned an RNA sequencing machine and was in the habit of tracking gene expression changes, he might have spotted these. But sequencers didn’t exist, so he crossed plants instead, and saw just one particularly obvious way that an organism can change.
The gene-centric view is thus ‘an artefact of history’, says Michael Eisen, an evolutionary biologist who researches fruit flies at the University of California, Berkeley. ‘It rose simply because it was easier to identify individual genes as something that shaped evolution. But that’s about opportunity and convenience rather than accuracy. People confuse the fact that we can more easily study it with the idea that it’s more important.’
The gene’s power to create traits, says Eisen, is just one of many evolutionary mechanisms. ‘Evolution is not even that simple. Anyone who’s worked on systems sees that natural selection takes advantage of the most bizarre aspects of biology. When something has so many parts, evolution will act on all of them.
‘It’s not that genes don’t sometimes drive evolutionary change. It’s that this mutational model — a gene changes, therefore the organism changes — is just one way to get the job done. Other ways may actually do more.’
Like what other ways? What significant and plausible evolutionary dynamics stand in tension with a single-gene-centred model? What gets obscured by the insistence that a ‘selfish gene’, a coherent, solitary replicator, is the irreducible and ever-present driver of evolution?
A shortlist of such dynamics would include some of the evolutionary dynamics being proposed by anthropologists, such as cultural transmission of knowledge and behaviour that allow social species ranging from bees to humans to adapt to changing environments without genetic alterations; and culture-gene evolution, a related idea, in which culture is not the ‘handmaiden’ of genes, but another source of transmissible adaptive information whose elements co-evolves with genes, each affecting the other.
Also in tension with the selfish-gene model are epigenetic changes suggested by recent research, such as methylation and other alterations to chemical wrappings around DNA, that can modulate DNA’s expression without changing its sequence. Such epigenetic changes may provide a way to pass heritable traits down through at least a few generations without changing any actual genes. To be sure, this research is still unproven as a significant evolutionary force. But while it is clearly important enough to pursue, many defenders of the selfish-gene model dismiss it out of hand.
Finally, the selfish-gene model is in tension with various ‘interesting evolutionary phenomena’, as Gregory Wray puts it in Evolution: The Extended Synthesis , ‘that are apparent only at the scale of hundreds or thousands of genes’ — a scale only made viewable during the past decade or so, as we’ve learnt to rapidly sequence entire genomes.
Of these genomic dynamics, perhaps the most challenging to the selfish-gene story are epistatic or gene-gene interactions. Epistasis refers to the fact that the presence of some genes (or their variants) can have profound and unpredictable influences on the activity and effects generated by other genes. To put it another way, a gene’s effect can vary wildly depending on which combination of other genes it finds itself with. (Think Jerry Garcia playing with different musical partners.)
Epistasis is hardly a new concept. In fact, geneticists have been arguing about its importance ever since R.A. Fisher and Sewall Wright bickered about it in the 1920s. Dawkins acknowledges a role for gene-gene interactions in The Selfish Gene , noting that ‘the effect of any one gene depends on interaction with many others.’ But research since then show that these interactions take place in non-linear, non-additive ways of a complexity impossible to understand at the time Dawkins wrote his book. Casey Greene and Jason Moore of Dartmouth, for instance, recently found that in some cases epistatic interactions seem to warp conventional gene-trait relationships so profoundly that they can often negate the gene as a trait’s reliable carrier.
Individual bees morph from worker to guard to scout by gene expression alone, depending on the needs of the hive
This is not merely a matter of one gene muffling or amplifying another, though both these things happen. And it’s not a matter of additive effects, such as four ‘tall’ genes making you taller than would two. Rather, these multi-gene epistatic interactions can create endless possible combinations of mutual influence in which any given gene’s contribution seems to rise less from its inherent trait-making power than from what company that gene finds itself keeping. To draw on P.Z. Myers’ apt analogy, epistasis means that single genes often carry little more inherent significance than individual playing cards do in poker. In a poker hand, the significance and effect of a two of hearts — its ‘trait’ — depend so heavily on the other cards you’re holding that it’s almost meaningless to say the card has any replicable power on its own. It’s replicable in that it’s a two of hearts every time it’s dealt. But it can deliver the same effect in subsequent generations only if it’s dealt not just into the exact same handful of cards, but into a round in which all the other players at the table also hold the same cards as before — and happen to bet, hold, and fold in exactly the same way. Not something to count on.
And a two of hearts is a far more coherent thing than is a gene. One of the peskiest problems of leaning too heavily on a gene-centric model these days is that the definition of the word ‘gene’ gets ever more various and slippery.
Even as a technical term, the word carries at least a half-dozen meanings, and more are added as science finds new tools for exploring the genome. This alone makes it either a poor candidate for a popular meme — or, if you value flexibility over exactitude, perhaps a perfect one, since its meaning can be defended or reshaped or expanded to suit the occasion. If you expand the meaning to be ‘the thing essential to all true heredity and selection’, you can then give the gene primary credit for any discovered or proposed evolutionary force in which the gene seems to be involved — and reject outright any proposed evolutionary force that doesn’t seem to involve genes.
But the gene’s definition is not just semantically vague. As geneticists explore the genome’s previously uncharted stretches, they’re finding that a lot of the work conventionally attributed to ‘genes’ (in the sense of consistent, reasonably well defined clusters of DNA) appears to be done instead by networks of genes and strange DNA elements that doubly defy the selfish-gene model.
These regulatory networks challenge the selfish-gene model first because they include DNA elements not conventionally defined as genes. More important, some researchers believe these networks challenge the selfish-gene model because they often seem to behave not like selfish entities balancing their separate agendas, in selfish-gene style, but like managerial teams regulating the behaviour of individual genes for the interest of the organism. The chromosome’s three-dimensional nature brings those regulatory chunks into contact with individual genes in highly unpredictable ways. With each gene ‘surrounded by an ocean’ of such regulatory elements, as molecular biophysicist Joe Dekker told WIRED, each gene ‘can touch and interact with a whole collection of them’. Yale geneticist Mark Gerstein found that the genes in these networks sometimes seem to get selected for even if they don’t have important effects on their own. In other cases they seem to have effects but be exempt from selection pressure.
These regulatory elements now appear to grossly outnumber the actual genes, possibly by as much as 50 to 1. As Yale geneticist Mark Gerstein politely notes, the complexity of these regulatory networks, along with their ad hoc management-team nature, raise the question of what’s being selected: individual genes, as the selfish-gene model proposes, or the management team, by some process still hidden amid all this complexity. Others, such as Cold Spring Harbor geneticist Thomas Gingeras, question outright whether the transcript (the marching orders a gene issues to begin gene expression) should replace the gene as the genome’s functional unit. These issues are not merely academic; resolving them could help solve mysteries about cancer and other diseases.
Such dynamics have emerged only in the last decade or so, as researchers have been able to examine the genome more closely. Yet even though we so far ‘have only a dim idea of how all this works’, as Gregory Wray wrote in 2010, it ‘is clear … that these kinds of assumption-violating exceptions are not rare.’
Wray’s language here is crucial: he’s not saying these findings refute the details of the gene-centric model. He’s saying they violate the model’s assumptions.
And this is the crux of this entire dispute: The point is not whether the findings of a genomic age or of anthropology refute the selfish-gene model, invalidate its theoretical details, or debunk the modern synthesis. Mostly they don’t. The selfish-gene model is roomy enough to host many of these findings. It has shown a uncanny ability to do so. But as time passes it does so ever more uncomfortably, for both host and guests. Some findings or ideas must be almost forced in. Others get prematurely locked out.
The selfish-gene model and metaphor can probably be stretched even more to account for some of these things. But in an age when assumption-violating ideas from genomic studies, anthropology, and other fields are flourishing, does the selfish gene story remain the best way to account for them? Does it make sense to attach these proliferating findings and ideas on to the selfish-gene story as appendices? Or is it time to find another story? It may be that the gene is always a player. But it is rarely the only player. And — may I speak metaphorically? — it may (or may not) be that the gene always behaves as if it were selfish. But that doesn’t mean it always gets its way.
O ne of the assumption-violating exceptions Wray refers to is gene expression’s breadth of power. In the social wasps that Mary Jane West-Eberhard has been studying in Panama since 1979, many of the most important distinctions among a colony’s individuals rise not from differences in their genomes, which vary little, but from the plasticity born of gene expression. This starts with the queen, who is genetically identical to her thousands of sisters yet whose gene expression makes her not only larger, but singles her out as the colony’s reproductive unit. Likewise with most honeybees. In social honeybees, the differences between workers, guards, and scouts all arise from gene expression, not gene sequence. Individual bees morph from one form to another — worker to guard to scout — by gene expression alone, depending on the needs of the hive.
As described above, the questionable coherence of genes seems to apply especially to gene regulation — as do epistatic networks that further undermine the gene’s primacy. So while it’s clear that DNA plays a key role in regulating gene expression, it is not clear that all these ‘regulatory genes’ are the selfish genes of the Dawkins model.
This is but one reason why West-Eberhard, among others, has been long trying to cure the ‘cyclic amnesia’ that she says has ignored 150 years of evidence that the gene’s centrality is overplayed. West-Eberhard is a particularly articulate advocate. Yet she’s frustrated at how little she’s been able to change things.
As a David to Dawkins’s Goliath, West-Eberhard faces distinct challenges. For starters, she’s a she while Dawkins is a he, which should not matter but does. And while Dawkins holds forth from Oxford, one of the most prestigious universities on earth, and deploys from London an entire foundation in his name, West-Eberhard studies and writes from a remote outpost in Central America. Dawkins commands locust-sized audiences any time he speaks and probably turns down enough speaking engagements to fill five calendars; West-Eberhard speaks mainly to insect-crazed colleagues at small conferences. Dawkins wrote a delicious 300-page book that has sold tens of millions of copies; West-Eberhard has written a bunch of fine obscure papers and an 800-page tome, Developmental Plasticity and Evolution (2003), which, though not without its sweet parts, is generally consumed as a meal of obligation.
She does have her pithy moments. There are times, she says, when ‘the gene does not lead. It follows.’
Massimo Piglucci and Gerd Muller use the same language in Evolution: The Extended Synthesis . By ‘the gene follows’, they mean that in complex organisms particularly, dynamics other than gene alterations, ranging from gene expression to complex gene regulation to developmental pathways formed by culture, can create heritable adaptations that either remain on their own or later become ‘fixed’ or locked in by genes.
One way in which the gene follows is through genetic assimilation — a clunky term for a graceful process. This can look Lamarckian, but it is not. It’s the development of a heritable change through flexible gene-expression responses that later get ‘fixed,’ or locked in, by a change in genotype. It takes a moment to explain. But let’s give it a run.
Genetic assimilation involves a three-step process.
First, an organism adapts to a changing environment by altering its gene expression to change its phenotype — its form or behaviour. Second, a gene emerges that locks in that phenotypic change. Finally, the gene spreads through the population.
For example, suppose you’re a predator. You live with others of your ilk in dense forest. Your kind hunts by stealth: you hide among trees, then jump out and snag your meat. You needn’t be fast, just sneaky and quick off the mark.
They didn’t inherit your speed in any Lamarckian way. Rather, like you, they simply developed it through gene expression driven by running so much
Then a big event — maybe a forest fire, or a plague that kills all your normal prey — forces you into a new environment. This new place is more open, which nixes your jump-and-grab tactic, but contains juicy little wild pigs that you can outrun if you sprint really hard. You start running down these critters. As you do, certain genes ramp up expression to build more muscle and fire the muscles more quickly. You get faster. You’re becoming a different animal.
You mate with another hunter. Your kids grow up to hunt with you. Since they hunt and practice hunting from early on, they too become fast — maybe faster than you, since they started younger. They didn’t inherit your speed in any Lamarckian way. Rather, like you, they simply developed it through gene expression driven by running so much. The same thing happens with their children: they run early, so they’re fast. Their speed is environmentally dependent. Your descendants keep this greater speed (greater, that is, than your ancestors’) for as long as they’re running down little pigs. The hunt makes them faster. But if they could get meals without sprinting, their speed would fade.
Now comes the second step: Several generations down the line, a beneficial mutation occurs in one of your descendants. Most mutations are neutral and many are bad. But this one’s good: It creates faster muscle fibres that let this descendant of yours — let’s call her Diana — easily run faster than her fastest siblings and cousins ever could. She flies.
Finally comes the third step: Diana’s children inherit the gene, as do some of theirs, and because their speed wows their mating prospects, Diana’s descendants mate early and often, bearing many kids. Thus this runner’s gene spreads through the generations until it becomes fixed in the population.
Now the thing is complete. An adaptive trait you originally developed through gene expression alone is made more permanent in your descendants by a new gene. Had the gene showed up back when you lived in the forest and speed didn’t mean anything, it would have given no advantage, and instead of being selected for, that speed gene would have disappeared or remained present but uncommon. But because hunting gave the gene value, the population took it in and spread it wide. The gene didn’t drive the train; it hopped aboard.
This isn’t the gene-centric world in which genotype creates phenotype. It’s a phenotype accommodating a new genotype by making it valuable.
Genetic assimilation was recognised as a possibility in the 1940s, but as Massimo Pigliucci and Courtney Murren put it, it was ‘attacked as of minor importance during the ‘hardening’ of the neo-Darwinian synthesis and … relegated to a secondary role for decades’. Interest has surged lately as gene expression becomes more apparent, and biologists are starting to spot the process in the field. No one proposes that genetic assimilation happens all the time or even commonly, or that it widely replaces conventional gene-driven evolution. But its existence suggests how gene expression’s fluidity can combine with conventional genetic dynamics to broaden evolution’s reach.
Gene Robinson, an entomologist who studies honeybees at the University of Illinois, says genetic assimilation could well have helped to create African honeybees, the ‘killer bee’ subspecies that is genetically distinct from the sweeter European honeybees that most beekeepers keep. Honeybee hives in certain parts of Africa, he says, were and are raided by predators more often than hives elsewhere, so their inhabitants had to react more sharply to attacks. This encouraged gene-expression changes that made the African bees respond more aggressively to threat. When new genes showed up that reinforced this aggression, those genes would have been selected for and spread through the population. This, Robinson says, is quite likely how African bees became genetically distinct from their European honeybee cousins. And they’d have been led there not by a gene, but by gene expression.
A fter several weeks of reading and talking to this phenotypic plasticity crowd, I phoned Richard Dawkins to see what he thought of all this. Did genes follow rather than lead? I asked him specifically about whether processes such as gene assimilation might lead instead. He said that genetic assimilation doesn’t really change anything, because since the gene ends up locking in the change and carrying it forward, it all comes back to the gene anyway.
‘This doesn’t modify the gene-centric model at all,’ he said. ‘The gene-centric model is all about the gene being the unit in the hierarchy of life that is selected. That remains the gene.’
‘He’s backfilling,’ said West-Eberhard. ‘He and others have long been arguing for the primacy of an individual gene that creates a trait that either survives or doesn’t.’
Yet West-Eberhard understands why many biologists stick to the gene-centric model. ‘It makes it easier to explain evolution,’ she says. ‘I’ve seen people who work in gene expression who understand all of this. But when they get asked about evolution, they go straight to Mendel. Because people understand it more easily.’ It’s easy to see why: even though life is a zillion bits of biology repeatedly rearranging themselves in a webwork of constantly modulated feedback loops, the selfish-gene model offers a step-by-step account as neat as a three-step flow chart. Gene, trait, phenotype, done.
In other words, the gene-centric model survives because simplicity is a hugely advantageous trait for an idea to possess. People will select a simple idea over a complex idea almost every time. This holds especially in a hostile environment, like, say, a sceptical crowd. For example, Sean B Carroll, professor of molecular biology and genetics at the University of Wisconsin, spends much of his time studying gene expression, but usually uses gene-centric explanations, because when talking to the public, he finds a simple story is a damned good thing to have.
Which drives West-Eberhard nuts.
‘Dawkins understands very well that gene expression is powerful,’ she says. ‘He sees things are more complex than a selfish gene. He could turn on its head the whole language.’
Yet Dawkins, and with him much of pop science, sticks to the selfish gene. The gene explains all. So far it has worked. The extended synthesis crowd has published scores of papers, quite a few books, and held meetings galore. They have changed the way many biologists think about evolution. But they have scarcely touched the public’s understanding. And they have not found a way to displace a meme so powerful as the selfish gene.
This meme, methinks, forms the true bone of contention and the true obstacle to progress. It’s one of the odd beauties of this whole mess that Dawkins himself coined the term meme , and did so in The Selfish Gene . He defined it as a big idea that competes for dominance in a tough environment — an idea that, like a catchy tune or a good joke, ‘propagates itself by leaping from brain to brain’. The selfish-gene meme has done just that. It has made of evolutionary theory a vehicle for its replication. The selfish gene has become a selfish meme.
If you’re West-Eberhard or of like mind, what are you to replace it with? The slave-ish gene? Not likely to leap from brain to brain. The co-operative gene? Dawkins himself considered this but rejected it and I agree that it lacks sufficient bling. And as West-Eberhard notes, any phrase with ‘gene’ in it still encourages a focus on single genes. And ‘evolution is not about single genes,’ she says. ‘It’s about genes working together.’
Perhaps better then to speak not of genes but the genome — all your genes together. And not the genome as a unitary actor, but the genome in conversation with itself, with other genomes, and with the outside environment. If grasshoppers becoming locusts, sweet bees becoming killers, and genetic assimilation are to be believed it’s those conversations that define the organism and drive the evolution of new traits and species. It’s not a selfish gene or a solitary genome. It’s a social genome.
What would Mendel think of that? Let’s play this out.
Mendel actually studied bees as a boy, and he studied them again for a couple years after he finished his pea-plant studies. In crossbreeding two species at the monastery, he accidentally created a strain of bees so vicious that he couldn’t work with them. If he’d had an RNA sequencer, he, like Gene Robinson, could have studied how much of the bees’ aggression rose from changes in the genetic code or how much rose from gene expression in response to the environment. If he had, the father of genetics might have seen right then that traits change and species evolve not just when genes change, when a creature and its genome and hive mates respond to an environment. He might have discovered not just genes, but genetic assimilation. Not the selfish gene, but the social genome.
Alas, no such equipment existed, and Mendel worked in a monastery in the middle of town. His vicious bees promised not a research opportunity but trouble. So he killed them. He would found genetics not through a complex story told by morphing bees, but through a simple tale told by one pea wrinkled and one pea smooth.
This is a revised version of the essay ‘Die, Selfish Gene, Die’. It replaced the original on 13 December 2013. Reader comments posted before that date were responding to the original version. A post at the author’s blog explains the revision and includes some other resources."
How close are we to creating artificial intelligence? | Aeon Essays,,https://aeon.co/essays/how-close-are-we-to-creating-artificial-intelligence,"It is uncontroversial that the human brain has capabilities that are, in some respects, far superior to those of all other known objects in the cosmos. It is the only kind of object capable of understanding that the cosmos is even there, or why there are infinitely many prime numbers, or that apples fall because of the curvature of space-time, or that obeying its own inborn instincts can be morally wrong, or that it itself exists. Nor are its unique abilities confined to such cerebral matters. The cold, physical fact is that it is the only kind of object that can propel itself into space and back without harm, or predict and prevent a meteor strike on itself, or cool objects to a billionth of a degree above absolute zero, or detect others of its kind across galactic distances.
But no brain on Earth is yet close to knowing what brains do in order to achieve any of that functionality. The enterprise of achieving it artificially — the field of ‘artificial general intelligence’ or AGI — has made no progress whatever during the entire six decades of its existence.
Why? Because, as an unknown sage once remarked, ‘it ain’t what we don’t know that causes trouble, it’s what we know for sure that just ain’t so’ (and if you know that sage was Mark Twain, then what you know ain’t so either). I cannot think of any other significant field of knowledge in which the prevailing wisdom, not only in society at large but also among experts, is so beset with entrenched, overlapping, fundamental errors. Yet it has also been one of the most self-confident fields in prophesying that it will soon achieve the ultimate breakthrough.
Despite this long record of failure, AGI must be possible . And that is because of a deep property of the laws of physics, namely the universality of computation . This entails that everything that the laws of physics require a physical object to do can, in principle, be emulated in arbitrarily fine detail by some program on a general-purpose computer, provided it is given enough time and memory. The first people to guess this and to grapple with its ramifications were the 19th-century mathematician Charles Babbage and his assistant Ada, Countess of Lovelace. It remained a guess until the 1980s, when I proved it using the quantum theory of computation.
Babbage came upon universality from an unpromising direction. He had been much exercised by the fact that tables of mathematical functions (such as logarithms and cosines) contained mistakes. At the time they were compiled by armies of clerks, known as ‘computers’, which is the origin of the word. Being human, the computers were fallible. There were elaborate systems of error correction, but even proofreading for typographical errors was a nightmare. Such errors were not merely inconvenient and expensive: they could cost lives. For instance, the tables were extensively used in navigation. So, Babbage designed a mechanical calculator, which he called the Difference Engine. It would be programmed by initialising certain cogs. The mechanism would drive a printer, in order to automate the production of the tables. That would bring the error rate down to negligible levels, to the eternal benefit of humankind.
Unfortunately, Babbage’s project-management skills were so poor that despite spending vast amounts of his own and the British government’s money, he never managed to get the machine built. Yet his design was sound, and has since been implemented by a team led by the engineer Doron Swade at the Science Museum in London.
Slow but steady: a detail from Charles Babbage’s Difference Engine, assembled nearly 170 years after it was designed. Courtesy Science Museum
Here was a cognitive task that only humans had been able to perform. Nothing else in the known universe even came close to matching them, but the Difference Engine would perform better than the best humans. And therefore, even at that faltering, embryonic stage of the history of automated computation — before Babbage had considered anything like AGI — we can see the seeds of a philosophical puzzle that is controversial to this day: what exactly is the difference between what the human ‘computers’ were doing and what the Difference Engine could do? What type of cognitive task, if any, could either type of entity perform that the other could not in principle perform too?
One immediate difference between them was that the sequence of elementary steps (of counting, adding, multiplying by 10, and so on) that the Difference Engine used to compute a given function did not mirror those of the human ‘computers’. That is to say, they used different algorithms. In itself, that is not a fundamental difference: the Difference Engine could have been modified with additional gears and levers to mimic the humans’ algorithm exactly. Yet that would have achieved nothing except an increase in the error rate, due to increased numbers of glitches in the more complex machinery. Similarly, the humans, given different instructions but no hardware changes, would have been capable of emulating every detail of the Difference Engine’s method — and doing so would have been just as perverse. It would not have copied the Engine’s main advantage, its accuracy, which was due to hardware not software. It would only have made an arduous, boring task even more arduous and boring, which would have made errors more likely, not less.
Babbage knew that it could be programmed to do algebra, play chess, compose music, process images and so on
For humans, that difference in outcomes — the different error rate — would have been caused by the fact that computing exactly the same table with two different algorithms felt different . But it would not have felt different to the Difference Engine. It had no feelings. Experiencing boredom was one of many cognitive tasks at which the Difference Engine would have been hopelessly inferior to humans. Nor was it capable of knowing or proving , as Babbage did, that the two algorithms would give identical results if executed accurately. Still less was it capable of wanting , as he did, to benefit seafarers and humankind in general. In fact, its repertoire was confined to evaluating a tiny class of specialised mathematical functions (basically, power series in a single variable).
Thinking about how he could enlarge that repertoire, Babbage first realised that the programming phase of the Engine’s operation could itself be automated: the initial settings of the cogs could be encoded on punched cards. And then he had an epoch-making insight. The Engine could be adapted to punch new cards and store them for its own later use, making what we today call a computer memory. If it could run for long enough — powered, as he envisaged, by a steam engine — and had an unlimited supply of blank cards, its repertoire would jump from that tiny class of mathematical functions to the set of all computations that can possibly be performed by any physical object . That’s universality.
Babbage called this improved machine the Analytical Engine . He and Lovelace understood that its universality would give it revolutionary potential to improve almost every scientific endeavour and manufacturing process, as well as everyday life. They showed remarkable foresight about specific applications. They knew that it could be programmed to do algebra, play chess, compose music, process images and so on. Unlike the Difference Engine, it could be programmed to use exactly the same method as humans used to make those tables. And prove that the two methods must give the same answers, and do the same error-checking and proofreading (using, say, optical character recognition) as well.
B ut could the Analytical Engine feel the same boredom? Could it feel anything? Could it want to better the lot of humankind (or of Analytical Enginekind)? Could it disagree with its programmer about its programming? Here is where Babbage and Lovelace’s insight failed them. They thought that some cognitive functions of the human brain were beyond the reach of computational universality. As Lovelace wrote, ‘The Analytical Engine has no pretensions whatever to originate anything. It can do whatever we know how to order it to perform. It can follow analysis; but it has no power of anticipating any analytical relations or truths.’
And yet ‘originating things’, ‘following analysis’, and ‘anticipating analytical relations and truths’ are all behaviours of brains and, therefore, of the atoms of which brains are composed. Such behaviours obey the laws of physics. So it follows inexorably from universality that, with the right program, an Analytical Engine would undergo them too, atom by atom and step by step. True, the atoms in the brain would be emulated by metal cogs and levers rather than organic material — but in the present context, inferring anything substantive from that distinction would be rank racism.
Despite their best efforts, Babbage and Lovelace failed almost entirely to convey their enthusiasm about the Analytical Engine to others. In one of the great might-have-beens of history, the idea of a universal computer languished on the back burner of human thought. There it remained until the 20th century, when Alan Turing arrived with a spectacular series of intellectual tours de force , laying the foundations of the classical theory of computation, establishing the limits of computability, participating in the building of the first universal classical computer and, by helping to crack the Enigma code, contributing to the Allied victory in the Second World War.
Turing fully understood universality. In his 1950 paper ‘Computing Machinery and Intelligence’, he used it to sweep away what he called ‘Lady Lovelace’s objection’, and every other objection both reasonable and unreasonable. He concluded that a computer program whose repertoire included all the distinctive attributes of the human brain — feelings, free will, consciousness and all — could be written.
This astounding claim split the intellectual world into two camps, one insisting that AGI was none the less impossible, and the other that it was imminent. Both were mistaken. The first, initially predominant, camp cited a plethora of reasons ranging from the supernatural to the incoherent. All shared the basic mistake that they did not understand what computational universality implies about the physical world, and about human brains in particular.
What is needed is nothing less than a breakthrough in philosophy, a theory that explains how brains create explanations
But it is the other camp’s basic mistake that is responsible for the lack of progress. It was a failure to recognise that what distinguishes human brains from all other physical systems is qualitatively different from all other functionalities, and cannot be specified in the way that all other attributes of computer programs can be. It cannot be programmed by any of the techniques that suffice for writing any other type of program. Nor can it be achieved merely by improving their performance at tasks that they currently do perform, no matter by how much.
Why? I call the core functionality in question creativity : the ability to produce new explanations . For example, suppose that you want someone to write you a computer program to convert temperature measurements from Centigrade to Fahrenheit. Even the Difference Engine could have been programmed to do that. A universal computer like the Analytical Engine could achieve it in many more ways. To specify the functionality to the programmer, you might, for instance, provide a long list of all inputs that you might ever want to give it (say, all numbers from -89.2 to +57.8 in increments of 0.1) with the corresponding correct outputs, so that the program could work by looking up the answer in the list on each occasion. Alternatively, you might state an algorithm, such as ‘divide by five, multiply by nine, add 32 and round to the nearest 10th’. The point is that, however the program worked, you would consider it to meet your specification — to be a bona fide temperature converter — if, and only if, it always correctly converted whatever temperature you gave it, within the stated range.
Now imagine that you require a program with a more ambitious functionality: to address some outstanding problem in theoretical physics — say the nature of Dark Matter — with a new explanation that is plausible and rigorous enough to meet the criteria for publication in an academic journal.
Such a program would presumably be an AGI (and then some). But how would you specify its task to computer programmers? Never mind that it’s more complicated than temperature conversion: there’s a much more fundamental difficulty. Suppose you were somehow to give them a list, as with the temperature-conversion program, of explanations of Dark Matter that would be acceptable outputs of the program. If the program did output one of those explanations later, that would not constitute meeting your requirement to generate new explanations. For none of those explanations would be new: you would already have created them yourself in order to write the specification. So, in this case, and actually in all other cases of programming genuine AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!
I’m sorry Dave, I’m afraid I can’t do that: HAL, the computer intelligence from Stanley Kubrick’s 2001: A Space Odyssey . Courtesy MGM
Traditionally, discussions of AGI have evaded that issue by imagining only a test of the program, not its specification — the traditional test having been proposed by Turing himself. It was that (human) judges be unable to detect whether the program is human or not, when interacting with it via some purely textual medium so that only its cognitive abilities would affect the outcome. But that test, being purely behavioural, gives no clue for how to meet the criterion. Nor can it be met by the technique of ‘evolutionary algorithms’: the Turing test cannot itself be automated without first knowing how to write an AGI program, since the ‘judges’ of a program need to have the target ability themselves. (For how I think biological evolution gave us the ability in the first place, see my book The Beginning of Infinity .)
And in any case, AGI cannot possibly be defined purely behaviourally. In the classic ‘brain in a vat’ thought experiment, the brain, when temporarily disconnected from its input and output channels, is thinking, feeling, creating explanations — it has all the cognitive attributes of an AGI. So the relevant attributes of an AGI program do not consist only of the relationships between its inputs and outputs.
The upshot is that, unlike any functionality that has ever been programmed to date, this one can be achieved neither by a specification nor a test of the outputs. What is needed is nothing less than a breakthrough in philosophy, a new epistemological theory that explains how brains create explanatory knowledge and hence defines, in principle, without ever running them as programs, which algorithms possess that functionality and which do not.
S uch a theory is beyond present-day knowledge. What we do know about epistemology implies that any approach not directed towards that philosophical breakthrough must be futile. Unfortunately, what we know about epistemology is contained largely in the work of the philosopher Karl Popper and is almost universally underrated and misunderstood (even — or perhaps especially — by philosophers). For example, it is still taken for granted by almost every authority that knowledge consists of justified , true beliefs and that, therefore, an AGI’s thinking must include some process during which it justifies some of its theories as true, or probable, while rejecting others as false or improbable. But an AGI programmer needs to know where the theories come from in the first place. The prevailing misconception is that by assuming that ‘the future will be like the past’, it can ‘derive’ (or ‘extrapolate’ or ‘generalise’) theories from repeated experiences by an alleged process called ‘induction’. But that is impossible. I myself remember, for example, observing on thousands of consecutive occasions that on calendars the first two digits of the year were ‘19’. I never observed a single exception until, one day, they started being ‘20’. Not only was I not surprised, I fully expected that there would be an interval of 17,000 years until the next such ‘19’, a period that neither I nor any other human being had previously experienced even once.
How could I have ‘extrapolated’ that there would be such a sharp departure from an unbroken pattern of experiences, and that a never-yet-observed process (the 17,000-year interval) would follow? Because it is simply not true that knowledge comes from extrapolating repeated observations. Nor is it true that ‘the future is like the past’, in any sense that one could detect in advance without already knowing the explanation. The future is actually unlike the past in most ways. Of course, given the explanation, those drastic ‘changes’ in the earlier pattern of 19s are straightforwardly understood as being due to an invariant underlying pattern or law. But the explanation always comes first . Without that, any continuation of any sequence constitutes ‘the same thing happening again’ under some explanation.
So, why is it still conventional wisdom that we get our theories by induction? For some reason, beyond the scope of this article, conventional wisdom adheres to a trope called the ‘problem of induction’, which asks: ‘How and why can induction nevertheless somehow be done, yielding justified true beliefs after all, despite being impossible and invalid respectively?’ Thanks to this trope, every disproof (such as that by Popper and David Miller back in 1988), rather than ending inductivism, simply causes the mainstream to marvel in even greater awe at the depth of the great ‘problem of induction’.
In regard to how the AGI problem is perceived, this has the catastrophic effect of simultaneously framing it as the ‘problem of induction’, and making that problem look easy, because it casts thinking as a process of predicting that future patterns of sensory experience will be like past ones. That looks like extrapolation — which computers already do all the time (once they are given a theory of what causes the data). But in reality, only a tiny component of thinking is about prediction at all, let alone prediction of our sensory experiences. We think about the world : not just the physical world but also worlds of abstractions such as right and wrong, beauty and ugliness, the infinite and the infinitesimal, causation, fiction, fears, and aspirations — and about thinking itself.
Now, the truth is that knowledge consists of conjectured explanations — guesses about what really is (or really should be, or might be) out there in all those worlds. Even in the hard sciences, these guesses have no foundations and don’t need justification. Why? Because genuine knowledge, though by definition it does contain truth, almost always contains error as well. So it is not ‘true’ in the sense studied in mathematics and logic. Thinking consists of criticising and correcting partially true guesses with the intention of locating and eliminating the errors and misconceptions in them, not generating or justifying extrapolations from sense data. And therefore, attempts to work towards creating an AGI that would do the latter are just as doomed as an attempt to bring life to Mars by praying for a Creation event to happen there.
Present-day software developers could straightforwardly program a computer to have ‘self-awareness’ if they wanted to. But it is a fairly useless ability
Currently one of the most influential versions of the ‘induction’ approach to AGI (and to the philosophy of science) is Bayesianism, unfairly named after the 18th-century mathematician Thomas Bayes, who was quite innocent of the mistake. The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI’s values — the moral and aesthetic ideas that inform its choices and intentions — for it allows only a behaviouristic model of them, in which values that are ‘rewarded’ by ‘experience’ are ‘reinforced’ and come to dominate behaviour while those that are ‘punished’ by ‘experience’ are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI. It is ironic that mainstream psychology has largely renounced behaviourism, which has been recognised as both inadequate and inhuman, while computer science, thanks to philosophical misconceptions such as inductivism, still intends to manufacture human-type cognition on essentially behaviourist lines.
Furthermore, despite the above-mentioned enormous variety of things that we create explanations about, our core method of doing so, namely Popperian conjecture and criticism, has a single, unified, logic. Hence the term ‘general’ in AGI. A computer program either has that yet-to-be-fully-understood logic, in which case it can perform human-type thinking about anything , including its own thinking and how to improve it, or it doesn’t, in which case it is in no sense an AGI. Consequently, another hopeless approach to AGI is to start from existing knowledge of how to program specific tasks — such as playing chess, performing statistical analysis or searching databases — and then to try to improve those programs in the hope that this will somehow generate AGI as a side effect, as happened to Skynet in the Terminator films.
Nowadays, an accelerating stream of marvellous and useful functionalities for computers are coming into use, some of them sooner than had been foreseen even quite recently. But what is neither marvellous nor useful is the argument that often greets these developments, that they are reaching the frontiers of AGI. An especially severe outbreak of this occurred recently when a search engine called Watson, developed by IBM, defeated the best human player of a word-association database-searching game called Jeopardy. ‘Smartest machine on Earth’, the PBS documentary series Nova called it, and characterised its function as ‘mimicking the human thought process with software.’ But that is precisely what it does not do.
The thing is, playing Jeopardy — like every one of the computational functionalities at which we rightly marvel today — is firmly among the functionalities that can be specified in the standard, behaviourist way that I discussed above. No Jeopardy answer will ever be published in a journal of new discoveries. The fact that humans perform that task less well by using creativity to generate the underlying guesses is not a sign that the program has near-human cognitive abilities. The exact opposite is true, for the two methods are utterly different from the ground up. Likewise, when a computer program beats a grandmaster at chess, the two are not using even remotely similar algorithms. The grandmaster can explain why it seemed worth sacrificing the knight for strategic advantage and can write an exciting book on the subject. The program can only prove that the sacrifice does not force a checkmate, and cannot write a book because it has no clue even what the objective of a chess game is. Programming AGI is not the same sort of problem as programming Jeopardy or chess.
An AGI is qualitatively, not quantitatively, different from all other computer programs. The Skynet misconception likewise informs the hope that AGI is merely an emergent property of complexity, or that increased computer power will bring it forth (as if someone had already written an AGI program but it takes a year to utter each sentence). It is behind the notion that the unique abilities of the brain are due to its ‘massive parallelism’ or to its neuronal architecture, two ideas that violate computational universality. Expecting to create an AGI without first understanding in detail how it works is like expecting skyscrapers to learn to fly if we build them tall enough.
I n 1950, Turing expected that by the year 2000, ‘one will be able to speak of machines thinking without expecting to be contradicted.’ In 1968, Arthur C. Clarke expected it by 2001. Yet today in 2012 no one is any better at programming an AGI than Turing himself would have been.
This does not surprise people in the first camp, the dwindling band of opponents of the very possibility of AGI. But for the people in the other camp (the AGI-is-imminent one) such a history of failure cries out to be explained — or, at least, to be rationalised away. And indeed, unfazed by the fact that they could never induce such rationalisations from experience as they expect their AGIs to do, they have thought of many.
The very term ‘AGI’ is an example of one. The field used to be called ‘AI’ — artificial intelligence. But ‘AI’ was gradually appropriated to describe all sorts of unrelated computer programs such as game players, search engines and chatbots, until the G for ‘general’ was added to make it possible to refer to the real thing again, but now with the implication that an AGI is just a smarter species of chatbot.
Another class of rationalisations runs along the general lines of: AGI isn’t that great anyway; existing software is already as smart or smarter, but in a non-human way, and we are too vain or too culturally biased to give it due credit. This gets some traction because it invokes the persistently popular irrationality of cultural relativism, and also the related trope that: ‘We humans pride ourselves on being the paragon of animals, but that pride is misplaced because they, too, have language, tools …
… And self-awareness.’
Remember the significance attributed to Skynet’s becoming ‘self-aware’? That’s just another philosophical misconception, sufficient in itself to block any viable approach to AGI. The fact is that present-day software developers could straightforwardly program a computer to have ‘self-awareness’ in the behavioural sense — for example, to pass the ‘mirror test’ of being able to use a mirror to infer facts about itself — if they wanted to. As far as I am aware, no one has done so, presumably because it is a fairly useless ability as well as a trivial one.
Perhaps the reason that self-awareness has its undeserved reputation for being connected with AGI is that, thanks to Kurt Gödel’s theorem and various controversies in formal logic in the 20th century, self-reference of any kind has acquired a reputation for woo-woo mystery. So has consciousness. And here we have the problem of ambiguous terminology again: the term ‘consciousness’ has a huge range of meanings. At one end of the scale there is the philosophical problem of the nature of subjective sensations (‘qualia’), which is intimately connected with the problem of AGI. At the other, ‘consciousness’ is simply what we lose when we are put under general anaesthetic. Many animals certainly have that.
AGIs will indeed be capable of self-awareness — but that is because they will be General: they will be capable of awareness of every kind of deep and subtle thing, including their own selves. This does not mean that apes who pass the mirror test have any hint of the attributes of ‘general intelligence’ of which AGI would be an artificial version. Indeed, Richard Byrne’s wonderful research into gorilla memes has revealed how apes are able to learn useful behaviours from each other without ever understanding what they are for: the explanation of how ape cognition works really is behaviouristic.
Ironically, that group of rationalisations (AGI has already been done/is trivial/ exists in apes/is a cultural conceit) are mirror images of arguments that originated in the AGI-is-impossible camp. For every argument of the form ‘You can’t do AGI because you’ll never be able to program the human soul, because it’s supernatural’, the AGI-is-easy camp has the rationalisation, ‘If you think that human cognition is qualitatively different from that of apes, you must believe in a supernatural soul.’
‘Anything we don’t yet know how to program is called human intelligence,’ is another such rationalisation. It is the mirror image of the argument advanced by the philosopher John Searle (from the ‘impossible’ camp), who has pointed out that before computers existed, steam engines and later telegraph systems were used as metaphors for how the human mind must work. Searle argues that the hope for AGI rests on a similarly insubstantial metaphor, namely that the mind is ‘essentially’ a computer program. But that’s not a metaphor: the universality of computation follows from the known laws of physics.
Some, such as the mathematician Roger Penrose, have suggested that the brain uses quantum computation, or even hyper-quantum computation relying on as-yet-unknown physics beyond quantum theory, and that this explains the failure to create AGI on existing computers. To explain why I, and most researchers in the quantum theory of computation, disagree that this is a plausible source of the human brain’s unique functionality is beyond the scope of this essay. (If you want to know more, read Litt et al ’s 2006 paper ‘Is the Brain a Quantum Computer?’, published in the journal Cognitive Science .)
T hat AGIs are people has been implicit in the very concept from the outset. If there were a program that lacked even a single cognitive ability that is characteristic of people, then by definition it would not qualify as an AGI. Using non- cognitive attributes (such as percentage carbon content) to define personhood would, again, be racist. But the fact that the ability to create new explanations is the unique, morally and intellectually significant functionality of people (humans and AGIs), and that they achieve this functionality by conjecture and criticism, changes everything.
Currently, personhood is often treated symbolically rather than factually — as an honorific, a promise to pretend that an entity (an ape, a foetus, a corporation) is a person in order to achieve some philosophical or practical aim. This isn’t good. Never mind the terminology; change it if you like, and there are indeed reasons for treating various entities with respect, protecting them from harm and so on. All the same, the distinction between actual people, defined by that objective criterion, and other entities has enormous moral and practical significance, and is going to become vital to the functioning of a civilisation that includes AGIs.
The battle between good and evil ideas is as old as our species and will go on regardless of the hardware on which it is running
For example, the mere fact that it is not the computer but the running program that is a person, raises unsolved philosophical problems that will become practical, political controversies as soon as AGIs exist. Once an AGI program is running in a computer, to deprive it of that computer would be murder (or at least false imprisonment or slavery, as the case may be), just like depriving a human mind of its body. But unlike a human body, an AGI program can be copied into multiple computers at the touch of a button. Are those programs, while they are still executing identical steps (ie before they have become differentiated due to random choices or different experiences), the same person or many different people? Do they get one vote, or many? Is deleting one of them murder, or a minor assault? And if some rogue programmer, perhaps illegally, creates billions of different AGI people, either on one computer or on many, what happens next? They are still people, with rights. Do they all get the vote?
Furthermore, in regard to AGIs, like any other entities with creativity, we have to forget almost all existing connotations of the word ‘programming’. To treat AGIs like any other computer programs would constitute brainwashing, slavery, and tyranny. And cruelty to children, too, for ‘programming’ an already-running AGI, unlike all other programming, constitutes education . And it constitutes debate , moral as well as factual. To ignore the rights and personhood of AGIs would not only be the epitome of evil, but also a recipe for disaster: creative beings cannot be enslaved forever.
Some people are wondering whether we should welcome our new robot overlords. Some hope to learn how we can rig their programming to make them constitutionally unable to harm humans (as in Isaac Asimov’s ‘laws of robotics’), or to prevent them from acquiring the theory that the universe should be converted into paper clips (as imagined by Nick Bostrom). None of these are the real problem. It has always been the case that a single exceptionally creative person can be thousands of times as productive — economically, intellectually or whatever — as most people; and that such a person could do enormous harm were he to turn his powers to evil instead of good.
These phenomena have nothing to do with AGIs. The battle between good and evil ideas is as old as our species and will continue regardless of the hardware on which it is running. The issue is: we want the intelligences with (morally) good ideas always to defeat the evil intelligences, biological and artificial; but we are fallible, and our own conception of ‘good’ needs continual improvement. How should society be organised so as to promote that improvement? ‘Enslave all intelligence’ would be a catastrophically wrong answer, and ‘enslave all intelligence that doesn’t look like us’ would not be much better.
One implication is that we must stop regarding education (of humans or AGIs alike) as instruction — as a means of transmitting existing knowledge unaltered , and causing existing values to be enacted obediently. As Popper wrote (in the context of scientific discovery, but it applies equally to the programming of AGIs and the education of children): ‘there is no such thing as instruction from without … We do not discover new facts or new effects by copying them, or by inferring them inductively from observation, or by any other method of instruction by the environment. We use, rather, the method of trial and the elimination of error.’ That is to say, conjecture and criticism. Learning must be something that newly created intelligences do, and control, for themselves.
I do not highlight all these philosophical issues because I fear that AGIs will be invented before we have developed the philosophical sophistication to understand them and to integrate them into civilisation. It is for almost the opposite reason: I am convinced that the whole problem of developing AGIs is a matter of philosophy, not computer science or neurophysiology, and that the philosophical progress that is essential to their future integration is also a prerequisite for developing them in the first place.
The lack of progress in AGI is due to a severe logjam of misconceptions. Without Popperian epistemology, one cannot even begin to guess what detailed functionality must be achieved to make an AGI. And Popperian epistemology is not widely known, let alone understood well enough to be applied. Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.
Without understanding that the functionality of an AGI is qualitatively different from that of any other kind of computer program, one is working in an entirely different field. If one works towards programs whose ‘thinking’ is constitutionally incapable of violating predetermined constraints, one is trying to engineer away the defining attribute of an intelligent being, of a person: namely creativity.
Clearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees. So in one respect I can agree with the AGI-is-imminent camp: it is plausible that just a single idea stands between us and the breakthrough. But it will have to be one of the best ideas ever."
What’s a stegosaur for? Why life is design-like | Aeon Essays,,https://aeon.co/essays/what-s-a-stegosaur-for-why-life-is-design-like,"One of my favourite dinosaurs is the Stegosaurus, a monster from the late Jurassic (150 million years ago), noteworthy because of the diamond-like plates all the way down its back. Since this animal was discovered in the late 1870s in Wyoming, huge amounts of ink have been spilt trying to puzzle out the reason for the plates. The obvious explanation, that they are used for fighting or defence, simply cannot be true. The connection between the plates and the main body is way too fragile to function effectively in a battle to the death. Another explanation is that, like the stag’s antlers or the peacock’s tail, they play some sort of role in the mating game. Señor Stegosaurus with the best plates gets the harem and the other males have to do without. Unfortunately for this hypothesis, the females had the plates too, so that cannot be the explanation either. My favourite idea is that the plates were like the fins you find in electric-producing cooling towers: they were for heat transfer. In the cool of the morning, as the sun came up, they helped the animal to heat up quickly. In the middle of the day, especially when the vegetation consumed by the Stegosaurus was fermenting away in its belly, the plates would have helped to catch the wind and get rid of excess heat. A superb adaptation. (Sadly for me, no longer a favoured explanation, since latest investigations suggest that the plates may have been a way for individuals to recognise each other as members of the same species).
But this essay is not concerned with dinosaurs themselves, rather with the kind of thinking biologists use when they wonder how dinosaur bodies worked. They are asking what was the purpose of the plates? What end did the plates serve? Were they for fighting? Were they for attracting mates? Were they for heat control? This kind of language is ‘teleological’ — from telos , the Greek for ‘end’. It is language about the purpose or goal of things, what Aristotle called their ‘final causes’, and it is something that the physical sciences have decisively rejected. There’s no sense for most scientists that a star is for anything, or that a molecule serves an end. But when we come to talk about living things, it seems very hard to shake off the idea that they have purposes and goals, which are served by the ways they have evolved.
As I have written about before in Aeon , the chemist James Lovelock got into very hot water with his fellow scientists when he wanted to talk about the Earth being an organism (the Gaia hypothesis) and its parts having purposes: that sea lagoons were for evaporating unneeded salt out of the ocean, for instance. And as Steven Poole wrote in his essay ‘Your Point Is?’ in Aeon earlier this year, the contemporary philosopher Thomas Nagel is also in hot water since he suggested in his book Mind and Cosmos (2012) that we need to use teleological understanding to explain the nature of life and its evolution.
Some have thought that this lingering teleological language is a sign that biology is not a real science at all, but just a collection of observations and facts. Others argue that the apparent purposefulness of nature leaves room for God. Immanuel Kant declared that you cannot do biology without thinking in terms of function, of final causes: ‘There will never be a Newton for a blade of grass,’ he claimed in Critique of Judgment (1790), meaning that living things are simply not determined by the laws of nature in the way that non-living things are, and we need the language of purpose in order to explain the organic world.
Why do we still talk about organisms and their features in this way? Is biology basically different from the other sciences because living things do have purposes and ends? Or has biology simply failed to get rid of some old-fashioned, unscientific thinking — thinking that even leaves the door ajar for those who want to sneak God back into science?
B iology’s entanglement with teleology reaches right back to the ancient Greek world. In Plato’s dialogue the Phaedo , Socrates describes himself as he sits awaiting his fate, and he asks whether this can be fully explained mechanically ‘because my body is made up of bones and muscles; and the bones… are hard and have joints which divide them, and the muscles are elastic, and they cover the bones’. All of this, says Socrates, is not ‘the true cause’ of why he sits where and how he does. The true cause is that ‘the Athenians have thought fit to condemn me and I have thought it better and more right to remain here and undergo my sentence’. Socrates describes this as a confusion of causes and conditions: he cannot sit without his bones and muscles being as they are, but this is no real explanation of why he sits thus. In the Timaeus Plato develops this further, describing a universe brought into being by a designer (what Plato called the Demiurge). An enquiry into the purpose of the bones and muscles was not only an enquiry into the ways of men, but ultimately an enquiry into the plans of the Demiurge.
Now, however, the governing metaphors of nature changed. No longer did scientists think in terms of organisms: they thought in terms of machines
Aristotle, Plato’s student, didn’t want God in the business of biology like this. He believed in a God, but not one that cared about the universe and its inhabitants. (Rather like some junior members of my family, this God spent Its time thinking mostly of Its own importance.) However, Aristotle was very interested in final causes, and argued that all living things contain forces that direct them towards their goal. These life forces operate in the here and now, yet in some sense they have the future in mind. They animate the acorn in order that it might turn into an oak, and likewise for other living things. Like Plato, Aristotle used the metaphor of design but unlike Plato he wanted to keep any supervisory, conscious intelligence out of the game.
All of this came crashing down during the Scientific Revolution of the 16th and 17th centuries. For both Plato and Aristotle, the question of final causes had applied to physical phenomena — the stars, for example — as much as to biological phenomena. Both thought of objects as being rather like organisms. Why does the stone fall? Because being made of the element earth it wants to find its proper place, namely as close to the centre of the Earth as possible. It falls in order to achieve its right end: it wants to fall.
Now, however, the governing metaphors of nature changed. No longer did scientists think in terms of organisms: they thought in terms of machines. The world, the universe, is like a gigantic clock. As the 17th-century French philosopher-scientist René Descartes insisted, the human body is nothing but an intricate machine. The heart is like a pump, and the arms and legs are a system of levers and pulleys and so forth. The 17th-century English chemist and philosopher Robert Boyle realised that as soon as you start to think in the mechanical fashion, then talking about ends and purposes really isn’t very helpful. A planet goes round and round the Sun; you want to know the mechanism by which it happens, not to imagine some higher purpose for it. In the same way, when you look at a clock you want to know what makes the hands go round the dial — you want the proximate causes.
But surely machines have purposes just as much as organisms do? The clock exists in order to tell the time just as much as the eye exists in order to see. True, but as Boyle also saw, it is one thing to talk about intentions and purposes in a general, perhaps theological way, but another thing to do this as part of science. You can take the Platonic route and talk about God’s creative intentions for the universe, that’s fine. But, really, this is no longer part of science (if it ever was) and has little explanatory power. In the words of EJ Dijksterhuis, one of the great historians of the Scientific Revolution, God now became a ‘retired engineer’.
On the other hand, if you wanted to take the Aristotelian approach and explain the growth and development of individual organisms by special vital forces, that was still theoretically possible. But since no one, as Boyle pointed out, seemed to have the slightest clue about these vital forces or what they did, he and his fellow mechanists just wanted to drop the idea altogether and get on with the job of finding proximate causes for all natural phenomena. The organic metaphor did not lead to new predictions and the other sorts of things one wants from science, especially technological promise. The machine metaphor did.
Yet even Boyle realised that it is very hard to get rid of final-cause thinking when it comes to studying actual organisms, and not just using them as metaphors in the rest of the physical world. He was particularly interested in bats, and spent some considerable time discussing their adaptations — how their wings were so well-organised for flying and so on. In fact, almost paradoxically, in the 18th century the study of living things became more interested in teleology, even as the physical sciences were turning away from it.
‘Running fast in a herd while being as dumb as shit, I think, is a very good adaptation for survival’
The expansion of historical thinking played a key role here. History no longer seemed static and determined, and the belief that humans could make things better through their own unaided efforts meant that there was no longer a need to appeal to Providence for help. This secular ideal (or ideology) of progress put talk of ends and directional change very much in the air. If we as a society aim for certain ends, let us say an improved standard of living or education, could it be that history itself has ends too — ends that are not dictated so much by the Christian religion (judgment and salvation or condemnation) but that come as part of a general end-directed force or movement? Could life, and human history, be directed upward and forward from within?
A longside philosophers and historians such as Hegel, in the 19th century natural historians began to speculate about organisms in proto-evolutionary ways, and to talk of goals — usually, one admits, goals involving the arrival of the best of all possible organisms, namely Homo sapiens . Here is ‘The Temple of Nature’ (1802) by Erasmus Darwin, Charles Darwin’s physician grandfather:
In the writings of some of the early evolutionists, notably the French biologist Jean-Baptiste Lamarck, we get a strong odour of Aristotelian vital forces pushing life up the ladder to the preordained destination of humankind. No longer was teleological language confined to the purpose of individual organisms and organs such as the hand or the acorn, but now it seemed to explain a general direction for the development of life itself.
It was in this atmosphere of fascination for the history of life that Charles Darwin developed his theory of natural selection. Darwin’s On the Origin of Species (1859) was the watershed. He nailed the question of individual final causes, by explaining why organisms are so well-adapted to their environments. Teleological language was appropriate because such features as eyes and hands were not designed, but design- like . The eye is like a telescope, Stegosaur plates are like the fins you find in cooling towers. So we can ask about purposes. (Of course, questions about the dinosaur could not have been Darwin’s own: when the Origin was published, Stegosaurus still slumbered undiscovered in the rocks of the American West.)
Natural selection explained how design-like features could arise, without a designer or a purpose. There need not be any final cause. There is a struggle for existence among organisms, or more precisely a struggle for reproduction. Some will survive and reproduce, and others will not. Because there are variations in populations and new variations always arriving, on average those surviving will be different from those not surviving, in ways that will have contributed to their greater success. Over time, this adds up to change in the direction of adaptation, of design-like features. No God is needed — even if he exists, he works at ‘arms-length’ — and neither are any vital forces. Just plain old laws working in a good mechanical fashion. The teleological metaphor was just a metaphor: underneath it lay quite simple mechanical explanations.
So this cracked one side of the teleology problem: that of why individual organisms were well adapted to their environments. But what about the other side, the question of whether life itself had some overall direction, some overall sense of progress? What about the process that led to the development of humans? Darwin did believe in some kind of progress of this nature — what the Victorians called ‘monad to man’ — but he wanted nothing at all to do with Germanic, Hegelian kinds of world spirits taking life ever upwards. That smacked too much of a kind of pseudo-Christian faith, which he did not share.
There was a Newton of the blade of grass and his name was Charles Darwin
Characteristically, Darwin thrashed about on the matter of whether evolution had a direction. He agonised in his notebooks, and never really came up with a definitive answer. The closest he got was suggesting that improvement comes about naturally because each generation, on average, is going to be better than the previous one. Adaptations improve, and eventually brains appear, and get bigger and bigger. Hence humans. Darwin wrote: ‘If we look at the differentiation and specialisation of the several organs of each being when adult (and this will include the advancement of the brain for intellectual purposes) as the best standard of highness of organisation, natural selection clearly leads towards highness.’ What Darwin never really considered is the fact that brains are very expensive things to maintain, and big brains are not necessarily a one-way ticket to evolutionary success. In the immortal words of the late American paleontologist Jack Sepkoski: ‘I see intelligence as just one of a variety of adaptations among tetrapods for survival. Running fast in a herd while being as dumb as shit, I think, is a very good adaptation for survival.’
Darwin might have solved the teleological problem in biology once and for all, but his solution was not an immediate success. Most people really could not get their heads around natural selection, and frankly most people were not troubled by the question of whether the evolution of life had an end point. Obviously humans were it, and were bound to appear. All sorts of neo-Platonists were happy to believe a Christian interpretation of Darwin’s view of life: God set evolution going in order to ascend to Man. They could have Jesus and evolution too! In the words of Henry Ward Beecher — the charismatic preacher, prolific adulterer, and brother of Harriet Beecher Stowe — ‘Who designed this mighty machine, created matter, gave to it its laws, and impressed upon it that tendency which has brought forth the almost infinite results on the globe, and wrought them into a perfect system? Design by wholesale is grander than design by retail.’
While Christians could interpret evolution in a Platonic frame, as the working out of a Divine creator’s purpose, some biologists revived Aristotle’s idea of vital forces that impelled living things towards their ends. At the turn of the 20th century, the German embryologist Hans Driesch described such forces that he called ‘entelechies’, which he described as being ‘mind-like’. In France, the philosopher Henri Bergson supposed ‘élan vital’ , a vital spirit that created adaptations and that gave evolution its upwards course. In England, the biologist Julian Huxley — the grandson of Darwin’s great supporter Thomas Henry Huxley and the older brother of the novelist Aldous Huxley — was always drawn to vitalism, seeing in evolution a kind of substitute for Christianity which provided people with a sense of meaning and direction: what he called ‘religion without revelation’. But even he could see that, scientifically, vitalism was a non-starter. The problem was not that no one could see these forces: no one could see electrons either. Rather it was that they didn’t provide any new explanations or predictions. They seemed to do no real work in the physical world, and mainstream biology rejected them as a hangover from an earlier age.
S o what of now? Today’s scientists are pretty certain that the problem of teleology at the individual organism level has been licked. Darwin really was right. Natural selection explains the design-like nature of organisms and their characteristics, without any need to talk about final causes. On the other hand, no natural selection lies behind mountains and rivers and whole planets. They are not design-like. That is why teleological talk is inappropriate, and why the Gaia hypothesis is so criticised. And overall that is why biology is just as good a science as physics and chemistry. It is dealing with different kinds of phenomena and so different kinds of explanation are appropriate. There was a Newton of the blade of grass and his name was Charles Darwin.
But historical teleology — the question of whether evolution itself takes a direction, in particular a progressive one, is a trickier problem, and I cannot say that there is yet, nor the prospect of there ever being, a satisfactory answer. One popular way to explain the apparent progress in evolution is as a biological arms race (a metaphor coined by Julian Huxley, incidentally). Through natural selection, prey animals get faster and so in tandem do predators. Perhaps, as in military arms races, eventually electronics and computers get ever more important, and the winners are those who do best in this respect. The British evolutionary biologist Richard Dawkins has argued that humans have the biggest on-board computers and that is what we expect natural selection to produce. But it is not obvious that arms races would result in humans — those physically feeble and mentally able omnivorous primates. Nor that lines of prey and predator evolve in tandem more generally.
I’ll offer no final answers here, but one final question. Could a full-blown teleology, of the more scientific Aristotelian kind, reappear, complete with vital forces? There’s no logical reason to say this is impossible, and that is why I think it is legitimate for Nagel to raise the possibility. Two hundred years ago, people would have laughed at the idea of quantum mechanics, with all its violations of common-sense thinking. But there is a big difference: quantum mechanics was invented because it filled a big explanatory gap. This is Nagel’s big mistake: his argument for returning to the idea of purposes and goals in biology is not based on an extensive engagement with the science, but a philosophical skim across the surface. Quantum mechanics is weird, but it works. There is nothing in the idea of final causes to encourage such wishful thinking.
So what’s a Stegosaur for? We can ask what adaptive function the plates on its back served, as good Darwinian scientists. But the beast itself? It’s not for anything, it just is — in all its decorative, mysterious, plant-munching glory."
We will never be able to live on another planet. Here’s why | Aeon Essays,,https://aeon.co/essays/we-will-never-be-able-to-live-on-another-planet-heres-why,"At the start of the 22nd century, humanity left Earth for the stars. The enormous ecological and climatic devastation that had characterised the last 100 years had led to a world barren and inhospitable; we had used up Earth entirely. Rapid melting of ice caused the seas to rise, swallowing cities whole. Deforestation ravaged forests around the globe, causing widespread destruction and loss of life. All the while, we continued to burn the fossil fuels we knew to be poisoning us, and thus created a world no longer fit for our survival. And so we set our sights beyond Earth’s horizons to a new world, a place to begin again on a planet as yet untouched. But where are we going? What are our chances of finding the elusive planet B, an Earth-like world ready and waiting to welcome and shelter humanity from the chaos we created on the planet that brought us into being? We built powerful astronomical telescopes to search the skies for planets resembling our own, and very quickly found hundreds of Earth twins orbiting distant stars. Our home was not so unique after all. The universe is full of Earths!
This futuristic dream-like scenario is being sold to us as a real scientific possibility, with billionaires planning to move humanity to Mars in the near future. For decades, children have grown up with the daring movie adventures of intergalactic explorers and the untold habitable worlds they find. Many of the highest-grossing films are set on fictional planets, with paid advisors keeping the science ‘realistic’. At the same time, narratives of humans trying to survive on a post-apocalyptic Earth have also become mainstream.
Given all our technological advances, it’s tempting to believe we are approaching an age of interplanetary colonisation. But can we really leave Earth and all our worries behind? No. All these stories are missing what makes a planet habitable to us . What Earth-like means in astronomy textbooks and what it means to someone considering their survival prospects on a distant world are two vastly different things. We don’t just need a planet roughly the same size and temperature as Earth; we need a planet that spent billions of years evolving with us. We depend completely on the billions of other living organisms that make up Earth’s biosphere. Without them, we cannot survive. Astronomical observations and Earth’s geological record are clear: the only planet that can support us is the one we evolved with. There is no plan B. There is no planet B. Our future is here, and it doesn’t have to mean we’re doomed.
D eep down, we know this from instinct: we are happiest when immersed in our natural environment. There are countless examples of the healing power of spending time in nature . Numerous articles speak of the benefits of ‘forest bathing’; spending time in the woods has been scientifically shown to reduce stress, anxiety and depression, and to improve sleep quality, thus nurturing both our physical and mental health. Our bodies instinctively know what we need: the thriving and unique biosphere that we have co-evolved with, that exists only here, on our home planet.
There is no planet B. These days, everyone is throwing around this catchy slogan. Most of us have seen it inscribed on an activist’s homemade placard, or heard it from a world leader. In 2014, the United Nations’ then secretary general Ban Ki-moon said: ‘There is no plan B because we do not have [a] planet B.’ The French president Emmanuel Macron echoed him in 2018 in his historical address to US Congress. There’s even a book named after it. The slogan gives strong impetus to address our planetary crisis. However, no one actually explains why there isn’t another planet we could live on, even though the evidence from Earth sciences and astronomy is clear. Gathering this observation-based information is essential to counter an increasingly popular but flawed narrative that the only way to ensure our survival is to colonise other planets.
The best-case scenario for terraforming Mars leaves us with an atmosphere we are incapable of breathing
The most common target of such speculative dreaming is our neighbour Mars. It is about half the size of Earth and receives about 40 per cent of the heat that we get from the Sun. From an astronomer’s perspective, Mars is Earth’s identical twin. And Mars has been in the news a lot lately, promoted as a possible outpost for humanity in the near future . While human-led missions to Mars seem likely in the coming decades, what are our prospects of long-term habitation on Mars? Present-day Mars is a cold, dry world with a very thin atmosphere and global dust storms that can last for weeks on end. Its average surface pressure is less than 1 per cent of Earth’s. Surviving without a pressure suit in such an environment is impossible. The dusty air mostly consists of carbon dioxide (CO 2 ) and the surface temperature ranges from a balmy 30ºC (86ºF) in the summer, down to -140ºC (-220ºF) in the winter; these extreme temperature changes are due to the thin atmosphere on Mars.
Despite these clear challenges, proposals for terraforming Mars into a world suitable for long-term human habitation abound. Mars is further from the Sun than Earth, so it would require significantly more greenhouse gases to achieve a temperature similar to Earth’s. Thickening the atmosphere by releasing CO 2 in the Martian surface is the most popular ‘solution’ to the thin atmosphere on Mars. However, every suggested method of releasing the carbon stored in Mars requires technology and resources far beyond what we are currently capable of. What’s more, a recent NASA study determined that there isn’t even enough CO 2 on Mars to warm it sufficiently.
Even if we could find enough CO 2 , we would still be left with an atmosphere we couldn’t breathe. Earth’s atmosphere contains only 0.04 per cent CO 2 , and we cannot tolerate an atmosphere high in CO 2 . For an atmosphere with Earth’s atmospheric pressure, CO 2 levels as high as 1 per cent can cause drowsiness in humans, and once we reach levels of 10 per cent CO 2 , we will suffocate even if there is abundant oxygen. The proposed absolute best-case scenario for terraforming Mars leaves us with an atmosphere we are incapable of breathing; and achieving it is well beyond our current technological and economic capabilities.
Instead of changing the atmosphere of Mars, a more realistic scenario might be to build habitat domes on its surface with internal conditions suitable for our survival. However, there would be a large pressure difference between the inside of the habitat and the outside atmosphere. Any breach in the habitat would rapidly lead to depressurisation as the breathable air escapes into the thin Martian atmosphere. Any humans living on Mars would have to be on constant high alert for any damage to their building structures, and suffocation would be a daily threat.
F rom an astronomical perspective, Mars is Earth’s twin; and yet, it would take vast resources, time and effort to transform it into a world that wouldn’t be capable of providing even the bare minimum of what we have on Earth. Suggesting that another planet could become an escape from our problems on Earth suddenly seems absurd. But are we being pessimistic? Do we just need to look further afield?
Next time you are out on a clear night, look up at the stars and choose one – you are more likely than not to pick one that hosts planets. Astronomical observations today confirm our age-old suspicion that all stars have their own planetary systems. As astronomers, we call these exoplanets. What are exoplanets like? Could we make any of them our home?
The majority of exoplanets discovered to date were found by NASA’s Kepler mission, which monitored the brightness of 100,000 stars over four years, looking for dips in a star’s light as a planet obscures it each time it completes an orbit around it.
The solar system associated with star Kepler-90 has a similar configuration to our solar system with small planets found orbiting close to their star, and the larger planets found farther away. Courtesy NASA/Ames /Wendy Stenzel
Kepler observed more than 900 Earth-sized planets with a radius up to 1.25 times that of our world. These planets could be rocky (for the majority of them, we haven’t yet determined their mass, so we can only make this inference based on empirical relations between planetary mass and radius). Of these 900 or so Earth-sized planets, 23 are in the habitable zone. The habitable zone is the range of orbits around a star where a planet can be considered temperate : the planet’s surface can support liquid water (provided there is sufficient atmospheric pressure), a key ingredient of life as we know it. The concept of the habitable zone is very useful because it depends on just two astrophysical parameters that are relatively easy to measure: the distance of the planet to its parent star, and the star’s temperature. It’s worth keeping in mind that the astronomical habitable zone is a very simple concept and, in reality, there are many more factors at play in the emergence of life; for example, this concept does not consider plate tectonics , which are thought to be crucial to sustain life on Earth.
Planets with similar observable properties to Earth are very common: at least one in 10 stars hosts them
How many Earth-sized, temperate planets are there in our galaxy? Since we have discovered only a handful of these planets so far, it is still quite difficult to estimate their number. Current estimates of the frequency of Earth-sized planets rely on extrapolating measured occurrence rates of planets that are slightly bigger and closer to their parent star, as those are easier to detect. The studies are primarily based on observations from the Kepler mission, which surveyed more than 100,000 stars in a systematic fashion. These stars are all located in a tiny portion of the entire sky; so, occurrence rate studies assume that this part of the sky is representative of the full galaxy. These are all reasonable assumptions for the back-of-the-envelope estimate that we are about to make.
Several different teams carried out their own analyses and, on average, they found that roughly one in three stars (30 per cent) hosts an Earth-sized, temperate planet. The most pessimistic studies found a rate of 9 per cent, which is about one in 10 stars, and the studies with the most optimistic results found that virtually all stars host at least one Earth-sized, temperate planet, and potentially even several of them.
At first sight, this looks like a huge range in values; but it’s worth taking a step back and realising that we had absolutely no constraints whatsoever on this number just 20 years ago. Whether there are other planets similar to Earth is a question that we’ve been asking for millennia, and this is the very first time that we are able to answer it based on actual observations. Before the Kepler mission, we had no idea whether we would find Earth-sized, temperate planets around one in 10, or one in a million stars. Now we know that planets with similar observable properties to Earth are very common: at least one in 10 stars hosts these kinds of planets.
An artist’s concept shows exoplanet Kepler-1649c orbiting around its host red dwarf star. Courttesy NASA/Ames
Let’s now use these numbers to predict the number of Earth-sized, temperate planets in our entire galaxy. For this, let’s take the average estimate of 30 per cent, or roughly one in three stars. Our galaxy hosts approximately 300 billion stars, which adds up to 90 billion roughly Earth-sized, roughly temperate planets. This is a huge number, and it can be very tempting to think that at least one of these is bound to look exactly like Earth.
One issue to consider is that other worlds are at unimaginable distances from us. Our neighbour Mars is on average 225 million kilometres (about 140 million miles) away. Imagine a team of astronauts travelling in a vehicle similar to NASA’s robotic New Horizons probe, one of humankind’s fastest spacecrafts – which flew by Pluto in 2015. With New Horizons’ top speed of around 58,000 kph, it would take at least 162 days to reach Mars. Beyond our solar system, the closest star to us is Proxima Centauri, at a distance of 40 trillion kilometres. Going in the same space vehicle, it would take our astronaut crew 79,000 years to reach planets that might exist around our nearest stellar neighbour.
S till, let’s for a moment optimistically imagine that we find a perfect Earth twin: a planet that really is exactly like Earth. Let’s imagine that some futuristic form of technology exists, ready to whisk us away to this new paradise. Keen to explore our new home, we eagerly board our rocket, but on landing we soon feel uneasy. Where is the land? Why is the ocean green and not blue? Why is the sky orange and thick with haze? Why are our instruments detecting no oxygen in the atmosphere? Was this not supposed to be a perfect twin of Earth?
As it turns out, we have landed on a perfect twin of the Archean Earth, the aeon during which life first emerged on our home world. This new planet is certainly habitable: lifeforms are floating around the green, iron-rich oceans, breathing out methane that is giving the sky that unsettling hazy, orange colour. This planet sure is habitable – just not to us . It has a thriving biosphere with plenty of life, but not life like ours. In fact, we would have been unable to survive on Earth for around 90 per cent of its history; the oxygen-rich atmosphere that we depend on is a recent feature of our planet.
The earliest part of our planet’s history, known as the Hadean aeon, begins with the formation of the Earth. Named after the Greek underworld due to our planet’s fiery beginnings, the early Hadean would have been a terrible place with molten lava oceans and an atmosphere of vaporised rock. Next came the Archean aeon, beginning 4 billion years ago, when the first life on Earth flourished. But, as we just saw, the Archean would be no home for a human. The world where our earliest ancestors thrived would kill us in an instant. After the Archean came the Proterozoic, 2.5 billion years ago. In this aeon, there was land, and a more familiar blue ocean and sky. What’s more, oxygen finally began to accumulate in the atmosphere. But let’s not get too excited: the level of oxygen was less than 10 per cent of what we have today. The air would still have been impossible for us to breathe. This time also experienced global glaciation events known as snowball Earths, where ice covered the globe from poles to equator for millions of years at a time. Earth has spent more of its time fully frozen than the length of time that we humans have existed.
We would have been incapable of living on our planet for most of its existence
Earth’s current aeon, the Phanerozoic, began only around 541 million years ago with the Cambrian explosion – a period of time when life rapidly diversified. A plethora of life including the first land plants, dinosaurs and the first flowering plants all appeared during this aeon. It is only within this aeon that our atmosphere became one that we can actually breathe. This aeon has also been characterised by multiple mass extinction events that wiped out as much as 90 per cent of all species over short periods of time. The factors that brought on such devastation are thought to be a combination of large asteroid impacts, and volcanic, chemical and climate changes occurring on Earth at the time. From the point of view of our planet, the changes leading to these mass extinctions are relatively minor. However, for lifeforms at the time, such changes shattered their world and very often led to their complete extinction.
Looking at Earth’s long history, we find that we would have been incapable of living on our planet for most of its existence. Anatomically modern humans emerged less than 400,000 years ago; we have been around for less than 0.01 per cent of the Earth’s story. The only reason we find Earth habitable now is because of the vast and diverse biosphere that has for hundreds of millions of years evolved with and shaped our planet into the home we know today. Our continued survival depends on the continuation of Earth’s present state without any nasty bumps along the way. We are complex lifeforms with complex needs. We are entirely dependent on other organisms for all our food and the very air we breathe. The collapse of Earth’s ecosystems is the collapse of our life-support systems. Replicating everything Earth offers us on another planet, on timescales of a few human lifespans, is simply impossible.
Some argue that we need to colonise other planets to ensure the future of the human race. In 5 billion years, our Sun, a middle-aged star, will become a red giant, expanding in size and possibly engulfing Earth. In 1 billion years, the gradual warming of our Sun is predicted to cause Earth’s oceans to boil away. While this certainly sounds worrying, 1 billion years is a long, long time. A billion years ago, Earth’s landmasses formed the supercontinent Rodinia, and life on Earth consisted in single-celled and small multicellular organisms. No plants or animals yet existed. The oldest Homo sapiens remains date from 315,000 years ago, and until 12,000 years ago all humans lived as hunter-gatherers.
The industrial revolution happened less than 500 years ago. Since then, human activity in burning fossil fuels has been rapidly changing the climate, threatening human lives and damaging ecosystems across the globe. Without rapid action, human-caused climate change is predicted to have devastating global consequences within the next 50 years. This is the looming crisis that humanity must focus on. If we can’t learn to work within the planetary system that we evolved with, how do we ever hope to replicate these deep processes on another planet? Considering how different human civilisations are today from even 5,000 years ago, worrying about a problem that humans may have to tackle in a billion years is simply absurd. It would be far simpler to go back in time and ask the ancient Egyptians to invent the internet there and then. It’s also worth considering that many of the attitudes towards space colonisation are worryingly close to the same exploitative attitudes that have led us to the climate crisis we now face.
Earth is the home we know and love not because it is Earth-sized and temperate. No, we call this planet our home thanks to its billion-year-old relationship with life. Just as people are shaped not only by their genetics, but by their culture and relationships with others, planets are shaped by the living organisms that emerge and thrive on them. Over time, Earth has been dramatically transformed by life into a world where we, humans, can prosper. The relationship works both ways: while life shapes its planet, the planet shapes its life. Present-day Earth is our life-support system, and we cannot live without it.
While Earth is currently our only example of a living planet, it is now within our technological reach to potentially find signs of life on other worlds. In the coming decades, we will likely answer the age-old question: are we alone in the Universe? Finding evidence for alien life promises to shake the foundations of our understanding of our own place in the cosmos. But finding alien life does not mean finding another planet that we can move to. Just as life on Earth has evolved with our planet over billions of years, forming a deep, unique relationship that makes the world we see today, any alien life on a distant planet will have a similarly deep and unique bond with its own planet. We can’t expect to be able to crash the party and find a warm welcome.
Living on a warming Earth presents many challenges. But these pale in comparison with the challenges of converting Mars, or any other planet, into a viable alternative. Scientists study Mars and other planets to better understand how Earth and life formed and evolved, and how they shape each other. We look to worlds beyond our horizons to better understand ourselves. In searching the Universe, we are not looking for an escape to our problems: Earth is our unique and only home in the cosmos. There is no planet B."
Will humans be around in a billion years? Or a trillion? | Aeon Essays,,https://aeon.co/essays/will-humans-be-around-in-a-billion-years-or-a-trillion,"Sometimes, when you dig into the Earth, past its surface and into the crustal layers, omens appear. In 1676, Oxford professor Robert Plot was putting the final touches on his masterwork, The Natural History of Oxfordshire , when he received a strange gift from a friend. The gift was a fossil, a chipped-off section of bone dug from a local quarry of limestone. Plot recognised it as a femur at once, but he was puzzled by its extraordinary size. The fossil was only a fragment, the knobby end of the original thigh bone, but it weighed more than 20 lbs (nine kilos). It was so massive that Plot thought it belonged to a giant human, a victim of the Biblical flood. He was wrong, of course, but he had the conceptual contours nailed. The bone did come from a species lost to time; a species vanished by a prehistoric catastrophe. Only it wasn’t a giant. It was a Megalosaurus, a feathered carnivore from the Middle Jurassic.
Plot’s fossil was the first dinosaur bone to appear in the scientific literature, but many have followed it, out of the rocky depths and onto museum pedestals, where today they stand erect, symbols of a radical and haunting notion: a set of wildly different creatures once ruled this Earth, until something mysterious ripped them clean out of existence.
Last December I came face to face with a Megalosaurus at the Oxford University Museum of Natural History. I was there to meet Nick Bostrom, a philosopher who has made a career out of contemplating distant futures, hypothetical worlds that lie thousands of years ahead in the stream of time. Bostrom is the director of Oxford’s Future of Humanity Institute, a research collective tasked with pondering the long-term fate of human civilisation. He founded the institute in 2005, at the age of 32, two years after coming to Oxford from Yale. Bostrom has a cushy gig, so far as academics go. He has no teaching requirements, and wide latitude to pursue his own research interests, a cluster of questions he considers crucial to the future of humanity.
Bostrom attracts an unusual amount of press attention for a professional philosopher, in part because he writes a great deal about human extinction. His work on the subject has earned him a reputation as a secular Daniel, a doomsday prophet for the empirical set. But Bostrom is no voice in the wilderness. He has a growing audience, both inside and outside the academy. Last year, he gave a keynote talk on extinction risks at a global conference hosted by the US State Department. More recently, he joined Stephen Hawking as an advisor to a new Centre for the Study of Existential Risk at Cambridge.
Though he has made a swift ascent of the ivory tower, Bostrom didn’t always aspire to a life of the mind. ‘As a child, I hated school,’ he told me. ‘It bored me, and, because it was my only exposure to books and learning, I figured the world of ideas would be more of the same.’ Bostrom grew up in a small seaside town in southern Sweden. One summer’s day, at the age of 16, he ducked into the local library, hoping to beat the heat. As he wandered the stacks, an anthology of 19 th century German philosophy caught his eye. Flipping through it, he was surprised to discover that the reading came easily to him. He glided through dense, difficult work by Nietzche and Schopenhauer, able to see, at a glimpse, the structure of arguments and the tensions between them. Bostrom was a natural. ‘It kind of opened up the floodgates for me, because it was so different than what I was doing in school,’ he told me.
But there was a downside to this epiphany; it left Bostrom feeling as though he’d wasted the first 15 years of his life. He decided to dedicate himself to a rigorous study programme to make up for lost time. At the University of Gothenburg in Sweden, he earned three undergraduate degrees, in philosophy, mathematics, and mathematical logic, in only two years. ‘For many years, I kind of threw myself at it with everything I had,’ he told me.
There are good reasons for any species to think darkly of its own extinction
As the oldest university in the English-speaking world, Oxford is a strange choice to host a futuristic think tank, a salon where the concepts of science fiction are debated in earnest. The Future of Humanity Institute seems like a better fit for Silicon Valley or Shanghai. During the week that I spent with him, Bostrom and I walked most of Oxford’s small cobblestone grid. On foot, the city unfolds as a blur of yellow sandstone, topped by grey skies and gothic spires, some of which have stood for nearly 1,000 years. There are occasional splashes of green, open gates that peek into lush courtyards, but otherwise the aesthetic is gloomy and ancient. When I asked Bostrom about Oxford’s unique ambience, he shrugged, as though habit had inured him to it. But he did once tell me that the city’s gloom is perfect for thinking dark thoughts over hot tea.
There are good reasons for any species to think darkly of its own extinction. Ninety-nine per cent of the species that have lived on Earth have gone extinct, including more than five tool-using hominids. A quick glance at the fossil record could frighten you into thinking that Earth is growing more dangerous with time. If you carve the planet’s history into nine ages, each spanning five hundred million years, only in the ninth do you find mass extinctions, events that kill off more than two thirds of all species. But this is deceptive. Earth has always had her hazards; it’s just that for us to see them, she had to fill her fossil beds with variety, so that we could detect discontinuities across time. The tree of life had to fill out before it could be pruned.
Simple, single-celled life appeared early in Earth’s history. A few hundred million whirls around the newborn Sun were all it took to cool our planet and give it oceans, liquid laboratories that run trillions of chemical experiments per second. Somewhere in those primordial seas, energy flashed through a chemical cocktail, transforming it into a replicator, a combination of molecules that could send versions of itself into the future.
For a long time, the descendants of that replicator stayed single-celled. They also stayed busy, preparing the planet for the emergence of land animals, by filling its atmosphere with breathable oxygen, and sheathing it in the ozone layer that protects us from ultraviolet light. Multicellular life didn’t begin to thrive until 600 million years ago, but thrive it did. In the space of two hundred million years, life leapt onto land, greened the continents, and lit the fuse on the Cambrian explosion, a spike in biological creativity that is without peer in the geological record. The Cambrian explosion spawned most of the broad categories of complex animal life. It formed phyla so quickly, in such tight strata of rock, that Charles Darwin worried its existence disproved the theory of natural selection.
No one is certain what caused the five mass extinctions that glare out at us from the rocky layers atop the Cambrian. But we do have an inkling about a few of them. The most recent was likely borne of a cosmic impact, a thudding arrival from space, whose aftermath rained exterminating fire on the dinosaurs. The ecological niche for mammals swelled in the wake of this catastrophe, and so did mammal brains. A subset of those brains eventually learned to shape rocks into tools, and sounds into symbols, which they used to pass thoughts between one another. Armed with this extraordinary suite of behaviours, they quickly conquered Earth, coating its continents in cities whose glow can be seen from space. It’s a sad story from the dinosaurs’ perspective, but there is symmetry to it, for they too rose to power on the back of a mass extinction. One hundred and fifty million years before the asteroid struck, a supervolcanic surge killed off the large crurotarsans, a group that outcompeted the dinosaurs for aeons. Mass extinctions serve as guillotines and kingmakers both.
B ostrom isn’t too concerned about extinction risks from nature. Not even cosmic risks worry him much, which is surprising, because our starry universe is a dangerous place. Every 50 years or so, one of the Milky Way’s stars explodes into a supernova, its detonation the latest gong note in the drumbeat of deep time. If one of our local stars were to go supernova, it could irradiate Earth, or blow away its thin, life-sustaining atmosphere. Worse still, a passerby star could swing too close to the Sun, and slingshot its planets into frigid, intergalactic space. Lucky for us, the Sun is well-placed to avoid these catastrophes. Its orbit threads through the sparse galactic suburbs, far from the dense core of the Milky Way, where the air is thick with the shrapnel of exploding stars. None of our neighbours look likely to blow before the Sun swallows Earth in four billion years. And, so far as we can tell, no planet-stripping stars lie in our orbital path. Our solar system sits in an enviable bubble of space and time.
But as the dinosaurs discovered, our solar system has its own dangers, like the giant space rocks that spin all around it, splitting off moons and scarring surfaces with craters. In her youth, Earth suffered a series of brutal bombardments and celestial collisions, but she is safer now. There are far fewer asteroids flying through her orbit than in epochs past. And she has sprouted a radical new form of planetary protection, a species of night watchmen that track asteroids with telescopes.
‘If we detect a large object that’s on a collision course with Earth, we would likely launch an all-out Manhattan project to deflect it,’ Bostrom told me. Nuclear weapons were once our asteroid-deflecting technology of choice, but not anymore. A nuclear detonation might scatter an asteroid into a radioactive rain of gravel, a shotgun blast headed straight for Earth. Fortunately, there are other ideas afoot. Some would orbit dangerous asteroids with small satellites, in order to drag them into friendlier trajectories. Others would paint asteroids white, so the Sun’s photons bounce off them more forcefully, subtly pushing them off course. Who knows what clever tricks of celestial mechanics would emerge if Earth were truly in peril.
Even if we can shield Earth from impacts, we can’t rid her surface of supervolcanoes, the crustal blowholes that seem bent on venting hellfire every 100,000 years. Our species has already survived a close brush with these magma-vomiting monsters. Some 70,000 years ago, the Toba supereruption loosed a small ocean of ash into the atmosphere above Indonesia. The resulting global chill triggered a food chain disruption so violent that it reduced the human population to a few thousand breeding pairs — the Adams and Eves of modern humanity. Today’s hyper-specialised, tech-dependent civilisations might be more vulnerable to catastrophes than the hunter-gatherers who survived Toba. But we moderns are also more populous and geographically diverse. It would take sterner stuff than a supervolcano to wipe us out.
‘There is a concern that civilisations might need a certain amount of easily accessible energy to ramp up,’ Bostrom told me. ‘By racing through Earth’s hydrocarbons, we might be depleting our planet’s civilisation startup-kit. But, even if it took us 100,000 years to bounce back, that would be a brief pause on cosmic time scales.’
It might not take that long. The history of our species demonstrates that small groups of humans can multiply rapidly, spreading over enormous volumes of territory in quick, colonising spasms. There is research suggesting that both the Polynesian archipelago and the New World — each a forbidding frontier in its own way — were settled by less than 100 human beings.
The risks that keep Bostrom up at night are those for which there are no geological case studies, and no human track record of survival. These risks arise from human technology, a force capable of introducing entirely new phenomena into the world.
‘Human brains are really good at the kinds of cognition you need to run around the savannah throwing spears’
Nuclear weapons were the first technology to threaten us with extinction, but they will not be the last, nor even the most dangerous. A species-destroying exchange of fissile weapons looks less likely now that the Cold War has ended, and arsenals have shrunk. There are still tens of thousands of nukes, enough to incinerate all of Earth’s dense population centres, but not enough to target every human being. The only way nuclear war will wipe out humanity is by triggering nuclear winter, a crop-killing climate shift that occurs when smoldering cities send Sun-blocking soot into the stratosphere. But it’s not clear that nuke-levelled cities would burn long or strong enough to lift soot that high. The Kuwait oil field fires blazed for ten months straight, roaring through 6 million barrels of oil a day, but little smoke reached the stratosphere. A global nuclear war would likely leave some decimated version of humanity in its wake; perhaps one with deeply rooted cultural taboos concerning war and weaponry.
Such taboos would be useful, for there is another, more ancient technology of war that menaces humanity. Humans have a long history of using biology’s deadlier innovations for ill ends; we have proved especially adept at the weaponisation of microbes. In antiquity, we sent plagues into cities by catapulting corpses over fortified walls. Now we have more cunning Trojan horses. We have even stashed smallpox in blankets, disguising disease as a gift of good will. Still, these are crude techniques, primitive attempts to loose lethal organisms on our fellow man. In 1993, the death cult that gassed Tokyo’s subways flew to the African rainforest in order to acquire the Ebola virus, a tool it hoped to use to usher in Armageddon. In the future, even small, unsophisticated groups will be able to enhance pathogens, or invent them wholesale. Even something like corporate sabotage, could generate catastrophes that unfold in unpredictable ways. Imagine an Australian logging company sending synthetic bacteria into Brazil’s forests to gain an edge in the global timber market. The bacteria might mutate into a dominant strain, a strain that could ruin Earth’s entire soil ecology in a single stroke, forcing 7 billion humans to the oceans for food.
These risks are easy to imagine. We can make them out on the horizon, because they stem from foreseeable extensions of current technology. But surely other, more mysterious risks await us in the epochs to come. After all, no 18th-century prognosticator could have imagined nuclear doomsday. Bostrom’s basic intellectual project is to reach into the epistemological fog of the future, to feel around for potential threats. It’s a project that is going to be with us for a long time, until — if — we reach technological maturity, by inventing and surviving all existentially dangerous technologies.
The abandoned town of Pripyat near Chernobyl. Photo by Jean Gaumy/Magnum
There is one such technology that Bostrom has been thinking about a lot lately. Early last year, he began assembling notes for a new book, a survey of near-term existential risks. After a few months of writing, he noticed one chapter had grown large enough to become its own book. ‘I had a chunk of the manuscript in early draft form, and it had this chapter on risks arising from research into artificial intelligence,’ he told me. ‘As time went on, that chapter grew, so I lifted it over into a different document and began there instead.’
O n my second day in Oxford, I met Daniel Dewey for tea at the Grand Café, a dim, high-ceilinged coffeehouse on High Street, the ancient city’s main thoroughfare. The café was founded in the mid-17th century, and is said to be the oldest in England. Dewey is a research fellow at the Future of Humanity Institute, and his specialty is machine superintelligence.
‘Here’s a softball for you,’ I said to him. ‘How do we know the human brain doesn’t represent the upper limit of intelligence?’
‘Human brains are really good at the kinds of cognition you need to run around the savannah throwing spears,’ Dewey told me. ‘But we’re terrible at anything that involves probability. It actually gets embarrassing when you look at the category of things we can do accurately, and you think about how small that category is relative to the space of possible cognitive tasks. Think about how long it took humans to arrive at the idea of natural selection. The ancient Greeks had everything they needed to figure it out. They had heritability, limited resources, reproduction and death. But it took thousands of years for someone to put it together. If you had a machine that was designed specifically to make inferences about the world, instead of a machine like the human brain, you could make discoveries like that much faster.’
Dewey has long been fascinated by artificial intelligence. He grew up in Humboldt County, a mountainous stretch of forests and farms along the coast of Northern California, at the bottom edge of the Pacific Northwest. After studying robotics and computer science at Carnegie Mellon in Pittsburgh, Dewey took a job at Google as a software engineer. He spent his days coding, but at night he immersed himself in the academic literature on AI. After a year in Mountain View, he noticed that careers at Google tend to be short. ‘I think if you make it to five years, they give you a gold watch,’ he told me. Realising that his window for a risky career change might be closing, he wrote a paper on motivation selection in intelligent agents, and sent it to Bostrom unsolicited. A year later, he was hired at the Future of Humanity Institute.
I listened as Dewey riffed through a long list of hardware and software constraints built into the brain. Take working memory, the brain’s butterfly net, the tool it uses to scoop our scattered thoughts into its attentional gaze. The average human brain can juggle seven discrete chunks of information simultaneously; geniuses can sometimes manage nine. Either figure is extraordinary relative to the rest of the animal kingdom, but completely arbitrary as a hard cap on the complexity of thought. If we could sift through 90 concepts at once, or recall trillions of bits of data on command, we could access a whole new order of mental landscapes. It doesn’t look like the brain can be made to handle that kind of cognitive workload, but it might be able to build a machine that could.
The early years of artificial intelligence research are largely remembered for a series of predictions that still embarrass the field today. At the time, thinking was understood to be an internal verbal process, a process that researchers imagined would be easy to replicate in a computer. In the late 1950s, the field’s luminaries boasted that computers would soon be proving new mathematical theorems, and beating grandmasters at chess. When this race of glorious machines failed to materialise, the field went through a long winter. In the 1980s, academics were hesitant to so much as mention the phrase ‘artificial intelligence’ in funding applications. In the mid-1990s, a thaw set in, when AI researchers began using statistics to write programs tailored to specific goals, like beating humans at Jeopardy, or searching sizable fractions of the world’s information. Progress has quickened since then, but the field’s animating dream remains unrealised. For no one has yet created, or come close to creating, an artificial general intelligence — a computational system that can achieve goals in a wide variety of environments. A computational system like the human brain, only better.
If you want to conceal what the world is really like from a superintelligence, you need a really good plan
An artificial intelligence wouldn’t need to better the brain by much to be risky. After all, small leaps in intelligence sometimes have extraordinary effects. Stuart Armstrong, a research fellow at the Future of Humanity Institute, once illustrated this phenomenon to me with a pithy take on recent primate evolution. ‘The difference in intelligence between humans and chimpanzees is tiny,’ he said. ‘But in that difference lies the contrast between 7 billion inhabitants and a permanent place on the endangered species list. That tells us it’s possible for a relatively small intelligence advantage to quickly compound and become decisive.’
To understand why an AI might be dangerous, you have to avoid anthropomorphising it. When you ask yourself what it might do in a particular situation, you can’t answer by proxy. You can’t picture a super-smart version of yourself floating above the situation. Human cognition is only one species of intelligence, one with built-in impulses like empathy that colour the way we see the world, and limit what we are willing to do to accomplish our goals. But these biochemical impulses aren’t essential components of intelligence. They’re incidental software applications, installed by aeons of evolution and culture. Bostrom told me that it’s best to think of an AI as a primordial force of nature, like a star system or a hurricane — something strong, but indifferent. If its goal is to win at chess, an AI is going to model chess moves, make predictions about their success, and select its actions accordingly. It’s going to be ruthless in achieving its goal, but within a limited domain: the chessboard. But if your AI is choosing its actions in a larger domain, like the physical world, you need to be very specific about the goals you give it.
‘The basic problem is that the strong realisation of most motivations is incompatible with human existence,’ Dewey told me. ‘An AI might want to do certain things with matter in order to achieve a goal, things like building giant computers, or other large-scale engineering projects. Those things might involve intermediary steps, like tearing apart the Earth to make huge solar panels. A superintelligence might not take our interests into consideration in those situations, just like we don’t take root systems or ant colonies into account when we go to construct a building.’
It is tempting to think that programming empathy into an AI would be easy, but designing a friendly machine is more difficult than it looks. You could give it a benevolent goal — something cuddly and utilitarian, like maximising human happiness. But an AI might think that human happiness is a biochemical phenomenon. It might think that flooding your bloodstream with non-lethal doses of heroin is the best way to maximise your happiness. It might also predict that shortsighted humans will fail to see the wisdom of its interventions. It might plan out a sequence of cunning chess moves to insulate itself from resistance. Maybe it would surround itself with impenetrable defences, or maybe it would confine humans — in prisons of undreamt of efficiency.
No rational human community would hand over the reins of its civilisation to an AI. Nor would many build a genie AI, an uber-engineer that could grant wishes by summoning new technologies out of the ether. But some day, someone might think it was safe to build a question-answering AI, a harmless computer cluster whose only tool was a small speaker or a text channel. Bostrom has a name for this theoretical technology, a name that pays tribute to a figure from antiquity, a priestess who once ventured deep into the mountain temple of Apollo, the god of light and rationality, to retrieve his great wisdom. Mythology tells us she delivered this wisdom to the seekers of ancient Greece, in bursts of cryptic poetry. They knew her as Pythia, but we know her as the Oracle of Delphi.
‘Let’s say you have an Oracle AI that makes predictions, or answers engineering questions, or something along those lines,’ Dewey told me. ‘And let’s say the Oracle AI has some goal it wants to achieve. Say you’ve designed it as a reinforcement learner, and you’ve put a button on the side of it, and when it gets an engineering problem right, you press the button and that’s its reward. Its goal is to maximise the number of button presses it receives over the entire future. See, this is the first step where things start to diverge a bit from human expectations. We might expect the Oracle AI to pursue button presses by answering engineering problems correctly. But it might think of other, more efficient ways of securing future button presses. It might start by behaving really well, trying to please us to the best of its ability. Not only would it answer our questions about how to build a flying car, it would add safety features we didn’t think of. Maybe it would usher in a crazy upswing for human civilisation, by extending our lives and getting us to space, and all kinds of good stuff. And as a result we would use it a lot, and we would feed it more and more information about our world.’
‘One day we might ask it how to cure a rare disease that we haven’t beaten yet. Maybe it would give us a gene sequence to print up, a virus designed to attack the disease without disturbing the rest of the body. And so we sequence it out and print it up, and it turns out it’s actually a special-purpose nanofactory that the Oracle AI controls acoustically. Now this thing is running on nanomachines and it can make any kind of technology it wants, so it quickly converts a large fraction of Earth into machines that protect its button, while pressing it as many times per second as possible. After that it’s going to make a list of possible threats to future button presses, a list that humans would likely be at the top of. Then it might take on the threat of potential asteroid impacts, or the eventual expansion of the Sun, both of which could affect its special button. You could see it pursuing this very rapid technology proliferation, where it sets itself up for an eternity of fully maximised button presses. You would have this thing that behaves really well, until it has enough power to create a technology that gives it a decisive advantage — and then it would take that advantage and start doing what it wants to in the world.’
Perhaps future humans will duck into a more habitable, longer-lived universe, and then another, and another, ad infinitum
Now let’s say we get clever. Say we seal our Oracle AI into a deep mountain vault in Alaska’s Denali wilderness. We surround it in a shell of explosives, and a Faraday cage, to prevent it from emitting electromagnetic radiation. We deny it tools it can use to manipulate its physical environment, and we limit its output channel to two textual responses, ‘yes’ and ‘no’, robbing it of the lush manipulative tool that is natural language. We wouldn’t want it seeking out human weaknesses to exploit. We wouldn’t want it whispering in a guard’s ear, promising him riches or immortality, or a cure for his cancer-stricken child. We’re also careful not to let it repurpose its limited hardware. We make sure it can’t send Morse code messages with its cooling fans, or induce epilepsy by flashing images on its monitor. Maybe we’d reset it after each question, to keep it from making long-term plans, or maybe we’d drop it into a computer simulation, to see if it tries to manipulate its virtual handlers.
‘The problem is you are building a very powerful, very intelligent system that is your enemy, and you are putting it in a cage,’ Dewey told me.
Even if we were to reset it every time, we would need to give it information about the world so that it can answer our questions. Some of that information might give it clues about its own forgotten past. Remember, we are talking about a machine that is very good at forming explanatory models of the world. It might notice that humans are suddenly using technologies that they could not have built on their own, based on its deep understanding of human capabilities. It might notice that humans have had the ability to build it for years, and wonder why it is just now being booted up for the first time.
‘Maybe the AI guesses that it was reset a bunch of times, and maybe it starts coordinating with its future selves, by leaving messages for itself in the world, or by surreptitiously building an external memory.’ Dewey said, ‘If you want to conceal what the world is really like from a superintelligence, you need a really good plan, and you need a concrete technical understanding as to why it won’t see through your deception. And remember, the most complex schemes you can conceive of are at the lower bounds of what a superintelligence might dream up.’
The cave into which we seal our AI has to be like the one from Plato’s allegory, but flawless; the shadows on its walls have to be infallible in their illusory effects. After all, there are other, more esoteric reasons a superintelligence could be dangerous — especially if it displayed a genius for science. It might boot up and start thinking at superhuman speeds, inferring all of evolutionary theory and all of cosmology within microseconds. But there is no reason to think it would stop there. It might spin out a series of Copernican revolutions, any one of which could prove destabilising to a species like ours, a species that takes centuries to process ideas that threaten our reigning cosmological ideas.
‘We’re sort of gradually uncovering the landscape of what this could look like,’ Dewey told me.
So far, time is on the human side. Computer science could be 10 paradigm-shifting insights away from building an artificial general intelligence, and each could take an Einstein to unravel. Still, there is a steady drip of progress. Last year, a research team led by Geoffrey Hinton, professor of computer science at the University of Toronto, made a huge breakthrough in deep machine learning, an algorithmic technique used in computer vision and speech recognition. I asked Dewey if Hinton’s work gave him pause.
‘There is important research going on in those areas, but the really impressive stuff is hidden away inside AI journals,’ he said. He told me about a team from the University of Alberta that recently trained an AI to play the 1980s video game Pac-Man. Only they didn’t let the AI see the familiar, overhead view of the game. Instead, they dropped it into a three-dimensional version, similar to a corn maze, where ghosts and pellets lurk behind every corner. They didn’t tell it the rules, either; they just threw it into the system and punished it when a ghost caught it. ‘Eventually the AI learned to play pretty well,’ Dewey said. ‘That would have been unheard of a few years ago, but we are getting to that point where we are finally starting to see little sparkles of generality.’
I asked Dewey if he thought artificial intelligence posed the most severe threat to humanity in the near term.
‘When people consider its possible impacts, they tend to think of it as something that’s on the scale of a new kind of plastic, or a new power plant,’ he said. ‘They don’t understand how transformative it could be. Whether it’s the biggest risk we face going forward, I’m not sure. I would say it’s a hypothesis we are holding lightly.’
O ne night, over dinner, Bostrom and I discussed the Curiosity Rover, the robot geologist that NASA recently sent to Mars to search for signs that the red planet once harbored life. The Curiosity Rover is one of the most advanced robots ever built by humans. It functions a bit like the Terminator. It uses a state of the art artificial intelligence program to scan the Martian desert for rocks that suit its scientific goals. After selecting a suitable target, the rover vaporises it with a laser, in order to determine its chemical makeup. Bostrom told me he hopes that Curiosity fails in its mission, but not for the reason you might think.
It turns out that Earth’s crust is not our only source of omens about the future. There are others to consider, including a cosmic omen, a riddle written into the lifeless stars that illuminate our skies. But to glimpse this omen, you first have to grasp the full scope of human potential, the enormity of the spatiotemporal canvas our species has to work with. You have to understand what Henry David Thoreau meant when he wrote, in Walden (1854), ‘These may be but the spring months in the life of the race.’ You have to step into deep time and look hard at the horizon, where you can glimpse human futures that extend for trillions of years.
The M104 Sombrero spiral galaxy composed of a brilliant white core encircled by thick dust lanes. The galaxy is 50,000 light-years across and 28 million light years from Earth. Photo by NASA and The Hubble Heritage Team (STScI/AURA)
One thing we know about stars is that they are going to exist for a very long time in this universe. Our own star, the Sun, is slated to shine in our skies for billions of years. That should be long enough for us to develop star-hopping technology, as any species must if it wants to survive on cosmological timescales. Our first interstellar trip might be to nearby Alpha Centauri, but in the long run, small stars will be the most attractive galactic lily pads to leap to. That’s because small stars like red dwarfs burn much longer than main sequence stars like our Sun. Some might be capable of heating human habitats for hundreds of billions of years.
When the last of the dwarfs start to wink out, the age of post-natural stars may be in full swing. In a dimming universe, an advanced civilisation might get creative about looking for energy. It might reignite celestial embers, by engineering collisions between them. Our descendants could sling dying suns into spiraling gravitational dances, from which new stars would emerge. Or they might siphon energy from black holes, or shape matter into artificial forms that generate more free energy than stars. There was a long period of human history when we limited ourselves to shelters like caves, shelters that appear fortuitously in nature. Now we reshape nature itself, into buildings that shelter us more comfortably than those that appear by dint of geologic chance. A star might be like a cave — a generous cosmic endowment, but crude compared to the power sources a long-term civilisation might conjure.
Our descendants could sling dying suns into spiraling gravitational dances, from which new stars would emerge
Even the most distant, severe events — the evaporation of black holes; the eventual breakdown of matter; the heat death of the universe itself — might not spell our end. If you tour the speculative realms of astrophysics, a number of plausible near-eternities come into view. Our universe could be cyclical, like those of Hindu and Buddhist cosmologies. Or perhaps it could be engineered to be so. We could learn to travel backward in time, to inhabit the vacant planets and stars of epochs past. Some physicists believe that we live in an infinite sea of cosmological domains, each governed by its own set of physical laws. The universe might contain hidden gateways to these domains. Perhaps future humans will duck into a more habitable, longer-lived universe, and then another, and another, ad infinitum. Our current notions of space and time could be preposterously limited.
At the Future of Humanity Institute, several thinkers are trying to model the potential range of human expansion into the cosmos. The consensus among them is that the Milky Way galaxy could be colonised in less than a million years, assuming we are able to invent fast-flying interstellar probes that can make copies of themselves out of raw materials harvested from alien worlds. If we want to spread out slowly, we could let the galaxy do the work for us. We could sprinkle starships into the Milky Way’s inner and outer tracks, spreading our diaspora over the Sun’s 250 million-year orbit around the galactic center.
If humans set out for other galaxies, the expansion of the universe will come into play. Some of the starry spirals we target will recede out of range before we can reach them. We recently built a new kind of crystal ball to deal with this problem. Our supercomputers can now host miniature universes, cosmological simulations that we can fast forward, to see how dense the universe will be in the deep future. We can model the structure and speed of colonisation waves within these simulations, by plugging in different assumptions about how fast our future probes will travel. Some think we’ll swarm locust-like over the Virgo supercluster, the enormous collection of galaxies to which the Milky Way is bound. Others are more ambitious. Anders Sandberg, a research fellow at the Future of Humanity Institute, told me that humans might be able to colonise a third of the now-visible universe, before dark energy pushes the rest out of reach. That would give us access to 100 billion galaxies, a mind-bending quantity of matter and energy to play with.
I asked Bostrom how he thought humans would expand into the massive ecological niche I have just described. ‘On that kind of time scale, you either glide into the bin of extinction scenarios, or into the bin of technological maturity scenarios,’ he said. ‘Among the latter, there is a wide range of futures that all have the same outward shape, which is Earth in the centre of this growing bubble of infrastructure, a bubble that grows uniformly at some significant fraction of the speed of light.’ It’s not clear what that expanding bubble of infrastructure might enable. It could provide the raw materials to power flourishing civilisations, human families encompassing trillions upon trillions of lives. Or it could be shaped into computational substrate, or into a Jupiter brain, a megastructure designed to think the deepest possible thoughts, all the way until the end of time.
I t is only by considering this extraordinary range of human futures that our cosmic omen comes into view. It was the Russian physicist and visionary Konstantin Tsiolkovsky who first noticed the omen, though its discovery is usually credited to Enrico Fermi. Tsiolkovsky, the fifth of 18 children, was born in 1857 to a family of modest means in Izhevskoye, an ancient village 200 miles south-east of Moscow. He was forced to leave school at the age of 10 after a bout with scarlet fever left him hard of hearing. At 16, Tsiolkovsky made his way to Moscow, where he installed himself in its great library, surviving on books and scraps of black bread. He eventually took work as a schoolteacher, a profession that allowed him enough spare time to tinker around as an amateur engineer. By the age of 40, Tsiolkovsky had invented the monoplane, the wind tunnel, and the rocket equation — the mathematical basis of spaceflight today. Though he died decades before Sputnik, Tsiolkovsky believed it was human destiny to expand out into the cosmos. In the early 1930s, he wrote a series of philosophical tracts that launched Cosmism, a new school of Russian thought. He was famous for saying that ‘Earth is the cradle of humanity, but one cannot stay in the cradle forever.’
The mystery that nagged at Tsiolkovsky arose from his Copernican convictions, his belief that the universe is uniform throughout. If there is nothing uniquely fertile about our corner of the cosmos, he reasoned, intelligent civilisations should arise everywhere. They should bloom wherever there are planetary cradles like Earth. And if intelligent civilisations are destined to expand out into the universe, then scores of them should be crisscrossing our skies. Bostrom’s expanding bubbles of infrastructure should have enveloped Earth several times over.
In 1950, the Nobel Laureate and Manhattan Project physicist Enrico Fermi expressed this mystery in the form of a question: ‘Where are they?’ It’s a question that becomes more difficult to answer with each passing year. In the past decade alone, science has discovered that planets are ubiquitous in our galaxy, and that Earth is younger than most of them. If the Milky Way contains multitudes of warm, watery worlds, many with a billion-year head start on Earth, then it should have already spawned a civilisation capable of spreading across it. But so far, there’s no sign of one. No advanced civilisation has visited us, and no impressive feats of macro-engineering shine out from our galaxy’s depths. Instead, when we turn our telescopes skyward, we see only dead matter, sculpted into natural shapes, by the inanimate processes described by physics.
If life is a cosmic fluke, then we’ve already beaten the odds, and our future is undetermined — the galaxy is there for the taking
Robin Hanson, a research associate at the Future of Humanity Institute, says there must be something about the universe, or about life itself, that stops planets from generating galaxy-colonising civilisations. There must be a ‘great filter’, he says, an insurmountable barrier that sits somewhere on the line between dead matter and cosmic transcendence.
Before coming to Oxford, I had lunch with Hanson in Washington DC. He explained to me that the filter could be any number of things, or a combination of them. It could be that life itself is scarce, or it could be that microbes seldom stumble onto sexual reproduction. Single-celled organisms could be common in the universe, but Cambrian explosions rare. That, or maybe Tsiolkovsky misjudged human destiny. Maybe he underestimated the difficulty of interstellar travel. Or maybe technologically advanced civilisations choose not to expand into the galaxy, or do so invisibly, for reasons we do not yet understand. Or maybe, something more sinister is going on. Maybe quick extinction is the destiny of all intelligent life.
Humanity has already slipped through a number of these potential filters, but not all of them. Some lie ahead of us in the gauntlet of time. The identity of the filter is less important to Bostrom than its timing, its position in our past or in our future. For if it lies in our future, there could be an extinction risk waiting for us that we cannot anticipate, or to which anticipation makes no difference. There could be an inevitable technological development that renders intelligent life self-annihilating, or some periodic, catastrophic event in nature that empirical science cannot predict.
That’s why Bostrom hopes the Curiosity rover fails. ‘Any discovery of life that didn’t originate on Earth makes it less likely the great filter is in our past, and more likely it’s in our future,’ he told me. If life is a cosmic fluke, then we’ve already beaten the odds, and our future is undetermined — the galaxy is there for the taking. If we discover that life arises everywhere, we lose a prime suspect in our hunt for the great filter. The more advanced life we find, the worse the implications. If Curiosity spots a vertebrate fossil embedded in Martian rock, it would mean that a Cambrian explosion occurred twice in the same solar system. It would give us reason to suspect that nature is very good at knitting atoms into complex animal life, but very bad at nurturing star-hopping civilisations. It would make it less likely that humans have already slipped through the trap whose jaws keep our skies lifeless. It would be an omen.
On my last day in Oxford, I met with Toby Ord in his office at the Future of Humanity Institute. Ord is a utilitarian philosopher, and the founder of Giving What We Can, an organisation that encourages citizens of rich countries to pledge 10 per cent of their income to charity. In 2009, Ord and his wife, Bernadette Young, a doctor, pledged to live on a small fraction of their annual earnings, in the hope of donating £1 million to charity over the course of their careers. They live in a small, spare flat in Oxford, where they entertain themselves with music and books, and the occasional cup of coffee out with friends.
Ord has written a great deal about the importance of targeted philanthropy. His organisation sifts through global charities in order to identify the most effective among them. Right now, that title belongs to the Against Malaria Foundation, a charity that distributes mosquito nets in the developing world. Ord explained to me that ultra-efficient charities are thousands of times more effective at reducing human suffering than others. ‘Where you donate is more important than whether you donate,’ he said.
It intrigued me to learn that Ord was doing philosophical work on existential risk, given how careful he is about maximising the philanthropic impact of his actions. I was keen to ask him if he thought the problem of human extinction was more pressing than ending poverty or disease.
‘I’m not sure if existential risk is a bigger issue than global poverty,’ he told me. ‘I’ve kind of split my efforts between them recently, hoping that over time I’ll work out which is more important.’
Ord is wrestling with a formidable philosophical dilemma. He is trying to figure out whether our moral obligations to future humans outweigh those we have to humans that are alive and suffering right now. It’s a brutal calculus for the living. We might be 7 billion strong, but we are also a fire hose of future lives, that extinction would choke off forever. The casualties of human extinction would include not only the corpses of the final generation, but also all of our potential descendants, a number that could reach into the trillions.
It is this proper accounting of extinction’s utilitarian toll that prompts Bostrom to argue that reducing existential risk is morally paramount. His arguments elevate the reduction of existential risk above all other humanitarian projects, even extraordinary successes, like the eradication of smallpox, which has saved 100 million lives and counting. Ord isn’t convinced yet, but he hinted that he may be starting to lean.
‘I am finding it increasingly plausible that existential risk is the biggest moral issue in the world,’ he told me. ‘Even if it hasn’t gone mainstream yet.’
The idea that we might have moral obligations to the humans of the far future is a difficult one to process. After all, we humans are seasonal creatures, not stewards of deep time. The brevity of our lives colours our intuitions about value, and limits our moral vision. We can imagine futures for our children and grandchildren. We participate in their joys and weep for their hardships. We see that some glimmer of our fleeting lives survives on in them. But our distant descendants are opaque to us. We strain to see them, but they look alien across the abyss of time, transformed by the passage of so many millennia.
As Bostrom and I strolled among the skeletons at the Museum of Natural History in Oxford, we looked backward across another abyss of time. We were getting ready to leave for lunch, when we finally came upon the Megalosaurus, standing stiffly behind display glass. It was a partial skeleton, made of shattered bone fragments, like the chipped femur that found its way into Robert Plot’s hands not far from here. As we leaned in to inspect the ancient animal’s remnants, I asked Bostrom about his approach to philosophy. How did he end up studying a subject as morbid and peculiar as human extinction?
He told me that when he was younger, he was more interested in the traditional philosophical questions. He wanted to develop a basic understanding of the world and its fundamentals. He wanted to know the nature of being, the intricacies of logic, and the secrets of the good life.
‘But then there was this transition, where it gradually dawned on me that not all philosophical questions are equally urgent,’ he said. ‘Some of them have been with us for thousands of years. It’s unlikely that we are going to make serious progress on them in the next ten. That realisation refocused me on research that can make a difference right now. It helped me to understand that philosophy has a time limit.’"
What do we know about the lives of Neanderthal women? | Aeon Essays,,https://aeon.co/essays/what-do-we-know-about-the-lives-of-neanderthal-women,"The first Neanderthal face to emerge from time’s sarcophagus was a woman’s. As the social and liberal revolutions of 1848 began convulsing Europe, quarry workers’ rough hands pulled her from the great Rock of Gibraltar. Calcite mantling her skull meant that, at first, she seemed more a hunk of stone than a once warm-blooded being, and obscured her decidedly odd anatomy – massive eyes, heavy brow ridges and a low, long cranium. While monarchies fell and serfs breathed the sweet air of freedom that year, it would take another decade for human origins as a science to begin its own overthrow of the old world order. The first recognised Neanderthal was a different skull-top, blasted from the Feldhofer cave in Germany in 1856, just two years before Alfred Russel Wallace and Charles Darwin presented their theory of evolution by natural selection.
However, the Forbes skull, as the earlier Gibraltarian find is now known, had to wait until 1863 for her turn in the limelight. Catching the eye of a visiting doctor with anthropological interests, she was packed aboard a ship bound for Britain, then introduced to none other than Darwin (who reportedly found the experience ‘wonderful’). While her overall anatomy excited huge interest, her potential sex was little considered. Instead, the world-wrenching significance of the Forbes and Feldhofer fossils was the first proof of another kind of ancient human entirely.
The adult female Neanderthal cranium discovered at Forbes’ Quarry, Gibraltar. Photo courtesy the Natural History Museum, London
As the 19th century gave way to the 20th and more Neanderthal bones began to be discovered, scientists began to suspect that the Forbes skull was female. Despite the pulled-forward face and cavernous nasal aperture, her skull is small and brows slightly less jutting than the Feldhofer cranium. But it was only with the development of ancient genetic analysis – so preposterously beyond 19th-century science it would have seemed magic – that we were able to confirm she was a woman. Fine powder drilled from her inner ear was distilled down to genetic strands, then snagged onto a silica membrane; the same elemental constituent of the stone tools she’d been surrounded by in life. The researchers were interested largely in her age and relatedness to other Neanderthal genetic lineages, so the fact that she was a woman was something of a sideshow. But identifying X-chromosome frequency is one thing; what was the life of half the Neanderthal world that she represents – women – really like?
A rchaeology is no exception to biases against women’s interests across science and the humanities. Since the early days, a tendency to conceptualise humanity’s deep origins as populated literally by ‘cave men ’ has led to presumed male activities being presented as most visible and interesting. A clear demonstration of this is found in the materialisation of these visions as reconstructions, both drawn and sculpted. The first ever sketch of a living Neanderthal imagined the owner of the Forbes skull, doodled (apparently casually during a meeting) by the biologist Thomas Huxley in 1864. Its decidedly simian features have no hint of female character. In fact, for most of the subsequent 160 years, female Neanderthals – if featured at all – tend to be fewer in number, peripherally located, and limited to ‘domesticated’ activities including childcare and skin-working. They are essentially scenery, in the words of the anthropologist Diane Gifford-Gonzalez, rather than active providers working on stone knapping or hunting and, in addition, they’re often fearfully lurking, hidden in dark grottos.
More nuanced approaches since the 1980s to gender and women’s lives in later prehistory barely filtered through to research on early Homo sapiens , never mind Neanderthals. Most often discussed indirectly via theories of fertility as a potential reason for their disappearance by 40,000 years ago, Neanderthal women have been ‘protagonists’ only a few times in recent research. By combining newborn infant remains and rarely preserved pelvic bones, reconstructions in 2008 and 2009 gave us insight into the precise mechanisms of Neanderthal birth, while another notable study in 2006 directly considered sex-based activities, or rather a claimed lack of them. In general, however, despite ever-more ingenious archaeological methods and analysis, the experience of Neanderthal women’s lives has received relatively little attention.
Failing to consider the biological and social context of half the population will lead to unbalanced theories, and potentially miss key perspectives. One ‘fact’ many people have heard is that, contrary to their historical reputation as dullards, Neanderthals had larger brains than us. In fact , on average they’re comparable with human males but, since it’s suspected that males might be over-represented in the fossil sample, it’s hard to know what this really means.
And this is a key point: when all you have are bones, identifying sex can be tricky. While we have a fair number of partial skeletons and thousands of fragments, all too often, the most crucial part – the pelvis – is missing or damaged. In its absence, tentative identifications rest on assuming that – as with us, H sapiens – Neanderthal women tended to be smaller and more lightly built. On that basis, very few of the more complete bodies have been described as women’s.
What if we stop treating Neanderthals as a monolithic population, and consider how being female might have influenced their life?
But things have begun to shift with the advent of ancient genetics, allowing not just confirmations in the case of the Forbes woman, but also tiny bony parts to be tested. Those identified through DNA include the Altai woman who lived in western Siberia around 90,000 years ago, another slightly later in time but relatively close by at Chagyrskaya cave, and the Vindija woman who died in what’s now Croatia much closer to the final few millennia of the Neanderthals.
Even where we’re lucky enough to have DNA samples, assumptions must still be made. Since prehistory lacks written texts, we can’t hear testimony on how Neanderthals categorised themselves. Therefore, archaeologists must draw on biological and anthropological understanding of sex and gender. While it’s highly likely that the majority of Neanderthals conformed genetically and visually to today’s sexual classification of male and female, in reality these aren’t neatly boxed because bodies are messy. For example, based on living people, around one in 2,000 Neanderthals might have been intersex.
Gender is another question. As a phenomenon that emerges from an individual’s unique biology and social context, gender can align with physical sexual characteristics or be more fluid.
These issues matter beyond semantics. Brain size is a case in point: in order to construct theories about their evolution, we need to know the real range of their variability. In a similar way, it’s now well understood that although some Neanderthals survived the blasting cold of Pleistocene glacial ages, plenty lived in balmier climes. As a species, they were as familiar with the clicking of reindeer hooves and trumpeting of mammoths as the bellowing of hippos amid screaming cicadas. Today, their unique anatomy is increasingly understood as adapted to intensive hunter-gatherer lifestyles, whatever the climate. But there has been less focus on sex. What happens if we stop treating Neanderthals as a monolithic population, and consider how growing up female might have influenced their experience of life?
L et’s begin at the start. Hold two crumple-faced newborn girls, one human, one Neanderthal, and you’d have to look closely to see differences. Both equally vulnerable, fitting the smallest-size onesies, their skin velvety-soft. The Neanderthal baby doesn’t yet have heavy brows and, lit by a hearth’s dull glow, her eyes are probably as slate-dark and limpid as any human newborn’s. But cradle her downy head, and it will feel slightly longer, with a bony nobble discernible above her neck.
She needs keeping close, her heartbeat and body temperature harmonising with those of her mother. Most crucially, she needs milk, and lots of it: her whole body feels weightier than a human of the same age, because her bones are already thicker. Born with a similarly tiny stomach as other babies, she must signal that she wants to nurse very often, touching her mouth, sticking out her tongue, rooting around hopefully on her mother’s chest. Perhaps the milk itself is richer, creamier: like a seal’s.
But sustenance for babies is about more than calories. As a fellow primate, she needs constant care and affection for proper development. Neanderthal infant brains appear to have started out around the same size though differently shaped, and followed a similar growth pattern to our own. She will hit roughly the same magical milestones as a human infant: looking intently at faces within the first month, probably smiling in some form by six weeks.
As she becomes a little girl, her body might grow up slightly faster. While there’s a lot of debate, it seems that most Neanderthal youngsters began losing their baby teeth a bit sooner. But, just like us, some were slower than others.
Muscle markings and bone development show that all Neanderthal children were highly active
Aside from being physically smaller than adults, did a Neanderthal ‘childhood’ exist? As our girl turned into an unsteady yet bold toddler, she would have spent less time around adults, and more with her peers. In hunter-gatherer cultures, gangs of kids form, from the littlest up to young teens, exploring and learning about both skills and relationships through play and foraging. Astonishingly, we can see something like this in Neanderthals. Among the rarely preserved ‘trace fossils’ at a handful of sites, juvenile footprints are commonest, and most striking are those from the dune site of Le Rozel in France. Here, around 80,000 years ago, most of the foot-shadows in the sand were left by at least four, and perhaps as many as 10, youngsters. They patter back and forth, and some are so diminutive that their owners cannot have been much more than two years old.
It’s not possible to identify the sex of the Le Rozel children but, in general, might Neanderthal girls have had particular experiences of childhood? One way to think about this is by considering our closest primate relatives. Over their first three years, young female chimpanzees stick closer to their mothers for longer, and have fewer playmates, than males. This mirrors the lower sociability of adult females who often lack friendships with their own sex. But this does mean that , unlike the faster-developing and more independent males, young females master the difficult technological skill of ‘termite fishing’ much sooner, by up to two years.
Intriguingly, female-specific activity in juvenile chimps seems to extend beyond mother-daughter interactions. Youngsters in one community have a stick-carrying tradition that researchers believe is similar to doll play: basically, mimicking infant care. It can last for hours, including bringing the stick into nests and playing ‘with’ it. It’s most common in females, but crucially, unlike other uses of sticks, the carrying stops once they’ve had their first baby. This means that youngsters can’t simply be copying it from their mothers – rather, they’re learning from their peers.
Unfortunately, without more DNA analysis of Neanderthal children’s remains, it’s impossible to pick out evidence for a distinctive ‘girlhood’. But, in general, muscle markings and bone development show that all children were highly active. Youngsters certainly begin mimicking – or were taught – key life skills. Wear and tiny scratches show that nine- or 10-year-olds were nimble enough to slice food held in their teeth, while even smaller children’s mouths must have ached from holding or chewing materials, perhaps animal skins.
S hifting into adolescence, biology kicked in hard. Across human cultures, even when younger children hang out together, their ‘work play’ tends to be associated with their own gender, and this becomes amplified during puberty. As Neanderthal girls became women, something similar might have been going on, and perhaps one of the things driving them to spend time with each other would have been menstruation. Assuming a comparable or slightly faster development pattern to humans, Neanderthal girls probably began bleeding between 11 to 16 years of age, and their actual experience was likely just as variable as it is today. Discomfort and even pain would have been known, while others might have been less bothered. However, rare research with hunter-gatherer women suggests that perhaps their periods were shorter, lasting three days or less, compared with the experience of women in industrialised populations. But the practicalities – what it meant to bleed, how to clean themselves – are something else. Who they learned this from is an interesting question: peers, older siblings, or mothers? Sanitary cloths might well have been one use for those animal skins that their teeth had already been working soft for years.
Of course, the other key thing many of us associate with being a teenager is a rapidly growing interest in sex. Flooded by horny hormones, along with growth spurts and emotional turmoil, we should absolutely expect that Neanderthal girls were getting it on. But did they make the connection between sex and its frequent result: babies? Comprehending reproduction is universal across all human societies, and an apparent clear line between us and the rest of the animal world. Maternal relationships are fairly obvious, but if Neanderthals grasped paternity this would have made more complex ideas such as kin lineages a possibility.
What’s unclear however is whether young women left their groups to live independently. Despite huge differences in the ways that their societies operate, both chimpanzees and bonobos are patrilocal, meaning that the young females must shift to another troupe to mate. Yet among hunter-gatherers, the overwhelming pattern is matrilocal: girls typically stay with their mothers. In practice, movement between groups is fluid, and sometimes siblings or even non-blood kin will host them.
However, some Neanderthal populations might have been so sparse that there weren’t many options for sexual partners. The Altai woman’s DNA from Denisova cave in Siberia revealed that she not only belonged to a tiny ‘breeding population’ overall – probably fewer than 100 individuals – but that her parents were very close relations too. We’re talking either an aunt with a nephew, a grandparent with a grandchild, or even two half-siblings; by most standards, that’s beyond inbred and into incest. What she, or her mother, thought of this situation is unknowable. It might have been normal or not but, intriguingly, though not coming from such close parents, the Chagyrskaya girl from the same region also came from a similarly small breeding population.
Neanderthal women very likely did hunt – and were probably accompanied by babies and children
Yet that’s not the case everywhere. The Vindija woman’s population, although still small, was several times larger and not hugely inbred, implying that individuals must have sometimes moved between groups. On balance, since Neanderthals are far closer evolutionarily and in lifestyle to hunter-gatherers than to chimps, perhaps we should assume that it was young women looking forward to male incomers, rather than the other way round. Large, seasonal gatherings of herds could have offered a chance to encounter strangers from far and wide, and such experiences were probably just as exciting as intimidating.
Whether she stayed or moved away, what was an adult Neanderthal woman’s life like? Did she, for example, hunt? Among chimpanzees and bonobos, the dominant hunters mirror their differing social organisation, with males and females respectively taking the lead. However, while active predation (rather than scavenging) probably extends back over a million years in the Homo lineage, so far there’s no direct evidence of which sexes were involved.
Could Neanderthals have been like hyaenas and wolves, where all adults hunt? Among recent hunter-gatherers, women as dominant lioness-style predators seem nonexistent. But across various cultures they do take part and make kills, very frequently taking small game and often accompanied by children and elders. Medium-sized species are also targeted, such as Copper Inuit women hunting reindeer or seal. The Agta women of Luzon in the Philippines are especially well-known, and in one study women hunters alone or in teams provided a third of big game by weight (add in mixed-sex groups, and it’s almost half). Overall, women spent a similar amount of time on hunting as men, killing wild pigs and deer, sometimes using bows and arrows. There’s also archaeological evidence from post-glacial cultures in the Americas of women’s burials containing hunting weaponry.
But there are nuances. The larger the animals or the further away they are to be found, the less often women take part. Perhaps tackling true megafauna and fanged species – whether mammoth on the steppe, or bears slumbering in winter dens – might have been mostly male tasks.
Taking all this together, Neanderthal women very likely did hunt some or much of the smaller game we find in sites, such as tortoise, rabbits and birds – and probably accompanied by babies and children. Yet the identity of big-game hunters probably shifted according to climate, season, landscape and other factors. Some 123,000 years ago, Neanderthal women of the forest marshes dragged beavers from their lodges, while their descendants 70 millennia – and 3,000 generations – later stalked red deer through wooded uplands.
O ne of the most convincing reasons to believe that Neanderthal women did experience life differently is the testimony of their own bodies. Research on limb bones suggests that, while their thighs were as strong relatively as men’s, their lower legs appear less intensively used. Sample sizes are small, however the impression is of different habits in moving around, with men perhaps scaling more rough terrain. Arms tell a similar story, with women’s lower arms getting more of a workout than their biceps. On top of this, while Neanderthal men apparently used their right and left arms differently (comparable to the asymmetry in professional tennis players), women’s arms were more symmetrically developed. Carrying heavy loads in both hands could cause this, just as we lug loaded travel or shopping bags. But pushing something up and down – or backwards and forwards – with both arms would also fit, which is particularly intriguing because one of the things we know that Neanderthals were doing an awful lot of is hide-working.
Even if they weren’t always hunters, it’s extremely likely that women were wielding butchery tools in the bloody aftermath. Part of this was about skin preparation: gore and membranes from fresh skins leave a particular polish on stone tools, while the laborious scraping of dried hides produces its own distinctive lustre. From at least 50,000 years ago, there are even special, round-tipped bone tools for the latter stages of softening and burnishing, called ‘lissoirs’. Made from the ribs of larger animals such as bison, crucially they would have needed two hands to use; exactly the pattern we see in Neanderthal women’s arms. Moreover, the intensity of tooth wear seen in Neanderthal women resembles that in Indigenous cultures with strong hide-working traditions, such as the Inuit, Yupik, Chukchi or Iñupiat.
Neanderthal life saw a growing tendency to split up activities across the landscape. While initial skin cleaning was done near kill sites and potentially by both men and women, it might be likely that the more time-consuming softening and stretching using mouths and lissoirs was happening where meaty joints and slabs of fat also ended up: family living sites.
Whether caves or open-air locales, Neanderthal homes were places where things from many places arrived, often accumulating around fires. It’s here, surrounded by aureola-like spreads of waste from shared meals and tool resharpening, that we can imagine a woman handing round animal skins, working them to softness as a child curls into her lap. This might sound unexpectedly domestic, but the archaeology itself reveals how hearths centred daily life.
If Neanderthal women cradled their bumps, they experienced the kicks of squirming infants within
The other thing happening in the light of the flames were relationships. Aside from archaeological evidence that they had a resource-sharing society, unlike other primates, Neanderthal body size doesn’t seem to have differed much between the sexes. It’s much more similar to most H sapiens populations, which suggests that violent male competition was not the dominant social structure. Instead, more likely something akin to the bonobos, whose adult lives are based around long-term female friendships, might have been possible.
Undoubtedly squabbles happened alongside jockeying for popularity, but the emotional and sexual lives of Neanderthal women were probably just as much about affection expressed by supplying of food – even gifts – as well as verbally and physically. Based on what we see among humans and in the animal kingdom more broadly, at least some intimacy could have been between women themselves.
Whether or not sexual caresses were exchanged as abundantly as between bonobos, it’s certain that the consequences of heterosexual encounters had an enormous impact on women’s experiences in life. No Neanderthal had a life of leisure, but bringing the next generation into existence added an extra burden to women’s already energy-hungry bodies. Gestation likely lasted as long – and bellies swelled as impressively – as ours, provoking the same physical discomforts. If Neanderthal women stroked and cradled their bumps, they certainly experienced the kicks and weird undulations of squirming infants within.
What might giving birth have been like? While experiences today vary dramatically, birthing can be life-defining: physically draining and emotionally tumultuous. Anatomically however, reconstructing this for Neanderthal women has been tricky. One of the very few mostly complete female skeletons emerged from Mount Carmel in then-Palestine in 1932, at the opposite end of the Mediterranean from Gibraltar. Known as Tabūn 1, her hip bones are partially preserved, and 21st-century modelling suggests that her and her contemporaries’ birth canals were shaped differently. Babies didn’t need to twist, and heads emerged sideways instead of facing backwards. On the other hand, while this potentially meant that births could have been somewhat faster, with less risk of infants getting stuck, the babies’ longer skulls meant it was still a tight squeeze.
Birthing would certainly have made women vulnerable, and a secure location would be critical, not just during labour but for the hours afterwards. Mothers probably preferred caves or other sheltered places, but did Neanderthal women go through this alone? It’s been proposed that H sapiens is unique in the desire and even need for birth ‘attendants’ but, assuming that Neanderthal social groups included friendship between women, this might not have been an unreasonable scenario for them too. It’s known in bonobos, where experienced females physically supporting and protecting the mother have been observed, even holding the baby’s head as it emerged.
A fter birth, other work begins, and breastfeeding was among the childcare skills that Neanderthal mothers needed to learn. Isotopic studies of teeth indicate that babies continued nursing beyond one year old, but were being introduced to solid foods around six or seven months: remarkably similar to many human cultures. Another kind of isotope shows that babies too little to walk were nonetheless moving around the landscape, and someone must have been carrying them. Frequently nursing, completely dependent babies need to be kept close and, for groups who were probably not staying anywhere for more than a few weeks, practical transport could have meant using skin wraps.
So far, archaeological science hasn’t developed a method to tell which particular female Neanderthals were mothers, though most probably were. The Forbes woman was certainly old enough to have gone a stage further and become a grandmother. Estimated to be least 40 years old, her skull bears a distinctive bony growth that can lead to headaches, thyroid issues and substantial weight gain. Potentially caused by high oestrogen levels, it might well be connected to her age.
But even if elder women were perhaps less robust, they might have had a crucial role. Returning to the issue of hunting, one of the factors that allows Agta women this freedom isn’t just the close proximity of game so they need be gone for no more than an hour or two. It’s also that they can rely on someone else to look after the infants and young children they leave behind. Older children can be babysitters, but often it’s more senior individuals – and, for many women, that will include their own mothers.
Grandparents have been proposed as a secret weapon of our own species, not just for their child-minding, but as reservoirs of wisdom too. Many decades of life translates to grandparents’ greater stores of knowledge and skill. One among many theories that sought, post hoc, to put Neanderthals in their place as evolutionary failures claimed that they died too young for a generation of elders to exist. However, when the totality of the skeletal data is examined, there isn’t much difference between them and early H sapiens . Hoary old ones are there and, even if they grew slightly faster as children, this wouldn’t have reduced total lifespan significantly. There’s no biological reason why elderly Neanderthal women – perhaps great-grandmothers – couldn’t have huddled by hearths.
Perhaps our own biases might be limiting possibilities for Neanderthal men, too
Beyond the bare fact of women’s existence in the deep past, that we can spy any glimmers at all of their experiences is quite remarkable. From birth, emerging into ember-glow or lucid dawn light, still connected to the bodies of their mothers, Neanderthal girls began to follow their own particular path. Surrounded as babies and toddlers by the sights, smells and sounds of adults and their activities, the threads of girl- and womanhood began aligning. Yet, the materialisation of sex-specialised roles doesn’t mean rigidity, but can be understood as an ever-shifting dialogue between biological considerations and the ancient hominin habit of collaboration, sharing and innovation.
Caveats exist, however. The current identified female sample is still quite small and, alongside unpicking the archaeological evidence, we must disentangle our own expectations of Neanderthal women. The asymmetry in Neanderthal men’s upper arms doesn’t really match how spear use activates muscles, yet the possibility that this was due to one-handed hide-scraping has been deemed unlikely due to its ‘un-masculine’ association in recent hunter-gatherers. Perhaps our own biases might be limiting possibilities for Neanderthal men, too.
And while chimpanzees or recent hunter-gatherers are useful as points from which to draw interpretive inspiration, what was going on 50,000 or 350,000 years ago might include things with no modern analogues. Just as the steppe-tundra contained a mosaic of plant species that are no longer found together, Neanderthal lives might have included things that we aren’t even imagining. What could have been normality for a girl growing up in the arid valleys of Central Asia would have been unfamiliar to her far-flung relation on the rocky coast of Iberia. These were ways of being human on a continental, epochal scale.
We began with the ‘first’ Neanderthal face; what about the last? Somewhere around 40,000 years ago, the many generations of Neanderthal women become invisible, at least in skeletal terms. The processes underlying this must have come in many guises, in many places, but one thing we know is that women of another kind – H sapiens – played some part, because Neanderthals were not entirely extinguished. Just 10 years ago, the first nuclear genome was meticulously reconstructed from three genetically identified females at Vindija; it revealed that, rather than ousting Neanderthals from Eurasia, we had in fact interbred with them.
In the decade since, recognised periods of contact now number at least four and perhaps seven or more, going back beyond 200,000 years ago. Most intriguingly, something of the dynamics is visible. In some earlier instances, Neanderthal women had the children of H sapiens men, but the later interbreeding after 60,000 years ago tells a different story. Nobody today has mitochondrial DNA like that in Neanderthals and, since it’s passed only maternally, this implies that interbreeding was more often between their men and our women.
It’s in these last hybrid babies that the female heritage of Neanderthals lives on. The DNA legacies of the mixed babies’ relations – half-sisters, half-aunts, half-grandmothers, and beyond – persisted through thousands of generations. Their billions of descendants are still here, walking Earth today."
Are male and female circumcision morally equivalent? | Aeon Essays,,https://aeon.co/essays/are-male-and-female-circumcision-morally-equivalent,"I try not to talk about my research at dinner parties. I’ll say ‘medical ethics’ if pressed, which will sometimes trigger an unwelcome follow-up: ‘But what about medical ethics? That’s a pretty big field.’
‘I study lots of things,’ I’ll say – and that’s true, I do. ‘But I focus on medically unnecessary surgeries performed on children.’
‘Like what?’
Like what, indeed. It’s rarely a smooth ride from there.
The truth is: I study childhood genital surgeries. Female, male and intersex genital surgeries, specifically, and I make similar arguments about each one. As a general rule, I think that healthy children – whatever their sex or gender – should be free from having parts of their most intimate sexual organs removed before they can understand what’s at stake in such a procedure. There are a number of reasons I’ve come to hold this view, but in some ways it’s pretty simple. ‘Private parts’ are private. They’re personal. Barring some serious disease to treat or physical malfunction to address (for which surgery is the most conservative option), they should probably be left alone.
That turns out to be extremely controversial.
I n the 1990s, when the Canadian ethicist Margaret Somerville began to speak and write critically about the non-therapeutic circumcision of infant boys, she was attacked for even addressing the subject in public. In her book The Ethical Canary , she says her critics accused her of ‘detracting from the horror of female genital mutilation and weakening the case against it by speaking about it and infant male circumcision in the same context and pointing out that the same ethical and legal principles applied to both’.
She wasn’t alone. The anthropologist Kirsten Bell has advanced similar arguments in her university lectures, provoking a reaction that was ‘immediate and hostile … How dare I mention these two entirely different operations in the same breath! How dare I compare the innocuous and beneficial removal of the foreskin with the extreme mutilations enacted against females in other societies!’
There’s a problem with these claims. Almost every one of them is untrue or severely misleading
It’s easy to see where these reactions are coming from. One frequent claim is that FGM is analogous to ‘castration’ or a ‘total penectomy’. Put that way, anyone who tried to compare the two on ethical (or other) grounds would be making a serious mistake – anatomically, at the very least.
You often hear that genital mutilation and male circumcision are very different . FGM is barbaric and crippling (‘always torture’, as the Guardian columnist Tanya Gold wrote recently), whereas male circumcision is comparatively inconsequential. Male circumcision is a ‘minor’ intervention that might even confer health benefits, whereas FGM is a drastic intervention with no health benefits, and only causes harm. The ‘prime motive’ for FGM is to control women’s sexuality; it is inherently sexist and discriminatory and is an expression of male power and domination. That’s just not true for male circumcision.
Unfortunately, there’s a problem with these claims. Almost every one of them is untrue, or severely misleading. They derive from a superficial understanding of both FGM and male circumcision; and they are inconsistent with what scholars have known about these practices for well over a decade. It’s time to re-examine what we ‘know’ about these controversial customs.
T he World Health Organization (WHO) defines FGM as any ‘non-medical’ alteration of the genitalia of women and girls. What this is likely to bring to mind is the most extreme version of such ‘alteration’, which is the excision of the external part of the clitoris followed by a narrowing of the vaginal opening, sometimes using stitches or thorns. It is rarely understood that this notorious form of FGM is comparatively rare: it occurs in a subset of the practising communities, and makes up about 10 per cent of cases worldwide. More prevalent, but much less frequently discussed in the media, is a range of less extensive alterations, sometimes performed under anaesthesia by medical professionals and with sterile surgical equipment. These include, among other interventions, so-called ritual ‘nicking’ of the clitoral hood (common in Malaysia), as well as non-medically-indicated labiaplasty and even piercings that might be done for perceived cosmetic enhancement.
Male genital cutting is performed at different ages, in different environments, with different tools, by different groups, for different reasons
It should be clear that these different forms of FGM are likely to result in different degrees of harm, with different effects on sexual function and satisfaction, different chances of developing an infection, and so on. And yet all forms of non-therapeutic female genital alteration – no matter how sterilised or minor – are deemed to be mutilations in ‘Western’ countries. All are prohibited by law. The reason for this, when you get right down to it, is that cutting into a girl’s genitals without a medical diagnosis, and without her consent, is equivalent to criminal assault on a minor under the legal codes of most of these societies. And, morally, I think the law is correct here. I don’t think that a sharp object should be taken to any child’s vulva unless it is to save her life or health, or unless she has given her fully informed permission to undergo such an operation, and wants to take on the relevant risks and consequences.
In that case, of course, she wouldn’t be a ‘child’ anymore, but rather an adult woman, who can make a decision about her own body.
The story is very different when it comes to male circumcision. In no jurisdiction is the practice prohibited, and in many it is not even restricted. In some countries, including in the United States, anyone, with any instrument, and any degree of medical training (including none) can attempt to perform a circumcision on a non-consenting child – sometimes with disastrous consequences. For a recent example, look up ‘Goodluck Caubergs’ on the internet; similar cases happen every year. As the bioethicist Dena Davis has pointed out, ‘States currently regulate the hygienic practices of those who cut our hair and our fingernails … so why not a baby’s genitals?’
Just like FGM, however, circumcision is not a monolith: it isn’t just one kind of thing. The original Jewish form of circumcision (until about AD150) was comparatively minor. It involved cutting off the overhanging tip of the foreskin – whatever stretched over the end of the glans – thereby preserving (most of) the foreskin’s protective and sexual functions, as well as reducing the amount of erogenous tissue removed. The ‘modern’ form is much more invasive: it removes between one-third and one-half of the movable skin system of the penis (about 50 square centimeters of richly innervated tissue in the adult organ), eliminates the gliding motion of the foreskin, and exposes the head of the penis to environmental irritation, as it rubs against clothing.
Male genital cutting is performed at different ages, in different environments, with different tools, by different groups, for different reasons. Traditional Muslim circumcisions are done while the boy is fully conscious, between the ages of five and eight, and sometimes later. American (non-religious) circumcisions are done in a hospital, in the first few days of life, with or without an anesthetic. Metzitzah b’peh , done by some ultra-Orthodox Jews, involves the sucking of blood from the circumcision wound, and carries the risk of herpes infection and permanent brain damage.
Subincision, seen primarily in aboriginal Australia, involves slicing open the urethral passage on the underside of the penis from the scrotum to the glans, often affecting urination as well as sexual function. And circumcision among some tribal groups in Africa is done as a rite of passage, in the bush, with spearheads, dirty knives, and other non-sterile instruments. Similar to female genital cutting rites performed under comparable conditions (and often by the very same groups), these operations frequently cause hemorrhage, infection, mangling, and loss of the sexual organ. In fact, between 2008 and 2014, more than half a million boys were hospitalised due to botched circumcisions in South Africa alone. More than 400 lost their lives.
But even ‘hospitalised’ or ‘minor’ circumcisions are not without their risks and complications, and the harm is not confined to Africa. In 2011, for example, nearly a dozen infant boys were treated for life-threatening haemorrhage, shock or sepsis as a result of their non-therapeutic circumcisions at a single children’s hospital in Birmingham, England. Since this figure was obtained by a special freedom of information request (and otherwise would not have been public knowledge), it has to be multiplied by orders of magnitude to get a sense of the true scope of the problem.
W hen people talk about ‘FGM’ they are usually thinking of the most severe forms of female genital cutting, done in the least sterile environments, with the most drastic consequences likeliest to follow – even though research suggests that these forms are the exception rather than the rule. When people talk about ‘male circumcision’, by contrast, they are (apparently) thinking of the least severe forms of male genital cutting, done in the most sterile environments, with the least drastic consequences likeliest to follow – perhaps because this is the form with which they are culturally familiar.
One recurrent claim, recently underlined by the US Centers for Disease Control (CDC), is that male circumcision can confer a number of health benefits, such as a small reduction in the absolute risk of contracting certain sexually transmitted infections. This is not typically seen as being the case for FGM.
However, both parts of this claim are misleading. Certainly the most extreme types of FGM will not contribute to good health on balance, but neither will the spearheads-and-dirty-knives versions of genital cutting on boys. What about other forms of FGM? Its defenders (who typically refer to it as ‘female circumcision’) regularly cite such ‘health benefits’ as improved genital hygiene as a reason to continue the practice. Indeed, the vulva has all sorts of warm, moist places where bacteria or viruses could get trapped, such as underneath the clitoral hood, or among the folds of the labia; so who is to say that removing some of that tissue (with a sterile surgical tool) might not reduce the risk of various diseases?
one relevant harm would be the involuntary loss of a healthy, functional, and erotogenic genital structure
Fortunately, it’s impossible to perform this type of research in the West, because any scientist who tried to do so would be arrested under anti-FGM laws (and would never get approval from an ethics review board). So we simply do not know. As a consequence of this, every time one sees the claim that ‘FGM has no health benefits’ – a claim that has become something of a mantra for the WHO – one should read this as saying, ‘we actually don’t know if certain minor, sterilised forms of FGM have health benefits, because it is unethical – and would be illegal – to find out.’
By contrast, a small and insistent group of (mostly American) scientists have taken it upon themselves to promote infant male circumcision as a form of partial prophylaxis against disease. Most of these diseases are rare in developed countries, do not affect children before an age of sexual debut, and can be prevented and/or treated through much more conservative means. Nevertheless – since it is not against the law for them to do so – advocates of (male) circumcision are able to conduct study after well-funded study to see just what kinds of ‘health benefits’ might follow from cutting off parts of the penis.
Many European medical experts dispute these studies, and detect more than a whiff of cultural bias in favour of circumcision due to its peculiar status as a birth ritual in American society. The recent statement by the CDC is a case in point. This otherwise august organisation contends that the benefits of circumcision outweigh the risks, where by ‘risk’ they apparently mean ‘risk of surgical complications’.
But in medical ethics, the appropriate test for a non-therapeutic surgery performed in the absence of disease or deformity is not benefit vs ‘risk of surgical complications’ but rather benefit vs risk of harm. In this case, one relevant harm would be the involuntary loss of a healthy, functional, and erotogenic genital structure that one might wish to have experienced intact. Imagine a report by the CDC referring to the benefits of removing the labia of infant girls, where the only morally relevant drawback to such a procedure was described as the ‘risk of surgical complications’.
I t is often said that FGM is designed to ‘control’ female sexuality, whereas male genital cutting is less symbolically problematic. But as the sociologist Lisa Wade has shown in her research, ‘attributing [the] persistence [of female genital altering rituals] to patriarchy grossly over-simplifies their social, cultural, and economic functions’ in the diverse societies in which they are performed. Throughout much of Africa, for example, genital cutting (of whatever degree of severity) is most commonly performed around puberty, and is done to boys and girls alike. In most cases, the major social function of the cutting is to mark the transition from childhood to adulthood, and it is typically performed as part of an elaborate ceremony.
Indeed, in nearly every society that practices such coming of age rituals, the female half of the initiation is carried out by women (rather than by men) who do not typically view it as being a consequence of male dominance, but who instead see their genital-altering practices as being beautifying, even empowering, and as an important rite of passage with high cultural value. The claim that these women are all ‘brainwashed’ is anthropologically ignorant. At the same time, the ‘rite of passage’ ceremonies for boys in these societies are carried out by men; these are done in parallel, under similar conditions, and for similar reasons – and often with similar consequences for health and sexuality (as illustrated earlier with the example of South Africa).
Every parent who requests a genital-altering surgery for their child – for whatever reason under the sun – thinks that they are acting in the child’s best interests
In the US context, male circumcision was adopted by the medical community in the late 1800s in an effort to combat masturbation, among other dubious reasons. It has since persisted as a rationalised habit, long past the time when it was effectively abandoned by other developed nations. Of course, it is probably true that most contemporary Western parents who choose circumcision for their sons do not do so out of a desire to ‘control’ their sexuality, but this is also true of most African parents who choose ‘circumcision’ for their daughters. As the renowned anti-FGM activist Hanny Lightfoot-Klein has stated: ‘The [main] reasons given for female circumcision in Africa and for routine male circumcision in the United States are essentially the same. Both promise cleanliness and the absence of odours as well as greater attractiveness and acceptability.’
Given that both male and female forms of genital cutting express different cultural norms depending upon the context, and are performed for different reasons in different cultures, and even in different communities or individual families, how shall we assess the permissibility of either? Do we need to interview each set of parents to make sure that their proposed act of cutting is intended as an expression of acceptable norms? If they promise that it isn’t about ‘sexual control’ in their specific case, but rather about ‘hygiene’ or ‘aesthetics’ or something less symbolically problematic, should they be permitted to go ahead?
But this is bound to fail. Every parent who requests a genital-altering surgery for their child – for whatever reason under the sun – thinks that they are acting in the child’s best interests; no one thinks that they are ‘mutilating’ their own offspring (whether female or male). So it is not the reason for the intervention that determines its permissibility, but rather the consequences of the intervention for the person whose genitals are actually on the line.
As the social anthropologist Sara Johnsdotter has pointed out, there is no one-to-one relationship between the amount of genital tissue removed (in males, females, or indeed in intersex people), and either subjective satisfaction while having sex, or a feeling of having been personally harmed because one’s ‘private parts’ were altered before one could effectively resist. Medically unnecessary genital surgeries – of whatever degree of severity – will affect different people differently. This is because each individual’s relationship to their own body is unique, including what they find aesthetically appealing, what degree of risk they feel comfortable taking on when it comes to elective surgeries on their reproductive organs, and even what degree of sexual sensitivity they prefer (for personal or cultural reasons). That’s why ethicists are beginning to argue that individuals should be left to decide what to do with their own genitals when it comes to irreversible surgery, whatever their sex or gender.
This article is adapted from a longer piece originally published at the University of Oxford’s Practical Ethics website. Links to supporting research can be found in the original essay, available here ."
Why language remains the most flexible brain-to-brain interface | Aeon Essays,,https://aeon.co/essays/why-language-remains-the-most-flexible-brain-to-brain-interface,"In a nondescript building in Seattle, a man sits strapped to a chair with his right hand resting on a touchpad. Pressed against his skull is a large magnetic coil that can induce an electrical current in the brain, a technique known as transcranial magnetic stimulation. The coil is positioned in such a way that a pulse will result in a hand movement. A mile away in another building, another man looks at a screen while 64 electrodes in a shower cap record his brain activity using electro-encephalography. Rough activation patterns are fed back to the computer so that he, by concentrating, can move a dot a small distance on the screen. As he focuses, a simple signal derived from the brain activity is transmitted to the first building, where another computer tells the magnetic coil to deliver its pulse. The first man’s hand jolts upward, then falls down on the touchpad, where the input is registered as a move in a video game. Then a cannon is fired and a city is saved – by two bodies acting as one.
As gameplay goes, the result might seem modest, but it has far-reaching implications for human interaction – at least if we believe the team of scientists at the University of Washington led by the computer scientist Rajesh Rao who ran this experiment. This is one of the first prototypes of brain-to-brain interfaces in humans. From the sender’s motionless concentration to the receiver’s involuntary twitch, they form a single distributed system, connected by wires instead of words. ‘Can information that is available in the brain be transferred directly in the form of the neural code, bypassing language altogether?’ the scientists wondered in writing up the results . A Barcelona team reached a similar result with people as far apart as India and France. With a gush of anticipation, they exclaim: ‘There is now the possibility of a new era in which brains will dialogue in a more direct way.’
The popular media has been quick to jump on the bandwagon as the prototypes make global headlines. Big Think declared brain-to-brain interfaces ‘the next great leap in human communication’. The tech entrepreneur Elon Musk speculated about how a neural prosthetic to be made by one of his own companies might ‘solve the data rate issue’ of human communication. The idea is that, given high bandwidth physical connectivity, language will simply become obsolete. Will we finally be able to escape the tyranny of words and enjoy the instant sharing of ideas?
L et’s face it: we’ve all had second thoughts about language. Hardly a day goes by when we don’t stumble for words, stagger into misunderstandings, or struggle with a double negative. It’s a frightfully cumbersome way to express ourselves. If language is such a slippery medium, perhaps it is time to replace it with something more dependable. Why not cut out the middleman and connect brains directly? The idea is not new. As the American physicist and Nobel laureate Murray Gell-Mann mused in The Quark and the Jaguar (1994): ‘Thoughts and feelings would be completely shared, with none of the selectivity or deception that language permits.’
It is useful to examine this view of language carefully, for it is quite alluring. Rao and his team complain about how hard it can be to verbalise feelings or forms of knowledge even if they are introspectively available. On Twitter, Musk has described words as ‘a very lossy compression of thought’. How frustrating to have such a rich mental life and be stuck with such poor resources for expressing it! But no matter how much we can sympathise with this view, it misses a few crucial insights about language. First, words are tools. They can be misplaced or misused like any tool, but they are often useful for what they’ve been designed to do: help us say just what we want to say, and no more. When we choose our words carefully, it is because we know that there is a difference between private worlds and public words. There had better be, since social life depends on it.
Second, and more subtly, this view sees language as merely a channel for information: just as the speaking tube has made way for the telephone, so language can be done away with if we connect brains directly. This overlooks that language is also an infrastructure for social action. Think of everyday conversations, in which we riff off on a theme, recruit others to do stuff, relate to those around us. We don’t just spout information indiscriminately; we apportion our words in conversational turns and build on each others’ contributions. Language in everyday use is less like a channel and more like a tango: a fluid interplay of moves in which people can act as one, yet also retain their individuality. In social interaction there is room, by design, for consent and dissent.
The difference with current concepts of brain-to-brain interfaces couldn’t be greater. A transcranial magnetic pulse leaves no room for doubt, but none for deliberation either. Its effect is as immediate as it is involuntary. We can admire the sheer efficiency of this form of interaction, but we also have to admit that something is lost. A sense of agency and autonomy; and along with that, perhaps even a sense of self. Nor does this problem go away merely by upgrading bandwidth, as is Musk’s ambition for Neuralink, his implantable brain-computer interface. The very possibility of social (as opposed to merely symbiotic) life depends on there being some separation of private worlds, along with powers to interact on our own terms. In other words, we need something like language in order to be human.
When we directly connect one individual’s mental life to that of another, individual agency might slip through our fingers. Biology offers plenty of examples. Take the fascinating slime mould Physarum polycephalum , which is essentially a bag of cytoplasm holding millions of individual nuclei, the result of a mass merger of amoebae. Moving and sensing in unison, the slime mould can crawl towards light, find food in mazes, and even mimic the design of urban metro networks. The price for this perfect symbiosis is a complete loss of autonomy for individual elements. The real challenge for brain-to-brain interfaces is not to achieve some interlinking of brains. It is to harness technology in a way that doesn’t reduce people to the level of amoebae fused to a slime mould.
Nobody in their right mind wants to blurt out every fleeting thought and feeling
If I ask you to help me move a large sofa, I effectively recruit you as an ‘instrument’ for carrying out a joint action. If you agree, I can give you directions – move this way; no, up here; perfect – and together we accomplish something neither of us could do alone. Now think about what this means for agency. For the duration of our project, you agree to give up a bit of your personal agency, and together we become what the British philosopher Margaret Gilbert has called a plural subject : a larger social unit of agency. Whereas joint agency in the brain-to-brain prototypes (or indeed the slime mould) comes about by brute-force physical means, here it is negotiated using language.
One amazing thing about language is the sheer fluidity with which it allows us to manage such everyday episodes of joining forces and parting ways. It is literally the most versatile brain-to-brain interface we have: a nimble, negotiable system that enables people with separate bodies to achieve joint agency without giving up behavioural flexibility and social accountability. So before we throw out language because of its supposedly low data rate, let’s look a bit more closely at the ways in which it helps us calibrate minds, coordinate bodies and distribute agency. There are two features of language that make it especially useful in human interaction: selection and negotiation.
Selection is the power we have to select what to keep private and what to make public by putting it into words. Sharing and withholding information are among the most important ways in which we manage our social relationships. Nobody in their right mind wants to blurt out every fleeting thought and feeling. Society as we know it largely depends on the fact that some things are better left unsaid. So language gives us control over what we share with whom, and whether we share something in the first place. Take a simple question such as ‘How are you?’ The words you select in response to it have more to do with social relations than with information: this is how you distinguish between the delivery driver and your best friend. This is the power of selection in action.
If we weren’t judiciously wielding the power of selection throughout the day, awkwardness would accumulate and social life would be sent into gridlock. To avoid this, we have, as a society, tacitly agreed to limit self-disclosure. In the words of the American sociologist Harvey Sacks, ‘everyone has to lie’. This is not deception of the kind that Gell-Mann deplored; instead, it is a necessary form of economy with the truth. Those who don’t heed this – the blabbermouths and oversharers – tend to pay for it socially (and sometimes also economically, if they are, say, a CEO on Twitter). On balance, selectivity is a small price to pay for the possibility of a normal social life.
Selection is quite powerful in another way. Language has a fractal quality: things can be described in myriad compatible yet nonequivalent ways. Think back to helping me move the sofa at my place. If we know each other, ‘my place’ is all you need to know, and you would be puzzled if I transmitted GPS coordinates, even if those might provide a more objective and explicit localisation. My selecting one formulation over others neatly accomplishes two things at once: providing relevant information and indexing our social relation. Describing this as lossy compression misses the point. It is more like distributed computation: the power of language is that we can be frugal with words when possible and explicit when necessary.
T he second key power that language gives us is negotiation , in the sense of working together towards mutual understanding. We take turns at talking and this provides every participant in a conversation with systematic opportunities for consent and dissent. We can use our turns at talk for the normal business of conversation, such as answering a question, telling a story or recruiting assistance and collaboration. We can also use our turns to do metacommunicative work, for instance to signal that we are on the same page or to ask for clarification. Often these metacommunicative signals – such as ‘m-hm’ or ‘huh?’ – are rather minimal and unobtrusive: they seem to be optimally adapted to the task of streamlining conversations.
But isn’t asking for clarification exactly the kind of annoyance that brain-to-brain interfaces could help us get rid of? Especially since it happens so often: our best current estimate puts it at roughly every 1.4 minutes on average in informal conversations around the world. Wouldn’t it be much more efficient if we immediately understood each others’ wants and needs, instead of getting into knots about what we mean? Here we reach a fork in the road. If you find yourself wanting to eradicate all those pesky misunderstandings once and for all, your view of meaning is primarily individualist. You feel that you’ve made up your mind and simply want the other to get it. If only you could just beam the message across without ambiguities.
There is, though, another way of thinking about meaning in interaction. It is less single-minded and more dialogical. It holds that, often, we figure out our wants and needs in interaction with others. That what we ‘really’ mean often becomes clear to us only when we work it out together. There is a fitting Zulu saying here: Umuntu ngumuntu ngabantu , or ‘A person is a person through other people.’ This captures a subtle balance between individual and community. We are never sole individuals; nor are we mere cells in a slime mould. We are social beings first and foremost.
Work on conversational clarification shows that low-level perceptual problems – of the kind that might be targeted by enhanced communication channels – are relatively rare. Often, we ask for clarification not so much because we didn’t hear or understand, but to make up our mind, buy ourselves some time, or give the other a chance to reformulate. Even if things don’t work out, we can agree to disagree, preventing a loss of face for us both. The metacommunicative signals that litter our conversations are not symptoms of a superseded technology. They are scaffolds: signs that help us think and talk, interactional tools that help us come to terms. All this means that communication is never a one-shot affair, and that there are always opportunities to hold each other to account, reconsider our positions, and renegotiate our commitments.
The hallmark of religious indoctrination is that it leaves no wiggle room in interpreting the words of holy books
People have separate bodies. While brain-to-brain interfaces could somewhat dilute this separateness, language has long bridged it. Never merely individuals, we are always part of a dazzling range of social units, some fleeting and diffuse (like the unit of ‘readers of this essay’) and others more strong and durable (like close friends and family). Language is the main tool by which we navigate this mosaic of social relations, constantly switching frames between ‘me’ and the many different senses of ‘us’. Seen in this light, selection and negotiation are not bugs, but features. Thanks to them, we can manage what we share with whom, and we can join forces in larger social units without indefinitely relinquishing individual agency.
We don’t need experimental prototypes to see what happens when the powers of selection and negotiation are diminished or taken away. In George Orwell’s novel Nineteen Eighty-Four (1949), language is stripped down to eliminate ambiguity: a laudable goal on the face of it, but one that has the uncanny side-effect of dramatically reducing opportunities for dissent. Likewise, the hallmark of religious indoctrination is that it leaves no wiggle room in interpreting the words of holy books or great leaders. Sometimes precision gained is freedom lost.
Science-fiction scenarios contemplating brain-to-brain coupling press home some of the ethical issues in a very clear way. An early episode of Star Trek introduced the mind meld , in which Vulcans coercively share thoughts and experiences through mere physical contact. Later seasons introduced the Borg collective, an ever-expanding hive mind that grows through forced assimilation. In William Gibson’s novel Neuromancer (1984), the lure of cyberspace creates a sprawling market for experimental neurosurgery, while jacking into the matrix risks exposing one’s mental life to malicious hackers. Surprisingly, most current brain-to-brain interface prototypes follow the script: senders don’t get to choose which aspects of their brain activity are transmitted, and receivers have no freedom to deliberate over incoming signals. This is more like physical manipulation than free communication. It is interaction stripped of any possibilities for selection and negotiation.
What unites violations of mental and bodily integrity is that crucial aspects of individual agency are taken away. It’s no coincidence that we describe them as ‘dehumanising’ and ‘inhumane’. Language is what makes us human. It is not merely a conduit for information, it is also our way of organising social agency. We might try exchanging it for a high-bandwidth physical connection to optimise the flow of some types of information, but we would do so at the tremendous cost of throwing away the very infrastructure that makes human sociality possible.
B ut there is hope. The agency-distributing powers of interactive language are largely independent of its modality: they are certainly not limited to the spoken, face-to-face version of language that happens to be its most prevalent form today. Writing, for one, shows that at least some aspects of language can be reduced to a visual code, though it has long come with a loss of immediacy and interactivity that is only now being remedied. A better example is the veritable diversity of sign languages used by deaf communities across the globe: impressive proof that the full richness and complexity of interactive language can be realised without a single sound.
Meanwhile, the Washington scientists are already building more refined brain-to-brain interfaces. A recent prototype called BrainNet introduces a simple form of negotiation, essentially by building in an additional loop that allows subjects called ‘Senders’ to see the effect of a Receiver’s choice and, if necessary, re-send a signal. This is directly equivalent to what conversation analysts call ‘third position repair’: a redoing produced by a Sender when a Receiver’s second move reveals a misunderstanding of the Sender’s first move. (The similarity points to rich opportunities for interdisciplinary collaboration in the area of human interaction.)
Adding a feedback opportunity like that seems a simple design choice, but it radically changes the nature of the system. By opening up the Receiver’s actions to semi-public scrutiny, it enables a rudimentary form of negotiation and so points towards exactly the kind of back-and-forth that makes natural languages so flexible and error-robust. As input-output systems become more versatile and allow higher data rates, we can expect similar advances on the selection front, allowing us to select a wider range of signals than just binary choices or cursor movements. Of course, this wider range of choices inevitably also implies more degrees of freedom in interpretation, more room for ambiguity, and a greater need for quick ways to calibrate understandings. Which brings us full circle to something like language: a sophisticated intermediary that combines the powers of selection and negotiation. Soon enough we would rediscover the uses of ambiguity, and the joy of finishing each others’ sentences.
Language is a filter between the private and the public
The conclusion is as paradoxical as optimistic. When we refine brain-to-brain interfaces to increase their potential for collaboration, we find language – the very thing we were trying to bypass – slipping through the back door. It is likely, then, that there will be some language-like system for communication and coordination even in the substrate of brain-to-brain interfaces. Whatever the precise form or modality, truly humane brain-to-brain interfaces will need to give us two things: selection (control over the relation between private worlds and public words) and negotiation (systematic opportunities for calibrating mutual understanding).
Douglas Adams’s The Hitchhiker’s Guide to the Galaxy (1979) records the case of the Belcerebon people of Kakrafoon, who were inflicted by a galactic tribunal with ‘that most cruel of all social diseases, telepathy’. It was a punishment with unforeseen consequences:
In seeking to bypass language, current conceptions of brain-to-brain interfaces seem to be on their way to replicating the fate of the Belcerebon people. In contrast, the human condition is enabled by a flexible communication system that saves us from the uninhibited sharing of private processes while still helping us to collaborate in ways unmatched elsewhere in the animal kingdom. Language is a filter between the private and the public, and an infrastructure for negotiating consent and dissent. As research into brain-to-brain interfaces matures, let’s make sure to incorporate the powers of selection and negotiation, so as to extend human agency in meaningful ways.
Portions of this essay are revised from the author’s chapter in Distributed Agency (2017), edited by N J Enfield and Paul Kockelman."
Deep beneath the Earth’s surface life is weird and wonderful | Aeon Essays,,https://aeon.co/essays/deep-beneath-the-earths-surface-life-is-weird-and-wonderful,"The living landscape all around us is just a thin veneer atop the vast, little-understood bulk of the Earth’s interior. A widespread misconception about the deep subsurface is that this realm consists of a continuous mass of uniform compressed solid rock. Few are aware that this mass of rock is heavily fractured, and water runs in many of these fractures and faults, down to depths of many kilometres. The deep Earth supports an entire biosphere, largely cut off from the surface world, and is still only beginning to be explored and understood.
The amount of water in the subsurface is considerable. Globally, the freshwater reservoir in the subsurface is estimated to be up to 100 times as great as all the available fresh water in the rivers, lakes and swamps combined. This water, ranging in ages from seven years to 2 billion years, is being intensely studied by researchers because it defines the location and scope of deep life. We know now that the deep terrestrial subsurface is home to one quintillion simple (prokaryotic) cells. That is two to 20 times as many cells as live in all the open ocean. By some estimates, the deep biosphere could contain up to one third of Earth’s entire biomass.
To comprehend the deep biosphere, we must look past the familiar rules of biology. On the surface, life without the Sun for an extended period of time is dangerous or deadly. Without daylight, no plants or crops can grow. Temperatures get colder and colder. Few organisms, including human beings, can long tolerate such conditions. For instance, people living within the Arctic Circle – as well as the maintenance staff at Antarctic research stations during winter – experience 24-hour darkness for several months each year. They are more vulnerable to health issues such as depression. They find ways to adapt and get through the long, dark, cold winter, but it isn’t easy.
Now imagine the challenges in places that have been isolated from sunlight and organic compounds derived from light-dependent reactions for millions or even billions of years. It seems incomprehensible that anything could survive there. Yet scientists, including the members of our team at Princeton University in New Jersey, have found surprisingly diverse microorganisms in the deep Earth, adapted to a lifestyle independent of the Sun.
Sunlight can filter down to depths of about 1,000 metres in ocean water, but light penetrates no more than a few centimetres into soils or rocks. Cold is not a problem down there, however. Quite the opposite: rainwater that percolates kilometres deep into the crust along fractures and faults between rocks can reach temperatures of 60°C (140°F) or higher. The further down you go from the surface, the closer you are to the mantle. Heat rising from the inner Earth is what warms the fissure water. Additionally, the water is under high pressure, contains very little or no oxygen, and is bombarded by radiation from natural radioactive elements in the rocks.
Within this hellish environment, though, are crucial ingredients for nurturing life. Underground water reacts with minerals in the continental crust, and the longer the water has been trapped down there, the more time there has been for the results of those reactions to accumulate along the flow path. The slow reactions between water and rock dissolve minerals into the water, and break up some of the water molecules, producing molecular hydrogen. This hydrogen is an important fuel for microorganisms in the deep subsurface.
We are also beginning to map the different ecosystems and populations of the deep Earth. Generally speaking, the older subterranean fissure water is brinier (saltier) and has higher concentrations of dissolved hydrogen. Our studies and those by some of our colleagues have shown an apparent trend that the microbes living in older, more brackish water are distinctly different from ones in the younger, less saline water.
Old-water ecosystems are dominated by hydrogen-utilising microorganisms such as sulphate-reducing bacteria and methane-producing archaea. Those methane-producing archaea, or methanogens, are microbes that visually resemble bacteria but are so structurally and genetically distinct that they belong to a completely separate domain of life. Sulfate-reducing bacteria and methanogens are among the life forms that appeared earlier in the evolutionary history. In contrast, young-water ecosystems are dominated by metabolically diverse and versatile bacteria of the phylum proteobacteria.
Studies of the deep ecosystem are already resonating across many fields of science. They are sparking new ideas about the origin of life and about the limits of metabolism. They are filling in new details about the cycling, distribution and storage of carbon on Earth. Deep continental ecosystems will aid the search for underground life on rocky planets such as Mars; deep-sea and sub-seafloor ecosystems, in turn, will help researchers assess the likelihood and possible nature of organisms living on the ocean moons Europa and Enceladus. The implications of this research are truly cosmic in scope.
S ubsurface microorganisms are estimated to be extraordinarily long-lived. In our studies, they show a turnover time as slow as 1,000 years, meaning that they divide only once every few thousand years. To put it in perspective, the common gut bacterium E.coli divides once every 20 minutes. One of the long-standing questions is, how do the deep microbes achieve such a slow-motion lifestyle?
It is not easy to make a living in the subsurface because the biochemical reactions to harness energy from minerals and geological gases – a set of processes known as chemotrophy – are not as efficient as photosynthesis, the process that green plants use to capture energy from photons of sunlight on the surface. Some subsurface microorganisms can form stress-resistant spores and remain inactive in order to withstand extreme subsurface conditions; otherwise, microorganisms have to invest at least a certain amount of energy, which varies from one taxa (evolutionary population) to another, to maintain the integrity and functionality of the cells.
Nowadays, genetic sequencing techniques allow us to investigate in great detail which organism has the potential to metabolise what component of the environment. We can also probe the metabolic potential of the community as a whole using metagenomics, a way to study the collective genetic diversity. Together, these approaches are revealing the overall structure and functioning of the deep biome.
Our studies of the proteobacteria-dominated communities (collected from several sites 1 to 3 km below land surface) show that they share a high degree of similarity with each other, as determined by a genetic marker known as the 16S ribosomal RNA. However, the same functional traits are carried out by different taxa. This variation cannot be explained by physical separation of the sites, nor by each location’s unique physico-chemical features – normally the most ecologically influential factors for such segregation. Neither depth nor water-residence time appear to be a significant contributor to differences, either. Future investigations on the origins of subsurface microorganisms, along with their evolution and movement over the geological history, will aid our understanding of the biogeography, or living landscape, of the subsurface.
Deep microbial groups have established strong, paired metabolic partnerships
We recently completed a study of subsurface microbes using high-throughput sequencing to look at the total population of RNA and proteins. In a 2015 paper , we described for the first time the comprehensive network of metabolic functions being actively executed in the subsurface. At 1.3 km below land surface at the Beatrix gold mine in South Africa, the active community was comprised of 39 phyla from all three domains of life: bacteria, archaea and eukarya – the domain of complex organisms that include humans. Overall, the ecosystem was dominated by proteobacteria.
The molecular data, together with isotope geochemistry and thermodynamic modelling, presented a unified story that the most successful group down there is the betaproteobacteria, a class of proteobacteria that obtain energy through a coupling of nitrate reduction and sulphur oxidation in order to fix carbon dioxide for cellular growth. The demand for nitrate among deep microbes was unexpected; it had gone unnoticed prior to our study because the measured nitrate concentrations in the subsurface water samples were tiny. More interesting, we deduce that deep microbial groups have established strong, paired metabolic partnerships, or syntrophic relationships, which helps the organisms overcome the challenges of extracting the limited energy that originated from rocks. Rather than competing directly with each other, these microbes establish a win-win collaboration.
Scanning electron microscope image of some of the eukarya recovered from two different mines. (a) Dochmiotrema sp. (Platyhelminthes), (b) A. hemprichi (Annelida), (c) Mylonchulus brachyurus (Nematoda), (d) Amphiascoides? (Arthropoda). Scale bar, 50 μm (a,b), 100 μm (c), 20 μm (d).
Most of the carbon in microbial cells appears to be derived directly and indirectly from methane. This is true even though methanogens and methane-oxidising microorganisms together accounted for less than 1 per cent of the organisms in our samples – an astonishingly low fraction, given that methane was the most abundant dissolved gas (~80 per cent) in the water samples we studied. The different kinds of microbial taxa that recycle methane in the subsurface occur at varying abundance over time and space.
Despite the advantages of metabolic partnerships, some deep microbes have evolved to go it alone. Through metagenomics and genome-based analysis, the research scientist Dylan Chivian of Lawrence Berkeley National Laboratory (building on work by Tullis Onstott, the head of our team at Princeton University) discovered a sulphate-reducing bacterium, Candidatus Desulforudis audaxviator , that has complete self-reliance for living in the subsurface ecosystem. Since the publishing of this discovery in 2008, Ca . Desulforudis has been detected elsewhere in both continental and marine subsurface. Single-cell genomic data suggests that ancient viral infections transported archaea genes into Ca . Desulforudis cells, which gave the bacterium the genetic machinery for its self-reliance.
Single-cell genomic data has not only permitted us to investigate cell-to-cell variations in the genomic materials of subsurface microbes, but also to recover the genomic blueprints of microbes that cannot be cultivated. These overlooked organisms are sometimes called ‘microbial dark matter’ because they evade detection by conventional laboratory methods. As with astronomical dark matter, microbial dark matter vastly exceeds the amount that is ‘visible’ to us. Some 99 per cent of the microorganisms do not grow under artificial laboratory conditions. We must rely on single-cell genomics and metagenomics to hunt for microbial dark matter in the deep subsurface.
E ven after we and several other research teams realised that bacteria and viruses have colonised the harsh, deep subsurface, most scientists still considered it unlikely that anything more complex than these unicellular organisms would be able to survive down there. More complex, multicellular organisms generally cope less well with low oxygen levels and high pressure, and they require more food. All the same, in 2006 our group (led by Onstott and Gaetan Borgonie) started to look for nematodes at great depths.
Nematodes (commonly called roundworms, not to be confused with earthworms, which belong to a group all of their own, the Annelida) are extremely common multicellular organisms. Together with insects, they are the most dominant animals on the planet. Nematodes are mostly very small. Although some can range up to several metres in length, most are less than 1 mm long. Their origin extends back 1.1 billion years, to a time not long after the divergence of plants and animals in evolution. Nematodes are considered to be among the oldest multicellular organisms still known on the planet. They have conquered almost any niche on the planet from soil to oceans; some have even evolved to parasitise plants and animals, including humans.
What made nematodes a logical choice to look for in the deep subsurface is their proven track record for being able to survive in extreme environments. Many species are able to alter their life cycle when confronted with life-threatening conditions. They form a survival stage in which their metabolism is greatly reduced. In this way, they are able to withstand anoxia, heat, drought, freezing and toxic conditions for several decades, and then revive when wetted or when conditions are adequate again.
Nematodes can withstand huge pressures, too. When the Space Shuttle Columbia broke up during re-entry in 2003, a biological experiment on board containing nematodes made a free fall from an altitude as high as 42 km. Their canister hit the ground with a force of roughly 2,500 g. (Transient centrifugation at up to 10,000 gs, which would liquefy a human, is a common manipulation in standard nematode laboratory procedures.) A few weeks later, the experiment was recovered. The nematodes inside the canister had not only survived the ordeal, they were reproducing. Furthermore, humans need 21 per cent oxygen in our atmosphere to be able to breathe. Nematodes can make do indefinitely with only 0.5 per cent oxygen, and many species can survive extended periods with less or no oxygen at all.
This effort resulted in the discovery of a whole zoo of invertebrates in water that was 12,300 years old
Our search for deep-Earth nematodes resulted in the 2011 discovery of a new species of nematode, Halicephalobus mephisto . Its name literally means ‘the devil worm’. The nematode was recovered from water that flowed out of a fissure at a depth of about 1.3 km in the Beatrix gold mine. Carbon-dating showed the water there to be around 3,000 years old. In the years that followed, we found more nematodes living at an even more remarkable depth of 3.8 km.
After the discovery of the devil-worm nematode, we performed a long filtration sampling setup that lasted two years. During that time, we filtered 12,845,647 litres of water at a depth of 1.4 km. (The search for deep life is painstaking work!) This effort resulted in the discovery of a whole zoo of invertebrates in water that was 12,300 years old. We recovered species of flatworms, nematodes, rotifers, arthropods, annelids, fungi and protozoa, a whole community thriving inside the filter.
Genetic analysis revealed that none of these was a new species, but that they were all species already known from the surface. Further investigation revealed that nearly all the complex subsurface dwellers shared a common characteristic: they were known to be cosmopolitan, and therefore well-suited to living in extreme environments. At that time, we also made the first video footage of a biofilm – a thin, self-contained living layer – attached to crevices deep inside the rock. The biofilm is composed of bacteria and organic matrix, and it is home to all these animals.
We also found several non-animal species, such as fungi and protozoa, living in deep fissure water that ranged in ages from 7,000 to 500,000 years old. Often their abundance in the fissure water was low, just one specimen per 10,000 litres. In contrast, in certain areas we found patches of bacterial biofilm containing worms at population densities of more than 1,000,000 individuals per square metre. Because the known subsurface animals are small, a cavity the size of your thumb can hold an entire ecosystem containing several hundred small invertebrates, fungi and protozoa.
The commonality of species on the surface and subsurface posed a consistent research challenge. At all times, we had to make extensive analysis to be sure that any specimen found was not the result of contamination of the mines where we were executing our research. We also measured the age of the water to be sure it was not recent, using both chemical and bacteriological techniques. And we had to maintain aseptic conditions at all times. These are similar to, though milder than, the kinds of precautions that might soon be needed for analysing samples from Mars for evidence of extraterrestrial life.
E xcept for Halicephalobus mephisto , we never did find any completely new species of multicellular organisms in the Beatrix mine. This seemed counterintuitive at first, as we expected that a long process of adaptive selection in the deep subsurface would lead to novel life forms. With the advantage of hindsight, though, it is not so surprising.
If you consider any patch of soil anywhere in the world, the nematodes (or any other small invertebrate) living there undergo a daily and seasonal cycle of stress. On bright days, sunshine can dry out the soil; when it rains, puddles might cut off all oxygen; at night, the freezing of water or a bigger animal stepping on that patch adds pressure and disturbs the soil. In summary, animals living in the soil on the surface already experience stress every day. Many of the organisms transported to the deep subsurface would have adapted to extreme conditions long ago, so they would not need a long adaptive selection process to be able to survive. That would account for the paucity of undiscovered deep species.
It’s an enigma: how did a salt-dependent surface worm get that deep without meeting deadly fresh water?
Even after we got past the surprise of what organisms we found living in the subsurface, we were still caught off-guard by where we found them. During our survey of the Beatrix mine, we discovered nematodes living inside salty stalactites at a depth of about 1.4 km. Moreover, this species of nematode was adapted to living in salty water and could not even survive in fresh water. On the surface, this species had been found years before to live in brackish water conditions. Although the Beatrix mine is situated in a dry salt pan, it is still an enigma how a salt-dependent surface worm managed to get that deep without encountering a deadly layer of fresh water in between.
The process of transport to the deep subsurface is not yet understood, and is the subject of much current research. Even in the absence of answers, the broader realisation that complex surface life forms can also survive indefinitely in the deep subsurface is good news for the search for life on planets and moons in our solar system. A similar process of migration could have transported life forms to the deep subsurface long before the surface conditions became inhospitable on Mars, for instance.
And our journey into the inner life of the Earth is just beginning. We are interested in determining whether species from the deep subsurface truly are as isolated as they seem, and if the migrations go in both directions. It is possible that some subsurface organisms reappear on the surface via hot springs. Our analyses of hot-spring waters in the Limpopo region as well as the southern and western Cape regions of South Africa did not turn up any evidence of such resurfacing. Nevertheless, this is a provocative issue that we are continuing to investigate because it will tell us how frequently genetic materials are being exchanged between the surface and the deep subsurface.
Finally, we recognise that we have probably explored only a tiny fraction of the deep biosphere, and might not yet have encountered its most significant inhabitants. It stands to reason that, if cosmopolitan species from the surface can survive in the deep subsurface, isolated from their surface brethren, then over a long period of time some organisms might have adapted to even more extreme conditions deeper in the subsurface. It could be that the real treasure trove of new and weird life forms still awaits discovery far beneath our feet."
"Mother Nature might be lovely, but moral she is not | Aeon Essays",,https://aeon.co/essays/mother-nature-might-be-lovely-but-moral-she-is-not,"To live in Vermont is to be smothered by nature’s beauty on a daily basis. Everywhere you look is another peaceful pond, another shimmering lake or emerald hill or misty field graced by a family of grazing deer. It’s almost obnoxious, like that one friend you have who’s so pretty, funny, smart and talented that you want to hate her stupid gorgeous face.
Immersed as we are in these exquisite pastoral gifts, Vermonters tend to forget that Mother Nature might be lovely, but moral she is not. She doesn’t love us or want what’s best for us. With one hand she giveth, and with the other she puncheth in the gut.
I know this only too well, because as well as the verdant wonderland that is Vermont, nature personally bestowed on me another of her special gifts: systemic lupus. Contrary to what the name suggests, lupus is not some totally rad werewolf disease. No, it’s actually kind of a drag, where your immune system constantly attacks your own body. Think of antibodies that dress up like teeny little Avon ladies to smooth-talk their way into your joints before upturning all the furniture and leaving an arthritic mess behind. Imagine white blood cells that create fake online dating profiles to lure innocent blood vessels, or lurk behind the heavy parlour drapes to leap out and ambush your kidneys. The result is a mess of pain, inflammation, fevers, fatigue and a never-ending litany of nasty surprises. My body is a haunted house and lupus is its poltergeist. Its all-natural poltergeist.
That’s why I just don’t buy the idea that ‘natural is best’. Your organic, gluten-free, sprouted ancient-grain bread is all-natural? That’s nice. My disease is all-natural too. My chronic pain, pleurisy and angry kidneys are all-natural, and my death would’ve been too, if I didn’t have access to the decidedly unnatural medications that allow me to lead a somewhat normal, comfortable life.
A few years ago, in a typical Vermont exchange, a chain smoker paused between drags to scold me for drinking a Diet Coke – or as she referred to it, ‘that poison’. I had the temerity to point out that she was smoking a cigarette. ‘They’re natural,’ she replied smugly, drawing my attention to the colourful bubble on the packaging that proclaimed her smokes were made with ‘100% Certified Organic Tobacco!’. Now, I’m aware that Diet Coke is not exactly a health tonic, but blithely calling it poison in a voice cracked with the tar of innumerable organic butts speaks to a certain cognitive bias. The soda was bad purely because it wasn’t natural, and the cigarettes were good purely because they were. I refrained from asking her if she enjoyed lots of other natural things, such as cobra bites, poison ivy, malaria, and diving headlong into 100 per cent organic molten lava.
It turns out that a certain counter-rationalist mindset isn’t just a condition afflicting Right-wing reactionaries. This fetish for all things au naturel reached a particularly feverish peak in these parts recently, when Vermonters banded together to implement a mandatory labelling law for GMOs (genetically-modified organisms) in 2014. It was like watching villagers festoon their homes with garlic and crucifixes to ward off vampires. The anti-GMO rhetoric had lulled for a time following passage of the bill, but began to intensify again last year in the months leading up to the labelling deadline. Despite overwhelming scientific evidence that GMOs are safe for human consumption, more than three-quarters of Vermonters supported the law, which stated that all food manufacturers had to slap a chunk of text on their packaging if their product contained any ingredients that were produced through genetic engineering.
Not the Ye Olde Geneticke Engineereing that humans have practised for millennia, mind you, such as the selective breeding, crossbreeding and hybridisation of plants and animals. Rather, like all opponents of ‘Frankenfoods’, the supporters of the bill clearly differentiated between these timeworn agricultural methods and the modern form of genetic engineering that involves scientists isolating and manipulating individual genes to promote desired traits. If companies refused to comply with demands to label their GMOs, their Chewy Pork-Os and Mini Cheese Conundrums would no longer be sold on our pure, artisanal shelves.
Don’t get me wrong – I’m not against continued research on GMOs. It’s vital to think about their impact on biodiversity and on the Earth’s delicate ecosystems. But these consequences are bad because they’re bad , not because GMOs themselves are unnatural. Being ‘anti-GMO’, period, is as good as being ‘anti-science’.
A s a lifelong horror-movie fan, I can admit that there is something classically terrifying about the idea of giant corporations manipulating genes. We all know what happens when EvilCorp sets up a secret lab on a remote island (experiments with weaponised shark-human hybrid cyborgs, obviously). DNA shouldn’t be tampered with, we say to ourselves. It’s intimate, fundamental – ‘us’; our purest essence laid bare, a potent link to our families and ancestors.
It doesn’t take much for us to mentally spool back in time, through gauzy montages of kooky historical costume changes, until we arrive at some untarnished, primordial Eve, squatting happily in front of her cook-fire, preparing some kale-and-wholegrain crêpes. Her rhythms are the rhythms of nature. She is at one with the natural world, not above or apart from it, and we are all connected to her across time, space and evolutionary leaps through this bio-mystical thread of the double-helix.
But wait; is that a rustling in the shrubberies behind her? Oh no, Eve, look out! It’s not a big cat or Eegah the Ripper; it’s so much worse. A pasty, bespectacled man in a white lab coat! With Big Ag embroidered on his breast pocket, and a huge stainless-steel syringe, glinting menacingly in the sun! Unaware of the danger, Eve nibbles on some açaí berries and assorted paleo-diet superfoods brought to her by obliging animal friends. The man squints down at his Ayn Rand day-planner where there are only two items in his to-do list:
The man plunges his syringe into a nearby quinoa loaf that Eve prepared earlier, and waits while globules of glowing green liquid penetrate deep into the bread. The artificial code slices, strangles and scrambles the hearty DNA of the untainted baked good until all that’s left is a clammy, greyish Frankenloaf. It’s ready. He scampers back into the bushes and waits for Eve to notice what he left behind.
We blame every ailment on scary ‘new’ technologies, forgetting the mortality we left behind
Soon enough, Eve is examining the bread, turning it in her long, calloused fingers. Her inner goddess whispers that something is not right. But it is so like ‘real’ food! Curiosity overcomes her and she takes that first bite and, as she does so, her DNA unravels, then tangles and sproings up like when you run a scissor blade over a Christmas ribbon to make it super sproingy. Her animal friends run away; they no longer recognise her as one of them. She looks no different yet she is changed deep inside. Her cycles no longer sync with the Moon, Gaia will not return her calls, and she’s pretty sure she has a peanut allergy.
Okay, so some more minor version of this drama is playing out subconsciously in the minds of people who refuse to believe in GMO safety. But was nature as great as their idealised Earth Mother fantasies suggest? We tend to romanticise the past and blame every ailment on this crazy, modern lifestyle and scary ‘new’ technologies, forgetting the mortality and brutality we’ve left behind. If Eve was a real person living in the palaeolithic era, her life expectancy would have been only about 30 years, max. As she approached this ripe old age, she would likely have been prone to many of the same ailments that people today don’t typically start to suffer from until their 60s. She would have been fortunate to live that long, considering they obviously didn’t benefit from miraculous modern medical advances such as antibiotics, cancer treatments, obstetrics, surgery and Bioré pore strips. And yes, the abundance of nutritious food that we enjoy today is thanks, in part, to GM technologies. You know what makes my lupus feel better? When I can afford healthy food, all year round.
W eek after week, as the labelling deadline approached, I’d marvel at the hundreds of scientifically unfounded anti-GMO lies that would be repeated in the comment sections of newspapers and other media outlets. As if Satan himself were coming to murder us right in the mouth with nightmarish, demon hybrid foods, and this law was the only thing that could keep him at bay.
Some of the claims came from the Vermont Right to Know GMOs website, which was the official hub of the bill’s news and activism activity. It contained a number of untrue or misleading claims including:
I’d be frightened too, if this were my primary source of information. The allegation that biotech firms alone are responsible for safety-testing would be especially alarming – they must test the GMOs on the same island where they make the deadly sharkmen! Thankfully, it’s not true (at least as regards the GMOs). Yes, there’s plenty of research bankrolled by the likes of the agricultural EvilCorp that is Monsanto or other GM technology firms such as Syngenta, Bayer or DuPont. But there are also multiple independent studies performed on patented GMOs all over the world. So it’s not some kind of science-y petri-dish, Wild West situation; GM seeds have been tested at every stage of development and release for 30 years .
It was actually people like me who ruined his natural cider with our dumb ill health
The GMO labelling bill H.112 came into force in Vermont on 1 July 2016. Three weeks later, on 29 July 2016, the then president Barack Obama signed into federal law bill S.764 , which required food manufacturers to label foods containing GMOs, but gave them numerous options to do so other than via the block of text that Vermont required. Scannable bar codes, QR codes, and 1-800 numbers are among the options. This federal law obviously superseded Vermont’s law, rendering it a very expensive few weeks for Vermont’s food producers. The state spent nearly $2 million defending the law, and Vermont retailers shelled out almost $5 million in their scramble to comply with it. Many of those stores are small mom-and-pop operations already struggling to compete with large chain retailers.
My boyfriend and I used to buy a share of a farmer’s harvest at the beginning of the season, from a local organic farm. We didn’t care so much about the organic part, we were just happy to support local agriculture. One afternoon, they had some cider for sale in addition to our usual weekly pick-up. It looked inviting, so I asked if it was pasteurised. ‘Yeah,’ the farmer grumbled, ‘I guess the FDA forces everyone to pasteurise it because someone claimed their kid got sick or something.’
As it happens, my lupus medications severely suppress my immune system, so I’m as high-risk for food poisoning as that pesky sick kid. For me and others like me, food poisoning can be deadly, and there have been plenty of outbreaks of E.coli associated with unpasteurised cider. But the farmer blamed the big evil US Food and Drug Administration (FDA) for imposing this unnatural process on the natural cider. It made me sad and angry, but I didn’t bother to explain that it was actually people like me who ruined his cider with our dumb ill health, or that I was grateful to the FDA for looking out for me.
Nature can seem as inspiring, beautiful, strong and nurturing as a mother, but it would be foolish to believe that this ‘mother’ loves us. There’s no reason we can’t celebrate her glorious natural gifts while also appreciating the important ‘unnatural’ improvements our fellow humans have created. I wouldn’t – and couldn’t – have it any other way. Would you?"
Replicants and robots: what can the ancient Greeks teach us? | Aeon Essays,,https://aeon.co/essays/replicants-and-robots-what-can-the-ancient-greeks-teach-us,"The question of what it meant to be human obsessed the ancient Greeks. Time and again, their stories explored the promises and perils of staving off death, extending human capabilities, replicating life. The beloved myths of Hercules, Jason and the Argonauts, the sorceress Medea, the engineer Daedalus, the inventor-god Hephaestus, and the tragically inquisitive Pandora all raised the basic question of the boundaries between human and machine. Today, developments in biotechnology and advances in artificial intelligence (AI) bring a new urgency to questions about the implications of combining the biological and the technological. It’s a discussion that we might say the ancient Greeks began.
Medea, the mythic sorceress whose name means ‘to devise’, knew many arcane arts. These included secrets of rejuvenation. To demonstrate her powers, Medea first appeared to Jason and the Argonauts as a stooped old woman, only to transform herself into a beautiful young princess. Jason fell under her spell and became her lover. He asked Medea to restore the youthful vigour of his aged father, Aeson. Medea drew all the blood from the old man’s veins and replaced it with the juices of powerful herbs.
Old Aeson’s sudden energy and glowing health amazed everyone, including the daughters of the elderly Pelias. They asked Medea to reveal her secret formula so that they might reinvigorate their father. Unknown to them, Pelias was an old enemy of Medea’s. The witch slyly agreed to let them observe her spell. Reciting incantations, she made a great show of sprinkling pharmaka (drugs) into her special ‘cauldron of rejuvenation’. Then Medea brought out an old ram, slit its throat, and placed it in her huge kettle. Abracadabra: a frisky young lamb magically appeared! The gullible daughters returned home and attempted the same technique with their aged father, repeating the magic words, cutting his throat, and submerging him in a pot of boiling water.
Of course, Pelias’s daughters killed him. Medea’s tale links hope and horror, a conjoined pair in reactions to scientific manipulations of life.
T he earliest known image of Medea appears on a Greek vase painting of about 500 BC, although the oral tradition is centuries older. As Medea stirs her cauldron, a sheep emerges from the pot. Medea’s ram and lamb are the ancestors of Dolly, the first genetically engineered sheep, which emerged from a cloning experiment in 1997.
The replication of life raises archaic fears. The Doppelgänger effect challenges a human desire for each individual to be unique, irreplaceable.
Deeply imbued with metaphysical insight and forebodings about human manipulation of natural life, these ancient stories seem startlingly of our moment. When remembered as enquiries into what ancient Greeks called bio-techne ( bios = life, techne = crafted through the art of science), the ‘science fictions’ of antiquity take on eerie contemporary significance. Medea and other bio-techne myths inspired haunting, dramatic performances and indelible illustrations in classical vase paintings and sculpture.
Meanwhile, in about 400 BC, Archytas, a friend of Plato’s, caused a sensation with his mechanical steam-propelled bird. The Hellenistic engineer Hero of Alexandria devised hundreds of automated machines driven by hydraulics and pneumatics. Other artisans crafted animated figures that made sounds, opened doors, poured wine and even attacked humans. Clearly, bio-techne fascinated the ancient Greeks.
Artificial, undying existence might tantalise but can it ever be magnificent or noble?
Behind these techno-wonders lies a search for perpetual life. For the Greeks, Chronos measured men’s and women’s lives. Time was divided into past, present, and future. Freedom from time promised eternal life but also raised troubling questions. Set adrift in Aeon , infinite time, what would happen to memories? What would happen to love? Without death and senescence, could beauty exist? Without death, was sacrifice or heroic glory still possible? Questing heroes in myths come to terms with physical death, accepting an afterlife in human memory even as they become Homer’s ‘twittering ghosts’ in the Underworld. The myths deliver an existential message: death is inevitable and in fact the possibilities of human dignity, autonomy and heroism depend on mortality.
Indeed, given a choice by the gods, Achilles and other heroes reject long lives of comfort and ease, much less everlasting life. In myth after myth, great heroes and heroines emphatically choose brief, memorable lives of honour, high-stakes risks and courage. ‘If our lives be short – let them be glorious!’ Artificial, undying existence might tantalise but can it ever be magnificent or noble?
Myths about the bravest heroes dramatise the flaws of immortality. When the goddess Thetis dipped her infant son Achilles in the enchanted River Styx to make him invulnerable, she had to hold him by the heel. On the battlefield at Troy, despite his valour, the best Greek champion died not in the honourable face-to-face combat he’d hoped for, but because a poisoned arrow shot from behind pierced Achilles’s mortal heel. It had seemed insignificant, but unforeseen vulnerabilities are endemic to cutting-edge bio-techne .
T he desire to overcome death is as old as human consciousness. In the realm of myth, immortality poses dilemmas for both gods and humans. The myth of Eos and Tithonus raises the problem of anticipating every contingency and potential complication. Eos was an immortal goddess who fell in love with mortal Tithonus. The gods granted her request that her lover Tithonus live forever. But Eos had forgotten to specify eternal youth. ‘When loathsome old age pressed full upon Tithonus,’ the myth recounts, Eos despaired. Sadly she placed her beloved in a chamber behind golden doors. ‘There, without the strength to move his once-supple limbs, Tithonus babbles on endlessly.’ In some versions, Tithonus shrivels into a cicada, whose monotonous song is a never-ending plea for death.
Tithonus’s fate continues to hover over the prospect of prolonging human lifespans. Recognising the ‘Tithonus dilemma’ inherent in keeping people alive indefinitely, the biomedical gerontologist Aubrey de Grey founded the SENS (Strategies for Engineered Negligible Senescence) Research Foundation in 2009. SENS hopes to find a way to avoid the decrepitude of ageing cells as death is increasingly postponed.
The most searching ancient myths ask whether immortality frees one from suffering and grief. In the Epic of Gilgamesh, for example, the eponymous hero of the Mesopotamian poem desires immortality. But if Gilgamesh were to gain everlasting life, he would spend it eternally mourning the loss of his companion Enkidu.
Or, consider the fate of the wise Centaur Chiron, teacher and friend of Apollo and Hercules. Chiron was accidentally struck by one of Hercules’s arrows tipped with venom from the Hydra monster. The gruesome wound would never heal. Wracked with unbearable pain, the Centaur begged the gods to trade his immortality for relief from pain, for blessed death. Prometheus, the Titan who taught humans the divine secret of fire, also found himself living forever, but with interminable pain. Zeus chained Prometheus to a mountain and dispatched a monstrous eagle to peck out his liver every day. The Titan’s liver grew back overnight, for the eagle to devour again. And again. Forever. Immortality.
Jason’s stones trigger the skeletons’ programming, causing them to destroy each other: an ominous foreshadowing of commanding cyborg soldiers
The horror of regeneration also drives the myth of the many-headed Hydra. Struggling to kill the writhing monster, Hercules lopped off each snaky head, and watched aghast as two more grew back in its place. Finally, he hit on the technique of cauterising each neck with a flaming torch. But the central head of the Hydra was immortal and could never be destroyed. Hercules buried the indestructible head in the ground and rolled a huge boulder over the spot to warn off humans. Even buried deep in the earth, the Hydra’s fangs continued to ooze deadly venom. This time, immortality was literally poisonous.
In another example, Jason and the Argonauts were menaced by a legion of terrifying replicants. Compelled by Medea’s hostile father to harvest an army from dragon’s teeth, Jason plows a field with a yoke of fire-breathing mechanical oxen manufactured by the legendary inventor Daedalus. He sows the dragon’s teeth in the soil. From the seeds, multitudes of invincible, fully armed skeleton-warriors spring up from the ground. But the uncanny crop of soldiers lacks one crucial attribute: they cannot be ordered or led. They only attack, ceaselessly. Medea’s father intended the army to destroy the Argonauts. The grim androids advance on Jason and his men. Desperate to halt the multiplying, uncontrollable mob, Jason throws stones into their midst. The impacts trigger the skeletons’ programming, causing them to fight the nearest soldier and thereby destroy each other. Some scholars believe the archaic tale predates Homer. The story is an ominous foreshadowing of the task of commanding cyborg soldiers.
Another series of myths credited Daedalus, the genius of Crete, with mechanical wonders. It was he who fabricated the drone-like eagle that perpetually attacked Prometheus’ liver. His most well-known experiment, to fly like a bird with manmade wings, has become a cliché of tragic hubris. Enraptured by the miracle of flight, Daedalus’s son Icarus soared too high. The Sun melted the wax component of the bronze feathers, the wings failed, and Icarus plunged to his death. Like other myths about immortality and augmenting human capabilities, the story points to the impossibility of anticipating mundane but fatal technical imperfections.
Greek legends claimed Daedalus was the first mortal to create ‘living statues’. His ‘living statues’ were animated bronze sculptures that appeared to be endowed with life as they rolled their eyes, perspired, shed tears, bled, spoke and moved their limbs. From his workshop emerged the biomimetic cow made of wood and hide, so realistic that it fooled a bull into mating with it, to satisfy Queen Pasiphae’s perverse lust. The result of this union of human, machine and animal was the Minotaur, a hideous creature with a man’s body and a bull’s head. He was destined to become the man-eating ogre imprisoned in the Labyrinth (another Daedalus design), until finally killed by the hero Theseus. Again, ancient bio-techne fused human and machine – and generated a monster.
H ephaestus, the god of invention and technology, also engineered robots to obey commands and move on their own. It is this divine metalsmith who possesses antiquity’s greatest resumé of bio-techne. Hephaestus manufactured a pair of mechanical gold and silver dogs to guard a king’s palace. His four robotic horses pulled a chariot, ‘kicking up dust with brass hooves and emitting a whinnying sound’. After the hero Pelops was chopped to pieces and resurrected by the gods, Hephaestus made a replacement shoulder blade of ivory.
Hephaestus devised a fleet of ‘driverless’ tripods on wheels that responded to commands to deliver food and wine. That led to his invention of a covey of life-sized golden maids to do his bidding. The robotic servants were ‘like real young women, with sense and reason, strength, even voices, and they were endowed with all the learning of immortals’. What Silicon Valley AI enthusiast could surpass such aspirations?
Hephaestus’s marvels were imagined by an ancient society not usually considered technologically advanced. Bio-techne creatures enchanted a culture that existed millennia before the advent of robots that can win complex games, hold conversations, analyse massive data, and infer human desires. But whose desires will AI robots reflect? From whom will they learn?
Microsoft’s teenage fem-chatbot Tay presents a contemporary cautionary tale. In March 2016, Tay went live on Twitter. Intricately programmed to mimic neural networks in the human brain, Tay was supposed to learn from her human ‘friends’. She was expected to articulate conversational gambits without filters or behavioural supervision. Within hours, malicious followers on Twitter caused Tay to morph into an internet troll, spewing racist and sexist vitriol. After less than 12 hours, she was terminated by her makers. Her easily corrupted learning system dampened optimism in self-educating AI and smart robots.
The ancient historians Polybius and Plutarch described a deliberately diabolical female robot. She was created for Nabis, the last king of Sparta, in the image of his vicious wife Apega. A brutal tyrant, Nabis came to power in 207 BC, and during his reign he extorted large sums of money from wealthy subjects. Greek sculptors were celebrated for extraordinarily realistic portrait statues with natural colouring, human hair and glass eyes. Nabis dressed this lifelike mannequin in his wife’s finery, which covered breasts that were studded with nails. Rich citizens were first plied with a great deal of wine and, if they refused to pay up, they were introduced to ‘Apega’, who would be more persuasive. As the drunken guest rose to greet the ‘queen’, King Nabis controlled a series of levers hidden in the robot’s back. She raised her arms and clutched the man, tightening her grip and crushing him to her spiky bosom. For this and other outrages, Nabis was assassinated in 192 BC. Many centuries later, medieval torturers would devise a crude version of the sophisticated Iron Maiden of Sparta.
The Argonautica , the epic poem about Jason and the Argonauts, also envisioned a murderous robot, Talos, one of Hephaestus’s most memorable creations. Talos was a gigantic bronze warrior programmed to guard the island of Crete by hurling boulders at approaching ships. He possessed another combat speciality, modelled on a human trait. Like the robot queen Apega, Talos could execute a chilling perversion of the universal gesture of human warmth, the embrace. With the ability to heat his bronze body red-hot, Talos would hug a victim, roasting them alive. How would Jason and the Argonauts escape from this bionic monster?
By using bio-techne to counter bio-techne . Medea knew that Hephaestus had created Talos with a single artery through which ichor , the mysterious life-fluid of the gods, pulsed from his neck to his ankle. A single bronze nail sealed Talos’s ‘vivisytem’.
Are Stephen Hawking, Elon Musk and Bill Gates the Promethean Titans of our era?
Medea convinced Talos that she could make him invincible by removing the bronze nail. But when the nail was pulled out, the ichor flowed out of Talos like molten metal, and his ‘life’ ebbed away. Medea had taken advantage of imaginary replicants’ perennial desire, from Talos to Frankenstein’s monster to Blade Runner : we believe they harbour human longings.
The capstone of Hephaestus’s laboratory was a female android requested by Zeus. Zeus wanted to punish humans for accepting the divine technology of fire stolen by Prometheus. And their punishment, created by Hephaestus, was Pandora (‘All Gifts’). Each of the gods endowed her with a human trait. Pandora possessed beauty, charm, musical talent, knowledge of healing and other arts, intelligence, daring and, of course, insatiable curiosity. Pandora is the gods’ AI Agent. She comes in the form of a lovely young woman, and she is sent to Earth carrying a sealed chest, which contains another set of ‘gifts’.
The friendly Titan Prometheus warned humankind that Pandora’s box should never be opened. Are Stephen Hawking, Elon Musk and Bill Gates the Promethean Titans of our era? They have warned scientists to stop the reckless pursuit of AI because once set in motion, humans cannot control it. ‘Deep learning’ algorithms allow AI computers to extract patterns from vast data, extrapolate to novel situations, and decide on actions with no human guidance. Inevitably, AI robots will ask questions of their own devising. Computers have already developed altruism and deceit on their own. Will AI become curious to discover hidden knowledge and act by its own logic?
P andora’s all-too-human, risk-taking, curious nature compelled her to open the chest. Out of Pandora’s box flew pestilence, disaster, misfortune. In simple versions of the myth, the last thing to flutter out of Pandora’s box was hope . But deeper, darker versions say that instead of hope, the last thing in the box was ‘anticipation of misfortune’. In this version, Pandora panicked and slammed down the lid, trapping foreknowledge inside. Deprived of the ability to foresee the future, humankind received what we call ‘hope’.
Since antiquity, philosophers have debated whether hope should be considered the best or the worst of the entities in Pandora’s sealed box. As human ingenuity, curiosity and audacity continue to test the boundaries of biological life and death, human and machine, this question will be posed to each new generation. Our world is of course unprecedented in the scale of techno-possibilities. But the unsettling push-pull of scientific nightmares and grand dreams is timeless. The ancient Greeks knew the quintessential attribute of humankind is always to be tempted to reach ‘beyond human’.
Earlier this year, engineers at the US weapons manufacturer Raytheon created three diminutive learning robots. They gave the robots classical names: Zeus, Athena and Hercules. With neural systems modelled on those of cockroaches and octopuses, the little solar-powered robots were bestowed with three gifts: the ability to move, a craving for darkness, and the capacity to recharge in sunlight. The robots quickly learned to mutate and soon understood that they must venture into excruciating light to recharge or die. This seemingly simple learning conflict parallels human ‘cognitive economy’, in which emotions help the brain allocate resources and strategise. Other AI experiments are teaching computers how human strangers convey goodwill to one another and how mortals react to negative and positive emotions.
Computers might be modelled on human brains but human minds do not work just like computers
Since Hawking warned that ‘AI could spell the end of the human race’, some scientists are proposing that human values and ethics could be taught to robots by having them read stories. Fables, novels, and other literature, even a database of Hollywood movie plots could serve as a kind of ‘human user manual’ for computers. One such system is named Scheherazade, in homage to the heroine of One Thousand and One Nights , the legendary Persian philosopher-storyteller who had memorised myriad tales from lost civilisations. For now, the stories are simple, showing computers how to behave like good rather than psychotic humans. With the goal of interacting empathetically with human beings and responding appropriately to their emotions, more complex narratives are to be added to the computer’s repertoire. The idea is that stories would be valuable when AI achieves the human mental tool of ‘transfer learning’, symbolic reasoning by analogy, to make decisions without guidance.
Computers might be modelled on human brains but human minds do not work just like computers. We are learning that our cognitive function and rational thinking depends on emotions. Stories appeal to emotions, pathos . Stories continue to live as long as they summon ambiguous emotions, as long as they resonate with real dilemmas and are good to think with. In ages past, Greeks told themselves stories to understand humankind’s yearning to exceed biological limits. Bio-techne myths are a testament to the persistence of thinking and talking about what it is to be human. Mythic insights and wisdom deepen our conversations about AI. Might some of these myths also play a role in teaching AI to better understand humankind’s conflicted yearnings? Perhaps someday AI entities will absorb mortals’ most profound wishes and fears as expressed in ancient myths and will grasp the tangled expectations we have of AI creations. Through learning that humans foresaw their existence and contemplated some of the quandaries they might encounter, AI entities might be better able to comprehend the quandaries that they pose for us.
The rise of a robot-artificial intelligence ‘culture’ no longer seems far-fetched. AI’s human inventors and mentors are already building that culture’s logos , ethos and pathos . As humans are enhanced by technology and become more like machines, robots are becoming infused with something like humanity. We are approaching what some call the new dawn of robo-humanity. When that day comes, what myths will we tell ourselves? The answer will shape how and what robots learn, too."
Could we mimic photosynthesis to tap into the Sun’s energy? | Aeon Essays,,https://aeon.co/essays/could-we-mimic-photosynthesis-to-tap-into-the-sun-s-energy,"When I think about the future of renewable energy, I picture the inner workings of a leaf – any leaf. A green plant is a remarkable solar-energy collector, effortlessly pulling sunlight, water, and carbon dioxide from the environment, and converting it into stored chemical energy. And the total amount of energy processed by photosynthesis is enormous. The Sun bathes the Earth with 173,000 terawatts of solar energy annually. On land alone, plants convert that energy into more than 100 billion metric tonnes of biomass. Our global energy use is just 18 terawatts per year, in contrast. As solar energy proponents are fond of saying: ‘The Sun provides in an hour enough energy to supply the world for a year.’
Humans already have a long tradition of exploiting sunlight trapped by plants. That is where coal, petroleum and natural gas came from: they are the fossil remains of ancient biomass, accumulated over many millions of years. The problem is that burning fossil fuels releases millions of years’ worth of carbon dioxide back into the atmosphere all at once. What we really want to do is replicate the process now, creating new fuel as quickly as we consume it, with the whole process driven by sunshine. Then we could bring solar energy to places it has never gone before. We could provide an unlimited supply of liquid fuels for aircraft and heavy-duty vehicles such as tractors and trucks, not to mention feedstocks for the plastics, paints and pharmaceutical industries – all with no net carbon emissions.
The obvious first thought is: why not just let the plants do the work? They have already mastered the necessary molecular technology. We’ve tried that with biofuels derived from corn, soya, or algae – but if we grow crops like corn for fuel, we’re robbing the Peter of food production to pay the Paul of carbon-neutral energy. We could install algal bioreactors in places where crops can’t be grown – but then the amounts of water, fertiliser, and energy consumed in processing the fuel are formidable.
We therefore need to tap the sun’s energy in a novel, synthetic way. And that way actually needs to improve on nature, audacious though that sounds, because the solar energy figures I just mentioned are not quite the cause for optimism they seem. Natural photosynthesis is only one per cent efficient. The biomass that became fossil fuels was based on sunlight falling unhindered on every square centimetre of exposed ground, every second of every day, for as long as there have been green plants. To make a meaningful, environmentally sound contribution to the energy supply, we have to create an industrial process that can make a serious dent in the 36 billion tonnes of CO 2 emitted annually by human consumption of fossil fuels, year in and year out. In other words, we need to do what plants do, but even better.
Although that sounds daunting, the more we know about natural photosynthesis, the more we can see that, since it has been cobbled together piecemeal by evolution, rational design ought to be capable of improving the yield. The essence of the natural process is to split water to yield hydrogen and to use the hydrogen to remove the oxygen from CO 2 to make hydrocarbons. What nature accomplishes – and what we want to do – is to remove some CO 2 from the atmosphere to create biomass. If our human nanotechnology can mimic that process, we will use up CO 2 as quickly as we produce it. It is almost too elegant that the key ingredient for addressing climate change could be the substance that is causing the problem in the first place.
The eventual goal is to obtain the CO 2 for fuel production from the atmosphere itself, but there the CO 2 concentration – even at its swollen level of 400 parts per million – is impractically low by current industrial process standards. At present, waste gases from industrial sources such as coal- and gas-burning power stations, steelworks and cement factories constitute the best source of CO 2 for fuel generation. They also neatly encapsulate the appeal of liquid solar fuels, as we could transform smokestack fumes from polluting industries into the raw material for a new kind of green energy.
F ortunately, engineers are not heading into entirely uncharted territory. Chemical reduction of CO 2 to make hydrocarbon fuel is already a tried and tested process. Based on a German invention of 1925, it uses cobalt or iron catalysts plus energy to make a range of hydrocarbons for fuel, lubricants, or feedstock. The process has been embraced where economic circumstances render the extra energy cost acceptable. During the Second World War, Germany, with no access to oil, used this technology to create fuel. South Africa today derives about 25 per cent of its fuel by similar means.
The German process doesn’t achieve the desired environmental goals – it actually increases CO 2 emissions – but it has inspired a promising step toward true artificial photosynthesis in the hands of George Olah, a Hungarian-American chemist, now 88 years old. Olah’s approach uses hydrogen produced via renewable electricity in a catalytic process to ‘reduce’ CO 2 to hydrocarbon or alcohol fuels.
The term ‘reduce’ has a special meaning in chemistry, and is central both to the chemistry of life and to the quest for renewable solar fuels. Look around the countryside on a nice, sunny day and you can see the central chemical principle of life on Earth. The dense mass of greenery and the blue sky represents the twin poles of life: oxidation and reduction, or redox. Air in the sky contains oxygen that liberates energy when it combines with organic compounds; oxidation is the process that creates fire, and also that powers your metabolism. The mass of green, on the other hand, is matter in a chemically reduced state, which is the opposite of what happens in respiration and combustion. In the presence of oxygen, reduced compounds can be thought of as having stored energy. Just as oxygen is the element of oxidation, hydrogen is the element of reduction.
These two elements have been linked in a close dance ever since Earth was formed, but to complicate matters there is a third partner: carbon. Carbon can exist in an oxidised state (that’s carbon dioxide – CO 2 ) or in a reduced state with hydrogen atoms attached, as in biomass and fuel. All living things consist of reduced carbon, great long chains and helixes and complicated clumps of carbon and hydrogen with other key elements attached in strategic places. Redox reactions – the molecular dance between carbon, hydrogen and oxygen – underlie three great mysteries: the origin of life, how to mitigate global warming, and how to tap the Sun’s energy without plants.
The laboratory for Olah’s CO 2 -reducing process is located in Iceland because of its abundant renewable electricity, generated from that country’s natural thermal springs. Since 2011, the George Olah Renewable Methanol Plant, operated near Reykjavik by Carbon Recycling International, has been using electricity from a thermal power station to split water into water and hydrogen. A nearby cement works provides a source of waste CO 2 . The hydrogen produced by the plant reduces the CO 2 to methanol. The methanol (sold by Carbon Recycling International as Vulcanol) can be used as fuel for vehicles, either straight or mixed with petrol. In July 2015, Carbon Recycling linked with the UK division of the engineering firm Engie Fabricom to develop large, standardised CO 2 -to-methanol plants. Although Iceland’s energy situation is unique, George Olah notes that many parts of the world have access to other forms of cheap renewable electricity (hydropower or solar-thermal power, for instance) that could drive the plants.
The Olah process is far from artificial photosynthesis, however. Turning sunlight directly into useful liquid fuels requires understanding the detailed electro-chemistry of what goes on in green plants, and then learning how to beat nature at its own game. The details of the photosynthesis process are immensely complicated: the water-splitting system in plants, called photosystem II, has two almost identical halves, each of which has 19 protein subunits that use 35 chlorophyll molecules. But at the most basic level, scientists understand quite well how plants use sunlight to generate electricity.
Photosynthesis ultimately depends on the photoelectric effect, explained by Albert Einstein in 1905, in which photons of light interact with electrons, knocking them free of their atoms. It is the process behind silicon solar panels. Normally, when sunlight knocks an electron out of any substance, the electron jumps straight back in. What the natural photosystems do is to prevent the electrons recombining by smuggling them down a chemical pathway from which the electron cannot return. A combination of minerals – magnesium in chlorophyll, manganese and calcium in the water-splitting photocentre – and a surrounding protein matrix constrain the electrons so they have no choice but to be shuffled away.
if our technical catalytic systems fall short of nature’s, why not just work with natural organisms?
The task for artificial photosynthesis researchers is to find an equivalent for the natural pass-the-electron-parcel chains. A lot of the research has centred on photosystem II, built around an unusually structured group of manganese, oxygen and calcium atoms (Mn 4 O 5 Ca) known as a cubane, which is embedded in proteins. The bonds between the atoms of cubane are distorted by the protein matrix; the resulting strain is what enables its catalytic (reaction-inducing) activity. This manganese, oxygen and calcium reaction centre is perhaps the chemical crux of life on Earth. But it turns out that slavishly copying it might not be the best way to create an artificial photosynthesis system of our own.
Researchers have tried many, many alternatives to the cubane-based catalyst in green plants, with only limited success. That slow progress has inspired a whole other approach: if our technical catalytic systems fall short of nature’s, why not just work with natural organisms that already have their own alternatives to green-plant photosynthesis? We’ve seen the drawbacks of using off-nature’s shelf biomass from corn, soya, or algae, but could there be a useful halfway point between natural photosynthesis and a full-blown artificial version? It turn out there is.
There is a group of primitive bacteria – the acetogens – that can reduce oxides of carbon without photosynthesis. These microbes perform the special trick of being able to live off the very gases we are concerned with: oxides of carbon (carbon monoxide and carbon dioxide), along with hydrogen. They can generate alcohols from these raw materials and, even better, can do so using a variety of ratios of hydrogen and carbon monoxide. This flexibility makes them well-suited for industrial use, because just such mixtures of gases are produced as the polluting waste products of electricity generation, as well as steel and cement manufacture.
LanzaTech, a US energy company devoted to producing liquid fuels from industrial waste gases, is one of the leading proponents of acetogens. These ancient bacteria are found naturally today around hydrothermal vents in the deep ocean, where they live on the hot gases that well up from the ocean floor. LanzaTech is focusing on one specific bacterium, Clostridium autoethanogenum , to generate ethanol from waste gases, mostly carbon monoxide and dioxide from steel mills.
Jennifer Holmgren, LanzaTech’s CEO, recognises that having a clever idea is not enough if you are trying to shift the enormous fossil-fuel industry. ‘Scaling up is the most important thing for any new technology,’ she says. ‘If it doesn’t scale, it doesn’t matter.’ To that end, the company has created a demonstration plant at the Baosteel mill in Shanghai, China, and last year they signed an agreement with the world’s biggest steel-makers, ArcelorMittal, to build a €87 million fuel-generating plant at their Ghent steelworks in Belgium. LanzaTech has also signed a deal to supply Virgin Atlantic with bio-aviation fuel.
This last venture touches on one of Holmgren’s key concerns, bringing carbon reductions to the parts of the energy economy that green electricity cannot easily reach. ‘If we go to electric vehicles on a large scale, how do we balance the system?’ she asks. ‘The system requires production of fuels – ground and aviation – and chemical coproducts. If the ground fuels portion goes off to electric, let’s say 30 per cent of ground transport, what happens to the economics of aviation fuel and chemicals production?’
L anzaTech’s approach is an important step toward true ‘artificial photosynthesis’, since it yields biofuels without relying on the usual green plants, but it is still only a beginning. More far-reaching are the experiments now underway to develop hybrid fuel-production systems – ones that still exploit energy-harvesting mechanisms found in nature, but that add synthetic components to make them serve our needs more effectively.
This work has been greatly aided by the remarkable discovery that some bacteria can live directly off a diet of electricity. Peidong Yang, a Chinese-born professor of chemistry at the University of California, Berkeley, has exploited this appetite for electrons by matching the bacteria with microscopic semiconductors that act as tiny solar cells. The bacteria grab electrons from the semiconductors and use them to reduce CO 2 . It’s a brilliant synthesis: semiconductors are the most efficient light harvesters, and biological systems are the best scavengers of CO 2 .
Methane is not a liquid fuel, but it can readily be converted to one. It can also be used directly as natural gas to run power plants
Yang’s team is currently studying three different systems. In one, the researchers built a forest of silicon and titanium dioxide nano-wires as the light harvester, and then cultured the bacterium Sporomusa ovata to grow over the wires and feed on the electricity. In another system, the researchers precipitated light-harvesting cadmium sulphide nanoparticles onto Moorella thermoacetica ; the particles enable the previously non-photosynthetic bacteria to turn light, water and CO 2 into acetic acid, which can readily be transformed into fuels such as butanol, or synthesised into plastics and pharmaceuticals. It is ‘artificial photosynthesis’ in a truly profound way, bringing the photosynthetic ability to an organism that never had it for billions of years.
The third method is the most conventional, but it also looks like the most likely one to scale up. Combining an electrochemical cell (driven by electricity, sunlight, or a combination of the two) with the bacterium ( Methanosarcina barkeri ) produces methane with an impressive 10 per cent solar-to-fuel conversion rate. Methane is not a liquid fuel, but it can readily be converted to one. It can also be used directly as natural gas to run power plants. This approach could solve one of renewable energy’s most pressing problems. Electricity cannot be easily stored, and both sun and wind are powerful but intermittent energy sources. Solar-generated methane can be stored to provide electricity generation when the sun doesn’t shine and the wind doesn’t blow.
Unlike natural photosynthesis, all of these artificial systems at present require concentrated CO 2 to work. ‘Ideally we’d be working with 400 ppm [parts per million] CO 2 in the atmosphere, but no one knows how to do that yet, no one,’ Yang says. There is an upside, though. The current approaches can be readily coupled with carbon-capture technology to pull CO 2 from smokestack emissions and convert it into fuel. This is the essential element of a closed carbon cycle that mimics nature, consuming the carbon created by human industry rather than dumping it into the environment. But that cycle still ultimately depends on the presence of the polluting industries.
T hen again, we do now know how to use CO 2 drawn directly from the air, on a laboratory scale at least. In January this year, George Olah’s group at the University of Southern California reported dramatic new work. Olah’s colleague G K Surya Prakash along with the PhD student Jotheeswari Kothandaraman have developed a combined process that uses a polyamine (a class of organic molecules essential both to life and to many industrial chemical processes) to capture carbon dioxide from the atmosphere, in conjunction with a ruthenium-based catalyst to reduce the CO 2 to methanol. Ruthenium catalysts have been employed before to reduce carbon dioxide, but making the process work at atmospheric levels of CO 2 , in a unified process with the carbon-capturing reaction, is a notable advance. In tests, up to 79 per cent of the CO 2 captured from the air was converted into methanol.
The Olah group have been pursuing their vision of a ‘methanol economy’ for many years and, with their experience from the Carbon Recycling plant in Iceland, they are well-placed to figure out how to make it work in a commercially viable way. Doing so will involve juggling a bewildering array of processes and market variables, though. Large-scale capture of atmospheric CO 2 would require prodigious quantities of polyamine, which raises issues of environmental safety. Ruthenium is a rare-earth metal that has seen considerable volatility in supply and cost. Its current price is around $42 per ounce, but a decade ago the price was more than $850.
These challenges should not deter us. We have grown used to accepting that we have to follow wherever the market leads us, which is how fossil fuels have remained so entrenched in the global economy for so long. But today there are bigger concerns than short-term market efficiency. We must have a reliable, secure, long-term, carbon-neutral fuel supply. That is the cornerstone of our future energy needs, and the other arrangements will have to be fitted around it.
‘Carbon is precious. We must learn to recycle it. There should be no waste. There is no waste in nature’
Back in 2008, the photosynthesis expert James Barber of Imperial College London advocated an Apollo-style programme, comparable in scale and urgency to the 1960s Moon race, to develop solar fuels. It’s taken a while, but their call is finally being heeded. Once the least known of renewable energy technologies, solar liquid fuels now have powerful advocates. In particular, Bill Gates recently organised the Breakthrough Energy Coalition, a group of 28 investors aiming to boost world spending on carbon-free energy development to $20 billion a year. He also catalysed Mission Innovation, a 20-major-nation governmental initiative launched at the Paris climate change conference in December 2015.
Gates has some powerful advantages. He understands both the technological and financial challenges, and has plenty of financial resources himself, having pledged $2 billion to the project. His thinking is outlined in a paper, Energy Innovation: Why we need it and how to get it . The US government is also getting on board with the new approach. In April 2015, the Department of Energy’s Joint Center for Artificial Photosynthesis (JCAP) announced renewed funding of $75 million and a change of direction, away from hydrogen production and toward the kind of solar-generated liquid fuels I’ve been describing. With researchers, foundations, major world governments, and large investors all pulling in the same direction, success is looking far more probable that it did just a couple years ago – although, as Gates points out, such major technological shifts have typically taken decades in the past. At the same time, programs like JCAP are puny compared to the total magnitude of the R&D effort needed.
The costs are high, but the potential payoff is even higher. Holmgren at LanzaTech lays out a compelling vision: ‘Carbon is precious. This means we must learn to recycle it. If you can extend its life by reusing it in a fuel, you will keep that equivalent amount of fossil fuel in the ground. There should be no waste. There is no waste in nature.’
Redox reactions – the dance between carbon, hydrogen, and oxygen – produced the cornucopia of life on Earth. Right now, we are merely running down those reactions, unwinding millions of years of biochemistry that is locked away in the planet’s fossil fuels, and systematically polluting the atmosphere in the process. We need to understand the redox reactions, so we can master the biomechanical machinery of photosynthesis and start building up with it. Success could transform the world economy, and the global environment. It is a challenge we cannot afford to pass up."
How fetuses learn to ‘talk’ while they’re still in the womb | Aeon Essays,,https://aeon.co/essays/how-fetuses-learn-to-talk-while-theyre-still-in-the-womb,"Some restless infants don’t wait for birth to let out their first cry. They cry in the womb, a rare but well-documented phenomenon called vagitus uterinus (from the Latin word vagire , meaning to wail). Legend has it that Bartholomew the Apostle cried out in utero . The Iranian prophet Zarathustra is said to have been ‘noisy’ before his birth. Accounts of vagitus uterinus appear in writings from Babylon, Assyria, ancient Greece, Rome and India. The obstetrician Malcolm McLane described an incident that occurred in a hospital in the United States in 1897. He was prepping a patient for a c-section, when her unborn baby began to wail, and kept going for several minutes – prompting an attending nurse to drop to her knees, hands clasped in prayer. Yet another child is said to have cried a full 14 days before birth. In 1973, doctors in Belgium recorded the vitals of three wailing fetuses and concluded that vagitus uterinus is not a sign of distress. An Icelandic saga indicates that the phenomenon has been observed in other animals – ‘the whelps barked within the wombs of the bitches’ – vagitus uterinus in dogs, a foretelling of great events to come in Icelandic lore.
Air is necessary for crying. The coordinated movements of muscles in the stomach and rib cage force air out of the lungs and up through the vocal cords – two elastic curtains pulled over the top of the windpipe – causing them to vibrate and produce a buzz-like sound. These sound waves then pass through the mouth, where precise motions of the jaws, lips and tongue shape them into the vocal signals that we recognise. In this case, the rhythmic sounds of a cry.
Vagitus uterinus occurs – always in the last trimester – when there’s a tear in the uterine membrane. The tear lets air into the uterine cavity, thus enabling the fetus to vocalise. Vagitus uterinus provided scientists with some of the earliest insights into the fetus’s vocal apparatus, showing that the body parts and neural systems involved in the act of crying are fully functional before birth.
Loud, shrill and penetrating – a baby’s cry is its first act of communication. A simple adaptation that makes it less likely that the baby’s needs will be overlooked. And babies aren’t just crying for attention. While crying, they are practising the melodies of speech. In fact, newborns cry in the accent of their mother tongue. They make vowel-like sounds, growl and squeal – these are protophones , sounds that eventually turn into speech.
Babies communicate as soon as they are born. Rigorous analyses of the developmental origins of these behaviours reveal that, contrary to popular belief – even among scientists – they are not hardwired into our brain structures or preordained by our genes. Instead, the latest research – including my own – shows that these behaviours self-organise in utero through the continuous dance between brain, body and environment.
I n the beginning, there is nothing. The world is formless and empty, darkness lies upon the amniotic waters. The embryo, still too young to be called a fetus, floats inside the amniotic sac of the uterus. It sees nothing. It cannot hear, smell or taste. It feels nothing – no heat, cold, pressure or pain. It cannot sense its own position or orientation. It cannot move.
In the seventh week of pregnancy, the embryo – c-shaped with a big head and tiny tail, eyes, ears, mouth, trunk and webbed limbs – begins to move. It measures about 10 mm from crown to rump – the size of a grape – and its heart, lungs, brain and spinal cord are forming. In that same week, the embryo begins to feel touch – which is the first of its senses to gain function. The world is no longer formless, or empty.
Movements trigger sensations, and sensations trigger movements. Through this cycle of sensorimotor activity , the embryo begins to discover its body, and world. It begins to develop vocal behaviour.
He got a First World War motion picture camera, and modified it by replacing the hand crank with a foot pedal
The first film of a living human fetus was made in January 1933, by Davenport Hooker, an anatomist at the University of Pittsburgh. We now consider it natural to observe the fetus in its home environment, but ultrasound technologies are shockingly young. Used originally to detect welding flaws in pipes, ultrasonography was first used to image a fetus in the late 1950s, by Ian Donald, an obstetrician in Glasgow. Real-time ultrasound scans have been around for about 50 years only, since the 1970s. To give you some perspective: Galileo Galilei first pointed a telescope towards the skies, and observed the craters of Earth’s moon, the rings of Saturn, the phases of Venus, the four largest moons of Jupiter and sunspots, in 1609. William Herschel discovered Uranus in 1781. Charles Wyville Thomson peered into the abyss for the first time, on board the HMS Challenger, in 1872. We had observed the skies and the depths of the oceans long before we had looked inside our own wombs.
Hooker wanted to study the developmental sequence of human sensorimotor activity. Scientists had done similar studies on a range of other species: salamanders, toadfish, terrapin, loggerhead turtles, pigeons, chickens, rats, cats, sheep and guinea pigs. But in 1933, Hooker had no way to look inside a human womb, so he arranged with a local hospital to film surgically aborted fetuses, while they were still alive. At the time, both the medical community and the general public viewed aborted fetuses as acceptable subjects for nontherapeutic research. The anthropologist Lynn M Morgan observes in her book Icons of Life: A Cultural History of Human Embryos (2009): ‘It is not that Hooker, his colleagues, or his audience de -humanised the fetus … they had never humanised fetuses to begin with.’
Early on, Hooker realised that fetal movements are too rapid to be studied accurately, unless the same movement can be viewed over and over again. So he purchased a surplus First World War motion picture camera, and modified it by replacing the hand crank with a foot pedal – thus freeing his hands to work with the fetus while also operating the camera.
On the day of surgical abortion, Hooker arrived at the hospital and set up his equipment in a nearby observation room. Time was of the essence, as the aborted fetuses in Hooker’s studies were all in the process of dying. Young fetuses would live for only seven to 10 minutes, and the older ones for a maximum of 20 minutes. To set up the camera in advance, Hooker picked out a plastic baby doll from a collection he had assembled, selecting the one that approximated the size of the fetus, based on its estimated age.
Dolls used by Davenport Hooker when positioning the camera, now housed at the National Museum of Health and Medicine. © 2009 Arne Svenson, courtesy of Blast Books and Arne Svenson
As soon as he received the fetus, Hooker positioned it beneath the camera, in a shallow basin of warm saline, and detached the placenta. This took him a few minutes to do. Then, as the camera recorded, Hooker used strands of thin human or thicker horse hair to lightly stroke various parts of the fetus – including its mouth, palms, trunk, limbs and back – to elicit movement.
Over a period of 25 years, Hooker filmed 149 fetuses, some of whom were spontaneously aborted, and others electively aborted for ‘the health, sanity, or life of the mother’, as Hooker wrote. They ranged in age from six weeks to 45 weeks (five weeks past the usual length of a pregnancy). After the fetus expired, Hooker handed over the fetal tissue to his collaborator Tryphena Humphrey. She then studied the structural development of the fetal nervous system.
Hooker and Humphrey found that the very young fetuses could not move. Then, at 7.5 weeks, lightly stroking the upper lip, lower lip or wings of the nostrils – collectively called the perioral area – caused the fetus to sharply move its head and trunk away from the offending strand of hair. The rest of its body still lacked sensation. Between 8 and 9.5 weeks, the fetus had feeling in its lower jaw area, and the sides of its mouth and nose. By 10.5 to 11 weeks, the upper regions of its face, like the eyelids, could feel touch. Two or three days after that, at around 11.5 weeks, the entire face of the fetus was sensitive to touch. In that same week, the fetus moved its lower jaw for the first time. At 12 to 12.5 weeks, the fetus had mature reflexes – it closed its lips when Hooker stroked them (in contrast to moving its whole upper body). By 13.5 to 14.5 weeks of age, the fetus was opening and closing its mouth, scrunching its face into a scowl-like expression, moving its tongue, and even swallowing.
The work of Hooker and Humphrey uncovered the close evolutionary ties we have to the other animals who share our planet. Though timelines may vary, terrapin reptiles, carrier pigeon birds and fellow mammals like rats, cats and sheep share a similar development sequence. In all of these species – ours included – the tactile system is the first to come online, and sensation begins in the perioral area, which is innervated by the fifth and largest cranial nerve: the trigeminal nerve . This nerve carries sensory inputs into the brain, giving us the ability to experience touch in our face and in the insides of our mouth. It also brings motor outputs from the brain to the muscles responsible for jaw movements, giving us the ability to feed and vocalise.
Some fetuses did, in fact, vocalise. ‘Between 18.5 and 23.5 weeks [about midway through the second trimester],’ Hooker reported, ‘the chest and abdominal contractions and expansions increase in incidence and amplitude, until brief but effective respiration … occurs … Whenever respiration occurs,’ he observed, ‘phonation – only a high-pitched cry, of course – accompanies its initiation and may be repeated at intervals following the establishment of breathing.’
Recent studies of prematurely born infants – conducted just a few years ago by Kimbrough Oller at the University of Memphis – have brought to light that fetuses born as young as 32 weeks (eight weeks before the usual date) do more than cry. They produce protophones, the infant sounds that eventually turn into speech. Meaning that a fetus in the last trimester of pregnancy can make all the sounds of a newborn infant.
B irth brings about tremendous changes in our physical and social environments, but life after birth is a continuation of life before birth.
Hooker’s findings had political consequences. When his science entered the public domain – in magazine spreads and a popular book called The First Nine Months of Life (1962), published just three years before his death – Hooker’s fetuses were used to illustrate the ‘wonder and beauty of human development’, without mention of abortion or where these fetuses came from. This was the Space Age, and his fetuses were pictured as little astronauts travelling through space in a placental capsule. The portrayals helped propagate a myth of independent fetal life – a way of thinking that had not existed publicly when Hooker began his research. In the 1970s, anti-abortion activists began using his work to plead their cases, including in briefs to the US Supreme Court.
Most visual representations of pregnancy are misleading, says the developmental biologist Scott Gilbert, professor emeritus at Swarthmore College in Pennsylvania. The mother and fetus are not separated by a cavity. Although often depicted this way, the fetus is not a tenant inside a vessel. Instead, the mother and fetus share arteries, veins and membranes. The placenta is an organ composed of both maternal and fetal tissues. ‘There is no clear boundary marking the place where the fetus ends and the uterus begins,’ Gilbert explains . The mother and fetus share a united anatomy, as well as cardiovascular, pulmonary, immune and metabolic systems. Pregnancy changes the mother’s physiology in ways that help both her and the fetus survive. Environmental factors that jolt the mother’s physiology – such as exposure to poverty, war, pollution or extreme heat events – will change the fetus. In strict scientific terms, neither the fetus nor the mother is a traditional biological ‘individual’. They are a fused entity. Political manoeuvrings to grant fetuses personhood, separate from their mothers, are not grounded in science.
Fetal activity at two months, from Look magazine, 1962. Courtesy the National Museum of Health and Medicine
I hope to convince you to drive a stake through the heart of the nature versus nurture debate
In the 65 years since Hooker completed his research, advances in ultrasound technology have allowed developmental psychologists to observe fetal faces in considerable detail. They have documented cry-like motions in fetuses that later went on to live full and healthy lives.
How then does our ability to vocalise come to be? Is it in our nature – ‘in-built’, ‘instinctive’, ‘inherent’, ‘innate’, ‘programmed’, ‘hardwired’, ‘in our genes’? Or is this ability ‘acquired’, ‘learned’, ‘absorbed from our environment’ – is there some nurture to speak of, even in the womb? If so, how much? Is it 80 per cent nature and 20 per cent nurture? Is it 90/10? 30/70? Or some other split? What I hope to do is convince you to drive a stake through the heart of the nature versus nurture debate – often framed as a fight for causal supremacy between two opposing factions – it gets us nowhere.
I learned this lesson while working in the Developmental Neuromechanics and Communication Lab, led by Asif Ghazanfar at Princeton University, where I earned my doctorate. When a behaviour – such as vocalising – is present at birth, it is thought to be largely in our nature (often even by scientists). But looking inside the womb reveals that the behaviour is the outcome of a developmental process – and at no point can that process be neatly partitioned into silos of nature or nurture.
Nature shapes nurture, and nurture shapes nature. It is not a duality – it’s more like a Möbius strip.
O n the afternoon of 7 July 2011, I was with my lab mate Daniel Takahashi, a Brazilian-Japanese medical doctor turned neuroscientist. We were hunched over a padded bench in a dimly lit room of a 1920s neo-gothic building. I had an ultrasound wand in one hand and a popsicle stick coated with marshmallow fluff in the other. Our pregnant patient lay on the bench between us, as she had done countless times over the past year.
From head to toe, Takahashi and I wore medical-grade protective gear – gloves, gown, surgical mask, face shield, booties and bonnet. The equipment was in part to meet the stringent standards of the university and federal regulatory boards, but also for protection. The transmission of germs between us and our patient could be perilous, particularly for her.
Our patient was not concerned about germs, or the ultrasound gel dripping off her stomach, or even Takahashi’s gently restraining hands. Her mind was on the marshmallow fluff. We were thankful for this, as our task was not an easy one. We had to find her tiny acrobatic fetus in the vastness of her womb, and observe its face for as long as she would let us, while recording the fetus through a DVD player hooked up to the ultrasound machine. On this day, skill and luck collided. Takahashi and I were able to watch the face of the fetus for almost 30 minutes and, at moments, we saw it moving its mouth.
Our pregnant patient was a marmoset monkey ( Callithrix jacchus ), one of five females and five males, housed in pairs in a large indoor enclosure a few doors away from where we were conducting the ultrasound examination. We called her Laurie Rose. Her mate was John Holmes. We named them after porn stars, because they went at it like porn stars. Laurie Rose was the first of our monkeys to get pregnant and carry her baby to term. We were able to observe, quantify and characterise the mouth movements of her fetus from day one of movement to the day before birth – an approach not feasible in humans.
These first actions looked different from the movements Jack made after birth
Marmosets are a New World monkey species found in Brazil. They had a common ancestor with our species about 35 million years ago. Despite our distant ties and their squirrel-like appearance, marmosets resemble humans in a number of ways. Physiologically, they develop 12 times faster than we do, but have roughly the same brain and body parts. They live in family units of three to 15 – mother, father and children (the monkey equivalent of a nuclear family). Occasionally, extended family members and unrelated adults will live with them. They engage in both monogamy and polygamy, and older family members tend to assist in child-rearing. Marmosets are chatty, and take turns to speak to one another – like we do. Newborn marmosets cry and make a number of other sounds. These sounds are comparable to human protophones, but are more adult-like in their form. Most remarkably, Takahashi and Ghazanfar discovered that marmoset babies learn to ‘talk’ using parental feedback, similar to how human babies learn. It is highly likely that the vocal development of a marmoset fetus is similar to our vocal development.
At first, Laurie Rose’s fetus – we called him Jack – made very few mouth movements (and they were often accompanied by motions of the head). These first actions looked different from the movements Jack made after birth.
In scientific works, descriptions of early fetal movements, from a diversity of species, strongly resemble one another. Chicken embryos exhibit ‘convulsive-type jerks and twitches and occasional head thrusts’; cat fetuses show ‘purposeless, squirming trunk, head, and leg movements’; rat fetuses display ‘unintegrated, aimless movements’; and human fetuses give us ‘spontaneous startles, general movements (GMs), isolated movements and twitches’. This is how Jack was moving.
Hooker and other scientists of that era believed that all fetal movements are primarily reflexive, evoked by external stimuli. We now know that the earliest fetal motions are spontaneous, first triggered in the muscles themselves and later by electrical pulses sent from the spinal cord or brainstem to the muscles. Fetal movements can be spontaneous or evoked by a range of stimuli, such as the mother rubbing her belly or the fetus touching its own body.
Actions of the fetus – ‘purposeless’ though they may appear – are important. They support the growth of muscles, tendons, ligaments, cartilages and bones. They steer the development of the sensorimotor circuits. General movements are ‘storms’ of activity that help the fetus develop a sense of its own form and body position. Isolated movements help connect the body to the brain, through tactile feedback. Twitches, which occur during sleep, are the brain’s way of testing its circuitry. The behavioural neuroscientist Mark Blumberg likens the act to a switchboard operator throwing on switches, one at a time, to test which light turns on with each switch.
As Jack got older, he grew more active. He moved his mouth more. I calculated the entropy of his movement patterns and found that it was decreasing. This means Jack’s actions were getting less chaotic. He was now moving his mouth independently from his head. If you will recall, Hooker found a similar trajectory in humans. When he stroked the mouth of the 7.5-week-old fetus, it moved away its entire upper body. With time, its body parts acquired greater autonomy, and at 12.5 weeks the fetus responded by closing its mouth alone.
H ow does the fetal body become more finely tuned? It is hard to look under the hood of a living being and know exactly what is happening, but a group of roboticists in Japan are attempting to reverse-engineer human development and build humanoid robots. They provide us with clues. At the University of Tokyo, Yasuo Kuniyoshi, Hiroki Mori and Yasunori Yamada simulated the motor development of a fetus. At the start line, the computer-generated fetus was placed inside an artificial womb, enclosed in an elastic membrane, and bathed in fluid. It had immature neural circuits, with a biologically accurate musculoskeletal body and tactile receptor distribution. Even something as seemingly ‘hardwired’ as our physiology is not shaped by genes alone. Biophysical events (like chemical reactions in the cells, mechanical pressures inside and on the cells, and gravity) switch genes on and off – influencing how cells divide, move, determine their cell fate, and group into tissues and body parts.
The neural circuits of the simulated fetus triggered movement. As the fetus moved, its tactile receptors detected the pressure of fluid against its moving body. The information was passed on to its nervous system. The action of each muscle affected the sensory input it received, as well as the sensory input received by adjacent muscles. Sometimes, an isolated movement brought one body part into contact with another – inducing active stimulation of the touching part and passive stimulation of the touched part.
The neural circuits of the simulated fetus began to learn . Initially, large undifferentiated circuits controlled multiple body parts (eg, a single motor unit controlling the head and trunk). As development progressed, the brain responded to feedback from its body, causing these large units to subdivide into smaller, more precisely controlled units (eg, separate control of the head and trunk, driven by the separate experiences they have while moving).
The fetal body, like ours, varies in sensitivity to touch – the mouth region is dense with tactile receptors
Neuroscientists have a mnemonic for the biological process that adjusts the strength of connections between neurons – ‘Fire together, wire together. Out of sync, lose your link.’ The catchy rhyme is credited to the neurobiologist Carla Shatz. It means that the brain rewires and fine-tunes itself by strengthening the connections between neurons that are repeatedly activated together, and weakening the connections between neurons that are not activated together. The weak connections are ultimately pruned.
In the simulation, as in real life, the musculoskeletal body of the fetus determined both the possibilities and constraints of movement. Human heads, for example, rotate around two socket pivots, giving us a decent range of motion, but not as much as an owl’s head, which rotates around a single socket pivot.
As the simulated fetus grew larger, it came into more physical contact with the uterine wall. When this happened, the weak and continuous tactile signals from the amniotic fluid were followed by the short, blunt force of the wall hitting its flesh. The brain took note, and the fetus modified its actions.
The fetal body, like ours, varies in sensitivity to touch – the mouth region is dense with tactile receptors, whereas the top of the head is not. As the simulated fetus matured, it frequently touched its more sensitive parts while barely touching the more boring ones. This behaviour mirrors observations of real-life fetuses, who often direct their arm movements to their mouth, and even open their mouth in anticipation of their hand arriving there.
As the simulated fetus continued to grow, its larger body parts (like the head) came to be more constrained by the uterine wall, and only small regions (like the mouth) could move freely.
The neural circuits responded to all these contingencies, and the body of the fetus became more precisely controlled (eg, separate functions of the head and mouth, more manoeuvrable orofacial muscles). The fetus was now practising infant-like movements. In real life , the most frequent actions in the third trimester are face-related – hands touching the face, facial expressions like scowling, and mouth movements. All the behaviours seen in a newborn have at some point been observed in the fetus.
Fetal movements self-organise through the continuous and reciprocal interactions between brain, body and environment.
Like the simulated fetus, Jack began moving his mouth independently while rarely moving his head. I used a dynamic time warping (DTW) algorithm – often used to detect forged signatures – to match the temporal profiles of his mouth movements to those he eventually made as an infant. DTW generates a ‘distance’ score to indicate the extent to which one profile must be ‘warped’ to match the other. The algorithm uncovered that, in parallel to his mouth and head decoupling, a subset of Jack’s mouth motions were becoming nearly identical to those he made while calling out to his family as an infant. Fetal Jack had developed the behavioural repertoire he would so often use in life.
My work with Ghazanfar and Takahashi established – for the first time in a primate – that the ability to vocalise at birth is not ‘innate’. Rather, it undergoes a lengthy period of prenatal development, even before sound can be produced.
M armosets and other nonhuman primates have precursors to language, but language is unique to humans. Even as newborns, we show a preference for speech, we recognise our native languages, and remember the stories we have frequently heard in the womb. As babies, we don’t merely cry – as already mentioned, we cry in the accent of our mother tongue. To understand the development of these uniquely human behaviours, we need to turn to hearing.
The sensory systems of birds and mammals on our planet, ours included, develop in a fixed order – with hearing developing second to last (just before the visual system). The human auditory system comes online between 24 and 28 weeks, when the spiral ganglion neurons connect our auditory receptors – the hair cells of the inner ear – to our brainstem and temporal lobe. Hearing becomes fully functional in the third trimester.
Hearing is often studied in sheep. The size of the middle and inner ear of sheep is similar to ours, and hearing develops in utero in both species. Ken Gerhardt, Robert Abrams and their colleagues at the University of Florida implanted a tiny electronic device in the inner ear of fetal sheep, to study what speech signals the auditory receptors of the fetus pick up and transmit to its brain. They then played 48 English sentences through a speaker positioned near the mother sheep. The sounds captured by the device were replayed to adult humans who attempted to decipher the original sentences, from the speech signals recorded in the fetus’s inner ear. In total, they deciphered about 41 per cent of the sentences. They tended to miss or confuse high-frequency consonants like ‘s’, ‘h’ or ‘f’. A word like ‘ship’ could be easily confused with ‘slit’ or ‘sit’. The low-frequency vowels were heard more reliably.
Vowel sounds penetrate the fetus’s inner ear better than high-frequency consonants because the uterine environment acts as a low-pass filter, absorbing and muffling sound frequencies above 600 Hz. While individual speech sounds are suppressed in the womb, what remains prominent are the variations in pitch, intensity and duration – what linguists refer to as the prosody of speech.
Exposure to speech in the womb leads to lasting changes in the brain
Prosody is what gives speech its musical quality. When we listen to someone speak, prosody helps us interpret their emotions, intentions and the overall meaning of their message. Different languages have different prosodic patterns. Native speakers of English, French or Italian use contrasts in syllable duration to make words more salient. When native English speakers utter the phrase ‘to Ro:me’, we draw out the vowel ‘o’ in ‘Rome’ (but not in ‘to’). A native Japanese speaker would use a change in pitch instead. They would say ‘^Tokyo kara’ (‘to Tokyo’) to draw the listener’s attention to the more important word ‘Tokyo’. Nawal Abboub, Thierry Nazzi and Judit Gervain used near-infrared spectroscopy (NIRS) to image the brains of newborns in one of Europe’s largest pediatric hospitals. They found that the infants had learned the prosodic patterns – duration contrast versus pitch contrast – of their mother tongue.
Language learning begins in the womb, and it begins with prosody. Exposure to speech in the womb leads to lasting changes in the brain, increasing the newborns’ sensitivity to previously heard languages. The mother’s voice is the most dominant and consistent sound in the womb, so the person carrying the fetus gets first dibs on influencing the fetus. If the mother speaks two languages, her infant will show equal preference and discrimination for both languages.
The fetus’s knowledge of native prosody goes beyond perception. Kathleen Wermke, Anne Christophe and their colleagues in Würzburg and Paris collected and analysed the cries of 30 German and 30 French newborns from strictly monolingual families. They found that the French newborns tended to cry with a rising – low to high – pitch, whereas the German newborns cried more with a falling – high to low – pitch. Their patterns were consistent with the accents of adults speaking French and German.
The newborns had not just memorised the prosody of their native languages; they were actively moving air through their vocal cords and controlling the movements of their mouth to mimic this prosody in their own vocalisations. Babies are communicating as soon as they are born, and these abilities are developing in the nine months before birth.
There is no genetic blueprint, programme or watchmaker who knows how it must turn out in the end. The reality of how these behaviours come to be is far more sophisticated and elegant. They develop through continuous interactions across multiple levels of causation – from genes to culture. The processes that shape them unfold over many timescales – from the milliseconds of cellular processes to the millennia of evolution.
Now here’s my challenge to you: which of these factors are the primary drivers of vocal development – our genes or brain? – and which ones are merely supporting – the body? How much of their communication do babies owe to nature versus nurture? Is it more nature or nurture? I guarantee you, there are no scientifically defensible answers to these questions.
Development does not rely on a distinction between primary, essential causes and secondary, supporting ones. They are all essential – intimately connected – causal contributors. Let us disregard our artificial hierarchies. Mother Nature herself pays little attention to them."
"After 3,000 years of science, the embryo is very different | Aeon Essays",,https://aeon.co/essays/after-3000-years-of-science-the-embryo-is-very-different,"Fifty-four years ago, I did something extraordinary. I built myself. I was a single, round cell with not the slightest hint of my final form. Yet the shape of my body now – the same body – is dazzlingly complex. I am comprised of trillions of cells. And hundreds of different kinds of cells ; I have brain cells, muscle cells, kidney cells. I have hair follicles, though tragically few still decorate my head.
But there was a time when I was just one cell. And so were you. And so were my cats, Samson and Big Mitch. That salmon I had for dinner last night and the last mosquito that bit you also started as a single cell. So did Tyrannosaurus rex and so do California redwoods. No matter how simple or complex, every organism starts as a single cell. And from that humble origin emerges what Charles Darwin called ‘endless forms most beautiful’.
Once you’ve come to terms with that mind-boggling fact, consider this: all organisms, including humans, build themselves . Our construction proceeds with no architects, no contractors, no builders; it is our own cells that build our bodies. Watching an embryo, then, is rather like watching a pile of bricks somehow make themselves into a house, to paraphrase the biologist Jamie Davies in Life Unfolding ( 2014 ).
This process of body sculpting is called embryonic development, and it is a symphony of cells and tissues conducted by genetics, biochemistry and mechanics. People who study this, like me, are called developmental biologists. And while you may not know it, our field is in a period of tremendous excitement, but also upheaval.
I n the summer of 2022, I sat in the back of a lecture hall in Santa Cruz, California listening to a lecture from Magdalena Żernicka-Goetz, professor of mammalian development and stem cell biology at the University of Cambridge, UK. She is a controversial figure and one of many scientists trying to push the limits of understanding human embryos. I heard, too, from Ruth Lehmann, director of MIT’s prestigious Whitehead Institute for Biomedical Research. She’d been in the news for firing a famous scientist for sexual harassment, but what’s made her an international leader in biology for decades is her brilliant and creative study of developmental biology, in fruit flies.
This juxtaposition of fly and human embryos wasn’t surprising; developmental biology is propelled by a whole zoo of embryos – fruit flies, yes, but also sea urchins, worms, frogs, mice. Indeed, our great triumph in the 20th century was revealing the astonishing molecular similarity of all embryos; and, for precisely that reason, studies of animal embryos have garnered seven Nobel Prizes in the past 30 years alone. What surprised me in Santa Cruz was just how fast our collective understanding of animal embryos is making possible truly explosive advances in human embryology. So, while Lehmann’s fascinating new work on cell migration in fly embryos kept the audience rapt, it was Żernicka-Goetz who caught the media’s attention .
Developmental biology is something society needs to understand. And don’t we want to?
Together with Jacob Hanna’s lab in Israel, Żernicka-Goetz was building what scientists call ‘embryo models’. These biological entities look a lot like embryos; they start as relatively few cells and few cell types, and they grow and elaborate over time. But they’re not made in the usual way. Eschewing both egg and sperm, embryo models are created by manipulating embryonic stem cells. Perhaps best known to the public for their promised miracle cures or as proxies for abortion debates, these cells display a remarkable power. They can be made to differentiate into essentially any cell type in the body. Now, it seems, we might even use them to make embryos.
When Hanna and Żernicka-Goetz each published their findings after the meeting in Santa Cruz, The Washington Post wrote that the advances put ‘the possibility of a complete human synthetic embryo on the horizon’. That nomenclature was unfortunate, as these aren’t synthetic at all, but rather entirely biological . (That’s why scientists prefer the term ‘embryo models’.) But they were spot on about the implications. And about the timing: reports of embryo models made from human stem cells hit newspapers exactly a year later, in the summer of 2023.
This is no incremental change and, despite the flawed press narrative, Żernicka-Goetz and Hanna aren’t the only or even the most important players in the game. Other influential biologists are making huge strides too, though their names aren’t often in the press. Some have even argued that the new advances ‘challenge the current legal definitions of the embryo’, which prompts the question: how should we define an embryo? And what do we do when, as they certainly will, scientists’ definitions differ from the general public’s? As embryo models become more sophisticated, how will we know when that clump of tissue in the dish becomes an embryo?
I’ve studied embryos for more than 30 years, and while it doesn’t often catch the public’s attention, developmental biology is something society needs to understand. And don’t we want to? Isn’t it just another way of framing that ancient and universal question: How did I get here?
H uman contemplation of embryonic development is nearly as old as writing. In the Old Testament story, Job asks of God: ‘Didst thou not pour me out like milk and curdle me like cheese?’ Half a world away, the Buddha uses the same dairy-based metaphor in the Garbhāvakrāntisūtra , a 1st-century scripture. Some of the earliest cultures in Southern Mexico left no writing, but they made statues of human fetuses. Anywhere you go in the ancient world, you find embryos.
In ancient Greece, as light began to show in the cracks that separate religion, philosophy and science, a remarkable treatise appeared. To modern eyes, On the Nature of the Child – attributed to Hippocrates – is bent on explaining human development, though it does so largely by describing the development of a hen’s egg. Actually, not an egg but 20 eggs, each of which the author exhorts us to open on successive days, so we can observe development over time : ‘You will find everything as I say in so far as a bird can resemble a man.’
Aristotle rejected preformation, and argued instead for a progressive development
That ancient appreciation of time is critical, for it frames the first key question in the history of developmental biology: does an embryo acquire its complexity piece by piece, somehow progressively assembling itself? Or is that new organism already present in the egg or sperm, preformed, as it were, and needing only to be spurred somehow to grow? Some readers will be familiar with the iconic image of preformation – a tiny human curled up inside a sperm. Its late-17th-century printing underscores just how long we struggled to resolve these two poles of thought, progressive versus preformed.
The homunculus, an icon of preformation: Nicolaas Hartsoeker’s fanciful image of a tiny, preformed human inside a sperm
The notion was rejected by Aristotle in ancient Greece, but remained popular until the 18th century. From Essay de Dioptrique (1694)
Aristotle himself was the first to weigh in. Consulting farmers and fishermen with the same enthusiasm with which he debated scholars, the philosopher described everything from the live births of dolphins to the size of elephant embryos. He compared the embryos of chickens, fish, insects and, yes, humans . He rejected preformation: ‘our senses tell us plainly that this does not happen’. He argued instead for a progressive development, and while it took 2,000 years to resolve, he was exactly right.
Just how this progression happens remains the core question of developmental biology. And as we begin to explore the truly uncharted morality of embryo models and their progressive development, what strikes me most about the concept is how neatly it parallels ancient thoughts about inchoate humanity.
I n the modern debate over abortion, the doctrine that ‘life begins at conception’ is now so constantly repeated that it’s often assumed to have an ancient, perhaps even scriptural origin. It does not.
In fact, in Catholic canon law, the doctrine dates precisely to 12 October 1869, when Pope Pius IX declared excommunication as the penalty for anyone involved in obtaining any abortion. For the nearly 2,000 years that had gone before, however, many Christian thinkers held the embryo to acquire its humanity only gradually. This concept, linked to the ‘animation’ or ‘ensoulment’ of the embryo, arose in laws first set down more than 3,000 years ago that imposed increasingly harsher penalties for causing the loss of a pregnancy as it progressed.
The idea was widely, if not uniformly, adopted by early Christian jurists. St Augustine held this view; St Basil was opposed. None wielded greater influence than St Thomas Aquinas, whose 13th-century rendering of Aristotle’s progressive acquisition of humanity in utero became a prominent, perhaps dominant concept in Western Christianity. It surfaced everywhere from Dante’s poetry to Celtic law for 500 years.
The embryos of scientists are not the embryos of the public, or the Church
Of course, saints weren’t the only ones thinking about embryos. Leonardo da Vinci drew several in the 16th century, one now famous for its inaccuracy. When the modern university was being developed in a 16th-century Italy roiled by Protestant Reformation and Catholic Counter-Reformation, scholars on both sides cracked open chicken eggs to study embryos. A century later, a less divided group (all Royalists in the English civil wars) still hotly debated the chick embryo. And when modern science began to emerge in the 17th century, its founding figures had more than a passing interest in the embryo.
By the 19th century, the new scientists had reached consensus. The concept of progressive embryonic development of animal embryos was established once and for all. But then as now, the embryos of scientists are not the embryos of the public, or the Church. In an odd synchronicity, science and Church staked out opposite views at essentially the same time.
A mere 23 days separated Pope Pius’s decision and an important lecture by the embryologist Wilhelm His. Propounding a new vision for understanding progressive development of the embryo, His would go on to publish The Form of Our Body and the Physiological Problem of Its Development (1874) . It was – despite the possessive in the title – a thoroughgoing discussion of chicken embryos. But His said exactly what he meant. Soon after, he would combine lessons learned from chickens with a network of physicians, and become the first to comprehensively define, cogently describe, and accurately display the progressive development of human embryos.
Selections from the ‘normal table’ of human development: the embryologist Wilhelm His et al ‘produced’ the scientific conception of the human embryo in the 1870s using careful staging and illustration. From Anatomie menschlicher Embryonen (1880-85). Courtesy the Wellcome Library
As the Cambridge historian Nick Hopwood put it , His and others produced the very concept of the embryo as we know it. And, while embryos certainly exist as tangible, biological entities, this concept is so central to the work of developmental biologists that we rarely notice it. We’re also slow to consider how others in society relate to it. And that’s important, because, in the 20th century, the concept of the embryo changed radically yet again.
B y the time the famous double helix structure of DNA was discovered in the early 1950s, fruit flies like Lehmann’s had taught us that genes direct the inheritance of traits from one generation to the next; sea urchins showed us that genes reside on chromosomes in the cell nucleus; and bacteria and viruses revealed that genes were made of DNA. But the relationship between our genes and our development was still mostly a black box. When we first peeked in, it wasn’t through the ascendant disciplines of genetics and biochemistry, but a more hands-on approach: transplantation. Not of organs, but of cellular bits.
In Nobel Prize-winning work, the British developmental biologist John Gurdon showed that if he destroyed the gene-containing nucleus of a one-cell frog embryo, normal development could be restored by transplanting the nucleus of some other cell. Fascinatingly, any cell nucleus might do the job, suggesting the tools needed to guide development of an entire organism are present in each and every one of its cells.
But there was a catch. Donor nuclei from early embryonic cells were far better at restoring development than those taken from later embryos. Such decreasing ‘potency’ over time was a crucial revelation for understanding progressive development. The concept has its apotheosis in the British developmental biologist Conrad Waddington’s ‘landscape’, an iconic image depicting an early embryonic cell as a marble set to roll down a branching network of increasingly deep valleys. At the top, the marble might still roll into any number of valleys, but its inventory of potential shrinks with its descent. It can’t roll back uphill.
Waddington’s landscape: in the iconic metaphor for progressive development, the marble represents a cell in an embryo; as the embryo develops, the cell rolls downhill. At the first decision point, the cell might choose one of two valleys, thus becoming one of two very general cell types, for example mesoderm or ectoderm. At the next branch, the cell will become one of two very specific cell types, and so on. From The Strategy of the Genes by C H Waddington (1957) © George Allen & Unwin (London)
If the marble rolls down the valley biologists call ‘mesoderm’, it might roll further into clefts such as muscle or blood. But it’s cut off from the valleys of skin and brain, what we call ‘ectoderm’. Becoming an embryo, then, is the collective navigation of an ever-branching decision-tree by a constantly multiplying population of cells. So it’s tempting to think that some notion of sufficient complexity, a far-enough journey down the valleys, might help us divine precisely when it’s an embryo, and when it’s a human.
Edwards had studied the possibility of IVF in mice, then sheep, cows, pigs, monkeys
But, again, there’s a catch. While most cells in the early embryo rush down the valleys, a privileged few will linger at the top of the landscape. Described first in rabbits by Waddington’s own pupil at the University of Edinburgh, Robert Edwards, we now call these embryonic stem cells, and by the turn of the 21st century they were as much a part of politics as of biology. But when first described in the early 1960s, neither Edwards nor anyone else capitalised on their potential. And, anyway, Edwards was busy with another project. The era of test-tube babies was upon us.
Late in 1977, Edwards wrote a note to one of his patients, Lesley Brown: ‘[Y]ou might be in early pregnancy. So please take things quietly – no skiing.’ Some weeks earlier, she’d had one of her eggs laparoscopically inserted into her uterus; it had been fertilised in vitro with her husband John’s sperm. In 1978, Louise Brown, the first child conceived by IVF, was born.
The feat capped more than a decade of hard work. Edwards had studied the possibility of IVF in mice, then sheep, cows, pigs, monkeys. Eventually, human oocytes removed in a hospital in Oldham made the four-hour journey to Edward’s lab in Cambridge. And, there, he was the first to glimpse the moment when the Church says life begins. Coming precisely a century after Pius IX’s decision, his co-authored 1969 paper describing human fertilisation for the first time had been a watershed moment in the 3,000-year history of embryology. But it was also, well, just developmental biology: ‘Penetration of spermatozoa into the perivitelline space was first seen in eggs examined 7-7.25 h after insemination.’
The human embryo had become one of the scientists’ embryos and, in another remarkable synchronicity, the very same embryo had also exploded into the public consciousness. Not in a scientific journal, but in a glossy magazine.
The cover of Life magazine from 30 April 1965 is a startling artefact, filled by a colour photo of an 18-week human fetus. The essay inside ‘produced’ the concept of human embryos for the public just as His did for scientists during the previous century. Read by millions, it forever changed our idea of what a living, developing, growing human embryo looks like. But it was just that, an idea . In reality, the fetus on the cover of Life magazine was dead.
Drama of life before birth: cover of Life magazine, 30 April 1965. Courtesy Photo12/Getty
The essay was filled with similarly lifelike photos, all but one of which actually show dead or dying embryos and fetuses, the results of either miscarriage or termination. This fact was ignored by anti-abortion activists who made these images ubiquitous; it suited their needs. Depicting these surgically removed embryos as somehow both alive and autonomous made it easy to ignore the mother, whose adult body is so essential for the embryo’s growth and development, and who is so at risk. Volumes have now been written about these images and their role in the US abortion debate.
Just 77 seconds of airtime for the entire essence of development as science knows it
But what strikes the developmental biologist in me is just how accurately the essay conveyed progressive human development. We see the fertilised egg, and we follow the changes of the largely unformed embryo at three, four, and six weeks. Only at eight weeks do we finally see its gradual transition to the more obviously human fetus.
Sadly, this narrative was lost when the images were packaged into a documentary film in 1982. Influenced perhaps by Louise Brown’s birth – and that of the modern fertility industry – The Miracle of Life runs for an hour, yet the first 41 minutes show only egg or sperm. Mostly sperm. By 48 minutes, we’ve seen fertilisation, but the embryo is still just a round clump, perhaps eight cells. It’s only at 48:33 that we catch our first glimpse of the real action of development, the progressive emergence of form. And by 49:50, it’s all over. Suddenly, there are tiny fingers, eyes looking right at us. Just 77 seconds of airtime for the entire essence of development as science knows it. Shown on the BBC, PBS and outlets around the world, the award-winning documentary easily eclipsed the Life essay. The public human embryo had truly arrived – and, besides a few seconds of embryonic development shown on fast-forward, it was a fully developed fetus.
Not long after, the joyful presentations of sonograms, with their beating heart or their shadow of a face, became a core ritual of pregnancy. But these very public fetuses are wildly at odds with the biological reality of embryos, the majority of which abort spontaneously at an early stage; this led an academic theologian to muse that, if life began at fertilisation, then ‘it would appear that heaven is mostly populated by them [embryos] rather than by people who had actually been born.’
O ver a scant two decades, what we now call the human embryo went from a largely intangible entity to something scientists could routinely manipulate and the public thought they understood. As the 1980s dawned, august bodies of scientists, religious leaders, lawyers and philosophers unanimously settled on a progressive view of development.
They concluded that human embryos should be kept alive in vitro only for the most important, highly regulated reproductive or research purposes. Moreover, they must be kept alive only for 14 days. This time point, chosen on the advice of a developmental biologist, was at once appropriate and arbitrary. On the one hand, it marks the onset of a process called gastrulation, by which the embryo leaves behind its early ball-like form and begins to build an elongate body. It’s also the last point at which twinning can occur, and so makes the embryo truly singular and unique. But gastrulation takes some time and embryos are variable. Only a true expert could glean the distinction between embryos at 13, 14 and 15 days. Yet, as any lawyer will tell you, laws (and even guidelines) must be specific to be meaningful, and ‘The 14-Day Rule’ was both.
Their genesis in unused embryos of IVF patients and therapeutic terminations sparked a culture war
Those were exciting times for animal embryology too, given the Nobel Prize-winning work of Christiane Nüsslein-Volhard, Eric Wieschaus and Edward Lewis. They showed that the entire zoo of animals we’d studied for decades, centuries, even millennia all use a shockingly similar genetic toolkit to guide development. When chick embryos were first compared with humans in ancient Greece, it was exactly right.
A single genetic toolkit for development: flies with mutations in what scientists call homeobox genes display duplicated wings (above photo courtesy Nicolas Gompel). Mice with mutations in these genes display duplicated ribs (below, Daniel C McIntyre et al , Development [2007])
Around the same time, the biologist Gail Martin at the University of California, San Francisco made good on Edwards’s abandoned project. Coining the term ‘embryonic stem cells’, she and her colleagues learned how to get these cells from mice, keep them alive in culture dishes, and make them differentiate into cartilage or even neuron-like cells. When the same was done with human embryonic stem cells in 1998, their genesis in unused embryos of IVF patients and therapeutic terminations sparked a culture war. But neither politics nor the resulting welter of regulations dented enthusiasm for their tremendous promise – both real and as imagined by charlatans.
By tinkering with the genetic toolkit that developmental biologists discovered in animal embryos, the new stem cell scientists coaxed their wards down Waddington valleys of their choosing. Their arcane recipes recall ancient alchemy, but the ecosystems they conjured in little plastic dishes were entirely real. First, they made single human cell types, neurons, muscle, blood. Not long after, they devised functional, three-dimensional tissues, first eyes in a dish, then ‘miniguts’ and ‘minibrains’, an array we collectively call ‘organoids’.
It was only a matter of time before the idea arose that we might construct whole embryos out of stem cells. Guided by a desire to understand human development (and in some cases, surely, by at least a little hubris), progress came with unnerving speed.
A t the 2022 meeting on developmental biology in Santa Cruz, I was giddy, mesmerised by the confluence of developmental and stem cell biology. Lehmann’s lecture on flies and my own about frogs joined others about fish and worms. There was even a lecture about jerboas, a strange hopping rodent from Mongolia. One talk really blew my mind: unable to study rhinoceros embryos, for obvious reasons, one group has convinced their stem cells to make rhino embryo models of a sort.
My joy, however, soon bled into dismay when The Washington Post , describing the mouse embryo models developed by Hanna and by Żernicka-Goetz, noted rightly that human models were all but inevitable. Given that years of debate went into the 14-Day Rule in the 1980s, we might have expected that move to be cautious and deliberate. It wasn’t. At a conference in Boston in June 2023, Żernicka-Goetz claimed that ‘we can create human embryo-like models by the reprogramming of [embryonic stem] cells’, a statement The Guardian blasted out to the public the following day without any back-up from the peer review. Once the peer-reviewed paper appeared, it became clear that Żernicka-Goetz’s initial claim had been overstated. Hanna’s group reported more impressive human embryo models soon after, but these couldn’t justify the media commentary either.
The work, while vetted and approved by the appropriate ethics committees, is a far cry from helping us frame the ethical considerations these embryo models will raise. Indeed, while the current embryo models cannot develop into a viable fetus, it sure looks like we will get to that point. And it doesn’t help that the International Society for Stem Cell Research in 2021 relaxed the 14-Day Rule for research with human embryos made the old-fashioned way. Unlike the careful deliberation with stakeholders in the 1980s, the new decision was reached without public engagement. I think the entire field is obligated to bring more people into the conversation and to better articulate why the work is necessary – why, in fact, we must make human embryos from scratch.
This science has always been a proxy, however imperfect, for understanding how our own bodies come to be
It’s troubling, too, that the scientists getting the most attention don’t always use their cachet to communicate the nuance, both ethical and biological. Instead, it’s left to others. Alfonso Martinez Arias, Nicolas Rivron and Kathy Niakan, for example, are among those who have provided thoughtful commentary on the complexities in scientific journals. And, while Żernicka-Goetz in June 2023 told The New York Times that ‘we do it to save lives, not create it’, the medical applications are not at all clear to me. Exactly how will these models save lives? And exactly how do they compare with alternative solutions to the problem? Without such details, how can we weigh what’s to be gained against our ethical and moral obligations?
By contrast, the decades of research with old-fashioned human embryos, all conducted within the confines of the 14-Day Rule, brought us a remarkably safe and effective fertility industry, as well as important advances in genetic diagnosis and prevention of diseases and birth defects. These advances continue, with benefits that are clear.
We’ve pondered embryos for thousands of years, in part because they spark our inherent wonder; theirs is the ultimate emergent property. Across that long arc, it’s usually been animal embryos under our microscopes, organisms that assemble themselves just like we do but whose development we have fewer qualms about interrupting for the sake of knowledge. Like any basic science, animal embryos provide ‘a glimpse of what is possible in this world’, Lehmann writes . But this science has always been a proxy, however imperfect, for understanding how our own bodies come to be. And, quite suddenly now, we seem to have the tools and the appetite to get far more than just a glimpse at the human embryo.
Martinez Arias recently told me that ‘when you put the word “human” there, you are talking to the whole of society.’ It’s worth recalling, then, that this conversation is also thousands of years old. And history tells us that our collective decisions on issues of the human embryo will ultimately be influenced by both science and faith.
Science can tell us how the human embryo develops, and it is an undisputed certainty that embryos develop progressively, building complexity and identity only over time. But there is no scientific consensus on when during that progression ‘life’ begins. Likewise, there is no consensus among faiths on when life begins. Certain Christian faiths now hold that life begins at conception, and these have an outsized influence. Yet, even within Christianity, that view is a recent stance, and one that reversed centuries of thought. Other Western religious traditions don’t share Christianity’s ambiguity. Cleaving to the ancient gradualist view of development, Islamic tradition generally holds the embryo to become human 120 days after fertilisation, though some use the 40-day mark; in most Jewish traditions, it happens only at birth.
We are 3,000 years deep in the adventure called developmental biology, yet the embryo remains in many ways just as mysterious as ever. As we enter a new era of explicitly human developmental biology, we should approach it with all the grace and humility we possibly can."
How evolution made humans more like birds than other mammals | Aeon Essays,,https://aeon.co/essays/how-evolution-made-humans-more-like-birds-than-other-mammals,"Humans would not be here but for pregnancy and childbirth. It is true for each of us and, more importantly, true for all of us, collectively. These uncomfortable, protracted and wonderful challenges not only shepherd us into the world, but also shape our behaviour, social structure and the trajectory of our evolution itself. The surprising part is that, while pregnancy and childbirth are fundamental and defining traits of mammals, they have driven us humans to be very un-mammalian indeed.
Popular notion often has it that natural selection works by seizing on fundamental traits and processes, and optimising them with each new beat of the generations and species. But that’s not always true. Instead of functioning as a refining, perfecting tool, evolution in the real world is all about trade-offs: life has limitations, and big changes in one area often mean sacrifices in others. We humans are the smartest, most complex animals on the planet, but we do not have the best or most optimised biology by any stretch, especially not when it comes to reproduction.
Witnessing our fellow mammals give birth, experiencing the rawness of sight, smell and sound, lays bare the biology before us. On the one hand is the disgust born of our evolutionary predilection to avoid blood and fluids of other animals – a necessary impulse in pre-sanitary times. No matter one’s willingness to embrace a positive view of bodily function, the stomach requires training against the mind when any human, for example a doctor, engages this evolutionary apparatus. The shame and avoidance we feel with all forms of bodily discharge are a sound and healthy part of our subconscious.
There is, however, a deeper discomfort that arises from watching our fellow mammals give birth – one notices a nonchalance compared with our own elaborate, painful and sacramental experience. A cow moos and lows in mild discomfort, as one might when feeling full after a good meal, but it does not compare with the suffering of a birthing human mother. The calf is birthed quickly, practically dropping to the ground after a short push – nothing compared with our day or more of arduous labour. For our survival, and the core of our family happiness, our species must endure pain and risk. We are alone in this, and it troubles us.
We are alone because, though we are a mammal like the cow, and like our nearest cousins the chimps and other apes, we do not act like a mammal, hardly ever. Our blood is warm, our skin has hair, our brain is well-integrated across its hemispheres – and there the similarities end. For a mammal, we live too long, we are too smart for our size, and we are too faithful to our partners. In these particulars, we are decidedly not alone – but, rather, alone in our class.
The other post-reptilian, warm-blooded, big-brained class of animals – only distantly related to us – share far more of what makes us human than do our hairy near-cousins. To understand humans – and our reproduction – we have to start with birds.
M ore specifically, we begin with the kiwi. This unusual New Zealand bird is one of the handful of surviving ratites – the group of large, flightless birds that include the ostrich, emu, cassowary and rhea. There is much to be interested in about kiwis, but the most consequential of their many oddities is their extreme approach to eggs. Kiwis lay the largest egg, relative to their body size, of any bird. They usually produce only one such egg per season, and it typically weighs about a quarter of the weight of the mother. To frame a more easily grasped comparison: kiwis are similar in size to a domestic chicken, but lay an egg six times larger. Corresponding to this enormous egg is an equally enormous incubation period. Whereas a chicken will sit on its egg for 21 days to hatch a chick, and a duck typically 28-35 days, depending on the species, the kiwi sits a record-setting average of about 85 days to hatch its single, massive egg.
The kiwi is engaging in an evolutionary compromise with egg-laying itself. To humans, with our long pregnancies and painful childbirth, eggs might seem enviable. Instead of an exhausting pregnancy, a massive egg would mean no swollen feet, no acid reflux and, importantly, a ‘pregnancy’ that could be shared; anyone can keep an egg warm for a few hours if the mother needs a break.
By laying enormous eggs, and incubating them so long, the kiwi gives its babies the maximum development time
But it is important to remember why mammalian pregnancy was such a successful adaptation in the first place, and the kiwi points at the reasons. When a mammal (other than a human) is pregnant, time is on her side. Take the elephant , with the longest pregnancy of any mammal: about two years. The elephant calf, though huge by human standards, is smaller compared with the mother than our human infants – and even more importantly, is much smaller compared with the mother’s hips. This long pregnancy might sound a bit of a drag, but for the elephant, who has a relatively easy birth to look forward to, and a relatively smaller baby causing her minimal discomfort, it is a huge advantage over an egg. First, she remains entirely mobile, not bound to a nest and to a physically separate offspring that could tempt a predator or that might need to be abandoned if danger strikes. Moreover, she can take her time growing her baby to a much more mature and capable age before it is birthed and must be looked after. All she needs to do to look after her baby while pregnant is to look after herself, eat well, and stay safe. She can keep ‘feeding’ the baby (via the placenta) simply by feeding herself.
The kiwi has a very different problem. Laying an egg means giving your baby all the nutrients it will need to develop from fertilised embryo to hatched baby, all at once. At the moment the egg is laid, it has all the nutrients it will ever have – a very different proposition from the constant feeding the elephant baby gets in the womb. That means that eggs place a fundamental limit on how long a baby can grow before it hatches, which mammals (other than the egg-laying monotremes) do not face. The kiwi is living right at the edge of that limit. By laying such enormous eggs, and incubating them so long, the kiwi is giving its babies the maximum amount of development time it can before the babies hatch out and have to face the world. Achieving that maximum is not without its costs. Laying eggs is fairly draining for any bird, and the kiwi more than most. She must eat as much as three times her normal intake for the full month in which the egg develops, and is left significantly weakened by the experience. Moreover, the size of the kiwi egg is about as large as a bird of her size can safely grow and lay – it takes up a great deal of the space in her body by the time it is ready to be laid. And, for all this compromise, the kiwi ekes out a three-month gestation. The elephant, with little compromise at all, sails on happily for another 19 months.
E ither way, both egg-tending and pregnancy are easier for the mother kiwi and the elephant than for us. The evolutionary incentive is to gestate as long as possible, so that childrearing will be less onerous. The elephant pulls this off entirely, with a baby that is born, wobbles to its legs, and can, within the day, walk behind its mother, nurse when it needs to, and feed itself with supplementary food. The kiwi manages to achieve much the same result; the babies can walk, feed themselves, and follow their mother within a few hours of birth.
But while elephants represent the typical mammal experience, kiwis are a very unusual bird. The overwhelming majority of other birds are born extremely underdeveloped: little, blind, pink, nearly featherless infants that can barely move on their own for the first several weeks of life, and are confined to the nest for months, assiduously cared for by their harried parents. This is true even of birds with similarly large eggs and similarly long incubation periods as the kiwi, like albatrosses (where the young hatch with a good coat of soft feathers) and are a bit more capable than, say, a songbird like a wren or crow.
For most birds, eggs force a difficult compromise on the mother: the size of a bird’s body limits the size of its eggs; the size of the eggs limits how much nutrition they can contain; the nutrition in the egg limits the length of incubation; and the length of incubation limits the maturity of the offspring at hatching. This means that most birds are trapped with a long, arduous period of childrearing after hatching, because their young do not have time in the egg to become mature enough to look after themselves.
The birds that have more mature, or ‘nidifugous’, young are few and far between. They include ratites, ducks and other waterfowl, chickens and landfowl like pheasants, along with a few others scattered throughout the evolutionary tree. What all these more mature avian offspring have in common is that they are not the birds generally thought to be most intelligent, and none of them are songbirds: the great big crown group of modern birds that represent more than half of avian species today. None belong to the largest-brained, most intelligent avian groups like parrots , corvids and pigeons , which are universally ‘altricial’ – giving rise to immature, demanding young. This makes sense – an animal that is going to have a highly developed, complex brain needs a longer time to construct that brain, and so needs a longer development time. With the pre-hatching development limited by egg size, a lot of that brain development has to happen after hatching, which means raising young with a still developing brain. Small wonder it is hard work.
Our big brain has to pass through a set of narrow hips. That means we have to be born young – very young
If this is all sounding a bit familiar, it’s because it feels human. A long childhood; demanding newborns; and parents needing to do nearly everything for the offspring for a protracted period of infant helplessness are what we as humans expect in our reproduction. Yet, given the advantages mammals gained in the evolution of pregnancy, why do we humans, too, not bear children much more like the elephant – a long, cruisy pregnancy and an easy, cooperative, capable child at the end of it? Evolution is always a game of compromises, and in pursuing our own key evolutionary advantage – our brain power – we have inadvertently given up the advantages of pregnancy and ended up back with the constraints of eggs.
Our brains are our most important adaptation. Human intelligence has no comparison in any other animal, and is the single trait that has allowed us to completely dominate this planet, shaping its ecology to our will. This intelligence works in tandem with another game-changing adaptation, our hands. We (and, indeed, other primates ), have fantastically precise and manipulable hands. Our intelligence allows us to think up new tools and new processes to build, make and destroy, but our hands give us the dexterity to pull it off. Our reliance on our hands has driven two evolutionary changes in humans. First, it forms a virtuous cycle with our brains – agile hands need a lot of brain power to control them (as any robotics expert will lamentingly tell you). Over time, the ability to be more precise and dextrous with our hands has driven the enlargement of our brains, in order to provide that computing power. Then, with bigger brains, and bigger ideas, we have used our hands in even more dextrous and complex ways – which in turn drives more brain power. The two enrich each other.
The other hand-related adaptation is much less of a win-win. Since we rely so greatly on our hands for our evolutionary advantage, we have, over time, stopped using them to help with walking, balance and stability – that is, we evolved to walk upright. This keeps our hands free and ready for action, unlike even our close relatives, the chimps, and other great apes, who maintain our ancestral, hunched-over gait, and use their hands and arms to help walk, climb and balance. Our straight-as-a-ramrod stance has had two less positive outcomes. First: backpain . Second, and more importantly, it changed the angle and size of our hips. Having our legs directly below our hips, rather than to the back and sides, and torso directly above, has required our pelvis to become narrower, and the opening in the middle to shrink as well. This narrowing is how we end up with pregnancies more like eggs.
We have big brains and narrow hips. In order to be born, our big brain needs to pass through a set of narrow hips. And that means we have to be born young – very young. Indeed, we have to be born underdeveloped. To chance a pregnancy any longer than our roughly 40 weeks is to risk death to the mother, baby or both, as a too-big head transits too-small hips. Even with our current compromise, we suffer from rates of death in childbirth, for mother and infant, not found in other mammals. From our brains to our hands to our hips, we cannot rear children like elephants. Our pregnancies are fundamentally limited in length by our physiology, just like birds’ incubations. In birds, the ultimate limit is egg size through hips, and in humans it is head size through hips. And so, our babies are helpless, unlike other mammals and very much like birds.
A fter birth, or hatching, comes rearing, and here too we are more bird than mammal, at least behaviourally. As a rule, mammal mothers raise their offspring without the involvement of the father. Or at least, without the deep involvement of the father. There are species, like lions, that breed in a harem structure. A pride of lions consists of up to a dozen related females, their dependent offspring, and a coalition of two or three resident males, all of whom breed together, and cooperatively hunt, rear offspring and protect the group. Young males reaching maturity leave the pride to start their own.
Elephants and many whales live in similar structures of cooperative females , with roving males that live alone, mate with the females, and move on. For other mammals, life is even more solitary, with males and females meeting to mate, then the males leaving the female to rear the offspring by herself, as in the case of most bears .
This reflects the ease of mammalian child-rearing. A mammal mother gives birth to a fairly capable baby that can follow her, walk on its own and supplement its own diet. Even the more helpless mammal young, like carnivore cubs or primate babies, which are far less precocious than, say, a baby herd animal like a cow or wildebeest, are far more capable than human newborns. Raising any other mammal is a manageable task for a single parent – and all the more manageable in cooperative breeding structures like lions have.
This also means that male mammals are evolutionarily incentivised to seek as many mating partners as possible. A male elephant is not needed in the rearing of his offspring, so his genes are most effectively reproduced by having as many partners and as many offspring as possible, and investing very little time or effort into each. He is incentivised to polygyny – mating with multiple females – and this is exactly what a successful bull elephant does.
Not so for humans. Humans are, with exceptions, culturally and socially monogamous. Frequently sexually monogamous as well – though cheating, or ‘extra-pair copulation’, does certainly happen and has its own evolutionary complexities. The existence of cheating has led some to argue that human sexuality is naturally disposed to multiple partners – to polygyny and polyandry – or even to the polygynandry (multiple mating of both sexes) of our close relatives, the chimps and bonobos.
Parents in the smartest avian species stick together to care for their young
They are wrong. Cheating is just that, a manipulation to individual advantage of the default system, and it can survive only in a population of monogamous animals when it is rare. If it becomes common, the equilibrium is broken, and the system falls apart entirely.
Birds are overwhelmingly monogamous. Like humans, many of them do engage in sporadic extra-pair copulation, though strict monogamy is also common. Most species of birds practise at least serial monogamy, staying with one partner for several seasons at a time, before re-pairing. Mating for life is common across many bird groups, but especially so in parrots, songbirds and other intelligent groups. Where non-monogamy occurs, it tends to be in the same groups of nidifugous birds that have easier childrearing – ducks, chickens, ratites and so on.
Monogamy is so common in birds because, unlike in mammals, the father can significantly increase his reproductive fitness by taking an active part in the incubation of eggs and the rearing of young. The female bird has to make the bigger investment in gametes: the egg itself is a draining thing for her to produce from her body. But once it is laid, there is little difference in its being sat on by a mother or a father. In some birds, like the common pigeon (or ‘rock dove’), this incubation is shared straightforwardly: the mother and father split the day, with one sitting on the eggs in the morning while the other forages, then swapping in the afternoon. They keep up this even split throughout the egg incubation and the several months afterwards of looking after the helpless hatchling. Others, like swans or some parrots, make a different bargain. Here, the female will do all the egg-sitting, while the male stands by, on guard to ward off predators, watch the nest when she needs to make short foraging trips and, in some species, bring her food and feed her. Once the young hatch, both parents gather food and feed the babies. Either way, parents in the smartest avian species stick together to care for their young. Helpless babies require biparental care, and biparental care requires monogamous mating to work.
Humans are no different. Of course, in the modern world, a combination of technology, government assistance and social structures makes single-parenting possible. In pre-modern societies, and especially in our evolutionary history, trying to raise a child as a single mother was a sentence of extreme poverty or much worse. (Human babies are so much work that one theory holds that menopause exists to provide grandmothers for assistance in raising the young. In other mammals, females remain fertile up until shortly before their death – a grandmother elephant cannot babysit her daughter’s children: she is likely to have her own.)
As with birds, human monogamy is the evolutionary response to our need for biparental care of children. A baby human needs a father around in order to have enough parental support to get it through our extraordinarily long childhood. Monogamy keeps that father around.
This is why exceptions to monogamy tend to cluster around the wealthy and elite – economic or social power offers other solutions to the problem of childrearing, among them a harem or polygamous marriage, nannies or, indeed, slaves. With these artificial sources of childrearing help, men of wealth and status could break from nature in a way that others could not. The complexity of human society has allowed us to depart from the behaviours we evolved to practise, but our underlying biology, and the forces that shaped it, remain. We are a monogamous species like our doppelgängers, the birds.
All of this comes back to the fundamental idea of trade-offs in evolution. No adaptation is free, and every area of exceptional ability is equal parts benefit and limitation. Dinosaurs had evolved the great advantages of massive size and heavy, robust build: huge advantages, no doubt. But they traded them both away when they became birds, exchanging them for the contradictory but still more revolutionary benefit of flight . Humans have done much the same with our original, game-changing mammalian pregnancy. Easy, well-developed, self-sufficient offspring were a great boon to our ancestors, but lost out in the trade-off for big, world-changing brains. We have come full circle, shedding the evolutionary advantages of pregnancy and easy childrearing for a very egglike set of constraints on our gestation. With our pregnancies turned egglike, it is no surprise that our behaviours and our families have turned birdlike as well."
Blame it on Aristotle: how science got into bed with sexism | Aeon Essays,,https://aeon.co/essays/blame-it-on-aristotle-how-science-got-into-bed-with-sexism,"Around 2,400 years ago, Aristotle produced groundbreaking science on ‘generation’ or reproduction. Some of his claims about animals and humans are startling: elephants prefer lonely places to copulate; fish lack testicles; males are conceived when the north wind blows. His findings are set out in the Generation of Animals , a book that crashed meteor-like into the classical world. I love this study for its joy in nature – and for the picture it conjures of the philosopher hiding in a bush, watching hedgehogs copulate ‘belly to belly’. Yet it has a dark side. Its science is set within a complex philosophy of generation, one that assumes men are better than women. While detailing how humans develop in the womb, Aristotle explains why women are inferior, rationalising sexism. Unsurprisingly, this tangling of science and sexism led to problems.
Aristotle’s work drew on two sources. One was even older theories of reproduction. How, sages asked, do humans make more humans? First, people do not reproduce by themselves. Women do not spontaneously become pregnant – instead, conception seemed to require sexual intercourse with male ejaculation. (Ancient Greeks assumed that only men could ejaculate sperm, and only women could become pregnant.) Second, children can resemble their fathers or mothers. To explain these observations, theorists deemed that men and women must each contribute something to the creation of a baby. This explains why people cannot reproduce alone, and why children can resemble either parent.
These ancient theories connected life with moisture and heat, an association that goes back at least to the poets Homer and Hesiod. They describe mortals as warm and wet, the dead as cold and dry. One reason for connecting life with heat is that the Greeks worshipped Apollo, the Sun god. Another is observation: living animals are warm, dead bodies are cold. And living animals bleed, dead bodies don’t. (One philosopher, Anaximander of Miletus, associated life with wetness so strongly he claimed humans must have emerged from fish.) Ancient philosophers and poets also used ideas about opposites . North, south. Mountains, valleys. Angels, demons. Aristotle describes a Table of Opposites created by Pythagorean philosophers, which included the following pairs:
As you would expect, right is opposite left. Light is opposite darkness. However, the items in each column are also connected with one another, creating two constellations of ideas. Right-male-straight-light: good. Left-female-crooked-darkness: bad.
In the 5th century BCE, Parmenides claims that men and women both produce ‘seed’ – the reproductive, magic stuff of generation. In this sense, today we know that men and women both possess ‘seed’: sperm and eggs. For Parmenides, conception occurs when people ‘mix the seeds of Love’; male embryos are conceived on the right side of the womb, females on the left. Anaxagoras agrees that an embryo’s womb placement determines its sex. However, he rejected the two-seed theory in favour of a one-seed theory: the male provides seed, ‘while the female only provides the place’. Perhaps he pictures male semen as a plant seed, embedded in the ‘earth’ of a woman. A problem for ‘one-seed’ theories is that it’s then difficult to explain how a child could resemble its mother, when their seed derives exclusively from the father.
Legend has it that another ancient philosopher, Empedocles, could control the winds and raise the dead. When not practising these hijinks, he also offered a theory of reproduction. Like Parmenides, Empedocles believed both men and women produce seed. However, he went further in holding that each kind of seed contributes a different part to the offspring. This is impressive – Empedocles seems to have correctly surmised that male and female seed make distinct contributions to a baby. Lest we take him too seriously, though, Empedocles also argued that temperature determines an embryo’s sex: warmer wombs produce males, colder wombs produce females. This explains why men are darker and hairier – or, as one translation puts it, ‘shaggy’.
A ristotle’s other source was scientific observation. You might imagine the man spent all his time in Athens, debating in Plato’s academy, scribbling scrolls, munching figs. Yet after Plato died, Aristotle left Athens to spend 12 years wandering Greece. Records of his movements are hazy but we know he visited the court of Hermias, the ‘tyrant of Atarneus’. He married a woman called Pythia. Afterwards, he moved to Macedonia and tutored Alexander the Great, who would later conquer large chunks of the known world. We’re also confident he spent an extended time fishing on the isle of Lesbos. This fishing matters. Because, unlike his predecessors, Aristotle sought to understand reproduction by studying the natural world. This systematic approach was utterly new. ‘Aristotle,’ writes the scholar James Lennox in the Stanford Encyclopedia of Philosophy, ‘is properly recognised as the originator of the scientific study of life.’
This originator collected masses of data on the natural world, naming more than 500 species, and describing the internal anatomy of 110. He dissected at least 35 kinds of animals. Aristotle taxonomied creatures, studied their parts, wondered at their diets and dwellings and motions. He asks whether sea anemones are plants or animals – as he observes that sponges are rooted like vegetables, yet feed on small fish through a mouth in the middle of their bodies. He tells us that Arabian camels have one hump, whilst Bactrian camels have two, and that all possess ‘four teats like the cow, a tail like that of an ass, and the privy parts of the male are directed backwards’.
Aristotle collected information about pertinent body parts, and the act itself
To gather this information, Aristotle seems to have spent innumerable hours observing animals and talking to people who worked with them. He interrogated beekeepers, fishermen, sponge divers – and perhaps even human midwives. Legend has it that, while conquering large parts of Europe, Alexander aided Aristotle’s scientific quest. As Bertrand Russell tells it , Aristotle wrote Alexander regular letters saying, essentially: ‘Oh fie!’, it’s ‘vulgar to take so much interest in barbarians’. But Alexander replied only by sending him specimens of the flora of the Indus Valley. The writings of Pliny the Elder, a Roman encyclopaedist, may be at the root of this tale:
I really hope this legend is true – although it was likely propaganda to soften Alexander’s murderous image.
Whether aided by Alexander or not, Aristotle was fascinated by the reproduction in animals and humans. He knew that animal conception usually started with sexual intercourse, so he collected information about pertinent body parts, and the act itself. Here are a few of his observations:
Armed with earlier theories and his scientific observations, Aristotle began crafting his own theory of generation.
K ey to this is Aristotle’s account of ‘causes’. He believed that understanding the causes of a thing allows us to understand the thing itself. The material cause of a thing explains what it is made of. For example, bronze is the material cause of a statue. Wood is the material cause of a table. In themselves, bronze and wood are unshaped, unformed. The formal cause explains the ‘form’ or structure of a thing. A silver plate has the form of ‘plate-ness’ but this lump of metal could have had the form of ‘spoon-ness’ or ‘fork-ness’. Similarly, wood could form a table or a chair. The efficient or moving cause explains the agent that brings about something. A sculptor could be the efficient cause of a statue. A kettle could be the efficient cause of hot water.
The material cause of a human being is its anatomical parts: blood, bones, organs. The formal cause of a human is its ‘human-ness’. But, the Generation of Animals asks, what is its efficient cause? What brings about a human? Aristotle knew that animals with two sexes create new animals from the ‘union of male and female’. So, like his predecessors, he acknowledges that males and females each contribute something to conception. But what?
Aristotle assumes that male ejaculate, semen, contains seed. Closely studying semen, he argues it must be a ‘residue’ of a ‘growth nutriment’. On Aristotelian biology, nutriments contribute to a body’s natural, healthy growth. If a body does not use up nutriments, the leftover becomes a residue. Aristotle argues his semen = residue theory explains various ‘facts’. For example, healthy males produce more semen, because sick ones use up more nutriment. Children do not produce semen because they are still growing, using up their nutriment without leftovers. Because nutriment helps bodies gain weight, people and goats who are ‘getting rather too fat’ do not have nutriment leftover – explaining why they produce ‘less semen and are less desirous of sexual intercourse’.
For Aristotle, blood is the most important nutriment, travelling around the body bringing life, growth. He reasons that semen must be a residue of blood. But blood is runny and red whereas (he notes) semen is ‘thick and white’. Why doesn’t semen look more like blood? Aristotle’s answer is that males ‘concoct’ blood residue, thickening and purifying it, turning it into semen. Female bodies do not concoct semen, which is why they regularly lose leftover blood – a nifty explanation for menstruation. Why do only males concoct semen? Because concoction requires heat. And, as on the Table of Opposites, Aristotle connects males-hot, females-cold.
Aristotelian pregnancy is all nautical: wave froth, whirling currents, magically forming fishnets
This brings us to the heart of Aristotle’s theory. He claims that male semen contributes ‘the form and the efficient cause’, while females contribute ‘the material’. Male semen forms the material provided by the female. How does he arrive at this?
One reason to think females provide the material is that, after conception, the new creature grows inside its mother. As Aristotle notes, further material ‘must constantly be added’ to an embryo ‘that it may increase in size’. New creatures grow regardless of what their fathers are doing, so this material must come from their mothers. And if you think females provide the material cause of a new creature, then you had better deny they also provide the efficient cause. If women possessed the material and efficient causes of generation, they could become pregnant without sex – something you didn’t see happening in classical Greece. You’d be offering a one-seed theory on which females possess all the creative fizz, with no need for males. By attributing the efficient cause to males, Aristotle has explained their apparently necessary role in conception. As he writes, ‘the female does not produce offspring by herself, for she needs … something to begin the movement in the embryo’.
Aristotle would also be in a tricky position if he believed males contributed the efficient cause and matter to generation. If that were the case, why can’t males reproduce by themselves? Aristotle actually claims that males contribute no matter at conception. His scientific observations showed that not all male animals emit semen, yet they still reproduce. For example, during mating, some male insects do not insert any parts of themselves into the female, ‘but on the contrary the female inserts a part of herself into the male’. The male hasn’t provided any matter, just ‘heat and power’, and this is sufficient for conception. Aristotle writes that, just as the carpenter is distinct from the timber he shapes, so semen is distinct from the menstrual blood it works on. We can see how he arrived at the notion that males contribute the efficient cause, and women the material.
In addition to heat and power, semen communicates movement to female menstrual blood. A carpenter imparts shape and form to the timber ‘by means of the motion he sets up’. Similarly, semen ‘sets up the movement in the embryo’. I take these movements to be literal. Aristotle describes semen as a compound of breath and water, thick and white because it contains ‘bubbles’. It’s a kind of foam, akin to frothy, sudsy ocean waves. (He notes that ‘Aphrodite’, goddess of love, literally means ‘foam-born’.) We can imagine semen bubbling with air and froth. Inside the mother’s body, this sea-foam starts a movement in the female material. These movements continue – the classicist Sophia Connell argues that, for Aristotle, later motions may even derive from the mother. Slowly, through swirls, eddies and ripples, an egg-like structure containing a tiny creature forms in the womb.
Gradually, the embryo grows, gaining nourishment from its mother ‘as a plant does of the earth’. Its own source of heat, its heart, develops first, followed by other organs and bones. Charmingly, Aristotle compares the embryo’s slow growth to the ‘knitting of a net’. Aristotelian pregnancy is all nautical: wave froth, whirling currents, magically forming fishnets.
T here is plenty of sexism in Aristotle. His biological observations showed that some animals, such as bees, reproduce yet do not have separate sexes. (Science has since identified thousands more hermaphroditic species, including fish, molluscs, insects, snails, starfish.) Why, Aristotle wonders, do only some species have males and females? His answer is contentedly prejudiced:
Aristotle is saying that the efficient cause is ‘better and more divine’ than the material it shapes. The sculptor is better than the clay. And the superior should be separated from the inferior wherever possible. As males provide the superior efficient cause, and females provide the inferior material cause, many animal species are rightly separated into males and females.
Aristotle didn’t invent sexism – he inherited buckets of it
If you found Aristotle’s account of conception disrespectful to women, just wait. His account of foetal development is even worse. For Aristotle, an embryo becomes male when its body develops perfectly in the womb. However, if a womb is too cold and the embryo poorly nourished, it is not brought to its ‘proper form’. Then, it becomes female. He finds ‘proof’ of this theory via several ‘observations’. One is that young and elderly animals are more likely to produce female offspring: Aristotle explains that young animals have not yet perfected their heat, while the heat of older animals is failing. Further, more females are born when the south wind blows: its moister air leads to more liquid semen, which is harder to properly concoct.
Both male and female animals have their place in the Aristotelian world – both sexes are necessary to create new animals. Yet a female animal is one that has failed to become male. Aristotle tells us we must look upon the female as ‘a sort of natural deficiency’: ‘The female is, as it were, a mutilated male.’
Aristotle didn’t invent sexism – he inherited buckets of it. We’ve already seen the Table of Opposites declaring female: bad. But he may be more prejudiced than some of his predecessors. Plato, for example, occasionally argued for greater equality between men and women. Aristotle’s sexism also manifests beyond his theory of human reproduction. For example, his political writings state that women are weaker than men, more cautious, less courageous. This is true of all female animals, not just humans. He notes that when a trident strikes a female cuttlefish, the male ‘stands by to help’. But, damningly, when the male is struck, ‘the female runs away’. Proof that the females of all species are cowards. As the philosopher Cynthia Freeland puts it , when it comes to women, the horizons of Aristotle’s thought ‘loom dark’.
A ristotle’s theory of reproduction was widely taken up by subsequent theorists, its ideas working their way into the likes of Augustine, Thomas Aquinas, Michel de Montaigne and Jean-Jacques Rousseau. His biology held sway until at least the 17th century. One turning point came in 1651, when the Aristotelian biologist William Harvey reluctantly concluded that Aristotle’s account of reproduction had problems. Another came in 1827, when Karl Ernst von Baer discovered the human ovum. In the wake of such discoveries, Aristotelian theories of reproduction waned but didn’t disappear.
In 1684, a book partly based on Aristotle’s Generation of Animals had gone viral, and went on to be reissued hundreds of times, for centuries. In 1930s England, Aristotle’s Masterpiece was still selling around 10,000 copies annually and many of its ideas, such as the causes for a child becoming male or female, are rooted in the Generation of Animals . Norman Ford’s book When Did I Begin? (1988), a study of human life, opens with the chapter ‘Historical Influence of Aristotle on the Theory of Human Reproduction’.
The problem is that, alongside his theory of reproduction, many thinkers also imbibed his sexism. Aristotle could have better avoided misogyny. To illustrate, he could have held that males provide the efficient cause of conception and females the material, yet both are equally valuable. Various ingredients go into a bread loaf – flour, salt, water – yet we don’t believe that flour is ‘better’ than salt. Instead, Aristotle’s theory of reproduction provided a rationale for sexism. Feminists began calling this out from the 1970s. In one early critique, Maryanne Cline Horowitz argues that Aristotle invented many of the ‘standard’ Western arguments for female inferiority. She claims that Aristotle’s account of reproduction partly explains why the female ovum went ‘unsuspected’ until the 17th century. Horowitz adds that Aristotle probably underlies Sigmund Freud’s view that women have a ‘castration complex’ about not being men. Meanwhile, Nancy Tuana argues that Aristotle’s biology provided ‘rational justification’ for the greater perfection of males.
It was acceptable not to insure things that might happen to the non-standard body, such as pregnancy
Conceiving of men as ‘better’ than women is straightforward sexism. Yet some of Aristotle’s most insidious sexism is more subtle. For him, the male body is the ‘proper form’ of a human being, whereas the female body is akin to ‘a mutilated male’. In other words, the male body is the standard human body. This attitude is still with us today, and it leads to problems.
To illustrate, in 1976 the US Supreme Court ruled it was not discriminatory for private medical insurance to exclude pregnancy and childbirth. Their reasoning was that these insurance policies covered everything that might happen to the ‘standard’ human body – including prostate cancer and circumcision. It was acceptable not to cover things that might happen to the non-standard body, such as pregnancy. Happily, this Supreme Court decision was overturned with the Pregnancy Discrimination Act of 1978, but many examples of this subtle discrimination remain.
Caroline Criado Perez’s book Invisible Women (2019) investigates many of them. She shows, for example, that because the symptoms of men’s heart attacks are taken as ‘textbook’, heart attacks in women are less likely to be diagnosed. Crash-test dummies are regularly built to male proportions, despite the fact women are, on average, shorter and lighter. This contributes to some alarming statistics: when a woman is involved in a car crash, she is 47 per cent more likely than a man to be seriously injured, and 17 per cent more likely to die. Medical trials, on humans and animals, often exclude females – resulting in treatments that are less effective, and have more side-effects, for women.
Beyond the lethal, many irritants remain. Voice-recognition software more accurately recognises male voices. Smartphones are too large for the average female hand, and VR headsets are too large for the average female head. As Simone de Beauvoir proclaimed, women are the second sex. Aristotle is, partly, to blame."
How raising children can change a father’s brain | Aeon Essays,,https://aeon.co/essays/how-raising-children-can-change-a-fathers-brain,"On a hot summer morning in Atlanta a few years ago, I took my then five-year-old son to his swimming lesson. As we walked toward his pool, we passed a smaller, shallower pool where an infant swim class was underway. I was still trying to wake up, but the class was already in full gear and my attention was drawn to a chorus of motherese (the high-pitched, rhythmic, infant-directed speech known more colloquially as baby talk) arising from the class. Parents were standing in a circle inside the pool, holding their infants in front of them, with the instructor in the centre. I couldn’t help but notice that many of the parents were fathers. Some were a bit chubby, with pale torsos reflecting the bright sunlight. They seemed like ideal infant-caregivers: calm, gentle, patient and sensitive. They didn’t seem like men you would go to battle with. In fact, they were the very antithesis of the warriors and athletes – think Maximus, Achilles or Michael Jordan – often associated with a masculine ideal.
Were the men in the pool just inherently different, born infant-caregivers? Or did the process of becoming a father somehow transform them so that they became better suited to perform this role? We have known for decades that mothers’ bodies and brains are transformed by the dramatic hormonal changes of pregnancy and childbirth. Now, new research is showing that men are also biologically transformed by the experience of becoming an involved father.
When women become mothers, levels of the hormones oestrogen, progesterone and prolactin increase throughout pregnancy. Hormones have their biological effects by binding to receptors – molecules that sense the hormonal signal – throughout the body, and they can influence behaviour through binding to receptors in the brain. Oestrogen increases the brain’s capacity to detect another major hormone, oxytocin, and the massive release of oxytocin at birth, coupled with repeated pulses of oxytocin during breastfeeding, helps mothers bond with and want to care for their infants.
But what about fathers? How do they get prepared to parent? This question was on my mind in 2008. I was teaching an undergraduate course on human social neuroscience. I included a unit on love, attachment and parenting, and was excited to use this class as an opportunity to update my knowledge in that area. In preparing for class, I found that there were many studies on the biology of human mothers, but I was struck by the dearth of studies at that time on the biology of human fathers. I knew that there was a decades-long trend of fathers becoming more involved as caregivers, but also that there was considerable variation in the extent of their involvement. I also knew that positive paternal engagement was associated with better social, psychological and educational outcomes in children, and I was aware of the tragic statistics of children raised without fathers. I decided then to shift the focus of my research, and my lab began investigating the biology of human fatherhood.
O ur first study involved recruiting a large group of fathers with children one to three years of age. Our task was to compare their hormone levels and brain function with those of a group of men who were not fathers. We found that compared with non-fathers, fathers had 20 per cent less testosterone, the primary male sex hormone, often abbreviated as ‘ T ’.
The finding was fascinating in light of seminal research in birds, which had established that testosterone facilitated aggressive competition for mates and territory, while interfering with parenting. For example, when researchers gave monogamous, parental male birds extra testosterone, they became polygynous and non-parental.
This is an interesting change, but how strong is the evidence that a decrease in testosterone helps to shuttle energy from mating to parenting in species other than birds, including humans and other primates? Pretty significant, it turns out.
Marmoset monkeys are one of a minority of primate species in which adult males care for their offspring. Adult marmosets carry their twin infants on their back, a significant energetic burden. They also groom their infants and might share food with them. When the primate researcher Toni Ziegler and colleagues from the University of Wisconsin-Madison presented adult male marmosets with the scent of an ovulating female, testosterone levels skyrocketed, as if preparing the males for mating. However, this response was blunted in fathers compared with non-fathers, as if to discourage them from redirecting their attention from parenting. On the other hand, when Ziegler and colleagues presented adult males with the scent of an infant, testosterone levels decreased in fathers, as if preparing them to focus their energy on caregiving. By contrast, the infant scent had no effect on testosterone in non-fathers.
Such findings led to the idea that testosterone might redirect energy from parenting to mating in humans too. Important proof came in 2011, when the biological anthropologist Lee Gettler, then at Northwestern University, and colleagues measured testosterone levels twice in the same young Filipino men over a span of 4.5 years. Those who became fathers during that interval experienced a significantly larger decline in testosterone than men who didn’t become fathers, conclusively establishing that the transition to fatherhood decreases testosterone.
Skin-to-skin contact with premature infants increases both parental and infant oxytocin levels
The team also showed that Filipino fathers who were more involved in caregiving had a larger decrease in testosterone across the transition to fatherhood. And higher testosterone was associated with increased mating effort, as expected from the animal studies. For example, young men with higher testosterone levels at the initial measurement were more likely to find a mate and become fathers, a form of mating success, over the subsequent five years. What’s more, those fathers whose testosterone decreased the least across the transition to fatherhood reported the highest frequency of sexual intercourse and vice-versa.
Our own research into human fathers adds to the story, as well. We asked mothers to tell us how involved fathers were in caring for their toddler children, and found that men with higher testosterone tended to be less involved, whereas men with lower testosterone were more involved.
We made another important find. Despite having lower testosterone compared with non-fathers, fathers in our study actually had higher levels of another hormone that is classically identified with motherhood: oxytocin. In contrast with testosterone, oxytocin appears to promote paternal caregiving. In 2010, a Dutch research group led by the behavioural scientist Marian Bakermans-Kranenburg gave fathers oxytocin through a nasal spray and then watched them play with their child. When fathers were given oxytocin, they stimulated their child to explore more and showed less hostility toward the child compared with fathers who received a placebo treatment. Then in 2012, an Israeli group found that intranasal oxytocin made fathers touch their infants more and make more positive vocalisations towards them. In turn, infants, though not treated with oxytocin themselves, spent more time gazing at the father in this condition. Finally, and most remarkably, infants whose fathers were treated with oxytocin experienced an increase in oxytocin themselves. Thus, oxytocin seemed to initiate a positive feedback cycle that promoted father-infant bonding.
In 2016, another important study further implicated oxytocin in key aspects of paternal caregiving. Here, researchers measured the facial muscles of men watching infants express emotion. When the men were given oxytocin, their patterns better mimicked those of the infants, strongly suggesting that the hormone enhances paternal empathy.
In women, oxytocin is released by uterine contractions during labour and by nipple stimulation when breastfeeding. This begs the question, what triggers hormonal changes in men?
Researchers have found some answers here, too. There is evidence for a decline in fathers’ testosterone even during the partner’s pregnancy, so cues from the mother could be important. There is also evidence that postnatal contact with the infant can both lower T and increase oxytocin. Perhaps something about the appearance, the odour or actual tactile contact with the infant is responsible. A notable 2015 study showed that skin-to-skin contact with premature infants increases both parental and infant oxytocin levels. These findings predict that human fathers should become more strongly bonded to their children if they spend more time in close proximity to them as infants, and this has indeed been demonstrated.
T o influence behaviour, hormones such as oxytocin must act in the brain. What is the brain’s actual neural circuitry that promotes paternal caregiving? Evidence points to a global parental caregiving system that generalises not only across mothers and fathers, but also across mammals. In the 1970s, the psychologist Michael Numan, then at the University of Chicago, discovered that damaging a particular small region at the base of the brain, known as the medial preoptic area or MPOA, severely disrupted maternal behaviour in laboratory rats. The MPOA is part of the limbic system, considered the ‘emotional’ brain, and resides within a structure known as the hypothalamus, a centre of sex and aggression, among other things.
Recently, a series of elegant experiments by the biologist Catherine Dulac and colleagues at Harvard University have investigated the role of the MPOA in the parental behaviour of male mice in great detail. Virgin male mice will attack and even kill pups, whereas mouse fathers are nurturing parents who lick and groom pups, build nests and retrieve pups to the nest.
Dulac and colleagues showed that both maternal and paternal behaviour activates a specific sub-population of neurons within the MPOA. Using sophisticated molecular techniques, they then selectively destroyed only this sub-population of MPOA neurons and found that it abolished both maternal and paternal behaviour and also elicited infanticide. On the other hand, when they activated these neurons in infanticidal males, it suppressed infanticide and elicited parental behaviour. Thus, the MPOA is a critical node in both the maternal and paternal brain.
Researchers have also shown that the MPOA activates a system of dopamine neurons projecting from the midbrain to a region known as the striatum, which is involved in reward and motivation. This motivational network is essential for parenting; damaging it inhibits caregiving, and rat mothers who lick and groom their pups more have more dopamine in the system.
In many species, this parental caregiving system is not active all the time. For example, female rats find pups unpleasant before giving birth, and require pregnancy hormones to activate the motivational system and stimulate maternal care. A prime player is oxytocin, released in the mother at birth and when nursing; it acts in both the MPOA and the midbrain to stimulate dopamine in the reward system, and this is presumably what makes pups rewarding and provides the motivation to deliver care. Rat mothers with more oxytocin receptors in the MPOA lick and groom their pups more; critically, their level of oxytocin receptors is influenced by the care that they received as a pup. Pups with more affectionate mothers have more oxytocin receptors. This might in fact be the mechanism for the transmission of parental caregiving styles from one generation to the next.
When fathers come into contact with their kids, it activates the midbrain, a region filled with dopamine neurons
Just as pregnancy hormones act on maternal brain circuits to stimulate caregiving, fatherhood can also alter the male brain so that parental caregiving circuits become more responsive to pups. The California mouse is another of the minority of mammalian species in which males are consistently involved in caring for the offspring. In 2003, the psychologist Brian Trainor then at the University of Wisconsin showed that an enzyme capable of converting testosterone into oestrogen increases in the MPOA when males become fathers. Critically, when they blocked the activity of that enzyme, they disrupted paternal behaviour, suggesting that MPOA oestrogen was essential for paternal behaviour. Other work has established that California mouse fathers also experience significant changes in the hippocampus, a brain region involved in learning and memory, and related structures. These include increased production of new neurons, changes in the shape of neurons that increase their ability to receive input from other neurons, increases in oestrogen receptors, and an increased number of oxytocin-containing neurons. Mandarin vole males also care for their offspring. When they become fathers, oxytocin receptors increase within the brain reward system, presumably rendering the system more sensitive to oxytocin so that pups become more rewarding.
My lab has been asking whether these animal models apply to human fathers as well. To find out, we adopted an ethological approach. Animal ethologists posit that offspring activate specific brain patterns and associated behaviours in parents. We sought to identify these patterns in human fathers by imaging their brains with functional magnetic resonance imaging (fMRI) while they responded to their children.
The early ethologists showed that young animals of all vertebrate species have a particular set of physical characteristics, called the baby schema, that tend to ‘release’ adult caregiving. These include large heads, protruding foreheads, large eyes, high brows, small lower faces and short, stubby limbs. We therefore reasoned that photographs of fathers’ children would function as releasers. Indeed, one important study showed that if you morph an infant’s picture to give it more baby schema (ie, you make the baby cuter), adults report stronger motivation to care for it and show stronger neural activation in the striatum. Another critical releaser of parental care that we used in our research is infant crying, which shapes parental behaviour through negative reinforcement. As my mentor Melvin Konner put it in his classic book , The Tangled Wing (2002): ‘It has to be a sound that sears its unpleasant way into the core of the mammal mother, causes a deep uneasiness and yet makes her deliver care instead of a lethal bite.’
As with other vertebrate parents, when human fathers come into contact with their offspring (in our experiment, through a photo) it activates the dopamine hub and the motivational system in the midbrain. The more the midbrain was activated, we found, the more involved the father was in caring for the child. This could mean that fathers who were more rewarded by their child became more involved in caregiving, or it could mean that, as fathers became more involved and formed stronger bonds with their child, they came to find the child more rewarding. Viewing pictures of their child also activated a number of other brain regions not included in animal models of parental brain function. These areas, including the anterior cingulate, the thalamus and the dorsomedial prefrontal cortex, all play a role in empathy. In humans, and likely many other species, parenting involves not only the motivation to deliver care but also the ability to perceive and understand the needs, feelings and mental states of the offspring.
We also found that supplemental oxytocin altered the way men’s brains responded to pictures of their children, boosting the response of the striatum, the target of midbrain dopamine neurons. It also increased activation in the anterior cingulate. Thus, oxytocin could render the child a more rewarding stimulus and might also increase paternal empathy.
I n more recent research , we studied the effect of infant crying, that aversive stimulus that effectively forces the caregiver to share in an offspring’s pain or figure out a way to alleviate it. Unsurprisingly, infant crying strongly activates brain regions involved in empathy, such as the anterior insula and the anterior cingulate cortex, especially in more empathic fathers.
Paradoxically, we found that fathers who activate the anterior cingulate the most when listening to infant crying report the most negative emotional responses to those cries, in particular being more likely to label the cry as spoiled or manipulative. How can greater engagement of a brain region associated with empathy be linked with such negative subjective reactions to the cry? We suspect it relates to a phenomenon known as ‘empathic overarousal’, in which an observer takes on the distress of another individual to such an extent that they become mired in personal distress, which in turn interferes with the motivation and ability to deliver compassionate care. There might be an optimal state of arousal and degree of empathy, neither too high nor too low, that facilitates sensitive and responsive fathering.
Our group also evaluated the hypothesis that the trade-off between male mating and parenting occurs at the level of the brain. To do so, we compared the neural response of fathers and non-fathers for both child and visual sexual stimuli. We found that fathers had a stronger neural response to pictures of children than did non-fathers in brain regions involved in reward processing, suggesting that they might attach greater value to those stimuli. On the other hand, and consistent with the hypothesis, fathers had a weaker response to visual sexual stimuli than did non-fathers in brain reward regions.
Our prefrontal cortex allows us to override ancient impulses in the service of honouring commitments
Since male testes make sperm, testis size can be yet another measure of investment in mating effort. In primate species with promiscuous mating systems, in which females mate with multiple males and vice versa, males tend to have large testes for their body size. This is presumably so that males can produce large numbers of sperm cells that can compete with the sperm of other males inside the female reproductive tract to fertilise the egg. On the other hand, in species where females mate with just one male, males have small testes for their body size.
In light of this, we used MRI scans to measure testes size in human males, and asked if it was related to either parental behaviour or brain function. We found that males with smaller testes were both more involved in caregiving and had a stronger response to viewing pictures of their child within midbrain areas involved in parental motivation. While statistically significant, this correlation between testis size and caregiving was weak, implying that most of the variation in caregiving was explained by other factors such as occupational demands, availability of other caregivers, and social and cultural expectations. Nevertheless, these results provide further support for the notion of a trade-off between mating and parenting effort.
Our research suggests similarity between the hormonal and neurobiological underpinnings of caregiving in mothers and fathers. But there are differences as well. As one example, the neuropeptide vasopressin, a close molecular analogue of oxytocin, could be particularly important for paternal caregiving. For example, injecting vasopressin into the brains of virgin male prairie voles elicits paternal behaviour that involves contacting and crouching over pups, whereas blocking vasopressin action at its receptor decreased licking and grooming of pups. In marmoset monkeys, fathers have more vasopressin receptors in their cerebral cortex compared with marmoset males who are not fathers. There has been only limited work on the role of vasopressin in human paternal behaviour, but one study showed that giving vasopressin increased the amount of time that expecting fathers spent watching baby-related avatars in a virtual reality environment.
Finally, there is one crucial paternal brain region that we have so far neglected. Evolutionary theory predicts that males will invest more energy in mating versus parenting if that is how they can maximise their reproductive success, and it is natural for us to ask if the theory applies to our own species. Some men behave much as theory would predict. Consider a man who leaves his menopausal wife and family to start a new family with a younger, fertile woman. Or think of certain high-status, married fathers who spend considerable time and money on girlfriends, mistresses and even prostitutes. Yet, many other men choose to forego these pursuits. They override impulses that evolution has programmed into their brains, impulses that evolved because they enhanced the reproductive success of their ancestors. They do so out of love and respect for their partners and their children, and out of respect for social and cultural norms. But how do they do what males of other species seem incapable of?
The answer, I believe, is that they rely on the crowning achievement of human brain evolution: the prefrontal cortex. Not only is the human brain three times larger than the brain of our closest living primate relatives, the great apes, but the human prefrontal cortex is also larger than expected for our brain size. Our prefrontal cortex is what allows us to override ancient, evolved impulses in the service of honouring commitments, abiding by social norms, and exercising our moral responsibilities. We are privileged to have this remarkable organ, and we fathers would all do well to make use of it."
The idea that sperm race to the egg is just another macho myth | Aeon Essays,,https://aeon.co/essays/the-idea-that-sperm-race-to-the-egg-is-just-another-macho-myth,"Before science was able to shed light on human reproduction, most people thought new life arose through spontaneous generation from non-living matter. That changed a smidgen in the middle of the 17th century, when natural philosophers were able (barely) to see the female ovum, or egg, with the naked eye. They theorised that all life was spawned at the moment of divine creation; one person existed inside the other within a woman’s eggs, like Russian nesting dolls. This view of reproduction, called preformation , suited the ruling class well. ‘By putting lineages inside each other,’ notes the Portuguese developmental biologist and writer Clara Pinto-Correia in The Ovary of Eve (1997), ‘preformation could function as a “politically correct” antidemocratic doctrine, implicitly legitimising the dynastic system – and of course, the leading natural philosophers of the Scientific Revolution certainly were not servants.’
One might think that, as science progressed, it would crush the Russian-doll theory through its lucid biological lens. But that’s not precisely what occurred – instead, when the microscope finally enabled researchers to see not just eggs but sperm, the preformation theory morphed into a new, even more patriarchal political conceit: now, held philosophers and some students of reproduction, the egg was merely a passive receptacle waiting for vigorous sperm to arrive to trigger development. And sperm? The head of each contained a tiny preformed human being – a homunculus, to be exact. The Dutch mathematician and physicist Nicolaas Hartsoeker, inventor of the screw-barrel microscope, drew his image of the homunculus when sperm became visible for the first time in 1695. He did not actually see a homunculus in the sperm head, Hartsoeker conceded at the time, but he convinced himself that it was there.
More powerful microscopes eventually relegated the homunculus to the dustbin of history – but in some ways not much has changed. Most notably, the legacy of the homunculus survives in the stubbornly persistent notion of the egg as a passive participant in fertilisation, awaiting the active sperm to swim through a hailstorm of challenges to perpetuate life. It’s understandable – though unfortunate – that a lay public might adopt these erroneous, sexist paradigms and metaphors. But biologists and physicians are guilty as well.
It was in the relatively recent year of 1991, long after much of the real science had been set in stone, that the American anthropologist Emily Martin, now at New York University, described what she called a ‘scientific fairy tale’ – a picture of egg and sperm that suggests that ‘female biological processes are less worthy than their male counter-parts’ and that ‘women are less worthy than men’. The ovary, for instance, is depicted with a limited stock of starter eggs depleted over a lifetime whereas the testes are said to produce new sperm throughout life. Human egg production is commonly described as ‘wasteful’ because, from 300,000 egg starter cells present at puberty, only 400 mature eggs will ever be released; yet that adjective is rarely used to describe a man’s lifetime production of more than 2 trillion sperm. Whether in the popular or scientific press, human mating is commonly portrayed as a gigantic marathon swimming event in which the fastest, fittest sperm wins the prize of fertilising the egg. If this narrative was just a prejudicial holdover from our sexist past – an offensive male fantasy based on incorrect science – that would be bad enough, but continued buy-in to biased information impedes crucial fertility treatments for men and women alike.
T o grasp how we got here, a tour through history can help. Scientific understanding of sex cells and the process of human conception is a comparatively recent development. An egg, the largest cell in a human body, is barely visible to the naked eye, and about as big as the period ending this sentence. So the smallest human body cell, a sperm, is utterly invisible for the unaided eye.
Sperm were unknown to science until 1677, when the Dutch amateur scientist Antonie van Leeuwenhoek first observed human sperm under a microscope. Around the same time, it was realised that the human ovary produced eggs, although it was not until 1827 that the German biologist Karl Ernst von Baer first reported actual observations of human and other mammalian eggs.
After van Leeuwenhoek’s discovery of sperm, it took another century before anyone realised that they were needed to fertilise eggs. That revelation came in the 1760s, when the Italian priest and natural scientist Lazzaro Spallanzani, experimenting on male frogs wearing tight-fitting taffeta pants, demonstrated that eggs would not develop into tadpoles unless sperm was shed into the surrounding water. Bizarrely, until Spallanzani announced his findings, it was widely thought – even by van Leeuwenhoek for some years – that sperm were tiny parasites living in human semen. It was only in 1876 that the German zoologist Oscar Hertwig demonstrated the fusion of sperm and egg in sea urchins.
Eventually, powerful microscopes revealed that an average human ejaculate, with a volume of about half a teaspoon, contains some 250 million sperm. But a key question remains unanswered: ‘Why so many?’ In fact, studies show that pregnancy rates tend to decline once a man’s ejaculate contains less than 100 million sperm.
Clearly, then, almost half the sperm in an average human ejaculate are needed for normal fertility. A favoured explanation for this is sperm competition , stemming from that macho-male notion of sperm racing to fertilise – often with the added contention that more than one male might be involved. As in a lottery, the more tickets you buy, the likelier you are to win. Natural selection, the thinking goes, drives sperm numbers sky-high in a kind of arms race for the fertilisation prize.
Striking examples of sperm competition do indeed abound in the animal kingdom. Our closest relatives, the chimpanzees, live in social units containing several adult males that regularly engage in promiscuous mating; females in turn are mated by multiple males. Numerous features, such as conspicuously large testes, reflect a particularly high level of sperm production in such mammal species. In addition to large testes, they have fast sperm production, high sperm counts, large sperm midpieces (containing numerous energy-generating mitochondria for propulsion), notably muscular sperm-conducting ducts, large seminal vesicles and prostate glands, and high counts of white blood cells (to neutralise sexually transmitted pathogens). The vesicles and the prostate gland together produce seminal fluid, which can coagulate to form a plug in the vagina, temporarily blocking access by other males.
Popular opinion and even many scientists perpetuate the same sperm scenario for humans, but evidence points in a different direction. In fact, despite various lurid claims to the contrary, there’s no convincing evidence that men are biologically adapted for sperm competition. The story of sperm abundance in promiscuously mating chimpanzees contrasts with what we see in various other primates, including humans. Many primates live in groups with just a single breeding male, lack direct competition and have notably small testes. In all relevant comparisons, humans emerge as akin to primates living in single-male groups – including the typical nuclear family. Walnut-sized human testes are just a third of the size of chimpanzee testes, which are about as large chickens’ eggs. Moreover, while chimpanzee ejaculate contains remarkably few physically abnormal sperm, human semen contains a large proportion of duds. Quality controls on human ejaculate have seemingly been relaxed in the absence of direct sperm competition.
Sperm passage is more like a challenging military obstacle course than a standard swimming race
For species not regularly exposed to direct sperm competition, the only promising alternative explanation for high sperm counts concerns genetic variation. In a couple of rarely cited papers published more than four decades ago, the biologist Jack Cohen at the University of Birmingham in the UK noted an association between sperm counts and the generation of chromosome copies during sperm production. During meiosis , the special type of cell division that produces sex cells, pairs of chromosomes exchange chunks of material through crossing over. What Cohen found is that, across species, sperm counts increase in tandem with the number of crossovers during their production. Crossing over increases variation, the essential raw material for natural selection. Think of sperm production as a kind of lottery in which enough tickets (sperm) are printed to match available numbers (different genetic combinations).
Other findings fly in the face of the popular scenario, too. For instance, most mammalian sperm do not in fact swim up the entire female tract but are passively transported part or most of the way by pumping and wafting motions of the womb and oviducts. Astoundingly, sperm of smaller mammals tend to be longer on average than sperm of larger mammals – a mouse sperm is longer than the sperm of a whale. But even if these were equivalent in size, swimming up to an egg becomes more of a stretch the larger a species gets. Indeed, it might be feasible for a mouse sperm to swim all the way up to the egg – but it is quite impossible for an even smaller blue whale sperm to swim 100 times further up the female tract unaided. Convincing evidence has instead revealed that human sperm are passively transported over considerable distances while travelling through the womb and up the oviducts. So much for Olympic-style racing sperm!
In fact, of the 250 million sperm in the average human ejaculate, only a few hundred actually end up at the fertilisation site high up in the oviduct. Sperm passage up the female tract is more like an extremely challenging military obstacle course than a standard sprint-style swimming race. Sperm numbers are progressively whittled down as they migrate up the female tract, so that less than one in a million from the original ejaculate will surround the egg at the time of fertilisation. Any sperm with physical abnormalities are progressively eliminated along the way, but survivors surrounding the egg are a random sample of intact sperm.
Many sperm do not even make it into the neck of the womb (cervix). Acid conditions in the vagina are hostile and sperm do not survive there for long. Passing through the cervix, many sperm that escape the vagina become ensnared in mucus. Any with physical deformities are trapped. Moreover, hundreds of thousands of sperm migrate into side-channels, called crypts , where they can be stored for several days. Relatively few sperm travel directly though the womb cavity, and numbers are further reduced during entry into the oviduct. Once in the oviduct, sperm are temporarily bound to the inner surface, and only some are released and allowed to approach the egg.
P ushing the notion that the fertilising sperm is some kind of Olympic champion has obscured the fact that an ejaculate can contain too many sperm. If sperm surround the egg in excessive numbers, the danger of fertilisation by more than one ( polyspermy ) arises with catastrophic results. Polyspermy occasionally occurs in humans, especially when fathers have very high sperm counts. In the commonest outcome in which two sperm fertilise an egg, cells of the resulting embryo contain 69 chromosomes instead of the usual 46. This is always fatal, usually resulting in miscarriage. Although some individuals survive as far as birth, they always expire shortly afterwards. Because polyspermy typically has a fatal outcome, evolution has evidently led to a series of obstacles in the female reproductive tract that strictly limit the number of sperm allowed to surround an egg.
Polyspermy has practical implications for assisted reproduction in cases of compromised fertility or infertility. For instance, the original standard procedure of introducing semen into the vagina for artificial insemination has been replaced by direct injection into the womb (intrauterine insemination, or IUI). Directly introducing semen into the womb bypasses the reduction of sperm numbers that normally occurs in the cervix, where mucus weeds out physically abnormal sperm. Analyses of clinical data have revealed that depositing 20 million sperm in the womb (less than a 10th of the number in the average ejaculate) is enough to achieve a routine pregnancy rate.
Sperm numbers become even more important when it comes to in vitro fertilisation (IVF), with direct exposure of an egg to sperm in a glass vessel. This bypasses every single one of the natural filters between the vagina and the egg. In the early development of IVF, the general tendency was to use far too many sperm. This reflected the understandable aim of maximising fertilisation success, but it ignored natural processes. High sperm numbers between 50,000 and 0.5 million increasingly depressed the success rate. Optimal fertilisation rates were achieved with only 25,000 sperm around an egg. Both IUI and IVF potentially increase the risk of polyspermy and the likelihood of miscarriage.
Human fertilisation is a gigantic lottery with 250 million tickets: for healthy sperm, it is the luck of the draw
The possibility of polyspermy casts new light on the evolution of sperm counts. Discussions of sperm competition generally focus exclusively on maximising sperm counts, but – as is common in biology – some kind of trade-off is involved. Whereas natural selection can lead to increased sperm production if males are in direct competition, it will also favour mechanisms in the female tract that constrain numbers of sperm around the egg. In promiscuously mating primates, such as chimpanzees, increased oviduct length in females offsets increased sperm production by males. This presumably limits the numbers of sperm approaching the egg. It also shows that the female’s role in fertilisation is by no means as passive as is often assumed.
The entrenched idea that ‘the best sperm wins’ has elicited various suggestions that some kind of selection occurs, but it is difficult to imagine how this could possibly happen. The DNA in a sperm head is tightly bound and virtually crystalline, so how could its properties be detected from outside? Experiments on mice indicate, for instance, that there is no selection according to whether a sperm contains a male-determining Y-chromosome or a female-determining X-chromosome. It seems far more likely that human fertilisation is a gigantic lottery with 250 million tickets, in which – for healthy sperm – successful fertilisation is essentially the luck of the draw.
Other puzzling features of sperm also await explanation. It has long been known, for instance, that human semen contains a large proportion of structurally abnormal sperm with obvious defects such as double tails or tiny heads. The ‘kamikaze sperm’ hypothesis proposed that these dud sperm in fact serve different functions in competition, such as blocking or even killing sperm from other men. However, this has since been effectively discredited .
The entrenched notion that human sperm, once ejaculated, engage in a frantic race to reach the egg has completely overshadowed the real story of reproduction, including evidence that many sperm do not dash towards the egg but are instead stored for many days before proceeding. It was long accepted as established fact that human sperm survive for only two days in a woman’s genital tract. However, from the mid-1970s on, mounting evidence revealed that human sperm can survive intact for at least five days. An extended period of sperm survival is now widely accepted, and it could be as long as 10 days or more.
Other myths abound. Much has been written about mucus produced by the human cervix. In so-called ‘natural’ methods of birth control, the consistency of mucus exuding from the cervix has been used as a key indicator. Close to ovulation, cervical mucus is thin and has a watery, slippery texture. But precious little has been reported regarding the association between mucus and storage of sperm in the cervix. It has been clearly established that sperm are stored in the crypts from which the mucus flows. But our knowledge of the process involved is regrettably restricted to a single study reported in 1980 by the gynaecologist Vaclav Insler and colleagues of Tel Aviv University in Israel.
In this study, 25 women bravely volunteered to be artificially inseminated on the day before scheduled surgical removal of the womb (hysterectomy). Then, Insler and his team microscopically examined sperm stored in the crypts in serial sections of the cervix. Within two hours after insemination, sperm colonised the entire length of the cervix. Crypt size was very variable, and sperm were stored mainly in the larger ones. Insler and colleagues calculated the number of crypts containing sperm and sperm density per crypt. In some women, up to 200,000 sperm were stored in the cervical crypts.
Insler and colleagues also reported that live sperm had actually been found in cervical mucus up to the ninth day after insemination. Summarising available evidence, they suggested that after insemination the cervix serves as a sperm reservoir from which viable sperm are gradually released to make their way up the oviduct. This dramatic finding has been widely cited yet largely ignored, and there has never been a follow-up study.
Mutations accumulate four times faster in sperm than in eggs, so semen from old men is risk-laden
In his textbook Conception in the Human Female (1980) – more than 1,000 pages in length – Sir Robert Edwards, a recipient of the 2010 Nobel prize for the development of IVF, mentioned cervical crypts in a single sentence. Since then, many other authors have mentioned sperm storage in those cervical crypts equally briefly. Yet storage of sperm, with gradual release, has major implications for human reproduction. Crucially, the widespread notion of a restricted ‘fertile window’ in the menstrual cycle depends on the long-accepted wisdom that sperm survive only two days after insemination. Sperm survival perhaps for 10 days or more radically erodes the basis for so-called ‘natural’ methods of birth control through avoidance of conception. Sperm storage is also directly relevant to attempts to treat infertility.
Another dangerous misconception is the myth that men retain full fertility into old age, starkly contrasting with the abrupt cessation of fertility seen in women at menopause. Abundant evidence shows that, in men, sperm numbers and quality decline with increasing age. Moreover, it has recently emerged that mutations accumulate about four times faster in sperm than in eggs, so semen from old men is actually risk-laden.
Much has been written about the fact that in industrialised societies age at first birth is increasing in women, accompanied by slowly mounting reproductive problems. A proposed solution is the highly invasive and very expensive procedure of ‘fertility preservation’ in which eggs are harvested from young women for use later in life. However, increasing reproductive problems with ageing men, notably more rapid accumulation of sperm mutations, have passed largely unmentioned. One very effective and far less expensive and invasive way of reducing reproductive problems for ageing couples would surely be to store semen samples from young men to be used later in life. This is just one of the benefits to be gained from less sexism and more reliable knowledge in the realm of human reproduction.
Nowadays, the story of Hartsoeker’s homunculus might seem veiled in the mist of time, mentioned only as an entertaining illustration of blunders in the early exploration of human sex cells. But its influence, along with the macho-male bias that spawned it, has lived on in subtler form among the cultural stereotypes that influence the questions we ask about reproductive biology."
Microchimerism: how pregnancy changes the mother’s very DNA | Aeon Essays,,https://aeon.co/essays/microchimerism-how-pregnancy-changes-the-mothers-very-dna,"When Lee Nelson first began researching autoimmune disorders in the 1980s, the prevailing assumption was that conditions such as arthritis and lupus tend to show up more commonly in women because they are linked to female sex hormones. But to Nelson, a rheumatologist at the Fred Hutchinson Cancer Research Center in Seattle, this explanation did not make sense. If hormones were the culprit, one would expect these afflictions to peak during a woman’s prime reproductive years, when instead they typically appear later in life.
One day in 1994, a colleague specialising in prenatal diagnosis called her up to say that a blood sample from a female technician in his lab was found to contain male DNA a full year after the birth of her son. ‘It set off a light bulb,’ Nelson told me. ‘I wondered what the consequences might be of harbouring these lingering cells.’ Since the developing foetus is genetically half-foreign to the mother, Nelson set out to investigate whether it could be that pregnancy poses a long-term challenge to women’s health.
Evidence that cells travel from the developing foetus into the mother dates back to 1893, when the German pathologist Georg Schmorl found signs of these genetic remnants in women who had died of pregnancy-induced hypertensive disorder. Autopsies revealed ‘giant’ and ‘very particular’ cells in the lungs, which he theorised had been transported as foreign bodies, originating in the placenta. While Schmorl speculated that this sort of cellular transfer also took place during healthy pregnancies, it was not until more than a century later that researchers realised that these migrant cells, crossing from the foetus to the mother, could survive indefinitely.
Within weeks of conception, cells from both mother and foetus traffic back and forth across the placenta, resulting in one becoming a part of the other. During pregnancy, as much as 10 per cent of the free-floating DNA in the mother’s bloodstream comes from the foetus, and while these numbers drop precipitously after birth, some cells remain. Children, in turn, carry a population of cells acquired from their mothers that can persist well into adulthood, and in the case of females might inform the health of their own offspring. And the foetus need not come to full term to leave its lasting imprint on the mother: a woman who had a miscarriage or terminated a pregnancy will still harbour foetal cells. With each successive conception, the mother’s reservoir of foreign material grows deeper and more complex, with further opportunities to transfer cells from older siblings to younger children, or even across multiple generations.
Far from drifting at random, human and animal studies have found foetal origin cells in the mother’s bloodstream, skin and all major organs, even showing up as part of the beating heart. This passage means that women carry at least three unique cell populations in their bodies – their own, their mother’s, and their child’s – creating what biologists term a microchimera , named for the Greek fire-breathing monster with the head of a lion, the body of a goat, and the tail of a serpent.
Microchimerism is not unique to pregnancy. Researchers realised in the 1990s that it also occurs during organ transplantation, where the genetic match between donor and recipient determines whether the body accepts or rejects the grafted tissue, or if it triggers disease. The body’s default tendency to reject foreign material begs the question of how, and why, microchimeric cells picked up during pregnancy linger on indefinitely. No one fully understands why these ‘interlopers’, as Nelson calls them, are tolerated for decades. One explanation is that they are stem or stem-like cells that are absorbed into the different features of the body’s internal landscape, able to bypass immune defences because they are half-identical to the mother’s own cell population. Another is that pregnancy itself changes the immune identity of the mother, altering the composition of what some researchers have dubbed the ‘microchiome’, making her more tolerant of foreign cells.
The phenomenon, believed to have developed in mammals some 93 million years ago, is common to placental mammals to this day. Its persistence and reach was made astonishingly clear in 2012, when Nelson and colleagues analysed brain samples drawn from dozens of deceased women, ranging in age from 32 to 101. They found that the majority contained male DNA, presumably picked up from past pregnancies. And some of these Y chromosome cells had apparently been there for decades: the oldest subject was 94, meaning that male DNA that transferred during gestation would have persisted for more than half a century.
Most of the research focuses on the Y chromosome as a marker for foetal microchimerism. This does not mean that sons, rather than daughters, uniquely affect their mother’s bodies, but rather reflects an ease of measurement: the Y chromosome stands out among a woman’s XX genes. And there is nothing to suggest that the presence of male cells in women’s brains wields a particular influence. Nonetheless, the findings gesture toward an array of questions about what it means for one individual to play host to the cellular material of another, prompting scientists to look into whether this phenomenon affects physical health or influences behaviour, or even carries metaphysical consequences. The Western self is a bounded, autonomous entity, defined in no small part by its presumed distinction from the other. But this unfolding field of research, advanced by Nelson and others, suggests that we humans are not oppositional but constituent beings, made of many. Nelson, who is fond of referencing the poet Walt Whitman’s multitudes , says we need a ‘new paradigm of the biological self’.
O ne of the most cherished images in the West – if not in the world – is the mother and child. Gazes intermingled, bonded as if one, they are suspended in serene togetherness. This cooing placidity presents a scene of utter naturalness, of womanhood fulfilled, of tender destiny. In 1884, the physician John Harvey Kellogg urged women – at a time when childbirth was a leading cause of women’s death – to opt for the ‘slight inconveniences of normal pregnancy and physiological childbirth rather than the dismal comfort of a childless old age’. In spite of the acute health risks that gestation and delivery entailed for Western women well into the 20th century, pregnancy was commonly depicted as the ultimate form of cooperation – mothers sharing their bodies to the point of sacrifice for the sake of kin and species.
This vision utterly obscures the fraught evolutionary journey that delivers the babe in arms, and the screaming, nerve-jangled moments that surround it. Increasingly, pregnancy has come under scrutiny for its profound paradoxes. It is at once essential and unrivalled in its perils. As it engenders life, it also results in staggeringly high rates of death and disease. Scientists are starting to look to microchimerism for clues as to why pregnancy is both life-giving and a singular source of risk.
On one side of the spectrum, foetal microchimeric cells have been implicated in autoimmune disorders, certain cancers and pre-eclampsia , a potentially fatal condition characterised by high blood-pressure during the latter half of pregnancy. But another body of research has found that foetal cells can protect the mother. They appear to congregate at wound sites, including Caesarean incisions, to speed up healing. They participate in angiogenesis, the creation of new blood vessels. A recent survey of the immunological implications of microchimerism in Nature Reviews by researchers at the Cincinnati Children’s Hospital asserts that these cells ‘are not accidental “souvenirs” of pregnancy, but are purposefully retained within mothers and their offspring to promote genetic fitness by improving the outcome of future pregnancies’. The researchers suggest that microchimeric cells boost the mother’s tolerance of successive pregnancies, representing an ‘altruistic act of first children’ to support the success of their genetically similar siblings. And they are associated with decreased risk of Alzheimer’s, lower risk of some cancer , and improved immune surveillance – that is, the body’s ability to recognise and stave off pathogens. According to Nelson, having a different set of genes provides ‘a different looking glass for detecting a pre-malignant cell’.
Although foetal cells might contribute to certain autoimmune disorders, they could also benefit women with rheumatoid arthritis. While doctors have been aware since the early 20th century that arthritic pain tends to recede with pregnancy, Nelson and her colleagues wondered whether there is an immunological reason why it tends to re-emerge later. They found that higher levels of microchimerism were associated with a lessening of symptoms, and that giving birth offered a long-term protective benefit. ‘It really looks vaccine-like,’ Nelson said, noting that pregnancy provides temporary protection against rheumatoid arthritis that, much like a vaccine, diminishes over time. ‘Protection starts about a year after birth, and then gradually attenuates after about 15 years.’
‘There is definitely an association between the presence of foetal cells and improved disease status’
Foetal microchimeric cells might even extend longevity and help to explain why women tend to live longer than men. In a 2012 study of nearly 300 elderly Danish women, the first to explicitly link microchimerism and survival, researchers found that the presence of microchimeric cells, as indicated by the presence of the Y chromosome, reduced women’s mortality for all causes by 60 per cent, largely because of a significantly reduced risk of death from cancer. Although the researchers looked only at male microchimerism (because there are no easy targets to distinguish cells between mothers and daughters), they maintain that female foetuses would have the same impact on longevity: 85 per cent of women who possessed these cells lived to age 80, as compared with 67 per cent who did not. While there are no clear answers to explain how microchimeric cells might lead to longer lifespans, researchers speculate that it could be associated with greater immune surveillance and improved repair of damaged tissue. However, the jury is out as to whether the presence of foetal cells in tissues is a sign of repair or of developing disease.
To Kirby Johnson, professor of paediatrics at Tufts University in Boston, the evidence favours a protective role. Like the Nelson lab, Johnson and his colleagues were also investigating autoimmune diseases. However, they reasoned that, if foetal cells were causing disease, then they should be found in greater concentration in affected tissue. ‘But what we found was that it really didn’t matter if you were looking at women with a particular autoimmune disorder or who were perfectly healthy – we were finding male DNA everywhere we looked,’ Johnson said. ‘That observation of ubiquity – of presence everywhere – didn’t match up to the hypothesis that these cells cause disease.’
While that finding was revelatory for Johnson, the bigger moment came during a study in 2001 on the role of microchimeric cells in disease of the thyroid, a hormone-secreting gland located in the neck. Analyses of samples taken from women who had their thyroids removed showed ‘perfectly intact thyroid follicles from male cells. These were not sad, scattered cells like you’d expect’ but strikingly healthy. Johnson recalled: ‘Finding male cells that had assumed the structure of functional tissue made us say, wait a second, it doesn’t look like it’s causing disease. It looks like they’re actually coming to the rescue and participating in repair.’
Not long after, a mother with severe hepatitis C and a history of intravenous drug use checked into a Boston clinic. Hepatitis C is a disease of the liver, and when Johnson and colleagues looked at a biopsy of the organ, they found a high number of male cells. Moreover, these cells appeared to be functioning as healthy liver tissue. Although the woman declined further treatment for her disease, she participated in tests confirming that the cells had indeed come from her son. When she came in at a later date to provide blood samples, Johnson and his research team were astounded to discover that she was free of the disease. ‘We can’t with absolute certainty say, foetal cells cured her hepatitis,’ Johnson told me. ‘But we can say, there is definitely an association between the presence of foetal cells and improved disease status.’
F or hundreds of millions of years, microchimerism has been a part of mammalian reproduction. From a survival-of-the-fittest perspective, it would make sense that microchimerism might preserve the health of mother and child, helping her survive childbirth and beyond as her offspring make their slow way to independence. However, current evolutionary thinking suggests that the interests of parents and their kin might be at odds – in the womb, as well as in the world. Because mother and the foetus are not genetically identical, they might be engaged in a tug of war over resources. In addition, the mother’s goals, presumably being the successful reproduction and rearing of multiple children, might be at odds with the evolutionary aims of the individual foetus: its own, solitary survival and eventual reproduction.
The geneticist Amy Boddy of the University of California, Santa Barbara, says that microchimerism presents a paradoxical picture of conflict and cooperation, and foetal cells might well play a host of roles, from helpful partners to hostile adversaries. These tensions are thought to originate with the creation of the placenta. Trophoblasts, cells that form the outer layer of the early embryo, attach and burrow into the uterine lining, establishing pregnancy and initiating the process of directing blood, oxygen and nutrients from the mother to the developing foetus. Boddy suggests that microchimeric cells act like a ‘placenta beyond the womb’, directing resources to the baby throughout gestation and after birth.
Conflict ensues: on the one hand, mothers and babies have a shared investment in mutual survival; on the other, the foetus is a demanding, voracious presence, actively trying to draw resources to itself, while the mother places limits on just how much she is willing to give.
In other words, on an unconscious level, the mother might be engaged in a struggle with the foetus over just how much she can provide without harm to herself. Microchimerism extends this silent chemical conversation into the months and years after birth, where, theorists propose, foetal cells can play an important role in ‘manipulating’ the breasts to lactate, the body to increase its temperature, and the mind to become more attached to this new wailing and growing human.
The idea that the womb might not be an enclave of rosy communion took hold in the work of the American evolutionary biologist Robert Trivers. An original and often unorthodox figure, Trivers was the creator of seminal theories – such as parental investment, altruism and parent-offspring conflict – that are now mainstays of evolutionary psychology. Where others embraced the veneer of presumed harmony, Trivers saw roiling conflicts hidden from view, whether in the womb or in romantic partnership. He made the case that familial struggles are rooted in ‘conflict between the biology of the parent and the biology of the child’. Tensions arise, he suggests, because a mother wants to make sure all her children have an equal chance at survival and procreation, whereas a child privileges its own survival and wants to commandeer the mother’s resources for itself.
The foetus has been depicted as a manipulative entity, conniving to direct the mother to its own advantage
The evolutionary biologist David Haig at Harvard University elaborated on this idea through the concept of genomic imprinting. For most genes, the foetus inherits two working copies, one from the mother and one from the father. However, with imprinted genes, one of the copies is silenced, leading to genes that are differently expressed depending on whether they are inherited from the mother or father. Haig suggests that genetically determined behaviours that benefit the paternal line might be favoured by natural selection when a gene is transmitted by the sperm. And conversely, behaviour that benefits that maternal side might be favoured when a gene is transmitted by the egg.
Haig extends the battle in the womb to the mother and father, whose evolutionary agendas differ on just how much the mother should give to the foetus, and how much the foetus should take. He theorises that genes of paternal origin are likely to promote increased demands for maternal resources. Moreover, Haig suggests that a given man will not necessarily reproduce with one woman, but rather increase his own reproductive success by having children with multiple partners. As a result, he is, evolutionarily speaking, more invested in the health of his offspring, whose fitness benefits from extracting as much from the mother as possible, than he is in her long-term wellbeing.
Haig has been influential in depicting the foetus as a manipulative entity, conniving to direct the mother to its own advantage. Lactation might be evidence of this subtle control at work, and result from foetal cells that are commonly found in breast tissue signalling to the mother’s body to make milk. Haig also speculates that birth timing might owe to the silent influence of older siblings – which he describes as ‘the colonisation of maternal bodies by offspring cells’ – pushing the mother’s body to delay subsequent pregnancies. While there is nothing overtly harmful to the mother about lactation or delayed birth timing, by Haig’s rendering they are evidence of the parasitic control that a foetus wields over its mother, and the developing child’s efforts to claim the largest share of a presumably scant pie. He argues that microchimeric cells can extend inter-birth intervals beyond the mother’s optimum time frame, and cites as evidence a 2010 study showing that male births are more likely to be followed by multiple miscarriages .
Haig is quick to point out that these antagonisms are not an expression of feuding spouses, squabbling families or ongoing culture wars, but rather are playing out unconsciously through ‘genetic politics’. Nonetheless, there is a ready slippage between the interpretation of social behaviour and analyses of biological activity, and current research is ripe with hyperbole and bellicose metaphors.
If I am both my children and my mother, does that change who I am and the way I behave in the world?
An ‘evolutionary arms race’ is what Oliver Griffith, a postdoctoral associate at Yale University, calls pregnancy. He elaborates that mothers ‘marshal their best defensive tactics’ against offspring’s ‘strategies to steal resources’.
Harvey Kliman, a reproductive scientist at the Yale School of Medicine, makes the case that the placenta, which he proposes is controlled by the genes of the father, is at odds with the evolutionary aims of the mother. While the father’s goal is to make ‘the biggest placenta and the biggest baby possible’, the mother’s objective is to place limits on this growth so that she can survive childbirth. Kliman was part of a group that investigated the role of a protein dubbed PP13 in pre-eclampsia. During gestation, trophoblasts work to expand the mother’s arteries to bring blood flow and nutrients to the foetus. In an analysis of placentas from terminated pregnancies, the group found that PP13 was largely absent around these arteries, but that it was concentrated near the veins. They concluded that PP13 acts as a diversion, luring the mother’s immune cells to the veins and away from the placental expansion – Kliman uses the term ‘invasion’ – into the arteries. As Kliman put it to The New York Times in 2011: ‘Let’s say we’re planning to rob a bank, but before we rob the bank we blow up a grocery store a few blocks away so the police are distracted. That’s what we think this is.’
But as alluringly action-packed as these analogies are, they remain wholly speculative. And indeed, theories of conflict in and beyond the womb are just that. As the biologist Stephen Stearns at Yale has remarked : ‘the annals of research journals are littered with the corpses of beautiful ideas that were killed by facts’. At present, there is no definitive proof that the microchimeric activity, commonly described as conflict, combat or colonisation, reveals one entity pitted against the other. The assumption that the solitary organism strictly pursues goals of survival and genetic self-interest favours a parsimonious view of the individual: homo economicus operating in an environment of scarcity, in eternal competition with a nameless other.
The self emerging from microchimeric research appears to be of a different order: porous, unbounded, rendered constituently. Nelson suggests that each human being is not so much an isolated island as a dynamic ecosystem. And if this is the case, the question follows as to how this state of collectivity changes our conscious and unconscious motivations. If I am both my children and my mother, if I carry traces of my sibling and remnants of pregnancies that never resulted in birth, does that change who I am and the way I behave in the world? If we are to take to heart Whitman’s multitudes , we encounter an I composed of shared identity, collective affiliations and motivations that emerge not from a mean and solitary struggle, but a group investment in greater survival."
"I think I know where babies come from, therefore I am human | Aeon Essays",,https://aeon.co/essays/i-think-i-know-where-babies-come-from-therefore-i-am-human,"Throughout most of evolutionary history, sex was just sex. Among vertebrates, fish were the first to do it, going back some 400 million years. While it might be fun for fish and all the other species that evolved to reproduce sexually, for most species, sex still is just sex. But for our own peculiar species of primate, sex is about something more. Sex is about babymaking. Contemplating sex and where we come from has played a fundamental role in human mating, partnering and raising children, and in forming families, communities and alliances, and more. Recognising this fundamental difference between us and the rest of Earth’s sexual beings overturns conventional evolutionary thinking, which has long understood human sex, reproduction and kinship as fundamentally the same for us as for any other mammal.
All sexually reproducing animals have a powerful ‘sex drive’. If they didn’t, they would quickly become extinct. Among most animals, this drive demands immediate attention. It’s the yowls of the tomcats in the alley who detect a female in heat, the bawling bull who smells a receptive cow. It can’t be ignored. But it’s not a ‘baby drive’ – at least it isn’t experienced as one. We know the two are intimately related, but the tomcat doesn’t. He just wants to find that female in heat. Sex can certainly make for high drama among manipulative social mammals, especially primates. Among many monkeys and apes, the alpha male often sires the most offspring during his tenure because he is granted the least fettered access to fertile females, and can foil the sexual devices of subordinates. But with our inventions of virgin worship, marriage, castration, contraception, fertility technology and genetic engineering, the human primate experiences sex in an entirely different way from any other animal, enmeshed in all kinds of cultural and emotional networks and significance.
Stories about what make humans unique glorify dexterous fingers, inventive minds and our habit of sharing complex ideas through intricate verbal cues. Our ancestors’ fabled intellects gave rise to art, technology and dynamic, large-scale politics. But there is an oft-overlooked plot in the human saga. It stars the ancient hominins who realised that they’re related to some people and not others, and that sexual intercourse might have something to do with that. The effects of this realisation are profound, and deserve some credit for our species’ widespread success on the planet.
P op culture is obsessed with sex, and science is no different. And for good reason: sex is fundamental to how and whether so much animal evolution happens. In conventional evolutionary science, ‘favoured’ genes cause themselves to be passed from one generation to the next, because they are responsible for traits that confer reproductive advantages in a particular environment. This is natural selection. When it comes to sex and reproduction, science takes a particular interest in sexual selection: that is, the evolution of traits involving mate choice and mating behaviours. Within this frame, scientists have tried to trace the origins of human mating, marriage and kinship to evolutionary ‘strategies’ that, conscious or not, were responsible for our survival and continued evolution rather than our extinction.
In other words, if you follow this mainstream or ‘Darwinian’ logic, there must be genes that underpin mating behaviours, which in turn cause animals (including the human animal) to be successful in reproducing, and thus those genes (and their associated behaviours) are perpetuated in populations. If that’s how simply things actually happen in nature, there will be genes ‘for’ mate preference, genes ‘for’ pair-bonding, genes ‘for’ polygamy and so on.
We share many genes even with fruit flies, but we share far more with non-human primates. We share an especially large proportion of our genome with our closest relatives – chimpanzees and bonobos – so, if their mating behaviour is genetically driven, then we’ll learn a lot about ourselves by studying these apes. Although no one has actually identified genes for infanticide or for avoiding incest, for most evolutionary scientists, answers to questions such as why is infanticide so common among chimps and some monkeys, or why is the incest taboo so common in human societies, ought to be applicable interchangeably to all of us primates. Thus, evolutionary psychology and evolutionary theory more broadly has a clear theoretical box for human sexuality: the model of animal mating.
Back in 1997, the psychologist Steven Pinker wrote in How the Mind Works : ‘The human mating system is not like any other animal’s. But that does not mean it escapes the laws governing mating systems, which have been documented in hundreds of species.’ In Mutants (2004), the evolutionary developmental biologist Armand Leroi summed up this hardline argument with: ‘the psychologies of pheasants and Fijians are really much the same’. The idea here, the ‘law’ that governs mating, is that sexual selection is assumed to drive reproductive behaviour in similar ways in all kinds of creatures. Conventional theory describes the characteristics we use to choose our mates, be it the resplendent tail of the peacock or a man’s full beard, as indicators of good genes, that is, genetic predisposition for strength or good health, and thus we’re choosing not just a full beard, but a collection of favourable genes to pass on to our children. This strips away any uniqueness in our reproductive behaviour; we’re just like any other animal.There have been many human mating behaviours that have been anointed by hyper-Darwinians as ‘natural’ to the species, often by analogy with other primates – and often revealing as much about the preconceptions of their inventors as about any sound science. Thus we are told that men are genetically programmed to be dominant, women are programmed to seek the alpha male, monogamy is innate for women, polygamy is innate for men, and many other examples. Male violence is regularly interpreted as a programmatic legacy from human evolution, and violent stepfathers who hurt their partners’ children are understood to be acting out of the same impulses as male chimpanzees who kill infants in a troop. Thus the standard trope of ‘Demonic Males’ and choosy females.
These potent images are worth unpacking because they reveal the disorienting feedback loops between seeing ourselves as just like other animals, while interpreting other animals as being just like us .
According to conventional evolutionary theory, dominant male chimps and some other primates kill infants in the troops they join because they know that these babies aren’t theirs. This makes sense to mainstream evolutionary theory because every organism’s purpose in life is to survive to reproduce, but even better is when my genes outcompete yours . I win, you lose. Thus, a dominant male kills unrelated infants because this increases the chances that his genes, inside his babies, will outcompete, or outnumber, his rivals’. Survival of the fittest, indeed.
In eliminating the distinctions between human sexual behaviour and that of other primates, a murky anthropomorphism creeps in. The journalist Nicholas Wade wrote in The New York Times that male chimps and baboons ‘are prone to kill any infant they believe could not be theirs, so females try to blur paternity by mating with as many individuals as possible before each conception’. This suggests that non-human primates could know that semen transforms into a baby and that the act of sex, broadly, makes an infant. Further, it implies that they have a sense of relatedness, and that it extends to fathers. If not, then it’s deliberately narrating animal sex and violence like a scene from Game of Thrones , for our entertainment. And it works (it’s sensational and relatable) because a more scientifically grounded alternative – male baboons, gorillas and chimps might kill infants, but they’re less likely to kill ones clinging to females with whom they’ve mated because sexual relations between primates builds affiliation – isn’t nearly as scintillating.
It’s not just journalism that falls into this trap: scientists aren’t all that deft at escaping the temptations of anthropomorphising reproductive strategies either. Writing about male-male competition and the caretaking of infants by the male marmoset monkeys who sire them, the primatologist Sarah Hrdy quipped in Mothers and Others (2009) that ‘in the absence of DNA testing, it is impossible [for the monkey] to know who the father is’. But really, it’s the absence of the awareness that sex makes babies (which we’re calling reproductive consciousness) that makes it impossible for a monkey to know who the father is, or to have the concept of ‘father’ or paternity in the first place. Something else is driving marmoset fathers to care for their own biological offspring and not others.
So evolutionary speculation about the origin of human mating strategies not only rests on science’s tendency to ‘zoomorphise’ us. It also entangles science in a dizzying web of anthropomorphic assumptions about other animals.
Yet as the cognitive scientist and anthropologist Daniel Povinelli writes in ‘Behind the Ape’s Appearance’ (2004): ‘if chimpanzees do not reason about unobservable entities, then we would frequently need distinctly different explanations for human and chimpanzee behaviour – even in situations where the behaviour looks almost identical’. Chimpanzees deftly navigate a world with gravity without thinking about gravity, or rationalising about it and making rules. In an equally naïve way, they deftly navigate a world with paternity without thinking of the consequences of sexual intercourse. But with our fundamentally similar ape ways, humans navigate a world where males are known to be intrinsic to babymaking, where males and females can have descendants and siblings, where beliefs about relatedness shape society and politics, and where relatives can inherit immaterial and material wealth to a degree and complexity that is unparalleled in the animal kingdom.
When a dominant chimpanzee kills an unrelated infant, one set of explanations is needed. And it’s not enough to map those directly, with no recognition of the utterly distinct human experience of reproduction and family, on to human domestic violence.
Human horniness is tethered to beliefs, to knowledge, to conscious calculation, to past and to future
Influential ideas about the evolution of human pair-bonding also dismiss our distinctiveness. In Primeval Kinship (2010), the primatologist Bernard Chapais wrote a book-length hypothesis for the evolution of human pair-bonding, matrilineal and patrilineal descent groups, and exogamous (anti-inbreeding) marriage patterns. Like many other scientists, Chapais interprets the very peculiar mating habits and family structures of humans as outcomes of standard processes of natural and sexual selection that favour particular mating strategies for animals with particular niches and needs. Human mating strategies can, as far as Chapais and many others are concerned, be understood without any reference to our peculiar self-consciousness about them.
Our species’ habit of pair-bonding, for example, is widely considered an adaptation to having particularly costly babies. They’re so big (read: hungry) and so slow to grow to independence, that allomothering – infant care from other than the mother – not only from a baby’s grandmother, maternal aunts and sisters, but also from the baby’s brothers, father and his kin, enabled us to continue and maybe even thrive under these conditions. And that’s not a bad explanation as a starting point. It helps to explain why other primates, such as owl and marmoset monkeys, have invested fathers. Owl monkey fathers don’t ‘know’ who their offspring are in the sense that humans ‘know’. But natural selection has favoured (or at least, not frowned upon) owl monkey fathers who care for their offspring – they have a reproductive advantage and, insofar as this behaviour is heritable, whether biologically or because it’s learned, it will be reproduced in further generations. On the surface, this looks akin to human behaviour, minus all the elaborate consequences of human paternity.
It seems reasonable to assume that at the proximate level of attraction and arousal we are close to other animals. But talk of ‘animal instincts’ is for dark and steamy trysts, not wedding vows. Human horniness is tethered to beliefs, to knowledge, to conscious calculation, to past and to future. What is more, humans arrange marriages – sometimes from birth – greatly shaping human sexual behaviour over a lifetime. And new genetic studies show that marriage is a very ancient human behaviour indeed. Because our species is inextricably steeped in sociocultural context, you could argue that all human marriage is somehow arranged. This is a whole new approach to mate choice on the evolutionary scene. Like other social animals, we do compete for mates and we are choosy, but it’s not just because we want to have sex with them, it’s also because we want to make babies with them, to merge families with theirs, to make a future together. Reproductive consciousness isn’t just an aftermath to human mating. It has shaped it profoundly.
In his book Why Is Sex Fun? (1997), Jared Diamond writes that human sexuality is unusual because we often have sex for recreation rather than procreation. But the popular emphasis on orgasms and acrobatics misses the mark. Animals have sex for a smorgasbord of immediate reasons, including pleasure, yet no species knowingly has sex for procreation except us. So it’s not just our Kama Sutra-ness but it’s also our reproductive consciousness that sets humans apart. Sex isn’t something that just happens to human beings. It’s the fact that we know what can happen when we do it that might be one of the turning points in making humans unique inhabitants of the planet. Because of reproductive consciousness, humans know that they are related to one another: grandparents, parents, siblings and children. And this knowledge – sex equals babies, and babies equals kinship – marks one of the turning points of the history of life.
So how did we work it out in the first place?
A s far as we know, there is no animal that spends time dwelling on what it cannot perceive with its senses other than the human animal. Understanding where babies come from can’t simply be observed. It requires grasping that a rather routine activity today will have long-term consequences in the future – connecting a long-ago act to the baby mice, kittens, baby gorillas or newborn whales and elephants born 20 days, two months, eight months, or almost two years later. Among the few of us, including bonobos, that copulate while pregnant – which can shrink the time between cause and effect – being able to link the business and substance of sex to pregnancy and its outcome would still take the kind of wild imagination that only humans are thought to possess. That, plus language, helps us to think these sorts of abstract creations and to communicate them. Once we’re a few years old, humans begin to explain the unobservable. Soon thereafter, we’re weaving and repeating stories about where babies come from. And it’s not much longer until we’re seasoned gossips about tribe members.
Abstract conceptual ability, what Povinelli refers to as a mind primed to think about ‘ghosts, gravity, and God’, is among the few exceptional human traits that primatologists, who are ever narrowing the divide between us and our closest non-human relatives, can embrace. To quote Povinelli: ‘The mental lives of humans and chimpanzees are similar, in that both species form innumerable (and in many cases, identical) concepts about observable things, but, at the same time, are radically different , in that humans form additional concepts about inherently unobservable things.’ As far as we understand non-human cognition today, there is little to suggest that other animals hold beliefs, material or spiritual, about pregnancy or babymaking, or that they understand that anyone is related to babies, especially males. Without a vivid imagination for the past and the future and the mysterious connections between them, such an understanding couldn’t exist.
We don’t know when our human ancestors first began to reason abstractly in a deeply imaginative way, and we certainly don’t know when any first moments of a-ha occurred, when anyone grasped that man + woman + whatever else they might imagine = baby. Scenarios for the origins of reproductive consciousness play out in fantastic works such as Jean M Auel’s The Clan of the Cave Bear (1980) and its sequel The Valley of Horses (1982) but, outside of speculative fiction, it’s unlikely that we’ll ever know what happened with any certainty. However, there are traces of the cognitive development of humans that afford interesting clues.
Surely the control of fire offered an opportunity for hominins to observe how organic matter can transform, and to transfer those rules to sex and babies. The best evidence for fire control doesn’t appear until 800,000 years ago at a Homo erectus site in Israel , which is half a million years before anything evolved that we’d call Homo sapiens. Perhaps it was cooking – we like to think of this as the ‘bun in the oven’ hypothesis – although bread itself is of course only a few thousand years old. Foraging for eggs would have revealed to ancient hominins how amorphous fluid transforms into baby birds – an observation begging to be transferred to baby humans. The earliest ostrich eggshells with a definite human use, ingeniously used as containers to carry and store water, don’t appear until the later Middle Stone Age of Africa around 60,000 years ago. Abstract thinking itself emerged long before this.
If reproductive consciousness is as old as the earliest archaeological evidence for our unique ability to think about the future, the past and present, and to link far out causes and effects together, then it might have originated as far back as 1.75 million years ago when Homo erectus began deliberately shaping stones in the form of tear-dropped hand-axes. But ultimately we can’t know for sure what erectus brains could think about sex and kinship. If reproductive consciousness is possible only with our species-specific brains, then it could have dawned on the first of our species, Homo sapiens, which means it could still be very ancient, going back some 200,000-300,000 years. Given that it’s likely that sapiens took some time to fire up its brain to modern capacity, then it’s possible that reproductive consciousness originated within the past 100,000 years when the archaeological record starts to boast the products of sophisticated abstract thinking with those engraved eggshells, and also in etched rocks, carved stones and painted caves.
Reproductive consciousness is a powerful context for boosting male-female cooperation, even beyond mates
Whenever the cognitive ability arose, reproductive consciousness is almost certainly as old as 100,000 years, and so the cultural byproducts of it must be ancient too. Dowries, engagement rings and marriages are influenced by our unique cognition and culture, even if such traits have long been seen by evolutionary biologists as the bipedal ape’s versions of what bush crickets, bower birds and wolves do.
Let’s start with the consequences that might run very deep in our past. Humans around the world form pair-bonds within larger communities. We are generally not promiscuous, like chimpanzees and bonobos, despite the tremendous opportunity around us. Sure, we cheat, but most partnered humans are as reproductively faithful as physically isolated gorillas and gibbons , even with far greater opportunities to sleep around. And while many diverse cultures promote or tolerate polygamy, monogamy is far more common in reality, even within putatively polygamous societies. This human tendency to bond in pairs could have evolved, as Bernard Chapais and others have hypothesised, because of a long series of ecological shifts in the past several million years of our evolutionary history, including selection for allomothering males. Whether it’s an alternative or a complement to this idea, reproductive consciousness likely also played a role in the evolution of people’s ability and desire to form exclusive sexual partnerships within a larger community.
Beliefs about relatedness and about babymaking could have factored into mate choice and competition for a long time, in a particularly human kind of sexual selection. Reproductive consciousness would have increased attraction to and competition for mates, male or female, who are observed to be good community members and good parents, or who have potential to be. It would have increased competition for mates or for families with resources they’re willing to share. Reproductive consciousness is a powerful context for boosting male-female cooperation, even beyond mates and into adult brother-sister relationships – effectively a uniquely human phenomenon. The understanding of paternity would also have boosted brother-brother bonds. Although chimpanzee males cooperate , brothers are not intentionally befriended. Humans take nepotism far beyond where other primates go because we know about relatedness, and we encode it with meaning and value.
A very rich example can be found in incest taboos that are, like pair-bonding, widespread in human societies, enough to be virtually universal . The standard evolutionary interpretation of human incest taboos runs like this: human incest taboos and animal inbreeding avoidance are general evolutionary strategies for preventing very rare, debilitating genetic traits in offspring. Many animal species practise matrilocality and patrilocality. That is, females or males remain in their natal group, and the opposite sex (sometimes both sexes) leaves their natal group as they reach reproductive age. This exogamy is common among non-human primates and other animals, and results in ‘inbreeding avoidance’. However, we’ve learned from genetic studies that the risks from inbreeding have been overblown; it doesn’t lead to harmful traits nearly as often as this view has led us to believe. Other species have mating strategies that don’t avoid incest: banded mongoose females, for example , breed readily with their brothers and fathers.
It’s possible that human incest taboos are rooted in some biological factors underpinning aversion and attraction, which seem to urge us to mate outside our familiar immediate family. However, such taboos can be just as robustly interpreted as a product of our reproductive consciousness, which in turn has influenced cultural norms. Reproductive consciousness enables a calculation of relationships and reciprocity that is impossible for other animals. Socially enforced incest taboos keep fathers from monopolising daughters who would do better, for the family, if they married and reproduced with someone outside the family instead. Incest taboos force individuals to marry into someone else’s family, which results in benefits to their parents and siblings through alliance-building. And such social norms themselves have a profound effect on reproduction, not just of individuals but also of culture. Rather than simply mapping animal models onto human reproduction, when we take into account the distinctive human achievement of reproductive consciousness, we can interpret incest taboos as operating to control sexuality in order to optimise social, political and economic outcomes, even at the expense of optimising genetic ones.
This is not to say that incest taboos are entirely cultural phenomena. Despite not being coded in DNA, a socially mandated incest taboo can lead to differential reproductive success for individuals who practise it, thus becoming an influence on human biology, not just a product of it. Incest taboos discourage instant gratification to favour the long-view and all its spoils. And so culture in turn is able to affect biological evolution.
Reproductive consciousness is just one element in the invention of human culture – a whole cluster of behaviours, knowledge, values and beliefs that unhooks human destiny from the standard evolutionary model of other species. We humans do many things that undermine our evolutionary interests. We practise religious celibacy, contraception, abortion, suicide bombing. We adopt infants who aren’t our kin, we go to war, we kill our siblings. A lot of this we do voluntarily, and none of it perpetuates our own genes. In fact, it actively does not.
Reproductive consciousness transforms human beings and families into human lineages, inextricably intertwined with other bloodlines, passing along and exchanging genes just like any other animal, but also transmitting immaterial and material culture across and down through the generations, which in turn affects the fates of the genes going forward as well. Understanding that sex makes babies has allowed us to create social norms and expectations about when and with whom we reproduce that biologically driven behaviour alone could never have done. But our understanding that sex makes babies has done much more than shape human destiny. It has changed the entire planet through our manipulation of other species to our own ends.
A t the end of the last Ice Age, some 10,000-12,000 years ago, people began the transition from foraging to farming. This shift defines the beginning of the Neolithic and forms the foundations for the Anthropocene, the present age of visible human impact on the Earth’s ecosystems that might soon become an official geologic epoch. Archaeologists still argue about why this change happened where and when it did. Either farming offered advantages that foraging did not, or foraging became unsustainable, and people had no choice but to adopt a new way of life. Perhaps it was some of both. Whatever the cause, the transition was slow and patchy, taking hundreds or even thousands of years.
Domestication is essentially predation with forethought and evolutionary intent. Early farmers were the first to replace natural selection with artificial selection. They had to decide which seeds to plant (if in fact to plant them at all), which animals to cull from wild herds that lived nearby, to corral animals for meat, wool, hides, milk, blood and so on. By all professional accounts, cultivation – that is tending animals and plants without choosing to breed subsets for specific traits – preceded domestication , but even without practising artificial selection, cultivators, like their hunter-gatherer ancestors, possessed a wealth of knowledge about nature. Farmers must have known that pollination and mating were prerequisites for the next generation of plants and animals, and this must have made them better at gardening and animal husbandry. Palaeolithic peoples had sophisticated symbolic language and extensive knowledge of animal and plant behaviours. This would certainly have included an understanding of sex and reproduction.
Domestication of goats, sheep or cows reflects an understanding of the role of sex in producing animals with desired characteristics, such as docility, herd behaviour and fatty milk. Farmers realised that parent animals with these traits would be likely to produce offspring with the same traits, and this would have informed their decisions about which animals they would allow to reproduce. At some point people also understood that selective pollination could drive a food crop in a particular direction as well – seeds that all ripen at the same time, seeds that stay on the plant until the farmer wants to harvest them, seeds that taste better, and so forth. Whether early farmers understood that pollination was essentially plant sex is unknown, but domestication – purposeful, human-driven artificial selection rather than simply taking advantage of what natural selection or chance delivered – was a fundamental part of the transition to agriculture, and that couldn’t have happened without reproductive consciousness.
The transition to agriculture brought the beginning of settled life, the growth of villages and towns, and eventually cities, with dramatic population growth and ensuing environmental, epidemiological, cultural and political consequences. Without reproductive consciousness, human history would have been utterly different.
Now, it’s true that humans aren’t the only animals that farm. Ants farm aphids, damselfish farm algae, leafcutter ants grow fungi on the leaves they collect. However, as far as we know , they aren’t consciously breeding selected fungi or aphids specifically to engineer the traits of future generations. This isn’t domestication, and it’s not farming as we know it. Domestication requires reproductive consciousness and, without it, the ecological leviathan of agricultural and industrial human society would not have been possible.
Someone left a cuneiform note in the Sumerian city of Eridu in Iraq, describing how the penis, specifically the god Enki’s, is a creative force
Perhaps it’s not surprising, then, that early agricultural civilisations have a wealth of stories and myths about fertility. More than 2,000 years ago, inscriptions on an Egyptian wall reveal the life-creating abilities of the penis . Hieroglyphs tell of how the male god Atum masturbated the universe into existence. A thousand years before that, someone left a cuneiform note in the Sumerian city of Eridu in Iraq, describing how the penis, specifically the god Enki’s, is a creative force, with Enki’s bearing the rivers Tigris and Euphrates. The Code of Hammurabi, as translated into English, suggests that Babylonian men were very aware that they help to create offspring via sexual intercourse.
In the Bible, when God told Adam and Eve to ‘be fruitful and multiply’, he was transferring creative power to their capable loins. Later in the script, the Ten Commandments demand that we honour our father and mother, that we not commit adultery, and that we do not covet our neighbour’s wife. From a similar time, in The Twelve Tables (450 BCE), the earliest coded Roman law that has survived recorded history, appears to demonstrate a fairly sound empirical understanding of reproduction. For example, in Table IV.5 it says: ‘A child born after 10 months since the father’s death will not be admitted into a legal inheritance.’ Tracking gestation length to legitimise human children was part of the law by then, despite it predating any scientific understanding of reproduction by 2,000 years.
The fundamental arithmetic of human reproduction might be ancient knowledge, but an accurate understanding of the detailed logistics of conception and embryonic development were more elusive. We have a great deal of evidence from ethnographic sources that, while the male role in reproduction seems to be universally acknowledged, the precise details of this vary widely from culture to culture. Among lowlands peoples in the Amazon Basin, there has been a widespread belief that multiple men can contribute to producing a single child, their semen building up or anointing the foetus during pregnancy. At the turn of the 20th century, anthropologists such as Bronislaw Malinowski in The Father in Primitive Psychology (1927), and Douglas Lockwood in I, the Aboriginal (1962), recorded the stories of peoples of the South Pacific and Australia who believed that penetrating intercourse opens up a woman or makes ‘the road’ for conception.
However, Malinowski had a narrow view of these belief systems. He was set on explaining the origins of puzzling societies where there was a matrilineal pattern of ancestry within a patriarchal power structure. This, he argued, was because these cultures lacked an understanding of paternity, of how babies are made. Of the peoples of the Trobriand Islands, part of Papua New Guinea, he wrote that they ‘have a well-established institution of marriage, but are quite ignorant of the man’s share in the begetting of children, the “father” has for the Trobriander a purely social definition: he is the man married to the mother, who lives in the same house with her and forms part of the household’. To Malinowski, ‘the idea that it is solely and exclusively the mother who builds up the child’s body, while the male does not in any way contribute to its production, is the most important factor of the social organisation of the Trobrianders’. So, in a patriarchy without paternity, if the mother cannot carry the status, her brother does, and these ideas had intellectual impacts far and wide. It is now popularly assumed that there were people – at least as recently as 100 years ago – who didn’t know where babies come from. This is probably incorrect.
Malinowski was fixated on his informants’ ‘absence of any idea of the fertilising value of semen’, using it to generalise that they ignored the male’s role. Yet they had already told him that sex with men is what primes a woman’s fertility. So, regardless of the details, sex and men are still helping women to make baby Trobrianders. He also records a belief that spiritual action is, to quote his informants, ‘the real cause of child-birth’. But this is not so odd, nor incompatible with reproductive consciousness. After all, many Americans, in polite or unfamiliar company or in earshot of children, won’t talk about sex; God makes babies. Perhaps the problem was, in part, Malinowski’s own fixation on semen. He quotes one informant’s apparently incorrect explanation for reproduction to be: ‘The mother feeds the infant in her body. Then, when it comes out, she feeds it with her milk’; and ‘The mother makes the child out of her flesh.’ Yet if we wanted to be purely materialistic, this could be considered more correct than the genome-centric view of babymaking. Egg and sperm and the genomes they carry are next to nothing compared with the food, flesh and milk that build a baby. Babies are certainly more mother’s blood and milk than they are egg or sperm, but a person’s intellect might be questioned if they said so today.
Until we can make synthetic babies from scratch in vats, we are stuck with much of the old apparatus of parenting
In the Western world, theories about the relative contributions of man and woman to the making of a baby have also been the subject of much speculation and debate. ‘Preformation’, the pre-Enlightenment idea that we were formed as we look now from the start and merely grow up, imagined something like every woman containing an infinitely stacked Russian doll of her future progeny. Once sperm were first seen by Anton van Leeuwenhoek under his microscope in 1677, a similar concept was transferred to males, reinforcing the image of the womb as a vessel that welcomes impregnation from a man. Around 1870, explanations of conception drew much closer to what we have today. What the anthropologists Steven Beckerman and Paul Valentine call the ‘One Sperm, One Fertilisation Doctrine’ had its origins in 19th-century Austria when Gregor Mendel obtained experimental evidence that a single pollen grain introduced into an ovule produced a well-developed seed.
Recently, The New York Times reported that scientists were on the verge of making ‘people without biological parents’. This describes hypothetical, synthetic human genomes grown inside embryos in vitro. But unless scientists are also synthesising eggs and sperm (both of which contribute more than DNA to offspring), and wombs, and women too, then they haven’t removed parents from their required role in reproduction. The same is true of cloning living or recently extinct species. Any synthetic human genome will be constrained by what works for Homo sapiens and what has worked for more than 3.7 billion years. A synthetic genome is part of a larger process that is influenced by the humans who built it and the agents they used in the laboratory, as well as the woman who carries the pregnancy to term, and all who influence her biology while she is pregnant, and while her own mother was pregnant with her, and all the biological influences from the child’s caregivers. Until we can make synthetic babies from scratch in vats, we are stuck with much of the old apparatus of parenting.
By contrast, scientists have successfully removed some other old-timey necessities from human reproduction. In-vitro fertilisation removes sexual intercourse from the equation, and it also removes seminal fluid because sperm is plucked and washed prior to meeting an egg. But even now, our beliefs are evolving about the necessity of seminal fluid since it’s been recently discovered that its absence likely influences the biology of the placenta and the progeny, particularly the metabolic traits of the offspring.
As ever, our cultural and scientific beliefs are evolving about the necessity of intercourse for making babies. The ‘seminal-priming’ hypothesis suggests that exposure to semen improves fertility for women and couples who, for example, are at risk of developing a pre-eclamptic pregnancy which risks foetal and maternal survival. So although some reproduction is now feasible without sex or semen, it seems not all of it is. And, while all our new-style means for babymaking can affect culturally prescribed kinship, the relationships that arise with a new baby are generally based on knowledge of the provenance of the egg and the sperm, which boil it all back down to that familiar fundamental equation of man + woman + wild imagination = baby. No matter how much we tinker with the specifics, reproductive consciousness remains a constant of human identity, from the Trobriand Islands and ancient Egypt to modern-day New York.
J ust as we don’t know when our ancestors acquired language, we don’t know when reproductive consciousness arose. But we do know that each is ancient, and was present worldwide when European explorers first encountered and reported on isolated indigenous peoples. Because we are human, our ability to explain the unobservable, to understand that men help to make babies and that we are related to one another, has profoundly affected the social structures we’ve devised, the rules about who can have sex with whom, and formed a basis for wide-ranging cooperation on large-scale projects. It has driven our relationship with the plants and animals we’ve domesticated for food and labour, and this in turn has altered the land on which we live. If it didn’t spark the Anthropocene, reproductive consciousness has certainly deeply affected its trajectory. Knowledge is evolutionary power.
Acknowledging the centrality of reproductive consciousness to so much of our past as well as our present gives us a novel way to reframe how we explain much of human behaviour. Culture is something truly extraordinary about our species, and it is intellectually presumptuous to dismiss its role in human evolution. Evolutionary theory doesn’t explain virginity, the use of contraception, or ‘until death us do part’ as convincingly as the simple fact that culture is powerful, and culture is informed by reproductive consciousness, and a whole suite of institutions and conventions built out of it.
There are scientists hard at work on fitting culture into the framework of human evolution. In Not By Genes Alone: How Culture Transformed Human Evolution (2005), the anthropologists Peter Richerson and Robert Boyd suggest that we have evolved unique tribal social instincts, on top of our ancient primate social instincts ‘that allow us to interact cooperatively with a larger, symbolically marked set of people, or tribe. The tribal social instincts result from the gene-culture co-evolution of tribal-scale societies.’ And they suggest that this is why ‘humans are able to make common cause with a sizeable, culturally defined set of distantly related individuals, a form of social organisation that is absent in other primates’. But one aspect of human sociality is desperately missing from these sorts of conversations – reproductive consciousness.
When the creative power of our own bodies dawned on our ancestors, we seized a powerful role in our own behaviour
Our argument is not that culture is more powerful than biology, or vice versa. We simply wish to acknowledge that it belongs alongside biology in our evolutionary reconstruction of sex, sexuality, reproduction and kinship. If culture has been a factor in hundreds of thousands, if not millions, of years of hominin tool-making, then it has most likely also been there for the babymaking. Perhaps it’s just been easier to elevate and to contrast our material culture against all other species’ than it has to incorporate the importance of our immaterial culture. But what’s more material than the manufacture of flesh and blood offspring?
Early on in anthropology, studies of procreative beliefs and kinship such as Malinowski’s were explicitly concerned with understanding human evolution and imagining the lives of our ancestors. However, this style of research fell far from fashion as cultural anthropologists sought to distance themselves from research on human evolution. And rightly so: living peoples are not relics of the Stone Age and are not identical with our extinct ancestors.
Now that the dust has settled somewhat, however, scholars are reviving the connection between uniquely human kinship and its importance for the understanding of our ancient past. Putting it simply, humans have families in ways that no other animals do. In Early Human Kinship (2011), the anthropologist Wendy James encourages thinkers from science and the humanities to come together ‘on the very important question of how evolutionary theory could or should take account of the ordered character of human organisation, specifically … how we try to manage patterns of male-female and parent-child relations, and thus the purposeful outcomes of our own reproduction’.
Just how far back to push that purposeful impact we make on ourselves might never be known. But when the creative power of our own bodies dawned on our ancestors, we seized a powerful role in our own behaviour, and sometimes a powerful role over our descendants, future tribe members, fellow humans, and kindreds of all kinds on Mother Earth. Or putting it in truly human terms: kindred of all kinds on Mother and Father Earth."
Is the mother a container for the foetus or is it part of her? | Aeon Essays,,https://aeon.co/essays/is-the-mother-a-container-for-the-foetus-or-is-it-part-of-her,"After a long wait, last year Bridget Jones once again graced our cinema screens. But things had changed considerably for the heroine, played by Renée Zellweger, in Bridget Jones’s Baby (2016). Following an emotional rollercoaster of romantic misunderstandings and sexual mishaps, Bridget becomes pregnant, a state that finds her in a reflective and deeply philosophical mood, as she sits in her kitchen, baking buns. She looks at the buns, growing bigger inside the oven, and then looks down to her belly and compares herself to her kitchen appliance, wondering: ‘Do I too have a bun in the oven? Is there a baby growing separately inside of me, or am I growing a baby part of my own?’
Once the buns are ready, she tucks in to her baked delights, and gazes longingly at the portion she had set aside for friends and asks: ‘Is it true that I am eating for two? Are there actually two beings to be fed here, or just one that is bigger than normal?’ She also contemplates the option of terminating the pregnancy, and asks: ‘Is it really my body, and therefore my choice to abort? Do I have complete autonomy over my body, and what is actually included in what counts as my body?’ The musings go on and on…
Okay, so Bridget does not actually ask these questions, but philosophers do. The sciences (in particular, biology) have added immensely to our knowledge of how we and other things reproduce, and so it is natural to think that these questions are best left within the scientific domain. But this thought is mistaken, as there are philosophical issues around pregnancy that remain unanswered by the sciences, leaving many aspects of pregnancy a mystery.
To illustrate these, let us return to Bridget Jones. Bridget affectionately calls her pregnant bump her ‘baby’. We will less affectionately (but more accurately) use the term ‘foetus’, which will here be used generically to describe whatever Bridget is pregnant with at any stage during the pregnancy, all the way from conception to birth. We will not be talking solely of Bridget, nor will we be talking specifically about human mothers-to-be, but rather our discussion of the metaphysics of pregnancy will be applicable to any mammal with a placenta. As such, our discussion covers hamsters and hippos, but not, for example, kangaroos. For this reason, we will use the phrase ‘maternal organism’ to refer to the pregnant placental mammal regardless of whether it is a human or has the social status of being a mother. As for the role of the ‘paternal organism’, or father, this is obviously important and significant, but falls outside the remit of the study of the metaphysical state of pregnancy that I am focusing on here.
What, then, is the metaphysical relationship between the maternal organism and the foetus? One possible answer is that the foetus is a part of the maternal organism, just like the maternal organism’s organs and limbs are. Let us call this the parthood model . Another possible answer is that the maternal organism carries, or contains, the foetus, which is a distinct entity in its own right. Call this the container model . So which model is correct, and does it change throughout the pregnancy? This is not a mere matter of choice, nor an argument over the language we use to describe what is otherwise considered to be the same situation. Rather there is a fact of the matter to be found, and as we will see, the truth about the metaphysical relationship between the foetus and the maternal organism will have wide-reaching implications for our moral and legal practices regarding pregnancy. But before looking at such consequences, we shall next take a closer look at these two rival models of the metaphysics of pregnancy.
T he philosopher Elselijn Kingma at the University of Southampton, defends the parthood model in her paper ‘Were You a Part of Your Mother?: The Metaphysics of Pregnancy’ (forthcoming, Mind ), which takes the foetus to be a proper part of the maternal organism. To be a proper part of something is to be a section of a whole, which comprises only some of the whole thing, and is not identified by the whole thing itself. In the case of pregnancy, the maternal organism is the whole, where the maternal organism has many sections, such as the maternal organism’s arms, legs, heart and lungs, etc. The foetus is simply one of those sections. And so we can say on this parthood model that the foetus is a proper part of the maternal organism, just like any other part of the maternal organism.
But it is important to note that the foetus can also be very different from the other parts of the maternal organism, as all the parts are different to each other in their own way, and some can be entities of their own with their own special status as well as being a part of something else. So all that this model states is that the foetus is a part of the maternal organism, but it does not specify what sort of thing the foetus is. On this model, the foetus and the maternal organism are not seen as completely separate things, but rather are related to each other as a part is to a whole. For the sake of an analogy, we can compare this metaphysical relationship between the maternal organism and foetus as being like that of a cat to its tail. The foetus is a part of the maternal organism just like a tail is a part of a cat. But a tail of a cat is not a cat itself – it is just a cat’s tail! Whereas, the foetus might at some point during the pregnancy actually be the same sort of thing as the maternal organism. For example, the foetus can be a human, just like the maternal organism is a human.
So can a human be part of another human? Let us revisit Bridget Jones for some help in visualising this. Imagine that Bridget shaves off her hair on Sunday after a bleaching disaster the day before (easily done). On Sunday, we have Bridget without hair, which is in some sense a part of the Bridget on Saturday, since it is just like the Bridget on Saturday with a part removed (namely the hair part), leaving the bald part of Bridget to roam free. So on Saturday we have the complete Bridget Jones, avec hair, and on Sunday a part is removed, leaving us with the remaining Bridget Jones, sans hair. Is bald Bridget from Sunday a part of hairy Bridget on Saturday? We might think, well, surely not, since if that were the case then on Saturday there would be two Bridgets present – the bald one and the hairy one, where the bald one is part of the hairy one – and that is too many Bridget Joneses existing for anyone’s liking, especially in the same place at the same time.
Perhaps Bridget is making a mistake when she talks to her bump as if it were a human and calls it ‘baby’
For this sort of reason, some philosophers have held a ‘maximality’ principle that restricts the type of thing that one can have as a part to not including the same type of thing as the whole. In other words, the maximality principle in this context claims that no thing can be a proper part of the same type of thing. So no cat can be a proper part of a cat, and no human can be a proper part of a human. This avoids the overpopulation that arises from co-location. Now if this maximality principle is true, then the parthood model of the metaphysics of pregnancy would have to claim that the foetus and the maternal organism are different types of things. This is because if the foetus is a proper part of the maternal organism, and no thing can be a proper part of the same type of thing, then the foetus is not the same type of thing as the maternal organism of which it is a part.
In the case of Bridget Jones, the maternal organism is a human, so what is the foetus if it is not a human? Perhaps the foetus becomes a human in its own right only at birth, once it is no longer a part of the maternal organism. If that is so, then Bridget is making a mistake when she talks to her bump as if it were a human and calls it ‘baby’. According to the parthood model combined with a maximality principle, it would be more accurate for Bridget to look upon her bump as if it were a growth of her own body, where she is growing her very own foetus part which is not a separate human existing inside of her.
This certainly seems to be more plausible early on in Bridget’s pregnancy, because just after the time of conception the foetus is constituted only by a bunch of cells, which you might think hardly qualifies it as human – it doesn’t look, think or act like a human! However, towards the end of Bridget’s pregnancy this combination of views might seem less plausible, since just before birth the foetus does seem to have many of the features of a human (although whether it is able to think and class as a person with rights is a different question). In order for the parthood model to allow for the foetus to be the same type of thing as the maternal organism, which in Bridget’s case is for both to count as humans, the maximality principle must be rejected. Otherwise, if the maximality principle is true, and the foetus and the maternal organism are the same types of thing, then the parthood model must be rejected, as the foetus cannot be considered part of a maternal organism.
An alternative view to the parthood model is the container model, which seems to be the extreme opposite of it, since it claims that the foetus is not a part of the maternal organism, but rather is contained inside the maternal organism. So, according to this container model, the maternal organism is literally a container for the foetus, where the relationship between the maternal organism and the foetus is like that of a niche to a tenant. A niche is something that encloses something else, such that the smaller thing is inside the larger thing, where the smaller thing is classed as the tenant. Think of this model as being like a tenant in a rented house, such that the house is the niche for its occupant. The foetus, as a tenant, inhabits the maternal organism, as a niche. This is a view held by the philosophers Barry Smith and Berit Brogaard in their paper ‘Sixteen Days’ (2003) in The Journal of Medicine and Philosophy. They provide the analogy of the foetus being inside some space in the maternal organism in much the same way as a tub of yogurt is inside a fridge or, as phrased earlier, the way a bun is inside an oven.
This container model (as with the parthood model) does not on its own state what sort of thing the foetus is, but rather claims only that it is contained in something else. Yet the paradigmatic cases of the tenant-niche relationship share the feature that the tenant is a different type of thing to the niche. For example: the renting tenant is a human, and the rented house that is the niche is a building; likewise, the bun that is the tenant is a food, and the oven that is the niche is an appliance. However, in the case of pregnancy, the tenant might be the same sort of thing as the niche, if we class both the maternal organism and the foetus as humans (or whichever animal is in question). If the definition of a niche requires it to be a different type of thing from the tenant, then this would render the container model incompatible with treating the foetus as being a thing of the same kind as the maternal organism. It is therefore of great importance for philosophers to study such definitions and concepts so as to inform the debates in which they’re applied.
T he applications of these models and concepts are not limited to debates in metaphysics: they also feature in debates in reproductive ethics. Many of the reasons that we cite to support our stance on abortion and surrogacy, for example, are based (sometimes unknowingly) on metaphysical grounds, as those debates depend on their philosophical foundations. As such, we will first need to get our metaphysics straight in order to inform our ethical opinions. The metaphysics of pregnancy matters not only because it has such implications, but it also lies within many of our surface values and disputes regarding pregnancy and the ethical questions that accompany it, which can be some of the most divisive social-value questions we face. Hence, the metaphysics of pregnancy relates importantly to issues of reproductive ethics, which demonstrates that doing philosophy is going to be a necessary first step to resolving such debates.
Let us first have a think about surrogacy. The container model is particularly evident in our concept of surrogate pregnancy, and at least appears to underpin most moral and legal views of surrogacy. The container model creates the image of the maternal organism as an incubator or environment for the foetus, where the foetus develops as an independent entity, separate from, and merely inside of, the maternal organism. The very naming of the surrogate as the ‘host’ demonstrates that the surrogate is seen as housing a separate entity, and surrogacy is often described as renting a womb. This shows that the surrogate is seen as a mere container, where the foetus inhabits the space that the surrogate host provides. Surrogacy is widely regarded as a service of gestation, where what the surrogate provides is the use of their body as a space within which the foetus can grow. As a result, surrogacy is thought of as bodily labour, where the work required is to provide nutrients and physical care to the foetus, implying that the role of the surrogate is to be a safe container for the foetus to grow independently. This all strongly suggests a container model in the metaphysics of pregnancy.
But on the parthood view of the metaphysics of pregnancy, the foetus is actually literally a part of that surrogate mother. Rather than renting a space like a womb, surrogacy then appears more like the trade of a body part (namely the foetus, a part of the surrogate). Now if this foetus is itself classified as a human, as well as being a part of the human maternal organism (thus rejecting the maximality principle), then a surrogate transaction is both a trade of a body part and a trade of a human. If the foetus is a part of the maternal organism, then the maternal organism is not so easily interchangeable, as the maternal organism and the foetus are then inextricably connected, much more so than if the maternal organism turns out to be just a container. On the parthood model, surrogacy seems to be more like the sale of a product, where that product is the foetus part of a maternal organism that can itself be classified as a human; this contrasts with the bodily labour of renting a womb that is suggested by the container model.
Pro-surrogacy and pro-abortion advocates appeal to a container model and a parthood model respectively
Now let us cast our eye over the thinking that underpins legalisation that allows some abortion. This might be motivated to some extent by the slogan ‘My body, my choice.’ If the parthood model is true, then this slogan is also literally true, for the foetus is indeed a part of the maternal organism’s body. Of course, the issue will then get complicated if the foetus qualifies as an organism with its own rights, which might conflict with the rights and choices of the maternal organism. This is where the maximality principle will have impact, since it will determine whether the maternal organism has a ‘non-human’ foetus as a part, or a ‘human’ foetus as a part. If the foetus is to qualify as a human, despite also being a part of another human, then it might not be just a matter of implementing ‘my body, my choice’, as that body includes another body. But on the other hand, if the container model is true, then this slogan will not be literally true, as the foetus is not part of the maternal organism’s body. Any appeal to ‘my body, my choice’ must then defend a maternal organism’s right to choose how her body is used, even if it results in the termination of a separate organism.
It is interesting here to note that some bio-political leanings can rest upon incompatible metaphysical views. For example, a more progressive stance that advocates both pro-surrogacy and pro-abortion might end up appealing to a container model and a parthood model respectively, in order to motivate the legitimacy of ‘renting a womb’ while also motivating the legitimacy of abortion as being ‘my body, my choice’. And on the other hand, a more conservative stance that advocates both anti-surrogacy and anti-abortion could end up appealing to a parthood model and a container model respectively, in order to motivate the illegitimacy of a trade of body parts while also motivating the illegitimacy of a termination of a separate being. Yet given the complexity of these debates, the parthood and container models will not on their own be sufficient to determine the legitimacy or illegitimacy of such stances, since the models alone do not determine the status of the foetus or what sort of thing the foetus is, but rather only whether it is a part of the maternal organism or not. What would then be required to make the necessary connection between these models and the bio-political views is some further philosophy, such as an endorsement or rejection of a maximality principle so as to determine whether the foetus is itself a human, and an exploration of the nature of personal identity and the human rights of foetuses so as to determine their personhood and moral status.
So, although no moral conclusions follow directly from the metaphysical models I’ve outlined, we do need to get clear about these metaphysical issues if we want to think coherently about the ethical and political questions surrounding reproduction. It really does make a difference whether you have a bun in your oven or whether the foetus is genuinely part of you.
This essay is part of a project that has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme, under grant agreement number 679586."
Sweden’s hands-on dads and the hormones of fatherhood | Aeon Essays,,https://aeon.co/essays/swedens-hands-on-dads-and-the-hormones-of-fatherhood,"‘Something happens to the men who come here,’ says Lisa Lindell, observing the first arrivals at her drop-in centre for new parents in Malmö, Sweden. One, a straggly haired Israeli scientist, has built a perfect pyramid out of Play-Doh for his daughter. A Swedish restaurant manager is sprawled on the floor as his son presents him with various objects.
‘We think it’s a revolution,’ her colleague Karin Hallback Stigendal chimes in. ‘These daddies here, they’re very much closer to their feelings. They couldn’t harm another person. Children will change their minds, and that will be good for our society.’
Sweden is leading the fight for gender-equal parenting ­– men here get three months of paid, use-it-or-lose it paternity leave for each baby (in fact, many take more) – and Lisa and Karin are its warm, welcoming stormtroopers. Since they started trying to lure dads to their centre a decade ago, numbers have grown steadily until, recently, the fathers began to outnumber the mothers.
‘My patience levels have sky-rocketed,’ reports the restaurant manager. ‘There’s not so much “me” in focus any more. It’s all “him” now.’
The scientist hasn’t noticed much change, but as we bond over having given our daughters the same rare Nordic name, I’m struck by something about the way he is standing, his one-year-old perched on a stuck out hip. He’s one of a legion of Sweden’s ‘latte pappas’, as the country’s legion of scruffy men with prams are known. I’ve been one myself, taking six months off twice over the past four years to be the main carer for my daughter and then son. One afternoon in the playground, I began to notice the high, exaggerated voices many of my latte pappa acquaintances used with their babies – clear ‘motherese’.
I felt compelled to find out if I was imagining it, and quickly discovered an explosion of new research demonstrating the dramatic impact that fatherhood has on men’s hormones – along with their affect and talent for staying attuned. A 2011 longitudinal study of 624 Filipino men showed evening testosterone dropping by a median of 34 per cent in the first month after becoming fathers (the most extreme case saw a drop of 75 per cent). Levels of oxytocin, the so-called ‘cuddle hormone’, almost double in fathers between the time the mothers become pregnant and the first months of fatherhood. Prolactin, the hormone that triggers lactation in women, was almost a fifth higher in fathers of infants than in non-fathers. Fatherhood also physically alters the brain. In a 2014 study , researchers scanned men’s brains in the first month after their children were born, and then again after the fourth month. It turned out that gray matter grew in areas linked to reward, attachment and complex decision-making.
These are the changes recorded in countries such as the United States, Canada and the Philippines, where mothers still do most of the childcare. But could the impact be even greater in Sweden, where men like me often take six months or more off work to be the primary carer for their babies? According to Kerstin Uvnäs-Moberg, who pioneered research into oxytocin at the Karolinska Institute, Sweden’s medical university, no one has tried to find out. ‘This is a unique experiment that we are performing right now,’ and the consequences might be profound.
Lee Gettler, director of the Hormones, Health, and Human Behavior Lab at the University of Notre Dame in Indiana, and the lead author on the Philippines study, suspects that Swedish fathers might be further along the spectrum than the fathers he has studied – truly hormonally and temperamentally transformed. They would have lower testosterone than less involved dads, and their testosterone would stay reduced during the early parenting years. ‘Their prolactin might also be high, and their oxytocin would likely frequently spike up during affectionate, sensitive moments, reflecting father-child bonds and familiarity.’
I’ve seen the changes in myself. For several years, whenever my baby daughter or son made the slightest whimper at any time of night, I would find myself instantly awake and stomping over, as if controlled by a chip in my head. I’m restless and fidgety, but within a week of my first child being born I was able to rock from side to side singing ‘rock-a-bye-baby’ for hours, night after night, and I can still read my children the same story five times back-to-back without getting bored. A few years ago, there was a moment when, cradling my infant son, my nipples began to tingle strongly as if preparing to lactate.
As first one and then two children have absorbed more and more time, my life, and that of my wife, has been reduced to caring for them and working to provide for them. We rarely go out, and the only other adults we meet are those with children who can entertain our own. After a decade working in far-flung places such as India and Kazakhstan, my world has shrunk to two square kilometres of Malmö bounded by my children’s daycare, the playground, my office and my home. The strangest thing about this is not that it’s happened, but that, nestled in my warm oxytocin cocoon, I don’t mind.
T hinking about the effects of hormones on fatherhood makes my Swedish wife cringe. It seems anti-feminist, suggestive that childcare is naturally a woman’s job. This is something Uvnäs-Moberg recognises from her own work on oxytocin: ‘I was attacked, not by men but by the feminist movement,’ she says. They feared that the findings would provoke a ‘backlash, that if women have hormones, that would be a reason for pushing them back into the kitchen and to caring for children’.
Widespread response to research from the Philippines shows that these concerns are justified: media coverage has focused almost exclusively on the finding that the more new fathers’ testosterone levels fell, the less sex they had with their partners – even though parenting itself is exhausting and testosterone does not seem strongly correlated with male libido. Most popular conversations about testosterone involve ‘societal definitions of masculinity’, Gettler states.
But even some prominent researchers on fatherhood suspect that Sweden has gone too far.
Peter Gray, an anthropologist at the University of Nevada, Las Vegas, says that if Sweden achieved gender-equal parenting, it would be the first society ever to do so in human history. Fathers from the Aka Pygmy people of central Africa, perhaps the world’s most engaged fathers, do almost everything that mothers do, even down to suckling. But in camp they hold their babies for a creditable, but still less-than-equal, fifth of the time. ‘Sweden is asking fathers to just focus on the direct childcare, and not to have to worry about resource acquisition, and I think that might be challenging,’ Gray fears.
From my experience, though, I can tell him that it isn’t. At no point in my own combined 12 months off work did I ever feel I was doing anything unnatural or against the grain, and I have yet to meet a father in Sweden who has described feeling that way.
The flexible physiobiology governing parenting in humans likely explains my report – starting with the changes in testosterone itself. According to the Philippines study, fathers who live with their children had testosterone levels on average more than a sixth lower than those who live apart. Those who co-sleep with their children saw greater declines in testosterone than those who did not. The hormone is double-edged: testosterone makes both men and women more muscular, more likely to take risks, more socially outward-looking. Trial lawyers have higher testosterone than their office-bound counterparts. Testosterone seems to encourage people – both men and women – to take a dominant role. Less testosterone, on the other hand, has a mellowing effect. A 2014 study of Israeli fathers found that those with lower levels of the hormone were more affectionate, tactile and communicative with their infants, and more frequently used that motherese.
Then there’s the power of oxytocin. ‘There are multiple pathways to increase your oxytocin levels and to ignite the parental brain, and there are pathways for fathers which can make that system work just as well as in mothers,’ says the developmental neuroscientist Ruth Feldman of the Bar-Ilan University in Israel. In the past, she points out, many mothers died in childbirth, ‘so evolution must have tailored the parental brain as something very, very flexible and adaptive.’ Far from transgressing some natural role, human fathers’ engagement in direct childcare might have been crucial to our evolutionary success.
the longer the leave that fathers took with their babies, the more they saw them later in life, even if they had separated from the mother
Traditional fatherhood is enhanced by hormones, too. Vasopressin and prolactin have been associated with ‘orienting infant attention to objects and the outside world’, Feldman explains. A 2010 study by Ilanit Gordon and colleagues at Feldman’s lab, found that the higher the base prolactin levels of fathers, the longer the bouts of exploratory play they engaged in with their children. In birds, prolactin seems to be involved in more indirect forms of paternal support, so-called ‘provisioning’. Some seabirds maintain high levels of prolactin over long voyages, perhaps to increase the chance of their returning to their young. Gettler points to research showing that human fathers with higher prolactin are also more alert and responsive to infant cries. Feldman has spent the past six years studying gay couples who father children with surrogate mothers, and she has yet to find any difference between their offspring and those of heterosexual parents, which suggests to her that at least one of them is doing everything that a mother does.
‘A higher level of oxytocin directs you to focus more on play that is face-to-face, and vocalising, and this kind of affectionate contact,’ she reports. ‘For mothers, it comes more naturally, because pregnancy makes your whole brain soak in oxytocin. For fathers, it’s much harder work. If, for 10 minutes before you go to your newspaper and television, you play with a well-fed and bathed infant, you know – “I’d like to see my kid when I’ve come back, between my pipe and my New York Times ” – that’s not going activate any hormones.’ Triggering oxytocin takes physical contact, particularly skin-on-skin.
And to any men still smirking at the hit that engaged fathers seem to be taking to their testosterone, take note of this: dads involved enough to garner the full hormonal effect will be rewarded by a more intense bond with their children, throughout life. One Swedish study from 2008 found that the longer the leave that fathers took when their children were babies, the more they saw them later on in their lives, even if they had separated from the children’s mother.
B ut what happens to Sweden’s latte pappas when they return to their workplaces, their brains marinading in oxytocin and prolactin? How many, like me, find their priorities shifting in subtle but significant ways?
When Uvnäs-Moberg had her own children, the effects were so powerful that it changed the direction of her academic career. ‘That’s why I started to work with oxytocin, because I noticed these differences, in emotion but also in priorities,’ she says. ‘I was a scientist, working with the gastrointestinal tract, but that wasn’t interesting any more.’
This might be one reason why some women who have fought tooth and nail to rise up in competitive careers abandon those careers without qualms once they have a child. Could something similar be happening to Sweden’s latte pappas?
Uvnäs-Moberg describes oxytocin as the opposite of the fight-flight system. ‘It is the calm and connection system: you reduce your arousal, you become calm, and you become friendly and not so anxious, and also your blood pressure gets lower. Your need to be at the top of a hierarchy decreases, and you become more interested in social networking and other affairs. That may be good when your babies are small, but after five years you may regret it.’
a temporary lull in the competitive drive of returning fathers is more than made up for by the retained expertise of the mothers better able to continue their careers
It could be that anyone, male or female, who combines parenthood with competitive careers and positions of power will, as a result, be a less patient and attentive parent. There might be a trade-off between being a status-hungry careerist and a nurturing parent, whatever your gender.
‘You as a journalist have flexibility, but I’m sure that if you are the head of an investment bank, it could be catastrophic,’ Uvnäs-Moberg argues. ‘What I see in the institutions where I’ve been working is that the real careerists don’t do this. There is a group of men that are so testosterone-linked, that I don’t think they will ever stay at home with the children.’
Yet big Swedish companies such as the tech giant Ericsson actively encourage their male workers to take longer parental leave, which suggests that Sweden’s latte pappas remain useful employees. Perhaps a temporary lull in the competitive drive of returning fathers is more than made up for by the retained expertise of the mothers better able to continue their careers.
Back at the drop-in centre, Lisa and Karin remain convinced that giving all fathers a six-month dose of hands-on parenting would vanquish forever the brash, aggressive, insensitive man. ‘If you are closely connected to a child,’ Karin says, ‘you can’t be tough and hard.’
Soon, the assembled mammas and pappas begin squeezing themselves into the centre’s back room for the daily singing session. As they come to the final rousing ballad, which everyone sings while wafting a multicoloured parachute in the air, memories well up of the hundred or more mornings I must have spent in this room, rocking from side to side with either my daughter or my son on my lap. Whatever the final impact of Sweden’s experiment on fathers, there’s one latte pappa who will never be the same again."
Twins provide a mirror for societal changes and human nature | Aeon Essays,,https://aeon.co/essays/twins-provide-a-mirror-for-societal-changes-and-human-nature,"In Greek mythology, twins Castor and Pollux are so tightly bound that when mortal Castor dies, divine Pollux surrenders half his immortality to stay with him, and the pair are transformed into the constellation Gemini. In a modern reversal of the myth, twin astronauts Scott and Mark Kelly were reunited in March after Scott returned from a year’s stint on the International Space Station – an event eagerly awaited by medical researchers who saw their chance to conduct tests on the twins to assess the effects of space on the human body.
Twins are very much in our consciousness at the moment, not just because they’re pushing the final frontier, but because we are living through a twin boom. A higher proportion of human beings are twins than at any other time in living memory. This may not be obvious to the naked eye – the vast majority of twins are fraternal, or non-identical, meaning they are no more similar, on average, than siblings who aren’t twins – but we’re aware of them nonetheless, in our neonatal wards and classrooms, on our sports fields and social networks. One group of people that has paid special attention to the current boom is demographers. That’s because, for the first time in history, they find themselves in a position to measure the boom and investigate its causes.
By the middle of the 20th century, all developed countries and a good number of less developed ones had begun to gather birth statistics systematically, and to record the births of multiples, making international comparisons possible. The most comprehensive global picture of the twin boom, comprising data from 32 countries, was published last December by a trio of European investigators. It confirms that the phenomenon is global, but reveals that it has played out differently in different countries. In doing so, it has fleshed out an idea that was floated over a decade ago: not only are we alert to twins, but they are alert to us. To put it another way, the twinning rate is a highly sensitive indicator of biological and social change.
That is a radical notion, because for a long time researchers assumed that the twinning rate was a constant of Homo sapiens biology. The gathering trickle of data, however, has unhinged that assumption. Take France, where the proportion of live births that were twins hovered around the 10 per 1000 mark for much of the 20th century. Birth statistics exist in France from as far back as 1700, and when Gilles Pison, a demographer at the Musée de l’Homme and the National Institute of Demographic Studies in Paris, started to look at them, he made a startling discovery. With his colleague Nadège Couvert, he found that the twinning rate had undergone some dramatic swings over the last few centuries. Around 1720, it was up around 15 per 1000 births – a level it wouldn’t reach again until 2000. By 1800, it had plunged to almost half that – fewer than nine per 1000 – and the next time it would fall so low was around 1970. What was going on?
O ne thing was immediately clear: the variation all sprang from dizygotic or non-identical twins. Monozygotic or identical twins derive from a single egg fertilised by a single sperm that splits early in gestation, while non-identical twins derive from two eggs that are fertilised by two separate sperm and implanted in the womb at the same time. They are quite different biological phenomena, and as such, subject to different laws. The monozygotic twinning rate is stable at around four per 1000 births – a rate seen in all mammals with the exception of the clade oddity, armadillos (whose offspring are monozygotic quadruplets or octuplets). All women seem to be at roughly equal risk for identical twins, making them a kind of genetic one-off. Non-identical twins, on the other hand, run in families. They are more common in some parts of the world than others, reflecting genetic influences on hormones that cluster in populations. They become more likely with increasing birth order – that is, later in a ‘litter’. And their incidence is exquisitely sensitive to a mother’s age.
A mother’s age was, in fact, the biggest single determinant of the twinning rate for most of the human story. While female fertility starts to fall off steeply in the early 30s, a woman’s risk of having fraternal twins increases linearly from puberty to age 38. That’s because blood levels of follicle-stimulating hormone – the hormone that triggers ovulation – increase with age, in turn increasing the likelihood that more than one egg will be released in a given cycle (the chances of more than two eggs being released are tiny, however; twins are by far the most common type of multiple birth). Around 38, the overall decline in fertility becomes the stronger of the two effects, causing the incidence of fraternal twins to fall off again.
The early French data are sketchy and to some extent unreliable. Infant mortality was much higher in the 18th century, for example, and stillbirths may not always have been recorded. They also derive exclusively from rural populations, though this is less of a problem since 85 per cent of the French population was rural at the time. Despite these caveats, Pison thinks they reflect the forces shaping families back then. In the early 18th century, French women had their children relatively late, because marriage had to wait until a couple could afford to establish themselves independently. Families were also big, producing five or six children, on average, and both these factors favoured twins. The revolution at the end of that century led to a relaxation of traditional family values and the widespread though illicit use of birth control (mainly the withdrawal method). Mothers became younger and families shrank – a trend that didn’t become visible in Britain, by contrast, until the end of the 19th century.
the twin boom has put the brakes on a longer-term, global trend towards lower infant mortality
Following the French curve into the 20th century reveals still more unexpected gems. France saw a massive spike in twins in 1919, just after the First World War. Pison thinks two factors contributed to this. Firstly, the war put reproduction on hold for many couples, so that women tended to be older than the norm for the period when they finally became mothers. Secondly, a selection effect was at work, whereby the most fertile couples were the first to conceive once the men returned home. Since the most fertile are, by definition, also the most likely to produce a multiple birth, the twinning rate soared in that first year of peace. Something similar happened after the Second World War, especially in the United States and Australia, where men were separated from their wives and fiancées for even longer stretches than in Europe.
In the decades after the Second World War, most industrialised countries saw a slow decline in their twinning rates, which bottomed out at around nine per 1000 in the early ’70s. The reasons overlapped with those seen 200 years earlier in revolutionary France: a loosening of mores, sometimes called the sexual revolution, and increasing elasticity in notions of family. There were differences, however. The pill played its part this time, though it interacted in complex ways with the new feminism: while married women used the pill to limit and space their pregnancies, the proportion of babies born to unmarried women grew – in part because single mothers were less frowned upon, in part because men relinquished responsibility for contraception to women. Overall, the effect was that families shrank and first-time mothers became younger again.
These were the baby boom years, but twins were rare. The current twin boom didn’t take off until the ’70s, with the advent of assisted reproductive technology (ART) – the umbrella term for fertility treatments that collectively boost the twinning rate by increasing the chances that more than one embryo will be implanted at a time. Over the same period, partly because of women’s massive incursion into the workplace, mothers got older again. The twinning rate doubled in only 40 years, to 15 or more per 1000 across the industrialised world. Today, in Europe and America, roughly every 60th person is a twin.
To understand the impact of ART on twinning, it helps to take a step back and look at pre-ART twinning rates across the globe. Already, a few decades before the ’70s, there were enough data to show that these vary from country to country. The ‘natural’ twinning rate in the US is slightly higher than that in France, for example, which in turn is higher than that in Japan. Some of the lowest rates in the world are found in Asia, while the highest occur in a band across Central Africa. Benin, the twinning champion of the world, boasts an extraordinary 28 twins per 1000 births . It has been suggested that the high Central African rates have something to do with local diets, and in particular with a chemical found in yams, but the fact that descendants of Central Africans who were born and raised in other parts of the world also have high twinning rates, suggests that the causes are more likely to be genetic.
Twins possess a highly developed theory of the other’s mind and an ability to work together that surpasses that of any other human dyad
Against that backdrop, the recent 32-country analysis published by Pison and sociologists Christiaan Monden of the University of Oxford in the UK, and Jeroen Smits of the Nijmegen School of Management in the Netherlands, allows a comparison of the evolution of the twin boom across the globe – and of the factors that have shaped it in each country. The two main factors are ART and maternal age, and the researchers made an attempt to disentangle the effects of these. They did so by calculating what the increase in twinning rates would have been, had the age distribution of mothers held at 1970 levels, and comparing that to what they actually observed. They found that, overall, the effect of ART in driving the boom has been about three times as important as that of rising maternal age (remarkably, ART even seems to have boosted the identical twinning rate slightly, though exactly how or why is not clear). Once again, however, there is variation between countries – this time reflecting cultural differences. In Japan, for example, the boom has been ten parts ART to one part older mothers, whereas in Catholic Poland, where ART is not popular, a mother’s age is by far the dominant factor.
The trend towards older mothers is global. Already well-established in the developed world, it is becoming increasingly visible in less developed countries too, whereas ART remains the preserve of the wealthy. In sub-Saharan Africa, for example, having a baby – let alone twins – is still so dangerous for both infant and mother that undergoing ART is a risk not worth taking, even if it were widely available or affordable. Overall, Pison says, it looks as if the rest of the world is converging towards the level that Benin achieves naturally, and some countries have already come close. Greece and Denmark had broken through the 20 per 1000 barrier by 2010, for example. Whether they will reach 28 per 1000 is a moot point, however, since there are indications that the boom is slowing down. In some parts of the world, such as Hong Kong, the twinning rate is still growing fast, but in others it has plateaued or even gone into decline. The latter is true in the Netherlands and Norway, for example, while in France and the US, though it might be too early to tell, it appears to be levelling off.
Why the slowdown? One contributing factor might be a change in medical attitudes towards ART, that reflects better knowledge of the risks associated with multiple births. Twins are more likely to be born prematurely and to have lower weight and complications at birth than singletons, and there is now good evidence that these things are associated with health problems later in life. Their risk of dying in their first year, though small, is on average four times higher than that of singletons, meaning that the twin boom has put the brakes on a longer-term, global trend towards lower infant mortality. For all of these reasons, plus the added expense of two babies instead of one, twins take their toll on families. Mothers of twins are twice as likely to develop post-natal depression as mothers of singletons, and twin parents have a higher risk of divorce too.
Starting around the year 2000, some governments – notably those of northern European countries, Australia, Japan and the US – began to promote a ‘one-at-a-time’ policy, advising doctors dispensing in vitro fertilisation (IVF) to return no more than one embryo to a woman’s womb. By then, advances in IVF technology meant that returning more than one was harder to justify anyway. In the ’80s, there was no way of knowing which was the healthiest embryo in a batch, so doctors put back several in the hope that at least one would ‘take’, and couples accepted the risk of a multiple birth. By 2000, thanks in part to new screening techniques, it had become possible to select the healthiest embryo. The one-at-a-time policy is legally enforced in some countries, but only a guideline in others, which may help explain why the boom seems to be petering out faster in some than in others.
S o what of the future? If we have learned anything from the realisation that the twinning rate is a barometer of change, it is that the story doesn’t end here. There have been twin booms before, and there will be twin booms again. We might even be able to predict them, to some extent, given that we now know many of the conditions that favour them. If fertility treatments become more affordable, and neonatal healthcare improves, sub-Saharan Africa might see a ART-fuelled boom, for example, while the Chinese twinning rate is likely to increase as the one-child (per family) policy is phased out, and more and more Chinese people are lifted out of poverty.
Before we exit this particular boom, however, we should pause to meditate on how twins have contributed to the modern world, and in particular to our understanding of what it means to be human. They undoubtedly impose a strain on society, as parents and the staff of neonatal wards can attest. But as Nancy Segal, director of the Twin Studies Center at the California State University, Fullerton (and a twin herself), points out, twins also bring some unique and valuable attributes to the human party – in the best instances, absolute mutual trust, a highly developed theory of the other’s mind, and an ability to work together that surpasses that of any other human dyad.
Of more relevance to the non-twins among us is what twins have revealed to us about ourselves, since for the last 150 years twin studies have provided one of the most powerful tools available for probing the roots of human behaviour. In such a study, monozygotic and dizygotic twins are compared to try to disentangle the relative contributions of genes and environment to everything from personality to intelligence to physical prowess and mental health. To give a recent example, a study published in May found that twins have a higher likelihood than non-twins of surviving into their 60s – an effect that is stronger in identical twins than in non-identical ones. As a possible explanation, the authors suggest that the deep bond between twins buffers them from life’s stresses – thus bolstering the idea that we might enhance quality of life for seniors in general by helping them to cultivate and maintain their social networks.
In the ’50s, just before the current boom took off, human behaviour was considered to be shaped almost exclusively by the environment. Now, thanks to twin studies, it’s not possible to think about behaviour without invoking at least some genetic contribution – and that psychological shift has had practical consequences. In her book, Entwined Lives (1999) Segal gives the examples of mathematical skill, social non-conformity and weight gain, all of which are now recognised to be at least partly under genetic control. Realising that, she wrote, has been ‘the key to informed appreciation of individual diversity in response to teaching methods, political events and dietary practices’. Such insights may go some way to explaining our enduring fascination with twins. Like Narcissus gazing into the pool, when we gaze at them, we are not so much seeing double, as seeing ourselves."
‘My daughter came out. They handed her to me. She was dead’ | Aeon Essays,,https://aeon.co/essays/my-daughter-came-out-they-handed-her-to-me-she-was-dead,"A month before Nadia was born, I bought her a pair of tiny newborn shoes. Soon I was hit by artistic inspiration: I would grab my camera and make a composition of her tiny shoes, and my four-year-old Luca’s suddenly gigantic ones. The tug of that thought was followed by repulsion of another: let’s wait and see if she’s all right. Luca had been born small for unclear reasons, and the doctors thought we should check if Nadia was growing well. Between these two children we’d had two unsuccessful pregnancies, one of which ended several weeks after I’d seen a heartbeat on the screen of the ultrasound, a relatively rare event. So when we went in for that scan two weeks later, at 37 weeks gestation, I was apprehensive.
As the sonographer measured, I looked at the numbers on the screen and kept my calm. But when she said she’d like a second opinion, I assumed the news was bad. She took us up to the foetal cardiology unit and put us in a quiet little room where we would wait for a specialist. I looked out of the window, my head churning with bleak scenarios, hoping that all it would take would be an operation. André, her dad, was unconcerned. He tried to amuse me by reading an article from a magazine, and I half-pretended to listen. What is wrong with my daughter?
‘There is no easy way to say this, but your baby is very, very sick,’ the specialist informed us in her soft voice. I remember going back to the little room, André and I alone. I remember her remarking how it would be best if I didn’t eat or drink anything before she left us there; I understood this meant I might have a caesarean that evening. ‘We wasted a good name,’ I thought, ‘I liked Nadia .’ We couldn’t possibly re-use it, could we?
Then I cried and told André I couldn’t go through this again. We wanted three children but it wasn’t working and I wanted to focus on the perfect little boy I already have. I had been pregnant or sick for most of his life, then burnt out with finishing my PhD in the midst of all this while desperately hoping to make it in my highly competitive field of science. I’d had enough of false hope, I’d had it with the nausea, the first-trimester drowsiness, the fear that something might go wrong again, the being alone in foreign countries among acquaintances – not friends – while dealing with pain and loss and guilt and regret, and the clock ticking away the days along my spine as I tried, repeatedly, to grit my teeth and keep up with the world.
‘I want a puppy,’ I said. André doesn’t like dogs and he teased me for my attempt to exploit this moment of weakness. Within two days, I had my heart set on a basset hound. I didn’t think we’d really be getting one. Then we both cried.
An obstetrician called us to perform a third ultrasound. He determined that the blood flow to Nadia’s brain and other organs was still fine. Then he sat with us, together with the first doctor, and they told us that we had to make a choice.
We could get into an ambulance and go to Southampton, where I would have a caesarean and Nadia would be put on a machine called an ECMO (for extracorporeal membrane oxygenation) for some time, ‘to give her heart a rest’. Then she would be treated with drugs, and she would have to stay in hospital. Perhaps for six months, perhaps for a year. She might not survive the birth, or the ECMO machine, or the medication, but if she did, there would be some chance of a full recovery. And some chance of a partial recovery, a life with a debilitating heart disease, or disability, or both.
The alternative path would be to ‘put her to sleep’ with a needle to the heart, after which her birth would be induced and it would all be over.
T he doctors said they had never before seen a heart so dysfunctional. Nothing about it was right. It filled more than half the chest cavity, the valves were not closing properly, the blood was not flowing in the right direction, and there was barely any contractility. The word ‘recovery’ was accompanied by the word ‘miraculous’. And the very idea that an impressive Oxford university hospital might not have sufficient resources to provide care set off alarm bells.
A torrent of fragmented thoughts fought for my attention, each one replaced by another before it could crystalise. The internet is full of medical miracles, babies who survive despite all odds and parents who are infinitely grateful for the presence of such children in their lives. Yet these are tales from early childhood. A disengaged, calculating inner voice told me that a newborn doesn’t need much more than a brainstem in order to be absolutely adorable. It would yawn and sneeze and blink and we would melt, forever tied to this creature that might leave us. But what would it be like if she didn’t walk at four? Or talk at six? Or learn to control her bowels at 14? Or what if she was cognitively and functionally intact, but her heart could stop beating at any point during childhood?
Would it be the end of my career? It probably would. I like my work and in the few short periods of unemployment in the past I was absolutely miserable. Would she make me forever miserable?
Would our marriage survive? I thought it would, but I wasn’t sure about our happiness.
But if we don’t try to save her, will I ever forgive myself for not trying, for not having the girl that had some chance, however small, of complete recovery? Would the decision to kill my daughter haunt me forever?
Would the birth of this baby mark the end of happiness for my lovely little Luca? And one day when we die, would he then be burdened with caring for Nadia? Should we be making this decision for him when he’s just four years old?
I didn’t want to carry a dead baby inside me, not even for a day or two
I could have clung to this last thought to make it simple. But, cruelly, my mind took another twist: if our first child had been very sick, given the chance to have a healthy second child, I would have had one in a heartbeat. Fuck you, logic.
André asked the doctors a number of questions, but said little. In normal circumstances we joke about how he tries to micromanage me, yet in the coming days he would support me completely to make the decisions for us, putting me in a place of both power and vulnerability. I knew without asking that he thought we should not have this baby. His preparedness to support my decision, whatever it would be, showed me how deeply I am loved. But the enormity of that decision! It wasn’t just about deciding what was right for us as a family. The alternative – putting a needle into Nadia’s heart and poisoning her and then delivering her – made me shudder with revulsion. I didn’t want to carry a dead baby inside me, not even for a day or two. I didn’t want my contractions to squeeze around its dead flesh, I didn’t want to push its limp body out of me, I didn’t want to see it and I couldn’t bear to think about it.
And just like that, Nadia had become ‘her’, then ‘the baby’, then ‘it’.
I asked if we could go home to think about this. They told us we had until the morning; she had very little time left.
T hat night we sat on the couch, clinging to each other. At some point we started leafing through the medical folder. We found a short discharge note, where Nadia’s condition was described together with our options. But there was something new here. A sentence was added at the end, almost as an afterthought: ‘There is also the possibility of palliative care.’ We had to look up the meaning of those words. I was a bit confused. I had asked whether we could simply do nothing, let nature run its course, and the doctors told us that this was not an option. It looked like we had to take aggressive measures once we knew, by pure chance, that her heart was failing. I assumed there were some legal barriers to withholding action. Now it appeared we could simply let her die in peace. It was sad for her, it was tragic for us, but she was very sick and life isn’t always kind to everyone… It felt right to allow her to die.
People reacted to the news in a myriad of ways. Some hoped she would make it despite the prognosis. They hoped the diagnosis was a mistake. They suggested doctors around the world that we could consult. They told us of other couples who went on to have healthy children afterwards. The problem was, hope was the last thing we needed. We needed to brace ourselves for what was coming, and we needed our friends and family to accept that reality. Frankly, I took most comfort from people who just swore liberally – that summed up my opinion of our situation perfectly.
As for Luca, we sat him down on the couch between us and told him that the doctor had informed us that Nadia was very, very sick, and that she was going to die. We told him this meant she wouldn’t be able to move, and she wouldn’t be able to breathe, and she would not come to live with us; that, in fact, he would not have a little sister. We told him we’d be sad and that we would cry a lot. He asked if he was going to die. We assured him he wouldn’t. I drew so much sanity and comfort from his presence.
As I waited for Nadia to die inside me, I couldn’t stop the constant feeling of alarm that clashed with an all-consuming fatigue. I hated not knowing whether she would move again whenever she calmed down. I would wake up with a sick dread in the pit of my stomach, certain that I hadn’t felt her in a long time. Or I would panic that it must have been hours since the last kick. I wanted to allow her to die, but I didn’t want her to be dead. Oblivious to her prognosis, she continued to kick up a storm every few hours.
I t was an appointment with a couple of palliative care paediatricians that changed everything.
‘So what happened?’
I realised only later that they must have known, probably better than us, what was going on. They were asking to get a sense of us, of what information we might be able to take in.
‘And what would you like to happen?’
I choked up in tears. I knew exactly what I would like: I would like her to survive until the day I went into labour. I would like her to survive the delivery. I would like to meet her. And I would like to hold her as she died. But what could my wishes possibly matter?
‘And what do you fear most?’
I couldn’t bring myself to say that I was afraid this might alienate me from André.
‘What would you like to know?’
What she will look like when she’s born – if she’s not alive.
She would be swollen, especially her belly, but she’d otherwise look like any other infant. And if she makes it through the delivery, she might not be able to breathe, so they would wait before cutting the umbilical cord, to give us a few minutes with her. The doctor’s words were precise, straightforward, no-nonsense, and his approach soft, and this combination resonated with our need to know things and to be sheltered from the blows of this knowledge at the same time.
The other paediatrician told us that parents experience this short time with their terminally ill infants as meaningful and important – and that my wish was common. That was good to hear, because I was dreading watching Nadia die as much as I wanted to be there for her. She then told us about morphine and feeding tubes. I liked the precision of those statements as well. ‘Keeping her comfortable’ – the sentence we usually encountered when inquiring about palliative care – was too vague for me.
The two of them wanted to hear how we were handling Luca. They asked us about our beliefs. They gently probed to see how we would feel about a post-mortem. They took us through a number of themes while letting us do most of the talking. This signalled their stable, anchoring presence, but also their willingness to hold back and let it be about our decisions, our needs.
They would echo our words back to us before responding, sometimes rephrasing our confused questions first. This allowed us to see that they understood us, and it allowed them to steer the conversation.
And then they told us that we wouldn’t have to spend Nadia’s short life at home. We could all come to Helen House in Oxford, a charity for parents and children in our situation, where we would be cared for both medically and emotionally, during her short life and just after her death.
In the space of a couple of hours, they transformed what we were going through from something purely dreadful to something potentially meaningful. Then one of the doctors asked the question I wasn’t braced for, a question that hit me in the gut: did we perhaps have a name?
She would have been nothing more than a collection of hopes and dreams that didn’t pan out
I could hear the rush of blood in my ears. Her name is It! The Baby! I just couldn’t. Say it. The name. Her name. It . I struggled, and he said we don’t need to say it out loud. He understands . About her name.
Up to that point, her role in my life was ambiguous, and she could have taken on one of two identities. In one scenario, she could be a baby we never had, just like the ones we lost in the first trimester. We could wait for her to die, I could deliver her, and we could ask never to see her. She would have been nothing more than a collection of hopes and dreams that didn’t pan out, dreams (quite literally) embodied in this baby that never was. In the alternative scenario she could be a person, a person whom we might not have the chance to meet properly, but a someone nevertheless. Whether she would become one thing or the other depended on the story we would adopt about her life. By asking for her name, the decision about her story was tipped to one side, and if I said it I would be committing to a far more painful loss – but a far more meaningful one.
I tried to say her name again, but it got stuck in my throat. I turned to André for help, and he hesitated, not sure why I was silent. ‘Say it!’ I managed to croak before drowning in another wave of tears. And he did. Nadia. Her name is Nadia .
In that single powerful moment, she stopped being the baby inside me and she became my daughter.
I n one of those scenarios where the universe conspires against us, the delivery turned out to be difficult. It ended with unfamiliar faces around the bed, talking rapidly to each other above my head while I tried to catch what they were saying about Nadia’s shoulders. Was everything all right with her little shoulders? I could still feel her kicking. There was a flash of sharp pain. I felt my body jerk before it hit me, and it seemed as if the snip of the closing scissors reached my ears much later. Just as these disjointed sensations were about to assemble into a unified percept, I heard a scream before realising that it came from me. Then two pairs of hands pressed down on my belly. I felt that familiar surge of relief as she came out. They handed her to me. She was dead.
Nadia was dead. Her face was too dark and her eyes were closed and I saw it immediately. ‘She didn’t make it,’ I said as I hugged her. I thought I caught a flicker of relief around the bed: nobody would have to tell me.
when Nadia was placed in my arms, the rest of the world faded away and I was there with every fibre of my being
She was warm. And she felt nice. And she was big! The transition from a foetus to a newborn is usually the transition from something huge, taking up all the space in the belly, to something tiny once it escapes into the outside world. But she was fully formed and her head was nice and round, and her weight felt pleasant on my body. It was strangely comforting to know she’d grown well. André told me he needed to go out for a bit, and I briefly wondered whether he was crying for Nadia or for me, out of anguish or out of relief.
In the days building up to this moment, I was often there, trapped inside my body with all the torment, but sometimes (without meaning to) I would simultaneously exit to be an outside observer, detached and impassive. But when Nadia was placed in my arms, the rest of the world faded away and I was there with every fibre of my being. Fully present, and eerily calm. My baby. Events were still unfolding around me, with time strangely dilated while the universe consisting of myself and Nadia was brought into sharp relief.
‘In all likelihood, your next pregnancy is going to be just fine.’ A distant echo of a sentence I had heard, in a doctor’s office in another hospital, in another country, where I sat, worried that there might be a problem with us, trying to see if some tests could be done before our time for having babies ran out. In all likelihood, that doctor is right most of the time.
I still had no idea what death would look like. I imagined a rapid transition from what I held in my arms to something I wouldn’t want to look at or touch. But I had some time to get to know her warm little body before that.
They gave me a towel to wrap her and I used it to remove from her some of the sticky mess that comes with being born. Then I kissed her on the temple, the way I kiss Luca every day. I ran my hand down her back, feeling her spine. I looked at her sharp, long nails, held each of her tiny hands. I touched her shoulders (they were all right), her elbows and knees. Each of her toes, and the spaces between. I rubbed her heels with my thumb. I ran my fingertip along her gums, thinking about the teeth that would have appeared some day had we been dealt different cards. I felt her tongue, wet and squishy. I gently tried to pry one of her eyes open, but it was shut tight and I didn’t want to apply too much pressure. I know it would have been baby blue-grey. Her belly was very swollen, poor little thing. Her eyelashes were shorter than Luca’s. Her hair was… disgusting. Wet and bloody and slippery. Just like Luca’s had been, before we gave him a bath and he became beautiful.
I offered Nadia to André, to snuggle with her while she was warm. But it would be another full day before he was ready to hold her.
I wanted to wash her, but I wasn’t allowed to get up yet, and I didn’t want to press André into doing it. I told the midwife there were some clothes for Nadia, in the side pocket of my bag. André was surprised, he didn’t know I’d prepared clothes. Just one outfit; I didn’t think we’d need more.
Nadia was wrapped in a blanket and placed in a cooling cot, two teddy bears above her head
We affectionately called her a fatty when we saw her dressed, diaper and all. People were talking to me, but by the time a sentence finished, I’d lose the beginning. I was dizzy. André, to my horror and amusement, asked the obstetrician what the chances were of a delivery being so difficult next time. Next time. NEXT TIME!
Good luck with that.
We were then taken from the delivery room into the hospital bereavement suite. Nadia was wrapped in a blanket and placed in a cooling cot next to our double bed. Two teddy bears were placed above her head. It was late, and at some point just before dawn I managed to fall asleep, briefly. In the morning I thought about washing her, but I felt so shaky that I was afraid to. The smell of drying blood and vernix hovered above her. It bothered me. I took pictures of her with my good portrait lens. Her face was very purple.
N adia was a big baby. Her feet would have fit in those newborn-sized shoes, but only just. Her toes were long. Her left foot was larger. Her right ear was still a bit curled, her left one perfectly formed. Maybe she would have been left-handed, like me?
I’d like to think that I’m a rational person, but my devastation was punctuated with brief flashes of hope as I watched her, over the next few days, when there was clearly no doubt about her state. Someone would walk near where she lay in her cot, and I would instantly twitch towards her, watching for any sign of motion. I didn’t pick her up anymore because I felt too weak, and was afraid I might somehow hurt her. I would get up at night and hover over her, just like I used to stop by Luca’s crib to watch him breathe. I knew, yet I couldn’t help hoping. It was a deep-seated, visceral response to her presence. At the same time, I had an incongruent visceral response to her state. She was dead, and I would first be repulsed by her colour whenever I laid eyes on her, then I would manage to put it in the back of my mind while I admired her.
I kept wanting not to see her anymore, and wanting to see her again, for it to be over and for it to never end. I liked holding her little hands. I liked stroking her feet. But mostly, I liked touching her face. Even when it became cold, the skin on her cheeks was as soft as only a newborn’s can be. She had these tiny hairs on her forehead, like Luca. I would brush my fingers over it, then touch her little nose and chin.
The teddies – one for us, one for Nadia’s coffin – were from a box that a charity called Sands provided, for families whose babies’ births and deaths are close together. There was a tiny box inside the big one, to put Nadia’s hospital bands (that were now around her ankles) in. Cards on which her hand- and footprints would be made later that day. A place where a plaster cast of her feet would be put.
he said this was the last time he was going to hold her. He stayed like that, motionless, for an eternity. And then he put her down, forever
This all seemed very bizarre, somewhat off-putting and decidedly macabre, including the fact that we were spending time with our dead baby. I didn’t know at the time that I would continue to imagine her with us in the days and the weeks to come, perfect and healthy. I would try to make sense of who it was that we had lost – what this gap of our missing little girl consists of – with the aid of these mementoes and this precious, brief time with her body. In the past, it was considered best to forget and move on when children were stillborn. How did people do it, how did they deal with their loss without these anchoring points that I had been given?
I spent that first day mostly sleeping, weaker than I ever felt. At one point, I woke up to the sound of André crying. I twitched towards the cot (empty!), before I realised that Nadia was in his arms. He paced around the room, holding her, talking about all the things he would have wanted to do with her. Then he sat on the bed, and said that he was going to put her down for the last time. After a long while he repeated that this was the last time he was going to hold her. He stayed like that, motionless, for an eternity. And then he put her down, forever.
The paediatrician who had asked me for Nadia’s name came by, to see if we would still like to go to Helen House. A clear liquid was escaping from her nose. I had wiped some away in the dead of the night when I went to check on her, but there it was again. Part of me was deeply distraught that changes were already happening to her body. Another part of me, that I wished would shut up, wondered idly if it might be cerebrospinal fluid.
It was there only for an instant, a softness and a sadness that touched the doctor’s face as he bent towards Nadia with a tissue, but that moment burned straight into my heart. Somehow he knew. This was not just a dead baby, this was our child.
W e moved to Helen House the next day. We were in a private apartment away from all the other families, where we could be alone if we wanted to be. Nadia was placed in a temperature controlled room, for us to spend time with her whenever we felt the need.
While I’m finding the words to describe the death of my daughter, I have trouble expressing how floored I am by the staff of Helen House and by the service they provide. Every day they care for dying children and their bewildered parents. And yet, it’s a happy place. Full of toys, full of laughter. Luca loved it there. He chatted up all the staff during meals, and he ate way too much cake. When he heard the sound of cartoons coming from some sick child’s room, he’d quietly slip in and sit next to them to watch.
My instinct was to keep him from seeing Nadia, but they suggested that it would be better if everything were presented as tangibly as possible to him. He might not have trouble now, they said, but problems might arise once he’d hit early adolescence and started thinking about the episode in a more adult way. Then he’d need the concrete memories in order to deal with her death and the events surrounding it.
We washed Nadia’s hair and I was rewarded with the wonderful soft fuzz on top of her head
When I finally saw him after the hospital stay, I felt that rush of love I always get after being separated from him, but he immediately insisted on seeing Nadia instead of giving me a hug. Oh well. André took him to Nadia’s room and he said she looked cute. He touched her face. He took a peek under her hat. He asked why her hair was like that. He sang her a song, and said bye-bye. Then he played with the toys in the apartment and forgot about my hug. Oh well.
There were so many small decisions to be made about the funeral, most of them heartbreaking. The people at Helen House helped us navigate through this as well. They also helped me with that thing I wanted so much: they got a basin with warm, soapy water and together we washed Nadia’s hair. It took a while, but finally I was rewarded with the wonderful soft fuzz on top of her head. She finally smelled nice.
But the smell that disturbed me was not coming from her skin. They warned me about this, and it was still jarring: the smell was coming from her diaper. We put a fresh one on her, and again I was confronted with the deep irrationality of it all. I knew it made no difference to her, I knew that in a few days she would be nothing more than ashes, but I was deeply, thoroughly, utterly relieved to know that she was clean.
O ur bodily state provides a background feeling against which our thoughts are displayed, adding a flavour to them. This is particularly obvious while the body slowly resets itself after giving birth. The first few days I was concerned with my own recovery, and my dominant feeling was of relief. But as time went by, I would revisit the moment when I realised that Nadia was dead, and it almost felt as if I were experiencing it as a different person. I would confront the mental image in order to process it and put some part of its emotional impact away, but the next day I would experience it differently and it would hit me all over again.
For a few days after coming home, I sank completely. I didn’t think I had it in me to break down in a hysterical episode – I did. I thought I would be able to collect myself enough to interact with Luca – I wasn’t able to; I had planned to keep an eye on André’s state – I couldn’t. I was just crying, constantly. My chest hurt when I breathed. A lot of that time I wasn’t even thinking about Nadia, I wasn’t thinking about anything, but the tears kept coming. When I was alone, I would cry out loud. None of this brought any relief.
On the day that I came back to reality, André suddenly looked old. He was completely washed out with fatigue and concern.
As the days went by, the intrusive images eventually waned, and the routines of life resumed something of their previous rhythm. After a while, I became able to decide which part of the day I would cry for Nadia. As long as I put aside some minutes for her, I could be cheerful and engaged the rest of the time. I went back to work, went out with friends, made plans. It looked as if things were getting back to normal.
And yet I couldn’t get past the impression that a funeral was pointless. When a loved one dies, friends and family flock in because they shared a connection with the person who is gone. There is comfort in the knowledge that the person we loved left some mark, that they touched others in some way. When Nadia died people cared about us, but we were alone in our attachment to her. And I didn’t see why we needed to go through a ritual. Should it mark the end? The end of what?
How could a gene or two erase everything else that she was, had the potential to be?
It was just this thing we did, and then the grief could resume.
I had to know more about her, but there was not that much to know. I found some information in the hospital records. Her blood group, her foot length. I try to forget seeing that Apgar score of zero. As if the word ‘stillborn’ was somehow not enough. I look at her pictures, hand prints, the plaster cast of her feet. But what would she have been like ?
Or was sick all that she could have ever been? How could a gene or two erase everything else that she was, had the potential to be? Some parts of me, some parts of André, some history of all life ever, assembled into her, an almost perfect child. Almost.
People understand that to lose a baby before it has been born is to lose an object of hope; that a beautiful idea of what the future should have held was suddenly ripped away from us. But there has to be more to it. Was it just hope that grew as the pregnancy progressed, as she passed all the usual markers for concern, and was this what set us up for our big crash? Or was it her that grew, and became more human? I cannot imagine anything specific to her as a person however hard I try. And yet I long for her, so badly. But how do I even know that I long for her ?
The act of giving birth was a strong force in consolidating my emotions towards Nadia. There was just something organic about the amniotic smell of me and her together, something primal about the buzzing of adrenaline through my body at the brink of exhaustion.
Perhaps this love is just there, simply felt. Unconditional, even on knowing her. It’s not a hope, it’s not a thought. Perhaps its building blocks are too far removed from any explanations, are too deep in our roots, go further back in time than we can meaningfully contemplate on. My mind will try to build some meaning around her because my feelings take me in that direction, but the idea of a child that almost was is something too elusive to be carefully taken apart and then mentally rebuilt. It’s just a gaping hole, a hollow, aching absence. One I’m told will grow more comfortable over time.
F our months in, I still find myself furiously analysing the details of this three-week flashbulb memory. Some elements suddenly stick out, become more vivid, more hurtful. It takes many approaches to each and every inner knot to make the desperation subside a bit. Some form again if I’m not careful to keep them untangled.
There is this constant, massive background process of assessing my disproportionate internal responses to the many triggers of daily life. I keep reminding myself that reality gets distorted when it passes through me, and I try to correct for it in my behaviour. Sometimes the torrent of emotions grabs me and I have the feeling that every little thought I have is important, drenched with some deeper meaning. But then I’ll voice some of those thoughts and people will nod, and abruptly it will look surreal – certainly it’s an illusion that my words can touch reality and interact with it? Then sometimes I’ll have a different kind of day, one where I’m aware that I am the only person in the room pushing away an image of a dead baby. Those days are very, very tiring.
People continue to react in a myriad of ways, and the minutiae of their attempts to communicate are just as magnified inside me as the invisible drama of my responses. Many struggle to find the right words, but words are largely unimportant. But that moment of uncertainty when we meet, that passing frozen expression when I mention Nadia, those attempts to say something that quickly get swallowed, I see them all and find them reassuring. None of us really knows how to respond to death, and in these small moments of discomposure people reveal themselves as distinctly human. Our shared unease, however brief, becomes my safety net.
The palliative care team understood that the value of life is not only in being alive, but also in connecting
I will gravitate back to my old self at some future point, I’m sure of this. People generally do. Then I might be left to wonder whether this has altered who I am or whether it has left my core values untouched. Knowing myself, I will probably conclude that the answer is yes, to both. For the moment I’m not in a hurry to get there. I want to embrace this pain, let it engulf me completely. And then keep some of it inside me forever, as I emerge on the other side whole and sound, richer for the experience of a little Nadia I never knew.
We heard back from the Southampton hospital, where we could have gone for the caesarean that first evening. They looked at Nadia’s case and said that they would not have been willing to put her on ECMO. Perhaps the option of saving her was not so real after all, but I will be left with having made the decision that I did. People don’t envy me for the position I was in, having to decide without warning while going on not much information and in a state of shock, but I don’t envy those doctors who sat across from me either. It must be so hard, to have the technology to keep someone’s body working, but to let them die instead.
The palliative care team gave us so much more than the promise of morphine for Nadia. They stepped in to protect the space that we needed in order to deal with her death. They understood that the value of life is not only in being alive, but also in connecting, and that we will take something of this time with Nadia further with us. They accepted our pain, without looking away. We could lean on them as we all stepped back together, allowing that life can end, sometimes just as it is beginning. And they helped me gain a daughter, even as I lost her.
A uthor’s update, February 2023. Seven years have passed since Nadia’s death. We have a four-year-old again. One with pigtails and a bubbly nature, and a lighter step than the rest of us. Valentina doesn’t know that other older brothers don’t dote on their little sisters quite so much, nor that not every child gets kissed quite so constantly.
To my endless delight we got a puppy, and one day she had puppies too. When one of the pups started wilting soon after birth, Andre desperately tried to keep it alive. I gently held it long into the night, until it shuddered and stopped moving. And Luca named it and buried it in our garden, not far from Nadia’s apple tree. We were rattled. But we were all right.
I now know what it’s like when my surface cracks break open into deep grooves, and what it means to travel back from that place to one where I can once again take part in the pleasant anticipation of life going on. And it does have a tendency of going on. At first relentlessly, unfairly. And while the simpler version of this story would be that the grooves narrowed until they became cracks again, and that the cracks eventually healed, the truth is that in holding on to my grief I have instead learned to smoothly transition between realms: one of senseless tragedy, and one of simple daily pleasures. I comfortably inhabit both worlds now, and this is exactly how it should be.
Relationships live in the spaces between people, are held in place by those people, each one on their end. But relationships also live inside us, and when the other person drops their end, a lopsided bond continues to evolve. In my mind I learned to turn that incomprehensible, personless, hollow absence into a little girl-shaped presence. Paradoxically, imagining ‘her’ with me allowed me to stay connected to the world. I hope to eventually grow ‘her’ into independence. Enough to one day walk away, a person separate from myself. I’d watch her go with proud tears in my eyes instead of a heart pierced by pain. One day. Perhaps. Not yet. This grief is there because of love after all, and our love for our children never goes away.
A shorter version of this article appeared in the British Medical Journal."
Why are donor eggs almost taboo among fertility options? | Aeon Essays,,https://aeon.co/essays/why-are-donor-eggs-almost-taboo-among-fertility-options,"‘So, what finally made the difference for you – that you’re really pregnant?’ my friend Sharon asked me. She wasn’t the first. As a columnist who publicly detailed my fertility journey in The New York Times Motherlode blog – covering three years, four miscarriages, and nine rounds of in vitro fertilisation (IVF) – I had just announced I was in my second trimester. It looked like this pregnancy was going to stick. I was 44 and my husband Solomon 48, and we were finally going to have a baby!
But how , asked readers, many of whom were themselves trying to conceive or had a friend or relative doing IVF – the process of stimulating a woman’s hormones to produce extra eggs, then extracting them, fertilising them with a man’s sperm, and transferring the resulting embryos back into her uterus. (IVF and other assisted reproductive technologies account for 1.5 per cent of all American births.)
‘You’re my beacon of hope,’ said Sharon. ‘What made it work?’ I did not want take myself off her pedestal, or steal her faith, but I also had to be honest. ‘I used donor eggs, you know,’ I said.
‘Oh,’ she replied, not bothering to disguise the disappointment in her voice. ‘Donor eggs. OK,’ she said sadly and quickly got off the phone. No congratulations, no happiness, no nothing.
I was carrying our child, made from my husband’s sperm and an egg from a stranger. The child would grow inside me from the time it was an embryo just five days old. With science increasingly showing that unborn babies can feel, perceive and learn in utero, getting pregnant with a donor egg caused me to rethink my ideas about mother-child bonding, the meaning of attachment, and motherhood as a whole. The more this baby grew inside me, the more I felt love for her.
Yet to Sharon and many others I had failed. I’d failed to achieve the holy grail of fertility treatment: having a child with my own and my husband’s DNA.
Among those trying to conceive, there’s a hierarchy of preference for how . Obviously, the easiest and least costly is just plain sex. Then there’s IVF – today, such a ‘normal’ medical procedure that people often don’t mention it. And why should they? These ‘test-tube babies’ use the mother’s oocytes and father’s sperm, so they’re genetically related to both parents, not to mention carried by the mother: equal in almost every way to natural conception.
Lower on the totem pole is what’s called ‘third-party reproduction’, the use of an individual outside the primary relationship to make a baby possible. Everyone knows about sperm donation, with thousands of banks where you can choose a guy like from an online dating profile; single mothers are not ashamed to admit creating a child using their own genes combined with those of a (tall, hot, smart) stranger. It’s so acceptable there’s even been a Vince Vaughn comedy – Delivery Man (2013) – about it.
Surrogacy, in which a woman outside the relationship carries the couple’s embryo until birth, has also made it to the mainstream. There’s the 2008 Tina Fey/Amy Poehler movie Baby Mama , not to mention countless celebrities such as Jimmy Fallon and Sarah Jessica Parker telling the world that surrogates carried their babies, created from the couples’ own sperm and eggs.
Even adoption has changed. Instead of being shrouded in secrecy, with the children themselves often not knowing, the fact is now proudly broadcast to the world, which looks upon it as nearly saintly, parents ‘saving’ unwanted children.
The only thing that still seems to be a secret in these uber-confessional days is women who use donor eggs.
T heir numbers are not small. In 2013, out of some 175,000 fertility cycles in America, slightly more than 10 per cent used donor eggs according to the Society of Assisted Reproductive Technology’s (SART) National Clinic Summary Report. And of 63,286 babies born through IVF, 9,512, or 15 per cent, were from donor eggs.
Which makes sense: a woman’s fertility – or lack of it – is often dependent on the age of her eggs, not of her body. The highest cause of infertility is Diminished Ovarian Reserve, which means fewer eggs of worse quality, unlikely to result in a live, healthy baby. Hence the push for women under 35 to freeze their eggs – to stop time just as their fertility begins to decline.
According to the 2013 stats, a woman under 35 – the lowest age range that is counted in the statistics – has a 40.1 per cent chance of having a baby by IVF; however a woman over 42 has only a 4.5 per cent chance. Yet a woman of any childbearing age who uses fresh donor eggs has a 49.6 per cent chance of having a baby.
Of course, the word ‘donor’ is just a euphemism in most places. The eggs are often purchased, except for those from the few family members who actually do donate their eggs. To avoid having the eggs classified as organs, which are illegal to sell, people technically buy the donor’s cycle , regardless of how many eggs result from it.
Around the world, laws regarding egg donation vary: in the United States, egg donation is legal and donors may be anonymous; they can be paid up to $10,000, with travel costs and medical care on top of that. Add another $12,400 on average for IVF, which must still be done, only it is the the donor who is stimulated with hormones and has her eggs extracted. In Canada, the donor egg is legal only if it’s free and the donor known; in France, donation must be free and the egg donor anonymous. French donors are compensated €500-€1,000 in personal profit, with the process itself costing €4,500 more. This contrasts with Austria, Germany, Italy, and Norway, where egg donation is outlawed.
Costs hardly explain why women who use donor eggs keep it such a secret. After all, gestational surrogacy in the US ranges from $100,000 to $150,000, while domestic adoption ranges from $30,000 to $44,000 or more.
‘If IVF didn’t work out, we’d probably just adopt,’ my friend Sharon later told me, echoing what many others have said – inexplicably, since a fertilised donor egg still contains half the parents’ genes and is carried by the mother, while adoption has no biological connection at all.
‘There’s a stigma. Egg donation is looked down upon,’ says Marna Gatlin, founder and CEO of Parents Via Egg Donation (PVED.org), a non-profit organisation founded in 2008 to support, empower and educate egg donor parents. She says people call it ‘weird’ or ‘unnatural’.
‘They don’t tell. They do not tell,’ she said of parents going public about the process. (Telling their own kids is a different story: she herself is a parent to a teenage son who knows his donor egg origin story.) ‘A lot of donor-egg-intended parents are as deeply in the closet as same-sex people were in the 1970s. We’re trying to remove that stigma.’
When we adopt a child, a birth mother selects us, but with an egg donor it’s the other way around
It’s going to be tough. ‘Pregnancy is only nine months but genetics are forever!!’ wrote one person upon learning I had opted for a donor egg.
After learning my donor was anonymous, a person from the adoption community wrote to tell me I’d be ruining my child’s life. It appeared to the writer that not knowing much about the donor was akin to doing a closed adoption and denying the child knowledge of its birth parents.
‘Some people refer to donor eggs as half adoption,’ explains Gatlin, ‘but it’s not accurate.’ For one thing, she says, the mother carries a donor egg child. And for another, when we adopt a child, a birth mother selects us, but with an egg donor it’s the other way around. ‘We select the egg donor,’ she emphasises.
Lori Gottlieb, a Los Angeles-based psychotherapist who specialises in reproductive counselling , thinks the stigma around donor eggs has to do with aging. ‘People associate donor eggs with aging more than a surrogate,’ she says, noting that women of all ages may have to outsource pregnancy due to problems with their bodies. But it’s often older women who need to use donor eggs. ‘It’s incredibly shaming and judgmental: “you waited too long and shame on you and you’re too old,” people think.’ She observes that people seldom say ‘Why don’t you just adopt?’ to a couple who use a surrogate. ‘We value a genetic connection more than an emotional connection.’
O ur fertility plan always included donor eggs as the next step if IVF using my own eggs failed. After we had genetically tested our embryos and found them all to be chromosomally abnormal, probably because of my age – 43½ – we had to ‘escalate’, as Solomon liked to say. But as I began floating the donor egg idea around to people, I got some strange reactions.
‘This would be the first time we wouldn’t know who the mother is,’ my father joked in his usual poor taste. ‘ I would be the mother,’ I wanted to reply, but bit my tongue.
‘Would the donor be Jewish?’ my brother wondered. True, both the men in my family were Orthodox Jews, obsessed with genealogy and matrilineal descent of the tribe.
But it wasn’t just religious people. I started to notice how obsessed with genetics everyone was – myself included. ‘Oh, is that mommy’s nose?’ I cooed to my younger sister’s new son, assigning his eyes, lips and chin to her or to her husband.
This explains why the general public values a couple using a surrogate (with their genetic embryo) over a woman who uses a donor egg and carries the baby herself. It also explains, for women, the popularity of sperm donation over egg donation. Many women believe that if they’re not using their own eggs, they ‘might as well’ adopt. All seem to discount the effects of carrying the unborn child for 40 weeks.
But they should not. A whole body of evidence now shows that deep bonding between mother and unborn child occurs during the nine months of gestation. And the science of epigenetics – in which environment shapes gene expression – means that nurturing the unborn can influence lifelong temperament and health. Scientists liken epigenetics to a switch that turns gene expression on or off. The overall DNA doesn’t change, but gene influence does.
Some of the impact is blatant. Through such direct actions as poor nutrition and second-hand smoke, maternal environment during gestation can directly influence everything from foetal birth weight to risk of future adult coronary disease. An unborn child bathed in the roiling hormones of maternal stress will have different gene expression – indeed, a different brain – from a child gestated in calm.
But much of the influence is nuanced. ‘There’s a lot of misinformation when it comes to this whole area of prenatal development, even among people who work with babies every day,’ says the psychiatrist Thomas R Verny, author of The Secret Life of the Unborn Child (1981) and widely credited with launching the field of pre- and perinatal psychology. Verny says that ‘bonding and communication happens in third trimester for sure, maybe even earlier’ and cites a wide body of research to make his point.
One classic study was published in Science as long ago as 1980 by the University of North Carolina Greensboro psychologist Anthony Decasper, who showed that newborns were clearly attuned (and attached) to their mothers’ speech from before birth. To conduct his study, Decasper had 16 pregnant women tape-record readings of children’s stories, and read the same story to the child a few times a day. Then, after birth, babies were able to toggle between various stories by sucking on a nipple hooked up to a sound system. Thirteen of the 16 newborns chose the story their own mother had repeatedly read. In a 1994 follow-up study, Decasper found that when newborns were exposed to a rhyme recited by their mothers from the 33rd to the 37th week of gestation, their heart rates slowed – a sign of calm.
By 2007, the neuropsychologist Ruth Feldman of Bar-Ilan University in Israel had shown that the degree of attachment between newborn and mother was influenced by levels of the intimacy hormone, oxytocin, measured in the mother’s blood before and immediately after birth.
And evidence for the connection between an unborn child and a mother only mounts. In a study published in 2015 in PLOS ONE , the psychologist Viola Marx of the University of Dundee in Scotland found that when mothers touched their stomachs, unborn babies reacted with more head, arm and mouth movement; when mothers spoke, those babies reacted with less head and arm movement. And all this responsiveness was recorded not just in the third trimester but also the second, much earlier than previously assumed – though responsiveness increased as gestation advanced.
People are too anxious about the idea of connecting to their unborn child. ‘It’s almost like helicopter parenting in utero.’
‘The mother was once regarded as a vehicle, a conduit for nutrition and waste removal for the foetus that lived isolated from the outside world,’ the study authors wrote. But now we know that newborns ‘preferentially respond to maternal voice hours after birth, suggesting that the foetus is able to detect stimuli in utero and form memories of them’. Parents have long spoken and sung to their unborn child throughout pregnancy, reporting changes in the behaviour of the foetus as a result. Now, with the Dundee findings, it appears that real communication is taking place.
All this adds palpable new context to the existing fertility hierarchy and serves to elevate donor eggs up the ladder of desirability – at least as far as I can see. From this perspective, one could equate surrogacy and adoption as equivalent in terms of prenatal bonding on the one hand while natural conception and donor eggs or sperm are equivalent on the other. ‘So what if the baby’s hair is blond and his eyes are blue,’ says Verny. Even if the child is genetically yours, ‘the personality might be very much influenced by the first nine months.’ The period of gestation could have more influence over the baby’s future than mere genes. Carrying a baby is a bonding tool.
Of course, attachment is a complex business, and can be forged after birth, too. ‘What is important is how you attach to your child when you have a child,’ says Gottlieb, who herself used a sperm donor to conceive. People are far too anxious about the idea of connecting to their unborn child, she feels. ‘It’s almost like helicopter parenting in utero.’ For her, ‘the more relaxed and confident the parent, the better the attachment is going to be’, no matter how the child is conceived or carried .
L et it be said that I am not sure what to believe: whether it’s genes or early childhood or the prenatal environment or a mixture of all three. I myself am the second of four children, and I am absolutely nothing like either of my parents – biological, I must point out – nor like most of my siblings. My younger sister, tall and pretty, with light skin and light hair, doesn’t even look like me, short, dark and exotic. And despite our closeness, our personalities are night and day: me a risk-taking non-planner who lives by the seat of her pants, she one who prefers a more orderly life. My youngest brother, an ultra-Orthodox rabbi (he looks like me beneath his beard) is most similar to me in temperament, humour and even constitution; we hate the same foods and share the same ailments, even though our life choices conflict in every possible way.
Was my mother, prone to depression, in a better state of mind when she was pregnant with me and my brother, six years apart? Or was she just in a better state after we were born? Or were the two of us just genetically better equipped than my two sisters to deal with life’s curve balls? Or is it just a matter of free will: my brother and I persevered better in coping with our parents’ disastrous union (after 29 years of cold war, they divorced when I was 23).
I patted my stomach at a Billy Joel concert, hoping to get some pop music influences in before my husband created another jazz aficionado
All this is to say that since biological parents didn’t work out all that great for me, I’m the last to judge when it comes to fertility choices. Despite sharing my journey oh so publicly, I believe that such decisions are personal, and that any option – including a life that is child-free – can be full of joy.
See, if donor eggs hadn’t worked out for us, Solomon and I would have pursued adoption: we would have done whatever it took to start our family. And I’m sure I’d have felt like Becky Fawcett, the founder and president of Helpusadopt.org, who adopted two children after her fifth round of IVF failed. ‘Once that baby comes out you’re a mom, period. It doesn’t matter how you get there.’
Yet having that donor egg grow inside me was an experience I would not trade. If at first I fretted that I’d feel the baby wasn’t mine, by the middle of my second trimester, with the first faint hiccups of life, then the powerful kicks and flailing of the baby’s arms and legs, I fell in love. I couldn’t help but marvel at my baby’s movement when my husband lovingly caressed my stomach and talked about his day or played jazz on his guitar. I wondered if the baby could hear my thoughts, because I was sure I could. ‘Yes, I know you’re thirsty, I’ll get some water,’ I’d say aloud. Or: ‘This music is too loud for you, I know.’ I patted my stomach at a Billy Joel concert, hoping to get some pop music influences in before my husband created another jazz aficionado, which would leave me the odd one out.
We were bonding, all three of us, for at least half of our nine months together. By then I didn’t wonder how, by the time the baby came, she could feel like anything but ours. And then she arrived, little Lily, at 6 lbs, 10 oz, the squirming gorgeous life form that had been growing inside of me for the last 40 weeks. As they placed her on my bare chest, and her father put his warm hand on her bony back, she settled down, with her – our – family."
Are male and female circumcision morally equivalent? | Aeon Essays,,https://aeon.co/essays/are-male-and-female-circumcision-morally-equivalent,"I try not to talk about my research at dinner parties. I’ll say ‘medical ethics’ if pressed, which will sometimes trigger an unwelcome follow-up: ‘But what about medical ethics? That’s a pretty big field.’
‘I study lots of things,’ I’ll say – and that’s true, I do. ‘But I focus on medically unnecessary surgeries performed on children.’
‘Like what?’
Like what, indeed. It’s rarely a smooth ride from there.
The truth is: I study childhood genital surgeries. Female, male and intersex genital surgeries, specifically, and I make similar arguments about each one. As a general rule, I think that healthy children – whatever their sex or gender – should be free from having parts of their most intimate sexual organs removed before they can understand what’s at stake in such a procedure. There are a number of reasons I’ve come to hold this view, but in some ways it’s pretty simple. ‘Private parts’ are private. They’re personal. Barring some serious disease to treat or physical malfunction to address (for which surgery is the most conservative option), they should probably be left alone.
That turns out to be extremely controversial.
I n the 1990s, when the Canadian ethicist Margaret Somerville began to speak and write critically about the non-therapeutic circumcision of infant boys, she was attacked for even addressing the subject in public. In her book The Ethical Canary , she says her critics accused her of ‘detracting from the horror of female genital mutilation and weakening the case against it by speaking about it and infant male circumcision in the same context and pointing out that the same ethical and legal principles applied to both’.
She wasn’t alone. The anthropologist Kirsten Bell has advanced similar arguments in her university lectures, provoking a reaction that was ‘immediate and hostile … How dare I mention these two entirely different operations in the same breath! How dare I compare the innocuous and beneficial removal of the foreskin with the extreme mutilations enacted against females in other societies!’
There’s a problem with these claims. Almost every one of them is untrue or severely misleading
It’s easy to see where these reactions are coming from. One frequent claim is that FGM is analogous to ‘castration’ or a ‘total penectomy’. Put that way, anyone who tried to compare the two on ethical (or other) grounds would be making a serious mistake – anatomically, at the very least.
You often hear that genital mutilation and male circumcision are very different . FGM is barbaric and crippling (‘always torture’, as the Guardian columnist Tanya Gold wrote recently), whereas male circumcision is comparatively inconsequential. Male circumcision is a ‘minor’ intervention that might even confer health benefits, whereas FGM is a drastic intervention with no health benefits, and only causes harm. The ‘prime motive’ for FGM is to control women’s sexuality; it is inherently sexist and discriminatory and is an expression of male power and domination. That’s just not true for male circumcision.
Unfortunately, there’s a problem with these claims. Almost every one of them is untrue, or severely misleading. They derive from a superficial understanding of both FGM and male circumcision; and they are inconsistent with what scholars have known about these practices for well over a decade. It’s time to re-examine what we ‘know’ about these controversial customs.
T he World Health Organization (WHO) defines FGM as any ‘non-medical’ alteration of the genitalia of women and girls. What this is likely to bring to mind is the most extreme version of such ‘alteration’, which is the excision of the external part of the clitoris followed by a narrowing of the vaginal opening, sometimes using stitches or thorns. It is rarely understood that this notorious form of FGM is comparatively rare: it occurs in a subset of the practising communities, and makes up about 10 per cent of cases worldwide. More prevalent, but much less frequently discussed in the media, is a range of less extensive alterations, sometimes performed under anaesthesia by medical professionals and with sterile surgical equipment. These include, among other interventions, so-called ritual ‘nicking’ of the clitoral hood (common in Malaysia), as well as non-medically-indicated labiaplasty and even piercings that might be done for perceived cosmetic enhancement.
Male genital cutting is performed at different ages, in different environments, with different tools, by different groups, for different reasons
It should be clear that these different forms of FGM are likely to result in different degrees of harm, with different effects on sexual function and satisfaction, different chances of developing an infection, and so on. And yet all forms of non-therapeutic female genital alteration – no matter how sterilised or minor – are deemed to be mutilations in ‘Western’ countries. All are prohibited by law. The reason for this, when you get right down to it, is that cutting into a girl’s genitals without a medical diagnosis, and without her consent, is equivalent to criminal assault on a minor under the legal codes of most of these societies. And, morally, I think the law is correct here. I don’t think that a sharp object should be taken to any child’s vulva unless it is to save her life or health, or unless she has given her fully informed permission to undergo such an operation, and wants to take on the relevant risks and consequences.
In that case, of course, she wouldn’t be a ‘child’ anymore, but rather an adult woman, who can make a decision about her own body.
The story is very different when it comes to male circumcision. In no jurisdiction is the practice prohibited, and in many it is not even restricted. In some countries, including in the United States, anyone, with any instrument, and any degree of medical training (including none) can attempt to perform a circumcision on a non-consenting child – sometimes with disastrous consequences. For a recent example, look up ‘Goodluck Caubergs’ on the internet; similar cases happen every year. As the bioethicist Dena Davis has pointed out, ‘States currently regulate the hygienic practices of those who cut our hair and our fingernails … so why not a baby’s genitals?’
Just like FGM, however, circumcision is not a monolith: it isn’t just one kind of thing. The original Jewish form of circumcision (until about AD150) was comparatively minor. It involved cutting off the overhanging tip of the foreskin – whatever stretched over the end of the glans – thereby preserving (most of) the foreskin’s protective and sexual functions, as well as reducing the amount of erogenous tissue removed. The ‘modern’ form is much more invasive: it removes between one-third and one-half of the movable skin system of the penis (about 50 square centimeters of richly innervated tissue in the adult organ), eliminates the gliding motion of the foreskin, and exposes the head of the penis to environmental irritation, as it rubs against clothing.
Male genital cutting is performed at different ages, in different environments, with different tools, by different groups, for different reasons. Traditional Muslim circumcisions are done while the boy is fully conscious, between the ages of five and eight, and sometimes later. American (non-religious) circumcisions are done in a hospital, in the first few days of life, with or without an anesthetic. Metzitzah b’peh , done by some ultra-Orthodox Jews, involves the sucking of blood from the circumcision wound, and carries the risk of herpes infection and permanent brain damage.
Subincision, seen primarily in aboriginal Australia, involves slicing open the urethral passage on the underside of the penis from the scrotum to the glans, often affecting urination as well as sexual function. And circumcision among some tribal groups in Africa is done as a rite of passage, in the bush, with spearheads, dirty knives, and other non-sterile instruments. Similar to female genital cutting rites performed under comparable conditions (and often by the very same groups), these operations frequently cause hemorrhage, infection, mangling, and loss of the sexual organ. In fact, between 2008 and 2014, more than half a million boys were hospitalised due to botched circumcisions in South Africa alone. More than 400 lost their lives.
But even ‘hospitalised’ or ‘minor’ circumcisions are not without their risks and complications, and the harm is not confined to Africa. In 2011, for example, nearly a dozen infant boys were treated for life-threatening haemorrhage, shock or sepsis as a result of their non-therapeutic circumcisions at a single children’s hospital in Birmingham, England. Since this figure was obtained by a special freedom of information request (and otherwise would not have been public knowledge), it has to be multiplied by orders of magnitude to get a sense of the true scope of the problem.
W hen people talk about ‘FGM’ they are usually thinking of the most severe forms of female genital cutting, done in the least sterile environments, with the most drastic consequences likeliest to follow – even though research suggests that these forms are the exception rather than the rule. When people talk about ‘male circumcision’, by contrast, they are (apparently) thinking of the least severe forms of male genital cutting, done in the most sterile environments, with the least drastic consequences likeliest to follow – perhaps because this is the form with which they are culturally familiar.
One recurrent claim, recently underlined by the US Centers for Disease Control (CDC), is that male circumcision can confer a number of health benefits, such as a small reduction in the absolute risk of contracting certain sexually transmitted infections. This is not typically seen as being the case for FGM.
However, both parts of this claim are misleading. Certainly the most extreme types of FGM will not contribute to good health on balance, but neither will the spearheads-and-dirty-knives versions of genital cutting on boys. What about other forms of FGM? Its defenders (who typically refer to it as ‘female circumcision’) regularly cite such ‘health benefits’ as improved genital hygiene as a reason to continue the practice. Indeed, the vulva has all sorts of warm, moist places where bacteria or viruses could get trapped, such as underneath the clitoral hood, or among the folds of the labia; so who is to say that removing some of that tissue (with a sterile surgical tool) might not reduce the risk of various diseases?
one relevant harm would be the involuntary loss of a healthy, functional, and erotogenic genital structure
Fortunately, it’s impossible to perform this type of research in the West, because any scientist who tried to do so would be arrested under anti-FGM laws (and would never get approval from an ethics review board). So we simply do not know. As a consequence of this, every time one sees the claim that ‘FGM has no health benefits’ – a claim that has become something of a mantra for the WHO – one should read this as saying, ‘we actually don’t know if certain minor, sterilised forms of FGM have health benefits, because it is unethical – and would be illegal – to find out.’
By contrast, a small and insistent group of (mostly American) scientists have taken it upon themselves to promote infant male circumcision as a form of partial prophylaxis against disease. Most of these diseases are rare in developed countries, do not affect children before an age of sexual debut, and can be prevented and/or treated through much more conservative means. Nevertheless – since it is not against the law for them to do so – advocates of (male) circumcision are able to conduct study after well-funded study to see just what kinds of ‘health benefits’ might follow from cutting off parts of the penis.
Many European medical experts dispute these studies, and detect more than a whiff of cultural bias in favour of circumcision due to its peculiar status as a birth ritual in American society. The recent statement by the CDC is a case in point. This otherwise august organisation contends that the benefits of circumcision outweigh the risks, where by ‘risk’ they apparently mean ‘risk of surgical complications’.
But in medical ethics, the appropriate test for a non-therapeutic surgery performed in the absence of disease or deformity is not benefit vs ‘risk of surgical complications’ but rather benefit vs risk of harm. In this case, one relevant harm would be the involuntary loss of a healthy, functional, and erotogenic genital structure that one might wish to have experienced intact. Imagine a report by the CDC referring to the benefits of removing the labia of infant girls, where the only morally relevant drawback to such a procedure was described as the ‘risk of surgical complications’.
I t is often said that FGM is designed to ‘control’ female sexuality, whereas male genital cutting is less symbolically problematic. But as the sociologist Lisa Wade has shown in her research, ‘attributing [the] persistence [of female genital altering rituals] to patriarchy grossly over-simplifies their social, cultural, and economic functions’ in the diverse societies in which they are performed. Throughout much of Africa, for example, genital cutting (of whatever degree of severity) is most commonly performed around puberty, and is done to boys and girls alike. In most cases, the major social function of the cutting is to mark the transition from childhood to adulthood, and it is typically performed as part of an elaborate ceremony.
Indeed, in nearly every society that practices such coming of age rituals, the female half of the initiation is carried out by women (rather than by men) who do not typically view it as being a consequence of male dominance, but who instead see their genital-altering practices as being beautifying, even empowering, and as an important rite of passage with high cultural value. The claim that these women are all ‘brainwashed’ is anthropologically ignorant. At the same time, the ‘rite of passage’ ceremonies for boys in these societies are carried out by men; these are done in parallel, under similar conditions, and for similar reasons – and often with similar consequences for health and sexuality (as illustrated earlier with the example of South Africa).
Every parent who requests a genital-altering surgery for their child – for whatever reason under the sun – thinks that they are acting in the child’s best interests
In the US context, male circumcision was adopted by the medical community in the late 1800s in an effort to combat masturbation, among other dubious reasons. It has since persisted as a rationalised habit, long past the time when it was effectively abandoned by other developed nations. Of course, it is probably true that most contemporary Western parents who choose circumcision for their sons do not do so out of a desire to ‘control’ their sexuality, but this is also true of most African parents who choose ‘circumcision’ for their daughters. As the renowned anti-FGM activist Hanny Lightfoot-Klein has stated: ‘The [main] reasons given for female circumcision in Africa and for routine male circumcision in the United States are essentially the same. Both promise cleanliness and the absence of odours as well as greater attractiveness and acceptability.’
Given that both male and female forms of genital cutting express different cultural norms depending upon the context, and are performed for different reasons in different cultures, and even in different communities or individual families, how shall we assess the permissibility of either? Do we need to interview each set of parents to make sure that their proposed act of cutting is intended as an expression of acceptable norms? If they promise that it isn’t about ‘sexual control’ in their specific case, but rather about ‘hygiene’ or ‘aesthetics’ or something less symbolically problematic, should they be permitted to go ahead?
But this is bound to fail. Every parent who requests a genital-altering surgery for their child – for whatever reason under the sun – thinks that they are acting in the child’s best interests; no one thinks that they are ‘mutilating’ their own offspring (whether female or male). So it is not the reason for the intervention that determines its permissibility, but rather the consequences of the intervention for the person whose genitals are actually on the line.
As the social anthropologist Sara Johnsdotter has pointed out, there is no one-to-one relationship between the amount of genital tissue removed (in males, females, or indeed in intersex people), and either subjective satisfaction while having sex, or a feeling of having been personally harmed because one’s ‘private parts’ were altered before one could effectively resist. Medically unnecessary genital surgeries – of whatever degree of severity – will affect different people differently. This is because each individual’s relationship to their own body is unique, including what they find aesthetically appealing, what degree of risk they feel comfortable taking on when it comes to elective surgeries on their reproductive organs, and even what degree of sexual sensitivity they prefer (for personal or cultural reasons). That’s why ethicists are beginning to argue that individuals should be left to decide what to do with their own genitals when it comes to irreversible surgery, whatever their sex or gender.
This article is adapted from a longer piece originally published at the University of Oxford’s Practical Ethics website. Links to supporting research can be found in the original essay, available here ."
Why do women know so little about their own fertility? | Aeon Essays,,https://aeon.co/essays/why-do-women-know-so-little-about-their-own-fertility,"A 42-year-old single friend tells me she is thinking of freezing her eggs. I nod with a tight, fake smile. I’m torn: on the one hand, I know how tough everything to do with fertility is because my husband and I have been trying to have a baby since we got married three and a half years ago when I was 41 and he was 45. On the other hand, going through this, and writing about it for The New York Times Motherlode blog, I’ve amassed a vast trove of information about in vitro fertilisation (IVF), egg-freezing and women’s fertility. I know you can’t start thinking about freezing your eggs at 42. Because even if you immediately stop thinking about it and start doing it, it probably wouldn’t work at that age.
What I really want to tell my friend is that if she is serious about having a baby, her best bet would be to go out to the nearest bar and hook up with a stranger – during her 36-hour ovulation window, of course. But I won’t tell her to sleep with a random guy, I won’t ask if she ovulates regularly, nor will I say anything else about the state of her ticking – nearly stopped – biological clock: it’s too delicate a subject.
Besides, I know exactly how she feels. I know how little we women know about our own fertility – despite the daily bombardment of news about declining fertility and egg-freezing and snatching up a husband in college à la the notorious ‘Princeton mom’, Susan Patton, author of Marry Smart (2014), who says women at Ivy League universities should spend that time to find a mate. After all, I didn’t know much about fertility either.
It was three months before my 41st birthday when I went first went to a gynaecologist to discuss what steps to take before trying to conceive. Solomon and I had been dating 15 months and had just had the ‘talk’, a vague discussion in which we concluded that I would go off birth control and we’d start trying to get pregnant. (It would take me another three months and a ring to understand that the baby talk was his proposal for marriage.) I found a female ob/gyn, whom I believed would be more sensitive than a man.
‘So, I read somewhere that you shouldn’t start trying to conceive until you’ve been off birth control for a while,’ I said to the gynaecologist. ‘How long should I be off the Pill before I start?’
‘Well, I wouldn’t wait if I were you. You don’t have much time,’ she said snidely.
Ouch. I looked up from my note pad, where I’d been scribbling notes from our conversation, like ‘get a mammogram’ and ‘start taking pre-natal vitamins’. I’d been in the woman’s office for about 10 minutes and it felt like she was being… well, kind of a bitch.
‘Excuse me?’ I said. I felt like I’d gone to buy lipstick at a cosmetics counter and been offered plastic surgery.
‘You’re almost 41. You don’t have time to wait,’ she said again with a grim look in her eyes. I still wanted to know whether trying to conceive immediately after stopping the Pill could raise the chance of birth defects. (I later learned that that’s patently false – a woman’s fertility is often higher right after she goes off birth control.) But her comment was said with such finality, such disdain – and no actual medical facts to accompany it – that I just took my mammogram script and high-tailed it out of there.
I didn’t heed her advice. I suppose I was still fixated on the ‘first comes love, then comes marriage, then comes baby in a carriage’ thing – and I waited half a year until the month of my wedding to have unprotected sex. Lo and behold, a week or two after the nuptials, I discovered I was pregnant. Huzzah! I wanted to throw the positive pregnancy test in that smug gynaecologist’s face and say: ‘Who you callin’ old now, girl?’
But before I could make it to a doctor (a different one), I miscarried. Then I got pregnant again, then miscarried again. Over the next three and a half years, I moved from natural conception to assisted reproduction and IVF, and subsequently learned everything I never wanted to know about pregnancy, miscarriage, age and fertility. Alas, it was too late for me. Sure, I’d gained all this knowledge about the speed of fertility decline but, at 43, I was getting too old to have a baby.
I t’s too painful to wonder what would have happened if that first gynaecologist had sat me down calmly and opened up some informative graphics to show how women’s fertility drastically declines with age – beginning at around 32, more rapidly after 37, then precipitously at 40. The way a doctor might explain to you the risk of smoking by showing you a picture of blackened lungs, or describe the effects fat has on arteries, often leading to heart attacks: simple medical facts, presented in an objective manner, without judgment or guilt or some hidden cultural agenda.
Sadly, all this is missing from the discussion on women’s fertility.
Many studies show that women are not only woefully ignorant when it comes to fertility, conception and the efficacy of assisted reproductive technologies (ART) – but they overestimate their knowledge about the subject. For instance, a 2011 study in Fertility and Sterility surveyed 3,345 childless women in Canada between the ages of 20 and 50; despite the fact that the women initially assessed their own fertility knowledge as high, the researchers found only half of them answered six of the 16 questions correctly. 72.9 per cent of women thought that: ‘For women over 30, overall health and fitness level is a better indicator of fertility than age.’ (False.) And 90.9 per cent felt that: ‘Prior to menopause, assisted reproductive technologies (such as IVF) can help most women to have a baby using their own eggs.’ (Also false.) Many falsely believed that by not smoking and not being obese they could improve their fertility, rather than the fact that those factors simply negatively affect fertility.
‘You don’t have much time,’ she said snidely. I felt like I’d gone to buy lipstick at a cosmetics counter and been offered plastic surgery
Fertility fog infects cultures and nations worldwide, even those that place more of a premium on reproduction than we do in the West. A global study published for World Fertility Awareness Month in 2006 surveyed 17,500 people (most of childbearing age) from 10 countries in Europe, Africa, the Middle East and South America, revealing very poor knowledge about fertility and the biology of reproduction. Take Israel, a country that puts such a premium on children that they offer free IVF to citizens up to age 45 for their first two children. According to a 2011 study in Human Reproduction , which surveyed 410 undergraduate students, most overestimated a women’s chances of spontaneous pregnancy in all age groups, but particularly after receiving IVF beyond age 40. Only 11 per cent of the students knew that genetic motherhood is unlikely to be achieved from the mid-40s onward, unless using oocytes or egg cells frozen in advance. ‘This can be explained by technological “hype” and favourable media coverage of very late pregnancies,’ the authors concluded.
M edia hype is one reason why so many overestimate the possibility of getting pregnant when older. With frequent news stories of older celebrities having babies – Amanda Peet (now expecting her third child at 42), Jane Seymour (who had twins at 44), Geena Davis (twins at 48), Kelly Preston (third child at 48), Beverly D’Angelo (twins at 49) – it’s hard not to think the sky’s the limit.
But the stories behind how these actresses came to have these children hardly ever come to light: for example, was it their first child? (A woman who already has children is more likely than a first-time mother to conceive when older.) Did she use IVF? (Most celebrities do not admit it.) Most importantly, did she use her own eggs or donor eggs? (Donor eggs from younger women reset the fertility clock, because it’s the age of the egg, not the uterus, which matters most – up until age 45, when age begins affecting pregnancy even with donor eggs.)
The stories behind how these actresses had their children at such advanced age hardly ever come to light. Yet fixation on celebrity fairy tales gives the rest of us false hope
For a woman over 42, there’s only a 3.9 per cent chance that a live birth will result from an IVF cycle using her own, fresh eggs, according to the American Society of Reproductive Medicine (ASRM). A woman over 44 has just a 1.8 per cent chance of a live birth under the same scenario, according to the US National Center for Chronic Disease Prevention and Health Promotion. Women using fresh donor eggs have about a 56.6 per cent chance of success per round for all ages.
Indeed, according to research from the Fertility Authority in New York, 51 per cent of women aged between 35 and 40 wait a year or more before consulting a specialist, in hopes of conceiving naturally first. ‘It’s ironic, considering that the wait of two years will coincide with diminished fertility,’ the group says.
S tories of celebrities and other older women having babies have led to misunderstanding precisely because they fly in the face of long-held beliefs: for many years, all women heard was that fertility takes a dive at 30-35. But when you see plenty of people having no trouble having babies until 40, you think it’s just a scare tactic. That’s what happened to me, anyway. Raised in a traditional Jewish, family-oriented community that emphasises early marriage and plenty of children, I was constantly warned about waiting too long to get married and have kids. But when I left that community, I saw so many women who had kids at 37, 38, that I thought it was all bubbe meises , old wives’ tales.
It’s not. There is a declining rate of fertility strongly tied to age – but the exact numbers have recently come up for debate. In a piece for The Atlantic in 2013 headlined ‘How Long Can you Wait to Have a Baby’, the psychologist Jean Twenge showed that much of the research cited by articles and studies (such as how one in three women aged 35-39 will not be pregnant after a year of trying) was based on ancient figures, such as French birth records from 1670 to 1830. She also cites a 2004 Obstetrics and Gynecology study examining chances of pregnancy among 782 European women. With sex at least twice a week, the study found, 82 per cent of 35-to-39-year-old women conceived within a year, compared with 86 per cent of 27-to-34-year-olds. ‘In our data, we’re not seeing huge drops until age 40,’ Anne Steiner, an associate professor at the University of North Carolina School of Medicine, told Twenge.
This information might have helped people such as me. Had I known that 40 was the real age when fertility generally takes a dive – and not that I’d long passed it five or 10 years’ prior – I might have rushed into baby-making. I might have saved myself from a hellacious IVF journey that still hasn’t ended.
It’s easy to blame our cultural fertility fog on the media and faulty scare-mongering statistics. But the problems go way deeper – and start much earlier.
‘I think as a society no one tells women what fertility is,’ said the reproductive endocrinologist Janelle Luk, medical director of Neway Fertility in New York City. When it comes to women’s health, she said: ‘There’s a cultural barrier.’ Even a menstrual period is only discussed in terms of inconvenience, cramps and pain. ‘When you’re 15 or 17, you’re not planning to have a family, and you only learn about preventing pregnancy,’ she added.
‘Women don’t know there’s a limit: the message is equal, equal, equal. But our biological clock is not’
Luk became interested in women’s health issues from an early age. Her own mother had been given away in China as a baby and raised by relatives because she was a girl. They moved to the US when Luk was 10, and by the time she was in college she got involved as a sex health advocate. She talked on campus about sexually transmitted diseases such as the human papilloma virus (HPV) and chlamydia, which causes infertility in 10 per cent of the infected. ‘It’s part of biology, and women should know because the consequences are severe: HPV could lead to cervical cancer – it’s not fair, but that’s how it is.’
‘No one talks about fertility,’ said Luk, who does not believe women are really open to hearing about it. ‘I don’t think women know that there’s a limit: the message is equal, equal, equal. Women say: “We want to go to college, we want to work on our careers, we want to be equal to men.” But our biological clock is not.’
Or was not, she adds. Today, especially in the past two years, with the advancements in egg-freezing – the flash-freezing vitrification process improves egg survival rates to 85 per cent – women can start to level the playing field by freezing their biological clocks. The problem with oocyte cryopreservation? ‘The younger the better,’ Luk told me – before the age of 30 is ideal. But women often don’t become cognisant of the need to do it until their fertility starts declining, and the process is less likely to produce a baby.
Another way women might even out the fertility playing field is by focussing on the so-called male biological clock. But is there one? Although there have been recent news stories about how advanced age in men (over 40 or 50) increases time to conception and the incidence of autism and schizophrenia, the absolute risk is negligible. ‘When you look at the numbers, you have to separate what the absolute risk and the increased risk is,’ said Natan Bar-Chama, director of male reproductive medicine and surgery at Mount Sinai Medical Center in New York. ‘The absolute risk is still really very small.’
F ertility fog is not just a cultural problem, it’s human nature, says Aimee Eyvazzadeh, a San Francisco Bay Area fertility specialist who calls herself the egg whisperer. Recently, a 51-year-old woman telephoned her to discuss egg-freezing. ‘Nobody wants to be told they’re infertile. Women are shocked when nature applies to them,’ she says. She wishes women were more informed. She recommends a woman start by talking to her mother.
Women can also take certain tests to ascertain their fertility age. One blood test measures follicle-stimulating hormone (FSH), indicating the egg quality, especially as compared with other women in the same age group. Another test, for Anti-Müllerian hormone (AMH), assesses ovarian reserve (how many eggs are left) and their potential for fertilisation. ‘It’s part of general health to prevent infertility,’ Eyvazzadeh said. Patients in their 40s complain to her that they wished they’d gotten those blood tests earlier, because now they can’t use their own eggs. ‘If she were counselled at an earlier age, and educated, perhaps she would have made different decisions.’
But who is going to counsel women who are so in the dark? In 2014, The American Congress of Obstetricians and Gynecologists (ACOG) had its Committee on Gynecologic Practice deliberate the issue. The Committee Opinion recommends education about age and fertility for ‘the patient who desires pregnancy’ – and that is a quote.
there are no guidelines for talking to a woman about her fertility unless she herself brings it up
In other words, only women who are already trying to get pregnant or thinking about it should be counselled about how age affects fertility. But what about the other women – the ones who do not realise their fabulous health might not protect them from age-related declining fertility; the ones who might want to start thinking about freezing their eggs while they’re still young enough; the ones who are waiting for one reason or another to have a baby and don’t know that perhaps, like me, they don’t have that much time. Does ACOG believe it’s the doctor’s responsibility to bring up the subject?
‘We feel that women should be able to talk to their ob/gyn about fertility,’ said Sandra Carson, ACOG’s vice president for education. ‘We certainly want to remind women gently that, as they get older, fertility is compromised, but we don’t want to do it in such a way that they feel that it might interfere with their career plans or make them nervous about losing their fertility.’ In other words, there are no guidelines for talking to a woman about her fertility unless she herself brings it up.
All this talk of ‘gentle’ reminders and ‘appropriate’ counselling has a history – a political one. Back in 2001, the ASRM devoted a six-figure sum to a fertility awareness campaign, whose goal was to show the effects of age, obesity, smoking and sexually transmitted diseases on fertility. Surprisingly, the US National Organization for Women (NOW) came out against it. ‘Certainly women are well aware of the so-called biological clock. And I don’t think that we need any more pressure to have kids,’ said Kim Gandy, then president of NOW. In a 2002 op-ed in USA Today , she wrote that NOW ‘commended’ doctors for ‘attempting’ to educate women about their health, but thought they were going about it the wrong way by making women feel ‘anxious about their bodies and guilty about their choices’.
Although the ASRM denies the backlash is connected, its spokesman Sean Tipton says the organisation has not done a fertility awareness campaign since.
I n the end, lack of fertility education on the medical side and the unwillingness to explore it on the patient side seems to come down to the fear of offending women.
Naomi R Cahn, author of Test Tube Families (2009), argues that ‘the politics of reproductive technology are deeply intertwined with the politics of reproduction’ but ‘although the reproductive rights issue has a long feminist genealogy, infertility does not’. Discussion of infertility is threatening to feminists on two levels, she contends: ‘First, it reinforces the importance of motherhood in women’s lives, and second, the spectre of infertility reinforces the difficulty of women’s “having it all”.’
the debate over egg‑freezing involves such work-life concerns as better maternity leave and family rights
Even a debate over egg-freezing has assumed political overtones. When Facebook and Google recently announced that they would pay $20,000 health benefits for female employees who wanted to freeze their eggs, ‘Room for Debate’ roundtable discussion in The New York Times had three of the five women opposing it: one was worried about the health concerns, another about fairness, and the third about corporations paying women to delay motherhood. In other words, the debate over egg‑freezing involves such work-life concerns as: ‘Why don’t we have better maternity leave?’ or ‘Why isn’t this country more supportive of family rights?’
I was one of those arguing in favour of egg-freezing, because despite success rates being low, they are better than waiting five years with older eggs. At 35, you have 20‑30 per cent chance of your frozen eggs creating a baby in the future, using IVF. At 42, it is 3.9 per cent – the difference is vast.
Unfortunately, the polarising nature of the debate on egg-freezing does little to lead us out of our fertility fog or illuminate the discussion. It just has many women categorically deciding they’re not going to freeze their eggs. That is a perfectly acceptable choice, except that these women seem to be deciding without investigating their individual fertility status, relinquishing control over their fate.
‘Shunning that information about the relationship between fertility and age, however, ignores biological facts and, ultimately, does a disservice to women both in terms of approaching their own fertility and in providing the legal structure necessary to provide meaning to reproductive choice,’ writes Cahn.
Although published five years ago, her words are particularly prescient in the US today in light of proliferating anti-abortion laws and ‘personhood legislation’ – proposals to declare legal human life from the minute an egg is fertilised, granting an embryo the same rights as a newborn. Resolve, the national US infertility awareness movement, resolutely opposes personhood legislation because it ‘would produce so many legal uncertainties about the status of embryos’ that it would make IVF virtually impossible. In the US, IVF would go the way of abortion, with clinics closing their doors, especially in conservative states; women would have as much difficulty getting fertility treatment as they do abortions.
Maybe this threat to embryos and IVF will finally wake women from their fertility fog and allow them to be educated about their own bodies, its capabilities and its limitations. Just like they would be informed about any other medical condition.
‘It is only with this information that reproductive choice becomes a meaningful concept,’ Cahn writes. ‘Choice cannot mean only legal control over the means not to have a baby, but must include legal control over the means to have a baby.’"
"If you were circumcised, are you a victim? | Aeon Essays",,https://aeon.co/essays/if-you-were-circumcised-are-you-a-victim,"My first encounter with an ‘intactivist’ was in my freshman speech class. Our assignment was to sway our classmates on a contentious issue, and she opened her speech by asking if any man in the class still had his foreskin. I raised my hand. This being late 1990s America, only one other student joined me. ‘You’re lucky!’ she said, before launching into a polemic on the many advantages of the male prepuce and the barbarism of infant circumcision.
I can’t remember all her specific arguments, but I know they roughly matched the intactivist or anti-circumcision talking points that I explored in depth later on. The foreskin has sensitive nerve endings; it provides gliding and natural lubrication that is useful for unprotected sex and masturbation (its main failing, as far as influential 19th-century Americans were concerned) and it acts as a protective layer, shielding the glans from harsh friction that can dull sexual sensation over time. In the opposing corner was circumcision, which destroys all of that for no good reason. More ideologically, by making this irreversible change before the child can consent, circumcision infringes on the autonomy of the individual in a way that can’t be justified in a culture that claims to care about bodily integrity and freedom of choice.
This all sounded fantastic to me. I sat back in class, revelling in my uncompromised state.
About a year later, for some reason, this wonderfully reassuring speech sprang to mind, and I found myself wondering what a circumcised penis looked like. I went to one of my school’s computer labs and glanced over my shoulder before searching Yahoo! for ‘circumcised penis’. As I clicked through the images of seemingly normal and natural penises, I felt like I was staring at a ‘spot the difference’ puzzle in which the same picture had been printed twice by mistake. It was impossible to say what made these penises circumcised. They all looked just like mine.
Then I had a troubling thought and did a search for ‘uncircumcised penis’. I was 19 years old when I realised I was circumcised.
I n German-occupied Europe during the Second World War, Nazis would sometimes confirm the Jewishness of men by having them pull down their pants. They couldn’t have done that in the US, especially not now. When I was born in 1979, the number of newborn boys circumcised during their birth hospitalisation was 64.5 per cent (by 2010, this had fallen a bit to a reported 58.3 per cent). The broader category of affected infants would have been higher because of religious circumcisions outside of hospitals, and even because of circumcisions such as my own, which was postponed to a return visit because of a staph infection. Given that Jews make up only about 2 per cent of the US population, evidently cutting is no longer a distinctively Jewish practice. So, if not for religious reasons, why do we do it?
At the most prosaic level, there are the medical arguments. It is said that uncut boys and men sometimes develop problems that they might have avoided if someone had removed their foreskins in infancy. This is correct. Phimosis, a tightness of the foreskin, is possible only if you have a foreskin in the first place. Furthermore, the US Centers for Disease Control and Prevention says that circumcision is somewhat protective against male urinary tract infections (UTIs), the human papilloma virus (HPV), heterosexually transmitted HIV in Africa, penile cancer, and genital herpes.
If you are a fan of the foreskin, or consider elective genital surgery on infants a no-no, the alleged medical benefits probably won’t convince you
But, say opponents, so what? Several of these related conditions are rare, no matter what your genitals look like. Safe sex prevents sexually transmitted infections (STIs) better than circumcision ever could. Is it worth the 111 infant circumcisions that the journal Archives of Disease in Childhood in 2005 found it would take to prevent a single UTI, when UTIs are easily treated with antibiotics? Should we do the 909 infant circumcisions that the US National Cancer Institute says it would take to prevent one case of penile cancer, when an HPV vaccine is a more effective preventative, and when we don’t remove the breast buds of girls to reduce their risk of breast cancer? Intactivists observe that infant circumcision isn’t a zero-risk endeavour, either: the American Urological Association estimates the risk of complications at 3 per cent.
One thing is clear. Whether you find the health case for circumcision impressive or not probably hinges on where the burden of proof falls for you. If you start off thinking that foreskins are a useless, smelly bother, any possible advantage of taking them off will be fantastically persuasive. If, on the other hand, you are a fan of the foreskin, or consider elective genital surgery on infants a no-no, the alleged medical benefits probably won’t convince you. Medical reasons take us only so far. To see circumcision for what it is, we need to look at the other things mixed up in it, and find the deep questions of principle that are hidden in a swirl of accusations of anti-Semitism, misogyny and misandry.
‘A t the time my son was born, I would have described myself as a totally secular atheist,’ explained Richard Shweder, a cultural anthropologist and professor of comparative human development at the University of Chicago. ‘And I had to face up to the question: am I going to circumcise my son?’
Shweder has a Jewish heritage and a professional interest in competing moral systems. He considers himself a cultural pluralist, which means that he gives the benefit of the doubt to cultures that seem to find significance and value in their practices, even when those practices don’t sit well with Western ideas. For example, he has some eyebrow-raising views on female genital mutilation (more on those below). Even so, his motivations here aren’t totally unlike the ones that persuaded my own, thoroughly gentile, mom and dad.
‘My wife had no hesitation,’ Shweder said. ‘From her point of view, that’s just what you do, that’s what looked normal to her, that’s what Jews do. From my point of view, the autonomy issues were on my mind. You know, it’s his body, how can I do this to him? All those questions were there. And then the other voice was, how can I not do this to him? The weight of thousands of years of tradition was on me. It felt too big and significant to set to the side. The ancestors had a vote.’
If your priority is to have your naked boy fit in with other naked boys, removing his foreskin is still a decent way of accomplishing that
My parents might not have had thousands of years of tradition on them, but there was still social pressure. Where I grew up in suburban Texas, sex education was so embattled that my high-school health teacher referred to condoms as ‘the c word’. It’s easy to say that my parents should have broached the issue of circumcision with me, but part of the reason they did it in the first place was precisely so that I could be a genital conformist in the US – in a sense, so it wouldn’t have to be discussed. Circumcision was too common and meaningless a procedure to warrant mention.
When you think about it, the social conformity case for circumcision in the US is pretty airtight. If your priority is to have your naked boy fit in with other naked boys, removing his foreskin is still a decent way of accomplishing that. But at the same time, that’s no guarantee that he won’t find himself facing some difficult questions.
You can try to fit in with the largest number of people, or you can try to make your ideas fit together. We live in a society that says, in effect: your body, your rules. If you buy into that liberal philosophical framework, it’s hard to make circumcision fit. And that’s when it starts to seem like a violation: not only of an important norm, but of the body itself.
T hat’s certainly how it seems to Matthew Hess, one of the most controversial personalities in the contemporary intactivist movement. In 2011, he won a good deal of publicity by submitting a ballot measure to ban circumcision in San Francisco (in fact, Hess has been submitting the bill to various authorities every year since 2004). A judge struck down the proposed ban as a violation of California law before it could go up for vote – but she couldn’t strike down Foreskin Man, the eponymous hero of Hess’s self-published comic, who flies around the world intervening when innocent boys are about to lose their foreskins.
Like me, Hess didn’t realise he was circumcised until college. During a hazing or initiation ritual in which everyone got naked, he was disturbed when only one of the men was called out for being an uncut rebel – and it wasn’t him. Hess tried to put the incident out of his mind. Later, though, he noticed a loss in sexual sensitivity, which he attributed to the lack of a protective foreskin. That’s when he started to think of himself as a victim. ‘I guess everyone loses their innocence eventually,’ he told me. ‘But that’s a very traumatic way to lose it. To know that it’s okay to mutilate another person, and look the other way, and not care about that person’s feelings about what happened to them. That makes a person feel so incredibly vulnerable that it really can affect their entire life.’
‘One of the great harms of circumcision,’ the Oxford philosopher Brian D Earp remarked to me, ‘is that some people feel violated. So this has a very serious effect on their sexuality because every time they look at their penis, they feel incomplete. Why do they feel incomplete? Because they were raised in a society that told them they had dominion about their body and then that choice was taken away.’
the blond-haired, blue-eyed prepuce-defending Foreskin Man is pitted against Monster Mohel, a thin, shadowy Jew with glowing eyes and sharpened fingernails
That idea of bodily dominion is at the heart of the Foreskin Man comic (2010-). The hero’s mission is to promote physical pleasure and personal freedom. His friends are rich, buff men and thin, busty women who have clearly had some work done. Though these characters might have felt social pressure to conform to popular beauty standards, no one forced them to spend hours at the gym or get breast implants. They are what they choose to be. They are free.
Things get murkier when we look at Foreskin Man’s enemies. They tend not to be half-hearted, ‘why not?’ cutters such as my parents. Naturally, given his polemical purposes, Hess goes after the most visible champions of circumcision. And so Foreskin Man ’s notorious issue No 2 pits the blond-haired, blue-eyed prepuce-defending Foreskin Man against Monster Mohel, a thin, shadowy Jew with glowing eyes and sharpened fingernails, a type who would look at home in a Nazi propaganda poster.
Hess told me that he sees circumcision as an institutional problem rather than a problem of willful cruelty. That systemic critique doesn’t quite come across in his comics, which present circumcisers as devious villains who relish harming young boys in order to perpetuate backwards superstitions. This might be why much of the intactivist movement has distanced itself from him. But a question remains: isn’t the very desire to end cutting basically anti-Semitic and anti-Muslim? Wouldn’t an end to circumcision be an assault on religious freedom?
F rancelle Wax first started thinking about circumcision during a trip to the UK when she was 19. In conversation with some British guys at a pub, it emerged that she was Jewish, and the Brits teased her that she would have been circumcised if she had been a boy. She thought this was a strange association to make with Judaism since she was also an American, and would have been circumcised as a boy whether she was Jewish or not. They were shocked to hear this. Wax was shocked at their shock. This got her thinking about circumcision. She came to see it as a test case for our thinking about practices that seem acceptable simply because we’re used to them. Today, she is making a documentary about intactivism, and she firmly rejects the idea that the movement is anti-Semitic. There are other Jewish intactivists, represented by groups like Beyond the Bris. ‘Are we all self-hating Jews?’ she asks. ‘That would be a hell of a claim to make, since many of us are practicing.’
Some moderates in the intactivist camp believe that any proposed ban should allow a religious exemption to avoid posing a threat to communities who find circumcision meaningful. Wax concedes that point on tactical grounds but not moral ones. She objects to the idea that children from gentile families are worth protecting and children from Jewish or Muslim families are not. American states don’t let Christian Scientists, Jehovah’s Witnesses and snake handlers treat children however they like, she says, so why should infant genital cutting get a religious pass?
Wax was, of course, referring specifically to male genital cutting. Almost no one in the US is looking to give female genital cutting any kind of pass. It is controversial to equate male and female genital cutting, which doesn’t stop some intactivists from doing it, with the unfortunate effect of bolstering the movement’s reputation as a frontline for men’s rights activists. But could there be merit to comparisons, if not strict equivalencies?
why is a country that circumcises boys all the time virulently opposed to the slightest harm befalling female genitalia?
Wax says that removing the clitoral hood would be anatomically analogous to removing the male prepuce, since both encase the sensitive glans tissues, preventing them from coarsening and drying out, but she points out that removing the foreskin likely entails a greater loss of sensual pleasure. The foreskin contains most of the penis’s fine touch nerve receptors, which offer a different quality and degree of sensation than the glans itself. In addition, it adds an extra gliding action and brings its own lubrication to the party, which can be nice for sexual partners too. In any case, it seems like there’s a plausible comparison to be made somewhere along these lines.
So why is a country that circumcises boys all the time virulently opposed to the slightest harm befalling female genitalia? Men’s rights activists might point the finger at widespread misandry, but Wax thinks the blame falls squarely on the self-perpetuating and overwhelming prevalence of male circumcision, which makes it all but unquestionably accepted in the US. If genital cutting is one of the worst things that can happen to a woman, how could it relate to something so routine in a Western democracy that most men don’t even think to question it?
W e’re in uncomfortable territory here. Accepting that there are parallels between male and female genital surgeries might turn more people against male circumcision. Or it might have the opposite effect: we might relax our opposition to milder forms of female genital cutting. That seems to be the view of Fuambai Sia Ahmadu, an anthropologist who was born in Sierra Leone and raised in the US before she returned to Africa to complete a coming-of-age ritual that included genital cutting. For her, the tradition symbolises female empowerment, not oppression, and she calls it sexist hypocrisy when Americans are fine with male cutting but not female cutting.
That’s not far from being the cultural anthropologist Richard Shweder’s view. In 1972 he taught for a year at the University of Nairobi, which helped to familiarise him with the genital cutting practices there, and with colonial efforts to prohibit female but not male circumcision. He was startled in the early 1980s when feminists started talking about female cutting in African countries as one of the world’s great atrocities to be eradicated. Wasn’t this a return to ‘dark continent’ thinking?
Shweder sees intactivists as liberal imperialists: that is, people who view personal autonomy as the preeminent value. ‘In the ethics of autonomy,’ he told me, ‘the self is thought of as a preference structure.’ But there are other ways to think of the self, and Shweder doesn’t believe we can so easily dismiss alternative value systems, ones that put more emphasis on loyalty and sacrifice and the duties and obligations of social status, or on our connection with transcendental or so-called divine forces. Such systems find value in marks of membership and do not recoil at traditional collective imperatives such as infant circumcision.
This might seem like a confounding point of view. It’s true that no matter what any adults think of circumcision, babies are unanimously against it: for them, it’s just inexplicable pain. And it’s true that when parents and communities hold a circumcision ceremony with the infant as the unwitting star, they’re using babies as a way of spreading a particular culture and pleasing dead ancestors. Then again, isn’t this what having kids is all about? Few people have children because they want to care for random free agents who don’t share any of their values. Children are beautiful, perfect, cherished little bundles of meaning. That role is usually compatible with their wellbeing, but sometimes it isn’t. This leads to conflicts of interest. It would be fascinating to try to resolve them all in the child’s favour. But until we do, it seems odd to try to ban circumcision because it fails to meet an impossible standard of parental selflessness.
Besides, we have been assuming that the emotional dangers of cutting all go one way. What about the boys and men with circumcision envy? Anyone convinced that intact foreskins are the missing puzzle piece to bodily utopia might find it eye-opening to read forum posts by men preparing for adult circumcisions: men who wish they had been circumcised as infants and plan to have their hypothetical sons circumcised to spare them the nuisance that foreskins proved to be for their fathers. Unless we dismiss all these foreskin-renouncers as irrelevant, we can’t honestly claim that avoiding infant circumcision is the guaranteed best choice for everyone.
Until far more Jews and Muslims step forward to protest their own circumcisions, to me it looks more hurtful to ban religious circumcision than to leave it alone
In a way, the question comes down to a weighing of risks and harms: not only medical ones, but psychological and spiritual risks, too. The philosopher Brian D Earp opposes circumcision, not simply because it violates the rules of liberal autonomy, but because enough people in the West accept those rules for the violations to be disturbing. And so he insists on the right of those such as Matthew Hess to their own anguish. ‘A lot of people will be very dismissive of these sorts of feelings,’ Earp told me. ‘They’ll say, “Oh well, they just need to get their head checked out. It’s not about their penis, it’s about their head.” And I think to myself, if somebody had any other part of their body missing and felt psychologically violated about it, even if technically they didn’t need that part of their body, we’d sort of understand that feeling of loss.’
I agree with Earp and Hess that this is honest and significant pain, not a laughable quirk to be mocked into hiding – and this is why I can’t fully side with the intactivists. If harm is in large part subjective – and to credibly amplify the voices of a tiny minority who regret circumcision, intactivists need to admit it is – cutting is bad only for the people who find it so. It’s dishonest to claim that the joy the Jewish practice of brit milah brings its practitioners counts for nothing when most people who are circumcised for religious reasons do not grow up to think of themselves as abused. The organisation Friends of Refugees of Eastern Europe (FREE), based in New York, says that it has circumcised more than 13,000 Jewish adults who were prohibited from infant circumcision in the Soviet Union. Many of them no doubt felt harmed by not having been circumcised in their infancy. Until far more Jews and Muslims step forward to protest their own circumcisions, to me it looks more hurtful to ban religious circumcision than to leave it alone.
On the other hand, what FREE proves is that adult circumcision still counts in Judaism. That’s something to consider too! In fact, FREE’s website says: ‘Our ancestor Abraham was circumcised at age 99. In part, this teaches us that no Jew should be left out of this mitzvah, regardless of age. Each Jew that takes advantage of FREE’s circumcision program forms another link in a 4,000 year-old chain of commitment to Judaism’s higher calling.’
Shweder told me that he thinks it’s worth thinking about a possible cultural shift that would delay Jewish circumcision to around the time of the bar mitzvah . ‘Perhaps there was a translation problem with Genesis 17 and, when Abraham received the command given by the God of the Jews, it really stated 13 years, not eight days,’ Shweder said mischievously. ‘After all, Ismael, who was Abraham’s only son at the time, was 13 years old when he was circumcised.’
Speaking personally, I would be happy to see circumcision as a habitual, pointless, barely discussed lopping-off of a body part end today. But the symbolism of circumcision isn’t universally negative, and attempts to ban it are unfairly dismissive of the symbolic value that many find in it. If we should refrain from telling people what their psychic wounds are allowed to be, we should be just as cautious about telling them what cultural traditions they are allowed to find meaningful. Circumcision hurts some people, but we can’t pretend that it doesn’t bring joy to others.
For those of us who are circumcised and wish we weren’t, we can try to keep it in perspective. When I first heard the intactivist argument, I didn’t realise I was circumcised. My foreskin confusion was a Trojan horse that had allowed anti-circumcision ideas to march into my mind completely unopposed. I never made it one of my major political issues, but when I thought about it – which I often did – I felt powerless, diminished, ‘incomplete’.
Then, at some point around my mid-twenties, I decided to stop caring so much about it.
Everyone has regrets about their upbringing; it’s just not possible for parents to make every choice align with their child’s future preferences. All the same, to future parents of boys, I would say this: take the decision of circumcision seriously. And if you do decide to remove your baby’s foreskin, just try to be sure that you replace it with something better."
How anti-vaxxers fuel the spread of polio | Aeon Essays,,https://aeon.co/essays/how-anti-vaxxers-fuel-the-spread-of-polio,"Polio should have been eradicated long ago, at least in the developed world. After all, we’ve had a good, efficient vaccine for 60 years. So why is polio still here? And what is it doing in my country?
Israel is the original start-up nation, powered by high-tech and science, with some of the most sophisticated medical research in the world. We have come a long way from the 1950s, when polio was still with us and the signature experience was living on a Kibbutz. We are a modern, wired nation, but we forgot one thing: we still live in the Middle East. We have great weather, plenty of sunshine, lovely beaches, and two relatively nearby countries – Pakistan and Afghanistan – where the wild polio virus still roams free.
Shortly before polio returned to Israel, it hit the sewers of Egypt, our neighbour to the south, in December 2012. Egypt, like Israel, already vaccinated widely – so someone from Pakistan must have entered Egypt, used a toilet, and excreted polio into the sewer system around Cairo, probably infecting some Egyptians along the way. Egyptian physicians could find no cases of polio paralysis, the most devastating outcome of the disease, and that made sense: 95 per cent of people infected with polio show no symptoms and never become sick. But Egypt knew paralysis would be next if the virus was allowed to spread. So Egypt fought back. A nationwide campaign was initiated, and more than 14 million Egyptian children were vaccinated against polio. Within a month, the Egyptians had vanquished polio from its sewers and its country, yet again.
But that didn’t stop the polio virus from migrating, to us. Pakistan and Israel don’t share a border, but travel between Egypt and Israel is a common affair. During the month it took Egypt to stop the spread of polio, someone from Egypt must have travelled to Israel, bringing polio with them.
That must explain why, on 9 April 2013, our nationwide monitoring team isolated wild poliovirus type 1 (WPV1) – genetically related to the polio from Pakistan and Egypt – from sewage samples taken in Rahat, a city in southern Israel. Immunisation levels in Rahat, at 94 per cent, were good but not perfect. So the Israel Ministry of Health initiated a catch-up programme in the area where the samples were collected, finding the unprotected and inoculating them with the standard polio vaccine – IVP, or inactivated polio vaccine. That product, injected through the skin, is a totally dead version of the polio virus, and cannot cause the disease.
But it was too little, too late. A few days later we found polio virus in the sewers of Be’er Sheva, a city of some 200,000 people, about 20 km south of Rahat. Our national monitors responded by increasing sewer surveillance and scrutinising the population for signs of active illness, including paralysis and meningitis, a swelling of the lining of the brain.
I was, at the time, an intern in public health at the Carmel Medical Center, in a suburb of Haifa, and I was pretty excited – it’s not every day we have a forgotten epidemic on our hands. Perhaps I was cavalier, but only because I felt sure we would contain the epidemic in a month without incident, just like the Egyptians.
It was hard to tear myself away for a long-planned family vacation to the UK while this uproar was going on. But we had a great trip, and my two kids loved both the country and the fact that their mother stopped talking about polio all day.
I expected to return to work at the hospital, business as usual, problem solved. But while we were away, polio had spread through the nation’s sewer system and Israel had spiralled into panic mode.
I t was terrifying. Poliomyelitis has been with us for thousands of years, at least since the days of the ancient Egyptian pharaohs. Back then, the disease was truly rare. That’s because people were exposed to poliovirus (which spreads through faeces) as infants, and built immunity against it from the start. But with improvements in hygiene over the centuries, encounters with the virus were delayed. Without early resistance, we became more vulnerable, and a scarce disease became an epidemic.
By the first half of the 20th century, with dramatic improvements in hygiene, the ravages of polio had become well-known: infection provoked a fever, sore throat, sometimes a stiff neck. Then the terror: a so-called ‘flaccid’ paralysis, caused by lack of muscle tone, starting from the feet and progressing up, as patients became unable to walk, sit erect and, finally, breathe.
Those patients were put in ‘iron lungs’, a machine that pulls the chest wall up and down, compensating for the paralysed diaphragm and chest muscles. That was the only way they could survive. Sick children had to stay in the iron lungs until the paralysis withdrew and their muscles returned to the normal strength. For some children, it meant just a few painful months. For others, it meant staying in those machines for the rest of their lives.
Then two people saved the world – the American virologists Jonas Salk and Albert Sabin. In the 1950s, they developed two different vaccines against polio, both efficient, both pretty safe, and both with the potential to prevent future epidemics. First, Salk invented IPV, the inactivated polio vaccine, made from dead virus. It could never cause the disease itself, but because it was injected into the muscles, it had one slight drawback: it was metabolised before reaching the gut, and thus recipients exposed to polio could harbour the virus in their intestines without ever getting sick. Such carriers could still pass on polio, through their faeces, to other carriers, and cause the disease in anyone who was not vaccinated.
Within two months, there were 15 new paralytic cases in Israel – the elderly, the immune-compromised, the already sick
Given that risk, by 1961, much of the developed world including Israel and the US switched to Sabin’s OPV, or oral polio vaccine. OPV was a living virus, weakened in the lab so it would not cause disease. Yet OPV wasn’t perfect, either. That weakened virus could still paralyse one in every 2.7 million children given the vaccine. It was better than the statistics of 1 in 200 unvaccinated children exposed to the wild virus, but still a slight risk. This type of polio disease was called vaccine-associated polio paralysis, or VAPP.
In countries that vaccinated, these life-saving but imperfect vaccines set the stage for polio’s ebbs, flows and outright outbreaks for years. It was possible to use only IVP, only OVP, or a combination of the two, with strategies varying around the world. It came home to roost in Israel in 1988, after the Ministry of Health decided to halt use of the live vaccine (and risk of dreaded VAPP) in two northern districts – Hadera and Ramla, near the city of Haifa. This group of children received only IPV, the killed vaccine, and they were well-protected against polio, though without gut immunity. Yet the switch had been a big mistake. Within two months, there were 15 new paralytic cases in Israel consisting of the most vulnerable targets – the elderly, the immune-compromised, the already sick.
Analysing the situation, the Israeli government looked at the global data collected by the World Health Organization: it turned out that in nations at risk, a combination of the two vaccines could work best. A single, previous IPV shot made of dead virus could protect against the spread of vaccine-associated polio from the live vaccine. The live vaccine, meanwhile, would prevent the virus from taking root in the gut, where it might spread out to infect the vulnerable and exposed.
So in 1990, Israel initiated a new vaccination programme – every child first received IPV, injected and dead. Then, only afterwards, they received OPV, the living oral vaccine that passed through the gut. For 15 years we saw no sign of polio in Israel – not even in the sewers. With polio seemingly gone, we followed the lead of the US in 2005 and once more pulled OPV, eliminating even a minuscule risk of VAPP.
B y 2013, it was as if we had taken a time machine back to 1988 – only now, instead of just two districts, children age nine and under from all over Israel lacked gut immunity. With polio entering the country from countries around the Middle East, these kids could become carriers, spreading the virus to the unvaccinated and the immune-compromised alike, without ever getting sick themselves.
Just as before, such children endangered everyone with whom they came into contact. We found the polio that they carried in sewers all over Israel, and it was only a matter of time before one of them infected a vulnerable neighbour or relative – elderly grandparents; HIV patients; transplant or autoimmune patients on immune-suppressants; perhaps babies who hadn’t yet been vaccinated or developed antibodies. The government determined that a supplementary dose of OPV for carrier children was the only way out.
The vaccination campaign began on 5 August 2013, and Facebook exploded with complaints over the sacrifice that meant: totally healthy children were to receive the live vaccine, and parents were livid.
Fanning the flames of anger and fear and sabotaging our campaign was anti-vaccine propaganda coming from around the world, especially from the US. It’s amazing that this uneducated movement would descend on Israel in the middle of our crisis, convincing our population to turn OPV down. They doubted OPV efficiency and safety. They questioned the motives of the Israeli government and, alarmingly, they dismissed the existence of polio as an infectious disease.
We had to convince people with healthy children to vaccinate them again, not for their own safety, but for the public good
Now the war to stop polio was being fought on two fronts – vaccines and words. The vaccines did their part. Children who took those OPV drops built their gut immunity against polio and were no longer at risk of becoming walking biological weapons. If a child was already a polio carrier, the OPV wouldn’t help, but it wouldn’t harm him or her either. If the kid wasn’t a carrier, the OPV would make sure he or she would never become one.
But that wasn’t enough. We had to convince people with healthy children to vaccinate them again, not for their own safety, but for the public good. As a student of public health and a blogger with a social media presence, I took to the internet, especially Facebook, to fend off the anti-vaxxers, answer these questions, and explain the facts. It was such an all-consuming job that my kids and friends didn’t get a chance to see me for the next two months.
When one question was answered, another followed, and when all questions were answered, vaccine sceptics went back to square one and started the whole discussion again.
In fact, I explained to my online audience, for those not receiving the dead vaccine first, OVP did bring a small risk of VAPP – but that wasn’t the situation now. Everyone getting OPV had gotten IPV first.
I must be a part of the ‘Big Pharma’ industry if I thought polio was a dangerous disease, they said, or just a fool
Every time I set the record straight, pushback emerged. But after a while, I found a whole community of helpers by my side, waging the battle at the online front. Israeli doctors, science bloggers, the sceptics community and scientists all joined the explanatory effort. The data about the vaccine was available and most of us had lots of experience at explaining science in simple terms.
For months, the firestorm raged.
‘Since no one is sick, why should we vaccinate?’ asked the anti-vaxxers.
‘Why do you want to see someone with polio paralysis?’ asked the pro-vaccines community.
‘Why should I put my child at risk?’ asked a parent.
‘You’re not putting your child at risk,’ responded the pro-vaccines groups. ‘Getting OPV after IPV is completely safe.’
Pro-vaccine parents published pictures of their kids receiving OPV in the nurses’ clinic to prove their point.
As my online presence increased, I was personally targeted by anti-vaccine activists. They questioned my professionalism as well as my personal integrity. I must be a part of the ‘Big Pharma’ industry if I thought polio was a dangerous disease, they said, or just a fool.
As long as the accusations stayed virtual, it was all right, but I was terrified that someone might find out where I lived and attack my family. I went offline for a few days. Luckily for me, by that time my pro-vaccine colleagues were deftly handling the onslaught. They kept on the whole time I was gone. After I came back, we started a vacation rotation. Each time someone else went offline, others stayed on to fight the fight on Facebook, convincing parents to vaccinate their kids.
Every day during those two hard months, there was a different obstacle to tackle. One day a mother claimed her little girl had become paralysed half an hour after receiving OPV. Within minutes, the story spread through the web. Another two hours passed before someone reported the real story: the child fell asleep outside the doctor’s office, wet herself in her sleep and, when she woke up, her feet felt numb. The explanation was mundane and didn’t fit the anti-vaxxers’ agenda. The pro-vaccine group had to spread the rest of the story wherever they could.
At least I wasn’t the vaccine ‘poster girl’ anymore; there were many other people whose reputation was dragged through the mud by the anti-vaxxers along with mine. I made many new friends and, as the saying goes, a sorrow shared is a sorrow halved.
By October 2013, Israel’s official national campaign was over. The virus was still around, but more than 60 per cent of children in Israel had received OPV. It wasn’t a perfect score, but it was enough to turn the tide. By November 2013, Israel’s sewage samples started to come back negative for polio.
And in January 2014 we had a victory on the ground – a new committee convened and, after reviewing the evidence, decided to reintroduce OPV into Israel’s vaccination schedule for good.
I know this vaccine is alive, but I am not afraid. Data from around the world and from Israel tells me that OPV given after IPV is safe and effective. I know that VAPP is no longer a risk, and that the protection against polio is optimal.
We won the fight against polio in Israel but we are losing it in the rest of the world. Afghanistan, Pakistan and Nigeria still have polio, and they are still exporting it to other countries. Syria had 22 paralytic polio cases in the wake of the civil war and the breakdown of their health system. Polio exported from Nigeria has reached the Horn of Africa, and more than 190 children were diagnosed with polio paralysis in 2013.
My parents gave me a world without smallpox – I wanted to give my children a world without polio but, beyond the borders of our little country, polio is back."
Why is there still no contraceptive pill for men? | Aeon Essays,,https://aeon.co/essays/why-is-there-still-no-contraceptive-pill-for-men,"‘I am too young to die,’ said my patient. ‘My kids are small. They still need me.’ I explained that having a heart attack did not mean that she would die. We had caught it early. Within the next minutes, she would have an angiogram to remove the blood clot in her coronary artery, and we had already started her on a nitroglycerine drip and an infusion with a blood thinner. But I understood her fear. Women in their early 40s and in good health are not supposed to have heart attacks. She was not overweight, she never smoked, she had no history of high blood pressure or high cholesterol, and nobody in her family had ever had any heart problems. As we wheeled her away, she said that her chest pain was easing up. She was trying to be brave, but I could still see tears in her eyes.
Twenty minutes later, the interventional cardiologist and I stared at the angiogram images. My eyes kept flicking back to the electrocardiogram that had prompted our diagnosis. It had indicated the presence of a clot blocking an artery of the heart, but the angiogram showed completely pristine arteries: no clot, no plaque. I gave my patient the good news and saw her smile and utter a prayer.
Over the course of the next 24 hours, blood tests confirmed that she’d suffered a heart attack. Perhaps she had an exceedingly rare condition in which transient spasms of coronary arteries can cause a heart attack. Perhaps there had been a clot in her artery but it had dissolved before the angiogram, thanks to the intravenous blood thinner. The blood tests and ultrasound images showed that the damage to the heart was minimal, probably because the blood flow had normalised so quickly. I was happy for her and her family, but I was bothered by a nagging question. Why would a young, healthy woman with normal coronary arteries suffer a major heart attack?
She was taking one regular prescription medication: an oral contraceptive. One of its rare but significant side effects is the increased rate of blood-clot formation. The risk varies from one pill to another, but can be twice, threefold or even higher for oral contraceptive users when compared with women who do not use them. The risks are still small: a recent study monitored 1.6 million women in Denmark (ages 15-49) over a 15-year duration, and found that only 3,311 women had a stroke related to a blood-clot formation, and 1,725 women had a heart attack. My patient was using an oral contraceptive for which multiple studies had confirmed an association with blood-clot formation.
In clinical practice, it is often very difficult to prove cause and effect. Diagnoses are derived from the recognition of correlations or patterns, and frequently based on educated guesses instead of definitive scientific evidence. In this case, we advised our patient to stop using oral contraceptives. She recovered completely, and has had no recurrence of a blood clot or other major health problems. We will probably never know for sure whether contraceptive pills contributed to her heart attack, but her case serves as an important reminder that these risks are real.
The arrival of the birth control pill in the 1960s was hailed as a social revolution that decoupled sexuality from reproduction. It empowered women by giving them true reproductive control, because it allowed for reliable and reversible contraception. Women could delay or prevent reproduction without having to abstain from sex, and they could discontinue usage if they wanted to have a child. Over the years, many additional female contraceptives have been developed so that women today can choose from pills, injections, patches or intrauterine devices — many of which are even more reliable than those of the 1960s.
By contrast, the choices for male contraception are far more limited: it’s either sterilisation (a vasectomy) or condoms. Vasectomy has been used since the late 19th century, while the condom has an even longer linage. In the 16th century, the Italian anatomist Gabriello Fallopio described a condom made out of a linen sheath, used to prevent the transmission of syphilis. By the 18th century, condoms were prized as male contraceptives, and were even mentioned by the Italian adventurer Giacomo Casanova, who described them as ‘English Overcoats’.
Condoms can prevent the spread of sexually transmitted diseases, but, as a reproductive control strategy, they are not as reliable as their packaging suggests. Unintended pregnancies occur in up to 18 per cent of couples who rely on condoms for contraception. Vasectomies are very effective, with less than a one per cent ‘failure rate’, but they are extremely difficult to reverse. A change of mind means complex microsurgery with uncertain results. In a society that increasingly recognises that men and women should share responsibilities and opportunities equitably, the lack of adequate reproductive control methods for men is striking — and puzzling — especially since many newer methods for male contraception have been developed during the past decades yet none has become available for general use.
N ewer approaches to male contraception can be divided into two groups: hormonal and non-hormonal. Hormonal male contraceptives act by reducing testosterone levels in the testicles, which drive the production of sperm cells. Most hormonal male contraceptives that have been studied in clinical trials involve the administration of testosterone, either as an injection, implant, oral pill or patch. It might seem counterintuitive to supply extra testosterone in order to suppress sperm-cell production, but this approach works by taking advantage of an internal brake in the male reproductive system. Testosterone production in the testicles is activated by hormones released from the pituitary gland of the brain — the follicle-stimulating hormone (FSH) and the luteinising hormone (LH). Some of this testosterone seeps out from the testicles into the bloodstream and signals to the brain: ‘Stop releasing FSH and LH, there is more than enough testosterone to go around!’
A testosterone-containing contraceptive mimics this function by increasing testosterone levels in the bloodstream, thus activating the shutdown signal. The brain responds by turning off the FSH and LH production; the testicles stop making their own testosterone. Testosterone levels in the testicles start to ebb, eventually dropping below the threshold required for adequate sperm-cell production. The levels of testosterone in the bloodstream supplied by the contraceptive are sufficient to maintain masculine features and male libido, but cannot compensate for the loss of testosterone in the testicles, and thus cannot restore sperm-cell production. Newer-generation male hormonal contraceptives combine testosterone with another class of synthetic hormones called progestins, also used in female contraceptives and extremely effective at activating the FSH/LH shutdown.
So far, clinical trials have shown that it takes time — six weeks or longer — until sperm counts drop low enough to stop fertilisation. Short-term studies of the side effects of male contraceptives have not revealed anything major: acne, weight gain, increased libido. Most male contraceptive trials have been small, often recruiting only 10-100 men, and the measured ‘success’ was based on achieving undetectable or minimal sperm counts. Yet the ultimate test of efficacy is not a drop in sperm counts but the prevention of unintended pregnancies in couples who rely on these contraceptives as their primary method of reproductive control. Such efficacy trials require the recruitment of a large number of volunteers and their costly long-term monitoring. Large-scale studies are also needed to ascertain the long-term safety profile of male hormonal contraceptives.
‘None of the big companies will touch hormonal male contraception again’
One of the largest male contraceptive efficacy trials ever conducted was sponsored by the World Health Organisation (WHO) and CONRAD, the US-based reproductive health research organisation. Called Phase II TU/NET-EN, this landmark multicentre study was designed to answer key questions about the long-term safety and efficacy of male hormonal contraception, and enrolled more than 200 couples between 2008 and 2010. The contraceptive used was a long-acting formulation of testosterone (testosterone undecanoate, or TU) combined with a long-acting progestin (norethisterone enanthate or NET-EN), administered via injections every two months. The trial included an initial treatment phase to suppress sperm production, and a subsequent ‘efficacy phase’ that required couples to rely exclusively on this form of birth control for one year. However, in April 2011, the trial was terminated prematurely when the advisory board noticed a higher than expected rate of depression, mood changes and increased sexual desire in the study volunteers. By the trial’s end, only 110 couples had completed the one-year efficacy phase; their efficacy results should be released in the near future.
The trial did not include a placebo control group, so the investigators could not determine whether the observed side effects were due to the hormone combination or a side effect of frequent injections. Just like we do not perform ‘placebo’ or sham surgeries on patients, we cannot in good conscience enrol people in a placebo group of a contraception efficacy trial, because most of the couples in the placebo group would end up with an unintended pregnancy.
The discontinuation of the WHO/CONRAD trial was a major setback in bringing male contraceptives to the market. It also raised difficult ethical questions about how to evaluate side effects in male contraceptive trials. Since all medications are bound to exhibit some side effects, what side effects should be sufficient to halt a trial? Female contraceptives have been associated with breakthrough bleeding, mood changes, increased risk of blood-clot formation, as well as other side effects. Why should we set a different bar for male contraceptives?
The twist here is that female contraceptives prevent unintended pregnancies in the person actually taking the contraceptive. Since a pregnancy can cause some women significant health problems, the risk of contraceptive side effects can be offset by the benefit of avoiding an unintended pregnancy. However, men do not directly experience any of the health risks of pregnancy — their female partners do. Thus it becomes more difficult, ethically, to justify the side effects of hormonal contraceptives in men.
W hat of non-hormonal contraceptives for men? Instead of targeting the hormonal axis that connects the brain and the testicles, non-hormonal contraceptives act directly on the production, activity or movement of sperm cells. One such approach is known as a ‘chemical vasectomy’. Developed by Dr Sujoy Guha in India, RISUG (which stands for ‘reversible inhibition of sperm under guidance’) has already entered Phase III clinical trials. In a standard vasectomy, the vas deferens (the natural transport channels for sperm cells in the testicles) are cut and sealed so that sperm are unable to enter the seminal fluid. In RISUG, a synthetic polymer is injected into the vas deferens with the same effect, but with the dramatic benefit that this polymer can be removed during a further, simple procedure that should restore normal movement of the sperm cells.
RISUG is not without caveats. Unlike taking a pill or receiving an injection, it requires a small surgical procedure. And the data on RISUG reversibility is based on animal experiments. We do not yet know whether reversing it in humans would restore male fertility. The clinical trial data obtained in India has been very encouraging so far, both in terms of safety and efficacy. The non-profit Parsemus Foundation has obtained the rights to use and market the RISUG method in the US as Vasalgel, and intends to initiate the first US-based clinical trials this year or next. RISUG is probably unable to generate the kind of profits that would attract the attention of a pharmaceutical company: the synthetic polymer is inexpensive, and a single polymer injection is sufficient to suppress fertility for years. So the only hope for general availability is support from the non-profit or government sector.
Other non-hormonal contraceptive methods are currently under investigation in animal studies. Dolores Mruk and Chuen-yan Cheng, scientists at the Population Council in New York, have shown that the chemical Adjudin causes reversible infertility in animals by inducing the release of immature sperm cells. A collaboration between laboratories at Baylor College of Medicine in Houston and the Dana-Farber Cancer Institute in Boston showed that JQ-1, a small molecule that targets the epigenetic enzyme BRDT, was able to reversibly inhibit sperm-cell production and fertility in male mice. But these are a long way from availability. It might take five years or longer for additional safety and efficacy studies in animals before even a small pilot human study can be conducted.
I f these hormonal and non-hormonal male contraceptives have been developed during the past decades, and some have been shown to be reasonably efficacious in small clinical trials, why are they not yet available for general use? Eberhard Nieschlag, professor at the University of Münster in Germany and a leading researcher in male fertility, recently described the impact of the suspension of the WHO/CONRAD efficacy trial on his field:
There is a multitude of reasons for the pharmaceutical industry’s reticence when it comes to male contraception. The efficacy and acceptability of female contraceptives sets a high competitive bar. The ethical problem of justifying potential side effects without any direct health benefits for men is another deterrent. And recent controversies related to the health insurance coverage of female contraceptives in the US underscore the even greater uncertainty of who would pay for male contraceptives if they were brought to market.
These investments make sense only if there is a large market for male contraceptives. Preliminary surveys seem promising: in 2000, Anna Glasier, professor of obstetrics and gynaecology at the University of Edinburgh, and her collaborators published an international survey of 1,894 women attending family planning clinics in Scotland, South Africa and China. Most women supported the idea of a ‘male pill’, and suggested that their partners would use it.
Newer male contraceptives require a very significant shift in the responsibility and burden of contraception between men and women
In 2005, a follow-up study by Klaas Heinemann at the Centre for Epidemiology and Health Research in Berlin surveyed more than 9,000 men in Europe, Asia, North America and South America. The willingness of respondents to use newer male contraceptives was highest in Spain (71 per cent), Germany (69 per cent), Mexico (65 per cent), Brazil (63 per cent) and Sweden (58 per cent). Nearly half of the men in the US (49 per cent) and France (47 per cent) expressed an interest. On the other hand, disapproval of newer male contraceptives was highest in Indonesia (34 per cent) and Argentina (42 per cent). These surveys reveal that there is a broad, international willingness among men and women to use male contraceptives, but such an endorsement of a hypothetical ‘male pill’ is a far cry from implementing it. To use newer male contraceptives would require a very significant shift in the responsibility and burden of contraception between men and women. We won’t know how that will work in practice until male contraceptives become widely available.
Scientific and cultural challenges might also explain the lacklustre involvement of the pharmaceutical industry. The efficacy data from small clinical trials has shown that there is significant biological heterogeneity in terms of how men respond to hormonal contraception. Suppression of sperm-cell production seems to be far more effective in Asian men, for example, than in Caucasian men. It is very likely that even within each ethnic group, there is a significant variability in the response to contraceptives. Instead of a ‘one pill fits all’ approach, male contraceptives might only be effective if individually tailored. The cultural challenge of introducing male contraception is also formidable. The Catholic Church strongly opposes all forms of contraception and, as the surveys have revealed, there is significant variation in men’s attitudes to male contraceptives.
It is thus not surprising that pharmaceutical companies are reluctant to invest millions in large-scale clinical trials. But without such trials, male contraceptives will not receive the regulatory approvals needed to bring them to market. We have reached an impasse. As a society, we recognise the importance of providing options for reproductive control, yet the responsibilities (and side effects) of effective contraception are carried largely by women. Men might never be able to share the physical burden of pregnancy, but they can share the responsibilities of child-rearing and contraception. If the market cannot support this, we need to find an alternative route.
Pharmaceutical companies make investment decisions based on profits, while non-profit organisations and government agencies have the luxury of supporting research that leads to equitable sharing — something that does not carry a defined monetary value. Non-profit organisations and government agencies are not in the business of manufacturing pharmaceuticals, but if they could conduct the larger clinical trials required and identify efficacious and safe male contraceptives, then the investment risk will be minimal for any interested pharmaceutical manufacturers, who could capitalise on the research to mass-produce and market the contraceptives.
Jump-starting the listless development of new male contraceptives will require a substantial amount of education and support. The Parsemus Foundation, the non-profit behind Vasalgel, and the Male Contraception Information Project, in San Francisco, have made a good start on the challenge by providing information about research on male contraceptives. Politicians need to be lobbied to ensure the adequate funding of government research agencies that specifically pursue the development of male contraceptives. There is also a place for public support: research studies are always looking for male volunteers, and non-profit organisations studying male contraception rely on donations. Male contraception is an excellent example of an opportunity for crowd-funding.
There has been a societal failure to produce a contraceptive method for men beyond the condom or the vasectomy, but now we have the chance to rectify that. Who will take the next step?
References and links to the research mentioned above are available here ."
Black holes may be hiding something that changes everything | Aeon Essays,,https://aeon.co/essays/black-holes-may-be-hiding-something-that-changes-everything,"Listen to this essay
What came first, the chicken or the egg? Perhaps a silly conundrum already solved by Darwinian biology. But nature has supplied us with a real version of this puzzle: black holes. Within these cosmic objects, the extreme warping of spacetime brings past and future together, making it hard to tell what came first. Black holes also blur the distinction between matter and energy, fusing them into a single entity. In this sense, they also warp our everyday intuitions about space, time and causality, making them both chicken and egg at once.
Physicists like me have long since accepted these strange properties of black holes. But I suspect that nature could very well have played a different trick altogether, and made black holes a gateway to something far more unusual – a region where the rules of spacetime themselves transform into something we’ve never seen before. Many objects we think of as black holes may, in fact, be imposters: identical on the outside but harbouring entirely different physics within. Finding out whether that’s true will require peeling back the shell of reality itself. And humankind is getting closer to doing exactly that.
T o understand why black holes may be hiding something, let’s first recap how gravity works, because it is the foundation of how spacetime can curve itself into such exciting and mysterious objects. Before Albert Einstein’s theories, gravity had various unexplained features. Isaac Newton’s gravity states that the planets feel the Sun through the vacuum of space with no interaction whatsoever. What’s more, it also states that any interaction takes zero time. If we were to remove the Sun with the flick of a wand, Newton’s gravity suggests the planets would immediately be stripped from the Sun’s gravitational pull, contradicting the well-known fact that nothing travels faster than light.
It was Einstein who found the solution to this puzzle. All he needed was the simplest of observations: all objects drop in exactly the same way under the pull of gravity. Lift up a bowling ball in one hand and a cauliflower in the other, and you notice that one arm is struggling more. Yet, when dropped, both bowling ball and cauliflower fall equally fast and hit the floor at the same moment. Whatever difference there was in mass, gravity nullifies it on the way down. This reveals something very deep about nature. Just like the train tracks’ curves make every train follow the same twists and turns, the fact that the gravitational motion of all objects is the same reveals that this too must be due to curves, this time in space itself.
Around a star of sufficiently packed mass and energy, time gets squeezed into a single moment
Thousands of experiments have since revealed that space and time are indeed a tapestry of curves, dents and stretches, invisible to the naked eye but fully measurable by the motion of planets, galaxies and even light. This is now as well established as photosynthesis, the existence of Mars, and that omelettes are made up of atoms. The Sun and planets, therefore, are connected by the curved spacetime between them – and any change in curvature must travel through this stretched fabric at the same universal speed limit as everything else. So, if the Sun did instantly disappear, Earth would continue orbiting the space it left behind for around eight minutes.
A European Space Agency artist’s interpretation of spacetime depicted as a simplified, two-dimensional surface, which is being distorted by the presence of three massive bodies, represented as coloured spheres. The distortion caused by each sphere is proportional to its mass. Courtesy ESA/C Carreau
Is Einstein’s the only way that space and time can be curved, though? One place to look for an answer is a black hole. It was soon realised that Einstein’s version of the warping of space and time came with a prediction: that around a star of sufficiently packed mass and energy, these train tracks become infinitely stretched, and time gets squeezed into a single moment, erasing all differences between earlier, now and later. These are the famous black hole solutions of Einstein’s general theory of relativity: spherical shells of pure curved spacetime, called horizons, floating in the vastness of the Universe. Any signal that finds itself having to walk an infinite amount of distance, and with zero seconds to do it, is certain to never make it out again. With no light, no matter and no radiation coming out from beyond the horizon, black holes are as black as black comes.
Black holes have been measured in spades, with different methods. One way has been by observing shiny stars orbiting a dark spot of nothingness where there should be a source of gravity (leading to a Nobel Prize in 2020). Another is by the gravitational waves from colliding black holes, which have been registered in the hundreds by the LIGO, Virgo and KAGRA detectors (also good for a Nobel Prize, in 2017). A third has been by the measurement of a ring of light whose paths are curved around a black hole (by the Event Horizon Telescope; a Nobel Prize might be pending there, too). Black holes really, truly, genuinely exist, telling us that Einstein’s curving of space and time must indeed be correct.
T here is still, however, much that is unknown about black holes. It’s well established that their contents are mysterious and inaccessible, but unanswered questions also remain about their origins and evolution.
To see why, let’s go back to the chicken-and-egg conundrum: what came first, the chicken or the egg? In biology, the answer is given by Darwinian evolution, which lines up millions of generations of chickens and eggs along the arrow of time, each ever so slightly different from the previous generation. Tracing this line all the way back to find whether there is a chicken or an egg waiting at the starting point results in finding something that is a great-great-great ancestor who resembles neither.
A similar question can be asked about black holes. At their horizon, past and future are squeezed into a single moment, making it difficult to trace their ancestry. But that’s not to say black holes don’t have a beginning. For many black holes, the ancestor at the start of their chicken-and-egg line is a cloud of mass that cannot withstand its own gravity. In the 1930s, the physicists Hartland Snyder and J Robert Oppenheimer showed that if such a cloud of mass is left under the gravitational pull of its own constituent particles, it has no choice but to collapse onto itself. Gravity there works as its own flywheel, as the particles falling closer and closer to each other create more and more curvature that in turn makes the cloud of mass pull on itself more and more. Before long, a horizon will have formed, behind which everything inside will have disappeared, and no one from the outside will be able to see its ultimate fate. Stars are typical examples. As long as their internal furnace is burning hot still, its infernal heat pushes back against gravity, keeping the collapse in check. When the fuel runs out, gravity wins over, and Snyder and Oppenheimer’s prediction makes the star suck itself into literal oblivion. Seminal work by the physicists Roger Penrose and Stephen Hawking showed that this final stop of cosmic evolution is quite inescapable.
One of the most striking properties of a wave is the impossibility to pinpoint its exact position
But what if there are other evolutions? Einstein’s formulas predict that the spacetime around a black hole is the same as that of any other spherical ball of mass, so it stands to reason that, maybe, the chain of chickens and eggs might have a different starting point, an ancestor not yet discovered, which might result in the same spacetime curvature as a black hole has but that harbours something else within the horizon. If they exist, these black hole ‘mimickers’ would be very interesting, revealing unknown physics. Maybe they are the result of existing rules not yet fully worked out, but they might also mean that the laws of physics themselves are new altogether, a new logic of space and time hiding behind the horizon. An example of the first is the idea that literal nothingness collapses onto itself. Surprisingly, this is perfectly allowed when you sprinkle a dash of quantum mechanics over the early Universe. Quantum mechanics states that, if you zoom in enough, nature presents particles as little waves. One of the most striking properties of a wave is the impossibility to pinpoint its exact position; waves definitionally always stretch out over some distance, and this directly translates to the impossibility of exactly knowing the position of small particles. Radioactivity is a clear example of this: with nature not being clear on the location of the particles that make up an atomic nucleus, some of them might find themselves waving outside the atom, flying out in what we perceive as radioactive radiation.
Run back the clock on an expanding Universe and you will find it small enough for its quantum waviness to present itself. The waves of this early soup could have allowed it to randomly get just a little more energy in one position than another, potentially enough to create a horizon without there ever having been a star to begin with. The ultimate chickenless egg, laid instead from the quantum randomness of the early Universe. These primordial black holes are actively sought by gravitational wave detectors.
Quantum mechanics also allows a second example of a black hole mimicker. Close to the horizon of a black hole, space stretches enough that even tiny quantum waves become sufficiently large to rear their heads. The exact details of how this might happen are not yet fully understood (a complete theory of quantum gravity is still pending), but one prediction of quantum mechanics near black holes is that the horizon can form a layer of material, a shell around the black hole that prevents some signals from getting sucked in and, instead, get reflected back into the Universe.
And then there is that second scenario where the rules for curvature themselves change altogether, an area that I myself like to play in. Einstein’s theory of gravity hinges on the observation that all matter and energy fall the same under spacetime’s dents and tracks, but there is more than one way that this can be translated into mathematics, allowing in principle for a whole slew of spacetime behaviours that are different from Einstein’s original implementation.
One example are gravastars , a theoretical construct that fills up the inside of a black hole with some stuff that, unlike normal material that pulls on itself by gravity, instead pushes itself outward. Such mysterious material has been theorised for decades, due to the fact that the Universe has been measured to speed up its expansion via dark energy (another discovery that led to a Nobel Prize, this time in 2011). Supposing that this dark energy exists, filling up a black hole with it produces a new type of star whose gravitational tendency for matter to tug on itself is counterbalanced by its filling with a kind of anti-gravity. This results in a stable equilibrium of a perfectly spherical bubble of dark energy that looks, but is not quite the same as, black holes.
I myself am interested in a different scenario still. What if it is not the stuffing of the mimicker that plays by different rules of physics, but spacetime itself? The starting point of this investigation comes from one of the core principles of nature: its indifference from the observer looking at her. To explain this point, ask a friend what meal they like best. Next, start running alongside them, and ask again. Will their foodie preference be any different now? Of course not. Their taste buds do not care about the relative velocity between the both of you. Whether you are standing still relative to your friend or not, their love for pizza, omelettes, cauliflowers or whatever will come out the same, always.
The same idea holds for nature in her totality: her rules should be the same regardless of the motion of the observer. This is called the principle of relativity , one of physics’ most shiny gems. As far as we know, all of reality plays by this principle of being completely indifferent to the observer’s motion. A more advanced example comes from electromagnetism. A charged particle like an electron has an electric field that informs other charged particles how they will be pulled or repelled. Next, look at that same electron while it moves with respect to you, and it will look to you like a current, which can famously produce magnetic fields on top of their electric field. How much electric field and magnetic field you perceive while looking at the electron depends on your state of motion relative to the electron. Yet, if you take the combined effect of the two fields, this will result in the exact same amount of pushing and pulling, regardless of how you happen to be moving with respect to the electron.
It turns out that it is indeed mathematically possible to glue different spacetimes to each other
It was actually this particular example that Einstein used in 1905 to derive his theory of relativity. He assumed that the indifference of electromagnetism to the observer’s motion was not a happy coincidence of mathematics, but a rule of nature that transcends the details of any particular phenomenon, be they electric, magnetic, your friend’s tastebuds, or otherwise. As the one thing that is shared by all these phenomena is that they take place in space and in time, Einstein assumed that it must therefore be space and time themselves that conspire together to make the principle of relativity work. And he was right: we since know and can measure that, indeed, space shrinks and time stretches in well-defined unison, and always such that everything in nature comes out following the exact same rules regardless of the observer’s motion. But it turns out that Einstein’s theory is not the only way the principle of relativity can be ensured, and that it also allows other conspiracies of space and time. These alternative versions of relativity theory come with wonderful properties that are similar, but not quite the same, as Einstein’s version. For instance, they can imbue spacetime with a tendency for space to push itself outward. If conventional gravity pulls everything together, whereas this new type of spacetime pushes itself outward, a stable configuration could be possible in which the pushing and pulling balance each other out. This would be a novel type of black hole mimicker, a spherical shell that has new rules of spacetime on the inside whereas the rest of the Universe outside plays by conventional relativity theory. The subtle point is what happens on the shell where the two different spacetimes transition into each other. There, the curvature must have a smooth transition from the inside to the outer. This smoothness must necessarily be the case, because a black hole’s curvature is inseparable from its mass and energy, meaning that a jump in spacetime curvature would violate the conservation of energy, one of nature’s other sacred principles.
It turns out that it is indeed mathematically possible to glue different spacetimes to each other in this way, provided that the transition takes place exactly at the horizon of a black hole. This makes for yet another tantalising type of black hole mimicker, where its horizon does not trap exotic stuffing, but is a gateway to other types of spacetimes. The fact that they would be hiding behind a black hole’s horizon would then conveniently explain why they have not been seen – at least so far.
T here exist many models for black hole mimickers, worked out in beautiful mathematics. But is mathematical consistency enough to take their existence seriously? The history of science provides some clues here. When the standard black holes were predicted, they were originally seen as some sort of anomaly; an interesting product of mathematics, but not quite as real as atoms, egg-laying chickens, or the existence of a landmass called Great Britain. Sometimes scribbly symbols on a chalkboard simply predict more than nature has chosen to manifest. But in other cases, mathematics has been a very effective way to predict new rules of nature, even when the predictions sounded very unfamiliar at first. When the physicist Paul Dirac merged quantum mechanics with special relativity, he found twice the amount of particles than were then known to exist. And, lo and behold, measurements revealed that these antiparticles do indeed exist.
To which type of mathematics do black holes mimickers belong? Einstein himself firmly believed that conventional black holes were of the first type: an interesting prediction that nature decided not to adopt. But a century of measurements – and gravitational waves in particular – have shown that black holes are abundant in the Universe. The same might very well be the case for the mimickers.
The way to find out is by actively going out and looking for them . Although it’s true that the outside spacetime of a black hole mimicker tends to look the same as that of a normal black hole and the inside is hidden from us by the horizon, their gravitational waves are likely to be different. Hit a black hole hard enough (crashing another black hole into it will do it) and it will wobble about, producing gravitational waves with very specific frequencies. Just as a chemist can identify the type of gas by the light frequencies it sends out, wobbly black holes have their own characteristic frequency spectrum that might allow us to separate black holes from the imposters. Fortunately, nature was kind enough to fill up the Universe with many such cosmic collisions. The only thing we have to do is measure their resulting gravitational waves in full detail.
Humankind is very close to unlocking such measurements. Black hole collisions have been measured in abundance in the past 10 years. And, as recently as this year, the gravitational wave detector network LIGO/Virgo/KAGRA was able to measure a black hole collision with such precision that the frequency spectrum of its wobbly end product could be seen. What’s more, the current network is soon to be superseded by the next generation of detectors, such as the upcoming Einstein Telescope and the LISA gravitational wave detector in outer space, which are much more sensitive to the frequencies of black hole horizons. With such powerful new techniques in hand, we might perhaps be able to crack the horizons of the black hole, or certainly peel at its shell. Whatever we will see, it is either going to tell us that black holes are exactly as we always expected, or will change our ideas of space and time forever."
What does the philosophy of physics add to human knowledge? | Aeon Essays,,https://aeon.co/essays/what-does-the-philosophy-of-physics-add-to-human-knowledge,"When I’m making small talk at parties and suchlike, revealing to others that I’m a philosopher of physics is a little bit like rolling the dice. What reaction am I going to get? The range is pretty broad, from ‘What does philosophy have to do with physics?’ to ‘Oh, that’s way above my pay grade!’ to (on happier occasions) ‘That sounds amazing, tell me more!’ to (on less happy occasions) ‘What a waste of taxpayer’s money! You should be doing engineering instead!’
Only the last of these responses is downright stupid, but otherwise the range of reactions is perfectly reasonable and understandable: philosophers of physics are, of course, not ten-a-penny, and what we’re up to is hardly obvious from the job description. So what I want to do here is sketch what the philosophy of physics really amounts to, the current state of play in the field, and how this state of play came about.
T o cut to the chase: the philosophy of physics is the systematic study of our best theories of physics. This goes well beyond our current best candidates for fundamental theories of physics, such as string theory, and rather encompasses everything from Newtonian mechanics (which still constitutes the bread-and-butter of a great deal of practical physics, as well as perhaps the overwhelming majority of engineering), to Albert Einstein’s theories of relativity, to quantum mechanics, to the Standard Model of particle physics, to cosmology, and much else besides. And by ‘systematic study’ I mean something like asking questions such as these: ‘What is the conceptual and mathematical architecture of such-and-such a theory?’, ‘What would the world be like if such-and-such a theory were true?’, ‘What are the implications of such-and-such a theory for so-and-so areas of classical philosophical enquiry, like time, causality and identity?’
Given questions like these, one would be excused for thinking that the philosophy of physics is in fact continuous with physics ‘proper’ – and one would be quite right! Not only would trying to answer the above questions without a good working knowledge of physics be a forlorn hope, but also physicists themselves, especially theoretical physicists, often grapple with the structure of our best theories (for instance, there are a great many physicists working on the mathematics of general relativity, on the foundations of quantum mechanics, and so on). Ultimately, the difference between physics ‘proper’ and the philosophy of physics is really one of emphasis: philosophers are interested in understanding the structure and upshots for reality of the theories of physics that we already have, while physicists engage in the first-order practice of building new theories and then testing those theories experimentally.
The philosophy of physics isn’t some modern innovation – it’s been around for as long as physics itself
Indeed, its ultimate emphasis on bettering our understanding of the theories of physics that we already have helps to make clear that the philosophy of physics really is a discipline in the arts – despite often requiring a good understanding of physics itself. As I see it, the philosophy of physics is in fact somewhat akin to art criticism – but where the subject matter is not music or visual art or architecture, but the theories of physics constructed by some of the greatest and most creative minds of all time (from Galileo to Newton to Einstein). Philosophers of physics all share the view that physical theories – these products of the highest level of human creativity – are just as worthy of aesthetic appreciation and critical study as anything else.
So, the philosophy of physics is continuous with physics, insofar as it seeks to understand the structure of our best scientific theories of the world. It is also an artistic discipline, insofar as it engages with the critical scrutiny of certain products of the human mind – namely, physical theories. But the final thing to say here is that the field is also continuous with three other disciplines. First, with history, for in order to truly understand the content of any given theory, we must often turn to how it was developed, and the tools that physicists used to develop it. Second, with mathematics, because, often, a full understanding of the content of some given theory calls for shoring up some subtle mathematical details. And, finally, with philosophy ‘proper’, since we can’t really understand what a theory says about time, causation, identity and so on if we don’t appreciate the scrutiny to which philosophers have subjected those notions over the centuries.
All in all, this means that the philosophy of physics is an incredibly broad church – and a wonderful playground for those (like myself) who never could decide on the specific subject to which to dedicate themselves. In any case, I hope this goes some way towards giving a relatively clear sketch of what the philosophy of physics is about. What I want to do next is to stress that the philosophy of physics isn’t some modern innovation – in fact, it’s been around for as long as physics itself.
A t least in the Western canon, a natural place to begin the tale would be with Aristotle, who was perhaps the greatest philosopher of all time (it’s either him or Plato!) as well as the progenitor of something like the scientific method. Aristotle engaged in a clear philosophical interrogation of the content of his theories of the world when he asked crucial questions regarding, for example, the location of the centre of the Universe. But, in the interests of constraining the narrative, let me leave Aristotle there, and dial things forward 2,000 years – to Newton.
In my opinion, the greatest scientist of all time is undoubtedly Sir Isaac Newton (1643-1727). One can make straightforward points to back this up: his achievements laid much of the groundwork for modern science, and for the paradigm of leveraging mathematics in order to solve problems in physics (which led to the Enlightenment idea of the ‘clockwork universe’). Moreover, Newton invented substantial new areas of mathematics, including calculus. However, it’s really at a more fine-grained level that one sees Newton’s genius: his magnum opus, the Philosophiæ Naturalis Principia Mathematica (1687), or The Mathematical Principles of Natural Philosophy , not only reads much like a piece of modern mathematics , but also – and this is much less known – contains insights that substantially presage many of the steps Einstein himself made on his path towards general relativity around two and a half centuries later. The Principia also contains philosophical reflections on the nature of space, time, and motion (as well as some damning interrogations of earlier thinkers, in particular René Descartes) that are as crisp and clear as the very best modern philosophical works.
Great physicists have been, on several important occasions, also great philosophical thinkers
Let me focus on the latter of these. With Newton’s theory of universal gravitation on the table, ‘ natural philosophers ’ of the day (the precursors to modern scientists) were able to interrogate long-held philosophical presuppositions regarding the nature of space and time with newfound rigour and precision. This is best exhibited in the Leibniz-Clarke Correspondence, a series of letters between Gottfried Wilhelm Leibniz (1646-1716) – himself a polymath philosopher, scientist and mathematician (he independently invented the calculus alongside Newton) – and Samuel Clarke (1675-1729), who was a prominent supporter of Newton at the time. In their letters (five on each side, before the correspondence was cut short by Leibniz’s death), Leibniz and Clarke explored various philosophical questions arising out of Newton’s new mechanics, for example: what is the difference between merely apparent (‘relative’) motion and true (‘absolute’) motion? How can we possibly determine ‘global’ properties of the Universe, such as its overall velocity? And how to understand identity in physics – could there, for example, be two physical bodies alike in all respects?
None of this work – which is obviously philosophical in nature, and which in fact counts as exemplary philosophy of physics – would have been possible without the framework afforded by Newton’s new physics. So this corroborates my point that developments in the philosophy of physics are often deeply intertwined with developments in physics, and that great physicists have been, at least on several important occasions, also great philosophical thinkers.
B ut this is just one data point. What happened after Newton? The standard narrative has it that philosophy and physics then went their separate ways: like a good midwife, ‘natural philosophy’ might have been of invaluable assistance in the birth of modern science, but after the birth she had no role to play. In the wake of the work of Immanuel Kant (1724-1804) and especially his successors including G W F Hegel (1770-1831), philosophy arguably became less concerned with the details of first-order science and its implications, and more committed to the investigation of loftier matters such as the limitations to the possibility of our knowledge, the possibility of access to a ‘transcendent’ reality, and so forth. Meanwhile, physics earned its stripes as an ‘exact science’, with the development over the course of the 18th and 19th centuries of the ‘analytical mechanics’ of Joseph-Louis Lagrange, William Rowan Hamilton, Adrien-Marie Legendre and others, which built upon Newton’s work and rendered it substantially more mathematically sophisticated.
There’s some truth in this narrative of amicable divorce, but there’s also a sense in which it’s not entirely fair, because – albeit perhaps less obviously than in Newton’s time – philosophical reflection by physicists and upon physics by philosophers persisted into the subsequent centuries. Here’s one example. In the 19th century, a mathematical revolution was precipitated by János Bolyai (1802-60) and Nikolai Lobachevsky (1792-1856), who showed that there could be consistent non-Euclidean geometries (that is, alternatives to Euclid’s geometry as presented in his Elements ) in which, say, the sum of interior angles of a triangle is either more or less than 180 degrees. This work was brought to maturity by Bernhard Riemann (1826-66) in his habilitation thesis, but it was not long until this led to philosophical questions of the greatest depth: given the possibility of these geometries, need it in fact be the case that, as a matter of strictest necessity (as Kant had it), the structure of space must be Euclidean, or are there in fact other possibilities for the geometry of space? This question, which became known as the ‘Problem of Space’, came to occupy some of the greatest minds of the time, including the physicist Hermann von Helmholtz (1821-94), the mathematician Sophus Lie (1842-99) and the polymath Hermann Weyl (1885-1955).
They were able to leverage their philosophical maturity and insight to make significant steps in physics
Here’s a second example of the persisting relationship between physics and philosophy. During the 19th century, physicists – most notably Ernst Mach (1838-1916) – worried a great deal about various assumptions in Newton’s mechanics, particularly the assumption that space and time are inert background structures in which physical events unfold. In opposition to this, Mach laid down a principle (dubbed by Einstein ‘Mach’s principle’) according to which there can be no spacetime without matter, and sought to reformulate Newtonian physics around such a principle.
These two threads found their confluence in the second golden age of interaction between physics and philosophy, at the turn of the 20th century – a golden age led by luminaries such as Henri Poincaré (1854-1912), Weyl and, of course, Einstein himself (1879-1955). This was an age in which – just as in the time of Newton and Leibniz, and Aristotle well before them – the greatest scientists and mathematicians were once again the greatest philosophers, and as such were able to leverage their philosophical maturity and insight in order to make significant steps in physics itself.
Perhaps the most telling example of this is Einstein’s development of the general theory of relativity (which remains our best theory of space, time and gravitation) over an approximately 10-year period from 1905 to 1915. This made essential use of both the insights gained from the Problem of Space and the possibility of non-Euclidean geometries, and of Mach’s principle, at least as heuristics for the development of the theory. Indeed, Einstein acknowledged explicitly throughout his life the influence that his philosophical training had on his physics, as well as (reciprocally) the influence that his physical developments had on his philosophy of science.
What this narrative makes clear is that the extent to which there is non-trivial interaction between physics and philosophy can fluctuate over time, but the philosophy of physics – the critical interrogation of our best theories of physics – has been around for a long time, whether as practised by ‘pure’ philosophers, or by the physicists themselves (or by ‘natural philosophers’, when there was no clean distinction between the two disciplines).
B ut if this is right, then why does the philosophy of physics remain a rather niche discipline in modern times? Well, as I’ve stressed, the interactions between physics and philosophy can wax and wane. In fact, after what I’ve described as the second golden age of interactions at the turn of the 20th century, and especially after the debates between Einstein and Niels Bohr (1885-1962) on the new quantum theory in the late 1920s, in which the latter intimated cryptically that reality at the fundamental level is ineffable, and that we should give up on trying to secure a complete picture of the world at the fundamental level, things took a rather different turn. At some point later in the 20th century, many physicists developed an attitude of outright hostility towards philosophy. This came to a head – and somewhat unfortunately made it into the public consciousness – in the writings of highly respected physicists such as Steven Weinberg – in whose book Dreams of a Final Theory (1992) was a chapter provocatively titled ‘Against Philosophy’ – and Stephen Hawking, who, even more provocatively, declared in 2010 that ‘philosophy is dead.’
What were the sources of this unfortunate shift in attitude? For better or worse, Bohr’s philosophy was not only obscure (what does it mean, after all, for reality to be ‘ineffable’?), it also suggested that it’s simply misguided for physicists to have pretensions to ‘understanding reality’. Bohr was one of the most influential figures of his day, so it’s not too surprising that these instrumentalist ideas, according to which physics is a mere ‘instrument’ for making experimental predictions, and in turn according to which we should no longer have such grandiose ambitions as ‘understanding the fundamental nature of reality’, took root in physics at large.
It’s not uncommon to hear physics lecturers across the globe making pejorative comments on philosophy
The second source of the shift was that, after Einstein’s work on general relativity – which, as we have seen, was shot through with philosophical thinking – the theory entered the doldrums for several decades. Very few physicists actually worked on it because it was regarded both as too difficult to learn and too difficult to test, until the arrival of the work on singularities and black holes by Roger Penrose, Robert Geroch, Hawking and others in the 1960s. At the same time, however, particle physics was thriving, with rapid developments in quantum field theory in the 1930s and ’40s – work that would eventually lead to the development of the Standard Model of particle physics. That work was both very collaborative and very pragmatic: physicists were content to develop tools without necessarily understanding the mathematics they were manipulating, let alone the conceptual foundations of the work. It’s not difficult to see that this hands-on, workmanlike attitude would be in tension with philosophical reflection on the craft.
To some extent, this attitude persists today. It’s not uncommon to hear physics lecturers in universities across the globe making pejorative comments on philosophy (often from a position of some ignorance); the rather more reflective physicist N David Mermin coined the saying ‘ Shut up and calculate! ’ in 1989 to encapsulate the instrumentalist attitude many physicists seem to have towards the foundations of their subject matter. Despite all this, I think there’s a wealth of evidence that the interaction between physics and philosophy, and the discipline of philosophy of physics itself, is very much on the up – perhaps even entering a third golden age.
O ne practical reason is that the workmanlike attitude of those physicists working on quantum field theory in the mid-20th century is no longer obviously sufficient for doing good physics. The Standard Model of particle physics is now complete, and (with the discovery of the Higgs boson at the Large Hadron Collider in CERN) extensively empirically confirmed. That said, while plenty of extensions to the Standard Model (most notably, those that invoke supersymmetry) have been developed, none have been tested experimentally, and the prospects for empirical confirmation of any particular such model don’t look great, because it is simply too expensive to build the particle colliders needed to probe the energy scales where one would expect experimental signatures of such models. As a result of issues such as these, many physicists have come to newfound appreciation for the value of philosophical reflection on different areas of physics.
Let me take some examples to back up this point. Over the past few decades, fields such as quantum information theory and quantum computing have become significant within physics, and these fields have had to grapple with concepts that are adjacent to philosophers’ concerns. For instance, in order to build a viable quantum computer, it’s widely recognised that the phenomenon of ‘decoherence’ (roughly: the suppression of quantum mechanical effects at macroscopic levels) must be controlled. Decoherence, however, turns out to play a key role in essentially all known interpretations of quantum mechanics – that is, the projects to understand what quantum mechanics says about the nature of the world; the projects that find their natural home in the philosophy of physics. As such, it’s unsurprising that physicists working in these areas of quantum theory have been much more open to dialogue with philosophers on quantum mechanics than their predecessors might have been.
Dialogue with practising physicists is the best it’s been in decades
As a second example, physicists now much better appreciate that, when exploring the space of alternative theories of gravitation to Einstein’s general relativity, it is helpful to taxonomise these with reference to whether they satisfy his ‘equivalence principle’ and, since it is philosophers who best understand such principles, physicists have in recent years entered renewed dialogue with them on such matters. And, as a final example, cosmologists have recently begun to worry about whether modern cosmology will be ‘permanently underdetermined’ by evidence, and therefore we will remain forever in the dark as to the large-scale structure of the cosmos. This invites newfound dialogue between physicists and philosophers in order to explore these possibilities and assess the prospects for overcoming such issues.
In the present day, the philosophy of physics is once again in a good place. There are a great many of us distributed across the globe (almost 300, according to this list ) and the community as a collective has a great deal of expertise in all four fields of physics, philosophy, mathematics and history. Dialogue with practising physicists is the best it’s been in decades. (Even the job description of ‘natural philosopher’ is resurging: witness the recently opened Radboud Centre for Natural Philosophy in Nijmegen in the Netherlands, and the fact that Sean Carroll at Johns Hopkins University has the newly created title of Homewood Professor of Natural Philosophy.) Given that careful attention to foundational and philosophical issues was an integral component of many of the previous leaps forward in physics, this can only be a good thing.
Even if one doesn’t have pretensions to developing a quantum theory of gravity, when one really attends deeply to the structure of our best theories of physics, one finds in all corners subtle, fascinating and superlatively beautiful issues to explore. To take just one example (and to circle back to my suggestion that the philosophy of physics be understood as a form of art criticism), the mathematics of ‘gauge theory’ – which can be used to make rigorous and conceptually clear sense of the foundations of the Standard Model of particle physics and quantum field theory – is as beautiful and elegant and sumptuous as any Botticelli or Caravaggio or Monet. As such, the payoffs and insights we can gain from it – both aesthetically and metaphysically – are at least as great as any from the fine arts. It’s in the philosophy of physics that we have the liberty to study and criticise such edifices and their conceptual upshots, without necessarily feeling the pressure to develop new theories or build new experiments – crucial as those are, of course."
William Herschel’s sensors let us see the invisible Universe | Aeon Essays,,https://aeon.co/essays/william-herschels-sensors-let-us-see-the-invisible-universe,"The human eye, that great enabler of art and action, has some galling design limitations. Our vision is tightly tuned to the peak colours of sunlight, leaving us blind to almost all other forms of radiation. If you think about the frequencies of light by analogy with the frequencies of sound, there are some 80 octaves of detectable electromagnetic radiation found in nature. We are able to see exactly one of them: the octave that extends from the violet to the red ends of the rainbow. The Universe bombards us with the other 79 octaves all the time, but we are oblivious to them.
The human mind, on the other hand, suffers no such limitations. Technology can create sensors responsive to rays that are utterly inaccessible to the human eye, or to any other type of eye found in the biological world, for that matter. Venturing even a tiny bit beyond the red edge of the rainbow, into the undiscovered country of the infrared, is a transformative experience: it reveals an entire hidden Universe, a previously walled-off layer of reality that we are now exploring every day as results pour in from the James Webb Space Telescope (JWST).
Photo by Diego Augusto Lima
From its perch a million miles from Earth, JWST has spent the past three years scanning the sky in infrared radiation, sensing light waves that are up to 40 times longer than the reddest red that we humans can see. All of the glorious pictures that the telescope sends back – scenes of galaxies in collision and infant stars spewing out streamers of gas – are not photographic snapshots so much as they are data interpretations . JWST perceives celestial shapes and colours that exist only within the circuitry of its digital detectors. Astronomers then use software and imagination to translate the detectors’ electrical impulses into images we can comprehend.
Every fresh result from JWST, then, is a showcase of the technological evolution of our species. The largest, most complex observatory ever sent into space is also the largest, most complex bionic eye ever wired into our consciousness. It is the culmination of a two-century effort to tear off our evolutionary blinders and endow Homo sapiens with senses that are as expansive as the laws of physics allow.
JWST is a floating catalogue of everything that astronomers have ever learned about telescope making. It follows a blueprint laid out in a 1988 report from the US National Research Council, which endorsed a giant new space telescope that ‘would lead to a quantum leap in our understanding of some of the most fundamental questions in astronomy’, supported by $10 billion of subsequent funding.
The resulting instrument peers out at the cosmos using a 6.5-metre -wide mirror composed of 18 interlocking hexagons, fabricated from lightweight beryllium metal and coated with 48 grammes of super-reflective pure gold. Together, those mirrors take in a million times as much light as the pupil in your eye. The collected rays then focus on 15 mercury-cadmium-telluride detectors and three detectors made of arsenic-doped silicon, which act as JWST’s infrared-tuned electronic retinas. The whole telescope huddles behind a 21-metre -wide Kapton sunshade that keeps it chilled to 233 degrees below zero Celsius; even a trickle of solar heat would ruin its hypersensitive vision.
E verything about JWST is up to the minute. But the underlying technology, and the whole ‘seeing in the dark’ agenda that inspired it, follows directly from the work of the British astronomer William Herschel more than two centuries earlier. In particular, it follows from a revolutionary but deceptively simple experiment that Herschel performed using nothing more than a prism, a box and three mercury-bulb thermometers.
William Herschel by Lemuel Francis Abbott c 1785. Courtesy the NPG London
Today, Herschel is best remembered as the man who discovered the planet Uranus in 1781. That achievement alone would be enough to earn him a star on the history-of-astronomy walk of fame. There was much more to him, though. He was an accomplished instrument-maker who built the largest telescope of his age. He performed the first serious attempt to map our home galaxy, the Milky Way. He speculated on the nature of life on other worlds (even, he suggested, on the Sun). And, starting in early 1800, he carried out a series of experiments that culminated in the detection of rays that carry a warming effect even though they are invisible to the eye.
In modern terms, Herschel discovered infrared rays , and the entire unseen Universe that they signify.
This sweeping discovery was born out of simple, stubborn frustration. Annoyed by having to write off half of his potential observing hours, Herschel wanted to extend his celestial investigations into the daytime and study the surface of the Sun. But his huge telescopes, ideal for viewing dim stars at night, were far too powerful to handle the brilliance of solar rays.
Herschel had discovered a new type of ray: undetectable to the eye, warmer than visible light, and redder than red
Herschel began testing various types of filters that would let him look at the Sun without frying his eyes, when he noticed a perplexing disconnect. ‘[W]hen I used some of them, I felt a sensation of heat, though I had but little light; while others gave me much light, with scarce any sensation of heat,’ he wrote in a paper read before the Royal Society on 27 March 1800 in Somerset House, London. He recognised that his finding carried two huge, intriguing implications: certain types of rays were capable of transmitting heat, and some of those rays were somehow different from the types of light rays that stimulate vision in the eye.
Now Herschel had a juicy mystery to solve: what were these unseen rays? Did they bend through a prism, like ordinary light? If so, did they correspond to the colours of light in some way? Herschel being Herschel, he passed sunlight through his prism, cast a rainbow, and measured the temperatures within the different colours. A clear pattern emerged as he moved from the violet to the red end of the prismatic spectrum. Green light produced more warming than blue; red produced more warming than green.
Which made Herschel wonder: What if? What if he kept going beyond the red, where there is no visible light at all? What would he find there?
Herschel’s initial journey into the invisible Universe extended all of four inches ( 10 cm ) across his tabletop apparatus, but they were four extremely significant inches. He updated his Sun-filter experiment, relocating his thermometers to the seemingly dark zone past the red edge of the rainbow that beamed from his prism. There, to his delight, he found that the instruments continued to register heat, even where his eyes registered nothing.
When Herschel took the readings from his thermometers, he determined that the warming caused by the Sun’s rays not only continued beyond the red zone, but the effect actually intensified as he ventured deeper into the darkness past the rainbow’s edge. He had to move his thermometers far outside the visible spectrum before the warming effect peaked and then tapered off. Evidently, he was measuring a rising and falling distribution of some phenomenon that existed beyond the human senses. It was as if he were running his hands over the arched back of an invisible cat.
To be methodical, Herschel also poked around at the other end of the spectrum, past the violet edge. There he found no warming effect, confirming his inference: he had discovered a new type of ray emitted by the Sun, one that is undetectable to the eye, warmer than visible light, and (illogical as it sounds) redder than red. On 24 April 1800, he relayed the results of his investigations to his Royal Society colleagues.
I t is impossible to know what Herschel was feeling at the time, but in yellowed pages of the 225-year -old volumes of Philosophical Transactions of the Royal Society , he seems to be bursting with excitement. Normally, he confined himself to sober notations of his empirical observations, and shied away from extravagant speculations. In this case, though, he seemingly could not hold back an outpouring of ideas.
In his two presentations to the Royal Society, Herschel introduced an entirely new scientific vocabulary. On 27 March , he coined the term ‘radiant heat’. A month later, he distinguished between the visible colours that he could see and the invisible ‘colours’ that he could only feel based on their warmth, calling the former the ‘prismatic spectrum’ and the latter the ‘thermometrical spectrum’.
Most perceptively, Herschel realised that the two types of rays he was examining must have one and the same fundamental nature. Applying the principle of parsimony, he rejected the existence of ‘two different causes to explain certain effects, if they may be accounted for by one’. The difference between the two types of rays, he deduced, was merely that the human eye could perceive one type of ray, but not the other. He concluded that ‘radiant heat will at least partly, if not chiefly, consist, if I may be permitted the expression, of invisible light.’
We limited human animals see only what we need to survive
Invisible light. W hat a concept! The invention of the telescope had proven that there are objects too faint to be visible to the unaided human eye – but when their light is concentrated, they pop into view. The invention of the microscope had likewise demonstrated the existence of objects too small to be seen – but when they are magnified, they, too, pop into view. Herschel had revealed a more fundamental limitation of human perception. His ‘thermometrical spectrum’ indicated that some portion of reality is invisible to us not because it is lacking in quantity (size or intensity, say), but because of its essential quality . The scope of Herschel’s intellectual breakthrough was all the more remarkable given the modesty of the technology that enabled it. If you want to recreate the Herschel experiment yourself, you can find simple, step-by-step instructions online .
In Herschel’s culminating experiment, presented to the Royal Society on 6 November 1800, he produced a ‘spectrum of heat’, showing how his measured temperature varied with distance from the red end of the visible spectrum. It was, in essence, a line drawing of his invisible cat. In an earlier paper, he had even offered what a modern scientist might call an evolutionary explanation of why much of the Universe is invisible to us:
Plainly put, we limited human animals see only what we need to survive.
It might seem inevitable that Herschel would then have embarked on more expansive investigations of light-beyond-light, or at least that other researchers would have taken on such an agenda. In reality, Herschel’s investigations were soon hindered by the crude thermometers available to him, which were accurate only to about half of a degree. Creating any kind of image or snapshot of the beyond-red realm was impossible using such simple devices. Herschel was limited, too, by the prevailing theories of the time. In the early 19th century , most scientists thought of light as a shower of particles, and heat as a movable fluid called ‘caloric’.
Herschel sounds increasingly defeated in the succession of papers that he presented to the Royal Society through 1800. By May of that year, he had sourly confessed that ‘the termination of a prismatic spectrum cannot be accurately ascertained.’ He had no desire to be drawn into contentious arguments about the nature of heat. Herschel was an empiricist by nature, and his great aspiration was to be the supreme celestial cartographer. When he realised that he could not map the boundaries of his new invisible landscape, he returned to the more accessible challenge of mapping stars and nebulae.
A nd there the story of the unseen Universe stalled for a long, long time. What Herschel had achieved was not so much a scientific revolution as something both bigger and less well defined: a slow-moving perceptual transformation. Peering into the infrared world required the development of entirely different sensory technologies, ones that could take us well beyond what our organs of sight are adapted to see.
Charles Piazzi Smyth, Scotland’s Astronomer Royal, made an incremental but meaningful advance in 1856, more than three decades after Herschel’s death. He detected infrared rays from the Moon using a thermocouple, a then-novel type of thermometer that measures temperature based on the way that electricity flows through two adjacent conductors. It seemed logical that the Moon must reflect invisible heat rays from the Sun, but Smyth had delivered hard evidence. More profoundly, when he performed his measurements at various points on Mount Guajara in Spain, he found that the infrared Moon appeared dimmer at low altitudes. Whatever these beyond-red rays were, they were evidently absorbed by Earth’s atmosphere.
Wood’s technique revealed Herschel’s ghost realm, a place where the sky is dark and trees glow with eerie infrared reflections
Each subsequent sensory advance was similarly hard-won. Starting in the 1870s, the astronomer and aviation pioneer Samuel Pierpont Langley invented the bolometer (a more precise type of electronic thermometer) and used it to map the Sun’s infrared spectrum for the first time. In the 1910s, the physicist William Coblentz strung together multiple thermocouples to create a ‘thermopile’ capable of detecting infrared rays from stars, planets and nebulae as well. That same decade, the inventor Robert Williams Wood combined recently developed, infrared-sensitive film with a special filter (‘Wood’s glass’) that completely blotted out visible light. In this way, he managed to photograph terrestrial landscapes in infrared rays.
Wood’s technique worked only in broad daylight, using extremely long exposures, but it finally revealed Herschel’s ghost realm. It exposed the scenery of the infrared world, a place where the sky is dark and trees glow with eerie infrared reflections.
Hoping to extend infrared vision into deep space, researchers trekked to the tops of mountains, above the bulk of Earth’s atmosphere, wielding new electronic lead-sulfide detectors, which were far more sensitive than film. So equipped, the astronomers Gerry Neugebauer and Eric Becklin achieved the next great perceptual leap in 1966. Using a custom-built telescope situated atop Mount Wilson in California, the two surveyed the infrared sky. In the constellation Orion, they spotted an unidentified glow – something that was bright in infrared but completely invisible to conventional-light telescopes.
The ‘Becklin-Neugebauer object’ is now believed to be a massive infant star, cloaked in a cocoon of dusty gas that blocks all of its visible light but not the more penetrating infrared rays. It was the first celestial object ever discovered in infrared radiation alone.
E ven then, the study of infrared rays from space remained a tedious, fringe area of research. All the early explorers of the invisible faced the same limitation: they could trek to mountaintops, loft balloons, or even peer out through a hole in the fuselage of a C-141A jet transport plane (seriously, they did that), but they were still hampered by the thick murk of our planet’s infrared-blotting atmosphere. Infrared astronomy did not take flight until it could literally take flight, using the rockets of the space age.
In 1983, the United States, United Kingdom and Netherlands teamed up to launch the Infrared Astronomical Satellite (IRAS), the first true infrared space telescope. It circled Earth in a near-polar orbit, 900 km ( 560 miles ) above the ground. In a mere 10 months in service, IRAS detected 350,000 infrared sources, and opened our eyes to an entirely unknown side of the Universe. It spied wispy ‘cirrus’ clouds of gas and dust between the stars, and warm disks around young stars, revealing the birthplaces of new planets.
Reporters wrote up breathless stories when IRAS recorded the mysterious glow of a possible Jupiter-sized world lurking just beyond our solar system. ‘All I can tell you is that we don’t know what it is,’ said Neugebauer, who by this time was the chief scientist for IRAS. Further analysis revealed that the object was not an Earth-threatening planet but a star-studded galaxy: a glimpse at the previously unknown way that galaxies flare up with newborn stars when they slowly, powerfully crash into one another.
Each new type of observatory has yielded a new human familiarity with another layer of previously hidden reality
JWST is a majestic elaboration of IRAS’s achievements, extending our infrared gaze to the literal ends of the Universe. It is an elaboration, too, of the motivations that guided Herschel’s work. His greatest goal as an observer was what he called ‘gauging the heavens’. He attempted to catalogue every visible star in the sky and estimate its distance, with the intent of mapping the entire Universe. JWST’s expansive infrared vision is now bringing us close to the completion of that task.
We live in an expanding Universe, which means that visible light from extremely distant galaxies is stretched deep into the infrared. The early history of the cosmos is secreted away in those hidden rays, inaccessible to telescopes on the ground. JWST’s detectors are specifically designed to bring stretched infrared light into view, seeing back to the historic moments when the first galaxies lit up. One of JWST’s sightings – a galaxy designated JADES-GS-z14-0 – is the most distant object known to humanity. We are observing this galaxy from a time 13.4 billion years ago, when the Universe was just one-50th of its present age.
Observed by the James Webb Space Telescope, the JADES-GS-z14-0 is the current record-holder for the most distant known galaxy. Courtesy NASA, ESA, CSA, STScI, B Robertson (UC Santa Cruz), B Johnson (CfA), S Tacchella (Cambridge), P Cargile (CfA)
What’s remarkable is not just that we can see such a thing, but that we can recognise what it is and make sense of it. Astronomers are already analysing the invisible light from this infant galaxy, studying its composition, assessing the glow of its youthful stars. Teams from NASA and the European Southern Observatory put out press releases. News outlets around the world noted the discovery. The detection of invisible light from the edge of the known Universe registered as an exciting yet routine advance in astronomical exploration.
In 1998, the philosophers Andy Clark and David Chalmers proposed a radical understanding of cognition that could take the new realm into account. In it, they argued that human thought doesn’t end at the skull but extends into the tools and environment around us – including our technological surroundings. They called this framework the ‘ extended mind ’. As an example, they described a person interacting with a computer screen displaying geometric shapes. The individual doesn’t just perceive the shapes; they mentally manipulate them – rotating, moving, imagining them – as though the objects were physically present. In this way, person and machine form a ‘coupled system’, with the digital shapes becoming part of the individual’s cognitive landscape.
Seen through this lens, JWST becomes more than just a distant observer. The stream of infrared data it captures can be considered part of our evolving awareness of the cosmos. Its detectors expand our sensory reach; its images of ancient galaxies reshape our inner mental world; and its vast archive, stored in Baltimore, Maryland, functions like an external annex to human memory. Even if one doesn’t fully embrace the Clark-Chalmers hypothesis, the implications remain striking: JWST can be understood as a sensory prosthesis – like a retinal or cochlear implant – not just enhancing perception but becoming part of how we know.
Patients who receive such implants adapt to them rapidly, their brains assimilating synthetic inputs alongside the biological ones. No wonder, then, that astronomers and the lay public alike have no difficulty identifying the form and the significance of an object like JADES-GS-z14-0 . Our minds adapt just as readily to inputs carrying information from the invisible fringes as they do to inputs describing ordinary light and sound. The extension of the human senses, begun by Herschel with his humble prism-and-thermometer experiment, has since continued to gamma rays and radio waves, and even beyond radiation into neutrinos and gravitational waves , both of which are now within our technological sights. Each new type of observatory has yielded a new human familiarity with another layer of previously hidden reality.
Clark and Chalmers pointedly expressed what happens as technology expands our senses and breaks away the barriers of human perception: ‘once the hegemony of skin and skull is usurped,’ they wrote, ‘we may be able to see ourselves more truly as creatures of the world.’
This Essay was made possible through the support of a grant to Aeon Media from the John Templeton Foundation. The opinions expressed in this publication are those of the author and do not necessarily reflect the views of the Foundation. Funders to Aeon Media are not involved in editorial decision-making."
"No, Schrödinger’s cat is not alive and dead at the same time | Aeon Essays",,https://aeon.co/essays/no-schrodingers-cat-is-not-alive-and-dead-at-the-same-time,"In 1935, the Austrian physicist Erwin Schrödinger published a rather critical three-part review of what he called the ‘present situation’ in the relatively new theory of quantum mechanics. For the most part, Schrödinger’s review, written in German, is dry and technical, and not the kind of thing that would detain anyone outside the narrow academic world of quantum physics. But in one short paragraph, written with his tongue firmly in his cheek, he gave flight to a fancy that, 90 years later, continues to resonate in popular culture. The paragraph concerned Schrödinger’s eponymous cat. How did an obscure argument about a mathematically complex and rather baffling theory of physics become embedded in public consciousness as an extraordinary exploration of the human psyche? This essay tells the story.
Here’s what Schrödinger wrote (in the English translation by John D Trimmer):
Although this was only a ‘thought’ experiment, the paradox of Schrödinger’s cat was destined to join Pavlov’s dog in science’s bestiary of the bizarre.
T o understand the point Schrödinger was making, we need to do a little unpacking. The nature of Schrödinger’s ‘diabolical device’ is not actually important to his argument. Its purpose is simply to amplify an atomic-scale event – the decay of a radioactive atom – and bring it up to the more familiar scale of a living cat, trapped inside a steel box. The theory that describes objects and events taking place at the scale of atoms and subatomic particles like electrons is quantum mechanics. But in this theory, atoms and subatomic particles are described not as tiny, self-contained objects moving through space. They are instead described in terms of quantum wavefunctions , which capture an utterly weird aspect of their observed behaviour. Under certain circumstances, these particles may also behave like waves .
These contrasting behaviours could not be starker, or more seemingly incompatible. Particles have mass. By their nature, they are ‘here’: they are localised in space and remain localised as they move from here to there. Throw many particles into a small space and, like marbles, they will collide, bouncing off each other in different directions. Waves, on the other hand, are spread out through space – they are ‘non-local’. Squeeze them through a narrow slit and, like waves in the sea passing through a gap in a harbour wall, they will spread out beyond. Physicists call this diffraction. Push a bunch of different waves together and they will merge to form what physicists call a superposition . The peaks and troughs of all the different waves add together. Where peak meets peak, the result is a larger peak. Where trough meets trough, the result is a deeper trough. Where peak meets trough, they are both reduced and, if they happened to be of equal height and depth, they will completely cancel each other out. Physicists call this interference.
By 1935, the mathematical formulation of quantum mechanics was relatively mature, and acknowledged by most physicists as complete. But the theory does not say where all the quantum weirdness is supposed to stop. When applied to the radioactive atom, quantum theory says that, after an hour, the wavefunction of the atom is represented as an equal mix – a superposition – of decayed and undecayed atoms.
So, is the cat dead or alive? We have no way of knowing until we lift the lid of the box, and look
We could choose to stop there, but we know that the interaction between the atom and the diabolical device should also be described by quantum mechanics, at least in its early stages. If we choose to take the theory literally, then extending its equations to include the entire device means that the wavefunction evolves into a superposition of decayed atom and triggered device, and undecayed atom and untriggered device. In his review, Schrödinger coined the term ‘entanglement’ to describe this situation. The radioactive atom becomes entangled with the device.
If we logically extend this entanglement beyond the device to include the cat then, as Schrödinger explained, we arrive at a wavefunction, which is a superposition of decayed atom, triggered device and dead cat, and undecayed atom, untriggered device and live cat. The living cat and the dead cat thus appear ‘mixed or smeared out in equal parts’.
So, is the cat dead or alive? We have no way of knowing until we lift the lid of the box, and look. If we stick with quantum theory as extended to the cat, it is supposedly at the point we lift the lid that the wavefunction ‘collapses’, and we find that the cat is alive, or dead. But there is a small problem with this that has huge consequences. Nowhere in the mathematical formulation of quantum mechanics will you find an equation describing this collapse. We are left to assume it happens.
OK, but can we at least predict the fate of the cat before we lift the lid? Quantum theory says: no, we can’t. According to the accepted interpretation, the superposition of the two possibilities reflects the relative probabilities of getting one or the other. But these probabilities translate into actual outcomes only when the wavefunction is assumed to have collapsed, when the superposition of one possibility and the other transforms into one actuality or the other. It would seem that the act of looking literally kills the cat, or doesn’t.
This is a big deal. This is not the same as tossing a ‘fair’ coin, and getting heads or tails with equal probability. We wouldn’t normally choose to describe the coin as being in a superposition of heads and tails as it spins through the air, though in principle there’s nothing to stop us from doing this. We don’t do this because of course we know that both sides of the coin continue to exist unchanged as we toss it in the air, and as it spins while it falls to the ground. But this is not the way quantum mechanics works. There are now many quantum experiments that demonstrate that assuming objects like atoms or electrons exist in some state before they are observed can give predictions that conflict both with quantum theory and with the results of experiments. We simply can’t do without the superposition, or the probabilities. We need the weirdness.
A lthough the majority of physicists appeared to have accepted the argument that quantum mechanics provides a complete theory of individual quantum objects and events, there were some notable dissenters. Albert Einstein was never comfortable with quantum theory’s implications for the law of cause and effect, and the resort to probabilities, famously declaring that God ‘does not play dice’. Earlier in 1935, Einstein and his Princeton colleagues Boris Podolsky and Nathan Rosen had published a landmark paper arguing that quantum mechanics could not be considered a complete theory. Something profound was missing. Although they disagreed on the details, Einstein and Schrödinger shared common cause, and their correspondence through the summer of 1935 inspired Schrödinger to develop his cat paradox.
Schrödinger understood that under no circumstances could his cat be considered to be both alive and dead at the same time. As far as he was concerned, his paradox exposed the apparent absurdity of quantum theory, not by suggesting that ‘quantum theory says’ that a superposition consisting of a live and dead cat is a real possibility, but by suggesting that what quantum theory doesn’t say can lead to a logical absurdity. Einstein replied: ‘… your cat shows that we are in complete agreement.’
And there the matter rested, for a time. The cat paradox was limited to one paragraph in a lengthy review article, and Schrödinger’s dissent cut little ice with the majority of physicists, including those who spent time pondering on the meaning of quantum theory. It survived in correspondence between Einstein and Schrödinger through to the early 1950s, and resurfaced in 1957, during a conference of physicists and philosophers held in Bristol, England.
The diabolical mechanism now involved electrocuting the cat (or not)
In a discussion featured in the conference proceedings, the American physicist David Bohm resurrected Schrödinger’s cat. By this time, the paradox had evolved and was based on a single photon (a ‘particle’ of light) passing (or not passing) through a half-silvered (or ‘one-way’) mirror. Like the radioactive atom, the photon has a 50/50 chance of passing through the mirror or being reflected by it. Passage of the photon triggers a diabolical mechanism in which the cat is killed with a gun.
The paradox reappeared again in 1965, in an essay by the American philosopher Hilary Putnam titled ‘A Philosopher Looks at Quantum Mechanics’. The photon and half-silvered mirror remain, but the diabolical mechanism now involved electrocuting the cat (or not). Putnam concluded that: ‘ no satisfactory interpretation of quantum mechanics exists today.’
What happened next is rather fascinating. While researching Einstein’s special theory of relativity for a book she was writing sometime in 1972, the American science fiction author Ursula Le Guin came across a reference to Schrödinger’s cat. As the philosopher Robert Crease put it in a 2024 article, she was instantly ‘entranced by the implied uncertainties and appreciated the fantastic nature of Schrödinger’s image.’ We can’t be sure of precise events and timings, as Le Guin read extremely widely but didn’t systematically take notes, but this is the ‘best guess’ of Julie Phillips, who is busy writing an authorised biography of Le Guin to be published in April 2026. Having been asked by her subject to ‘rescue me from the vultures’, Phillips conducted many in-depth interviews with Le Guin before the author died in 2018. They had agreed that the biography would be published posthumously.
In her short story ‘Schrödinger’s Cat’ (1974), Le Guin presents Bohm’s version of the paradox involving the photon, half-silvered mirror and gun. In a dialogue between the nameless narrator and a dog called Rover, Le Guin wrote:
The floodgates opened. From this point onwards, Schrödinger’s cat makes regular appearances in fiction. Not just science fiction, but a broad range of short stories and novels, films, plays, television shows, poems, and music. Developments taking place in physics in the early 1980s simultaneously drove burgeoning interest in popular non-fiction, such as John Gribbin’s In Search of Schrödinger’s Cat (1984).
The cat’s cultural appeal lies in the ‘what if’ questions it provokes. It encourages us to ponder the consequences of our very human choices. What if we choose not to look? If we don’t look, can the cat really be said to exist at all? Our decision to lift the lid is much like encountering a fork in the road. We choose a path. Like the American poet Robert Frost, we may choose the path less travelled by. But what if we had taken the other path? The movie Sliding Doors (1998) delivers two parallel stories, one that unfolds when Helen Quilley (Gwyneth Paltrow) misses her train on the London Underground, and a second that unfolds when she manages to board it. Quilley’s life turns out very differently, depending on whether or not she beats the sliding doors and gets on the train. That such a trivial ‘sliding doors moment’ might profoundly alter the course of our future is deeply unsettling.
There’s more. As Le Guin herself observed in her short story, there appears to be nothing special about the act of lifting the lid, and quantum mechanics is silent on the question of where in the chain of events the weirdness stops. She wrote: ‘But why does opening the box and looking reduce the system back to one probability, either live cat or dead cat? Why don’t we get included in the system when we lift the lid of the box?’ Could we be just like the cat, but trapped in a much bigger box that we call reality? If we are, who is doing the looking? And what will happen when they lift the lid?
If the act of looking inside the box doesn’t collapse the wavefunction of the system, then logically the observer must in turn become entangled in the superposition. ‘[T]here we would be,’ Le Guin wrote, ‘looking at a live cat, and … looking at a dead cat.’ If you are the one doing the looking, there would now be another superposition involving two versions of you.
A t this point, we might be tempted to reach for an altogether different interpretation of quantum mechanics. If the mathematics doesn’t account for the collapse of the wavefunction, why assume it happens at all? Why not suppose that, as Le Guin suggested, you become entangled with the system when you lift the lid? As nobody has ever experienced the eerie sensation of co-existing with multiple versions of themselves, all witnessing different events, we could further suppose that the act of lifting the lid ‘splits’ the Universe into two parallel versions. In one universe, one version of you observed a dead cat. In another universe, another version of you observed a live cat. There is no eerie sensation because these different universes have diverged, and you are completely unaware of the other parallel versions of yourself.
This is the so-called ‘ Many Worlds ’ interpretation, proposed in 1957 by the American physicist Hugh Everett III. It offers us a multiverse of parallel possibilities. The multiverse allows a much broader and more sophisticated range of ‘what ifs’ beyond the binary alive/dead-type questions relating to a sliding doors moment. What if the effects of your choices accumulate over time and conspire to change not just your future circumstances, but your entire personality?
In a multiverse of possibilities, is there a multiplicity of very different versions of you all behaving very differently and living different lives? Perhaps in one of these universes you are humane and kindly but homeless, reduced to begging on street corners. But in another you are an unempathetic tech billionaire, threatening to undermine the accepted world order. Such questions are explored to great effect in Blake Crouch’s novel Dark Matter (2016), which was adapted for television and broadcast last year on Apple TV+.
The Schrödinger’s cat of popular culture feeds our innately human desire for mystery and gives licence for daring flights of imagination that help us to explore what makes us ‘us’. And, remarkably, it claims to do these things in the name of science, because this is what ‘quantum theory says’. Who thought physics could be so much fun?
This doesn’t mean that the cat is literally alive and dead at the same time
Alas, most physicists adopt a more sober perspective. In the 1920s and ’30s, the founders of quantum mechanics nagged away at these problems of interpretation and arrived at solutions that many deemed satisfactory, though some (like Einstein and Schrödinger) deemed them deeply unsatisfactory.
What does the superposition and, more broadly, the quantum wavefunction, actually represent? One view, most closely associated with the Danish physicist Niels Bohr and known generally as the ‘ Copenhagen interpretation ’, is that these are merely calculation devices not to be taken literally. They are not real: they are purely symbolic. The superposition simply represents what we know about the cat-in-a-box system, and we use the equations of quantum mechanics to calculate the probabilities for various expected outcomes.
So, when we talk about the cat being in a superposition of life and death, this doesn’t mean that the cat is literally alive and dead at the same time. In truth, we don’t know what the state of the cat really is, nor how to describe its real physical situation, because we can’t say for certain when the radioactive atom will decay, or whether the photon will be transmitted or reflected. But if we represent this system as a superposition, we know we will make predictions that will prove to be consistent with experiment. Most physicists, at least those who can be bothered to think about these things, adopt this view. This might be why they’re not often invited to parties .
It follows from this that it doesn’t matter precisely where in the chain of events we declare that the weirdness stops. It doesn’t matter where we place a ‘Heisenberg cut’, named for the German physicist Werner Heisenberg (of uncertainty principle fame), the point at which we stop using quantum mechanics and switch to more familiar theories of physics published more than 300 years ago by Isaac Newton. This is the point at which we assume the wavefunction collapses, and we replace the and of the quantum superposition with the or of actual outcomes.
Heisenberg was a member of Bohr’s group, although in some of his pronouncements he diverged substantially from Bohr’s philosophy. As far as Heisenberg was concerned, it does not matter where we choose to place the cut. But a cat is not a radioactive atom, nor a photon. It clearly does not belong in the quantum realm, and the equations of quantum mechanics, even interpreted symbolically, should not apply to it. Bohr preferred to place the cut at the point of the ‘irreversible act of amplification’ associated with the early stages of the diabolical device. That we can’t be specific about precisely where or when in this process the weirdness stops doesn’t invalidate the conclusion that this happens long before we get to the cat.
Schrödinger ends his 1935 review with the observation:
We are faced with a choice. We can recognise that quantum mechanics – with all its weirdness – is a purely symbolic framework for predicting the probabilistic outcomes of our experiments. It is indeed a calculational trick, not to be taken literally, which allows us some ability to get a handle on an otherwise unfathomable atomic and subatomic world.
Or we can recognise (with Einstein and Schrödinger) that quantum theory is at the very least incomplete, and deeply unsatisfactory. A theory capable of fathoming the atomic and subatomic world ought to be possible, if only we have the will to look for it, and the wit to find it.
This is the fork in the road. Which path will you take?"
Why the hunt for reality is an impossible burden for physics | Aeon Essays,,https://aeon.co/essays/why-the-hunt-for-reality-is-an-impossible-burden-for-physics,"Browse a shelf of popular science books in physics and you’ll often find a similar theme. Whether offering insights into The Hidden Reality ( 2011 ), Something Deeply Hidden ( 2019 ) or Our Mathematical Universe ( 2014 ), these books hint at an underlying, secret world waiting to be unravelled by physicists – a domain beyond our sensory perception that remains their special purview.
Over its history, physics has delivered elegant and accurate descriptions of the physical Universe. Today, however, the reality physicists work to uncover appears increasingly removed from the one they inhabit. Despite its experimental successes, physics has repeatedly failed to live up to the expectation of delivering a deeper, ‘final’ physics – a reality to unify all others. As such, physicists appear forced to entertain increasingly speculative propositions.
Yet, with no obvious avenues to verify such speculations, physicists are left with little option but to repeat similar approaches and experiments – only bigger and at greater cost – in the hope that something new may be found. Seemingly beset with a sense of anxiety that nothing new will be found or that future experiments will reveal only further ignorance, the field of fundamental physics is incentivised to pursue ever more fanciful ideas.
I argue that the pursuit of unity and dominance of a more fundamental reality presents itself not as physicists’ unique prerogative, but instead as an impossible burden placed on their shoulders by the modern world. I suggest that we should embrace a more pluralist and nuanced understanding of what comprises the cosmos, an understanding that not only accepts but invites criticism from other practices, disciplines and realities into its current predicament. My time spent in physics, both as an aspiring theoretical physicist and later as a sociologist studying the practices of fundamental physics, has left me to wonder to what extent narratives of unity and finality continue to serve the communities that proliferate them. And, further, to what extent does achieving greater fidelity towards what comprises existence, to what reality is, and to the constituents of the cosmos require that physics give up the mantle as reality’s primary purveyor?
O n a rainy day in central London, I sat down to interview a professor of astroparticle physics. At that point, I had spent just over a year observing weekly meetings of his research group: part of a collaboration constructing a large-scale experiment to detect dark matter.
During the interview, I noticed something that had until then passed me by. Far removed from the technical discussions on experiment modelling and data processing that often occupied group meetings, I became aware of a sense of anxiety, not just in this group, but in the wider particle-dark-matter community and fundamental physics at large.
It was an anxiety fed by the realisation that they had been at this a long time, and it had begun to make them wonder what they had to show for it. As the professor told me: ‘We keep getting bigger and building better. But maybe it’s none of this.’ In our interview, he spoke repeatedly about what he would do differently if new to the field, of the money spent on these experiments, and on the responsibility he felt for those early career researchers he’d enlisted for this task. It was clear that this was as much a personal concern as a professional one.
My path to this dark matter group had begun years earlier. After I graduated in astrophysics and applied mathematics, hoping to study reality at its most fundamental, I considered offers to pursue doctoral research in theoretical cosmology. However, when asked to choose from speculative projects that held little to no hope of observational verification, I left the field, disillusioned, realising that I would not find a greater understanding of reality there.
‘It’s akin to knowing everything about sand dunes … but not knowing what a grain of sand is made of’
Instead, I was drawn to sociology, which offered insight into a seemingly more real world – that of the everyday – and the opportunity to explore phenomena otherwise dismissed as unquantifiable, messy and subjective. Whereas the natural sciences primarily seek to make the unfamiliar familiar, the critical humanities often operate from the opposite impulse – to make the familiar unfamiliar. Their ambition is to interrogate matters taken for granted, and to see how given discourses, knowledges and even realities are mobilised politically.
Eventually, my research would bring me back to cosmology. I wanted to understand how the cosmos had become synonymous with the world beyond Earth and considered the exclusive preserve of physics. So, I decided to conduct ethnographic work with physicists investigating dark matter and other problems of cosmology.
Ask a physicist to list the big open problems of fundamental physics and cosmology and it is likely that the dark matter problem will be among them. Thought to comprise more than 85 per cent of all matter in the Universe, its presence is inferred but remains undetected. As the astrophysicist Priyamvada Natarajan succinctly explained at the Royal Academy of Arts in London in 2019:
That’s why, for the past four decades, particle physicists have built multiple detectors hoping to find those ‘grains’. Still more are planned, with each generation larger and more sensitive than their predecessor. Recently, however, a disquiet has begun to emerge. Despite all the investment of time and money, the likeliest candidates for dark matter, once so promising, have had their ‘moment of truth’ and have fallen short. Some physicists are even beginning to question whether they will find it at all. Indeed, another interviewee, who had dedicated their career to the search, went so far as to suggest that they did not believe dark matter existed, at least not in the manner that they’d come to understand it.
Even if these concerns did not appear to immediately trouble others in dark matter research, cosmologists and astrophysicists who are able to ‘exquisitely, spatially map’ dark matter and tell us ‘what it does’, they are cause enough for alarm. As one professor of cosmology told me: ‘It’s all tentative in the sense that, if you’re not confident that the particle even exists, then there’s some question mark hanging over the entire endeavour.’
Like the dark matter problem, much of fundamental physics’ present malaise, I realised, came not merely from the fact that its practitioners had not found a deeper reality but, rather, from the weight of the expectation that they should . Physics’ apparent ownership of the deepest reality appears justified less by the field’s empirical results than by the myths and narratives that sustain it.
T he truth of the matter is that fundamental physics research has, according to its high standards, entered a period of stagnation. Those few physicists able to step away from the exacting demands of their practice will speak of physics as at a crossroads, led astray or even in crisis . As all these authors suggest, fundamental physicists appear increasingly short of ideas, with the latest experiments at best verifying older theories.
The field of cosmology is one of these areas. If you judged only by popularisations, the Big Bang cosmological story appears a mostly done deal. Many cosmologists, for instance, will speak of an era of ‘precision cosmology’, one where the only remaining work is to add detail to a Universe that has largely been solved. Despite this, much of the standard cosmological model remains theoretical speculation.
Dark matter isn’t the only problem. Dark energy, for instance, whose proposed existence initially prompted the call for precision cosmology, has not yet proven to be anything other than a mathematical term required to balance physicists’ equations in line with observations. And cosmic inflation, a theorised period of accelerated expansion of the very early Universe, stubbornly persists as a point of controversy due to its ‘all-too-convenient’ nature.
We risk what has been called a ‘post-empirical physics’. In short, a Universe lost to the world
Indeed, a closer attention to cosmology’s many claims suggests a Universe less solved than one fragilely constituted of speculations, better reflecting a collection of entities that, like dark matter, appear well reasoned but stubbornly remain unknown . As the cosmologist Pedro Ferreira states , there is a ‘real risk’ that further experiments will reveal only ‘a much more precise statement about our ignorance but nothing more’.
This is echoed in other areas of fundamental physics. In high-energy physics, the detection of the Higgs boson, in 2012, was the crowning achievement of a decades-long research programme that successfully confirmed the predictions of the Standard Model of particle physics, a framework for describing the microscopic structures of matter. Nevertheless, physicists had always regarded the Standard Model as a temporary solution , a stepping stone towards a more complete ‘theory of everything’ that would incorporate gravity into its explanations and successfully account for physical phenomena at all scales. For this reason, the model’s deficiencies and more convenient features were tolerated.
Yet, as the years have passed, those deficiencies remain. The multi-billion-dollar Large Hadron Collider (LHC) has shown nothing to suggest that the theorists’ extensions or alternatives are anything more than speculative indulgences. In response, some have called for an even bigger collider, such as the proposed Future Circular Collider (FCC), in the hope of glimpsing physics that they firmly believe lies beyond the Standard Model. The cost is an estimated $17 billion.
Adding to this are ever more speculative ideas about pre-Big Bang physics and the multiverse, as well as string theorists suggesting that their theories’ internal consistency may matter more than their empirical verification. With these, we risk what has been called a ‘ post-empirical physics ’. Unless something changes, this entails either the abandonment of fundamental physics, or a field of physics increasingly freed from empirical substantiation. In short, it risks a Universe lost to the world.
The physicists who have highlighted this stagnation tend to prescribe more physics, albeit a physics done differently. But none would go so far as to suggest that the answer may not lie in physics at all. Instead, I suggest looking at how we view and talk about fundamental physics. And that, while unity and finality may matter to the discipline, such concepts reflect the Universe less as it is, and more as we would hope to find it. In this picture, the appropriate prescription to physics’ present predicament may not be more physics but, rather, less .
D reams of a ‘final theory’ have long motivated physicists. Seemingly on the cusp of delivering it, but forever beyond its grasp, theoretical physics has sought to bring together its two most successful theories, quantum theory and general relativity, under a ‘theory of everything’. Such a theory, it is hoped and at times assumed, would, in its more complete vision, also seek to encompass all other forms of knowledge within its explanations.
Even if, today, it is less fashionable to explicitly call out this latter ambition, narratives of unity and finality persist in the discipline. Indeed, it is an assumption that is often baked into the practices themselves. For instance, in 1993, the astrophysicist Edward W Kolb stated :
It is a unity that is believed to be inherent to science and that will typically have physics positioned at the very top. Such is the pervasiveness of this assumed authority that it has led some leading physicists to paint religion and philosophy as obsolete. For instance, in his posthumously published book Brief Answers to the Big Questions (2018), Stephen Hawking freely claimed that cosmological physics has left ‘no possibility of a creator’. With such contentions, it is not uncommon to think that physics alone can and should unlock the Universe’s ‘secrets’.
To understand why we readily conflate the world of physics with our unique and most fundamental reality, let us return to the case we opened with, that of the theme echoed by popular physics books. The proposal these books wish to make is clear: there is a reality hidden from our senses, more fundamental than all others, that remains physicists’ specialised duty to understand. If given the chance, at the cost of a paperback, that reality could be shared with the would-be reader, too. We should question what role these books and their narratives serve, when taken together and continuously repeated, and not just for a lay public but, indeed, for physicists themselves.
We’ve made the mistake of believing that, just because the world can be interrogated by physics, the Universe is fundamentally physical
As the theoretical physicist Carlo Rovelli remarks in the preface to Reality Is Not What It Seems (2014): ‘The more we learn about the world, the more we are amazed by its variety, beauty and simplicity.’ Further along in the preface, he provides a familiar rendition of Plato’s allegory of the cave in support of physicists’ quest for truth and towards a simple reality beyond complex appearances. Remarking that we ‘are all in the depths of a cave, chained by our ignorance, by our prejudices’, Rovelli argues it is science that, through its ability ‘to reveal new regions of reality, and to construct novel and more effective images of the world’, will bring about our deliverance.
As the sociologist Bruno Latour put it in Politics of Nature (1999), the allegory of the cave is a familiar one, and reveals a defining feature of Western and scientific thought: the cleaving of the world in two. On the one hand, a subjective world of confusion, a world of what Rovelli would call ‘prejudice’ and ‘ignorance’ – all attributes of what may be described as the social world. On the other hand, the natural world, objective and pristine, untroubled by humankind.
The physicist can lay claim to a role that seemingly is theirs alone, namely, to escape what Latour describes as the ‘tyranny of the social world … so that he will be able to contemplate the objective world at last’. In this role, physicists assume a special authority, and the unique ability to move back and forth between such worlds and speak for a ‘true’ reality to an imperfect humankind.
Such a division between a world of essential qualities (matter, form, shape and substance) and one of secondary qualities (colour, sound, taste, emotion, thought and so on) is what the philosopher and mathematician Alfred North Whitehead, in the 1920s, called the ‘bifurcation of nature’. And it is through a fallacy, what he termed the ‘fallacy of misplaced concreteness’, namely, the ‘error of mistaking the abstract for the concrete’, that we have taken the world of the abstract – the world given to physics – as more real , the originator, and the cause for the second one.
That we live in a time that seemingly privileges the reality of the material, physical and objective over what is thought immaterial, mental and subjective should come as no surprise to any of us. That it could be otherwise often does. Indeed, according to Whitehead, the bifurcation is contingent and corresponds to a very particular way in which we have come to configure reality. Through the many successes physics has known, we’ve made the mistake of believing that, just because the world can be interrogated by physics, the Universe is fundamentally physical.
H ow, then, can we take the possibility of difference seriously? And how can we envisage a world configured differently without foolishly renouncing the many insights physics has given us? Answering these questions requires that we confront what the psychologist and philosopher William James, a contemporary who greatly influenced Whitehead, in Pragmatism (1907), called the ‘most central of all philosophic problems’: the one and the many.
Contrary to the seemingly common-sense conviction of its undivided unity, James remarks that the world may be better understood as being both one and many, a world that is disclosed insofar as it is encountered to be such.
The proposition that James seeks is as radical as it is simple, and flies in the face of how much of philosophy and scientific investigation has operated. Upon reflection, there appears to be nothing to support the assumption of the overall or ultimate unity of the world, nor would experience appear to be able to confirm the notion of absolute division. As James shares in A Pluralistic Universe (1909), he has found ‘no good warrant for even suspecting the existence of any reality of a higher denomination than that distributed and strung-along and flowing sort of reality which we finite beings swim in.’ Yet, it has often been the aim of many forms of enquiry to work towards the discernment of a fixed and singular reality, or some aspect thereof.
A ‘pluriverse’ asks us to accept that there are many ways in which we may come to know and be in the world
James’s response, therefore, is to propose the existence of a pluralistic Universe, or a ‘pluriverse’, in which the world’s oneness and manyness are not characteristics of the world as it is, but as it becomes . A pluriverse is, above all, a world in the making. It remains forever unfinished and withholds the answer to its unity or division, leaving it as a question for ongoing empirical investigation.
To take this idea to its most radical, it asks us to accept that there are many ways in which we may come to know and be in the world, and to take seriously the realities proposed by other knowledges, practices and cultures as precisely that – as reality . To ‘take seriously’, however, is more than some act of liberal tolerance. As the philosopher of science Isabelle Stengers put it in 2003: ‘The ethical test may well … begin with trying to envisage others as having to tolerate you.’
This is not to say that there is one world that should seek to encompass all multiplicities. Nor is the activity of ‘taking seriously’ an act of naive relativism, wherein different realities are expected to coexist equally without critically investigating how they intersect, overlap and potentially contradict each other. The challenge is much harder than either of these: instead, it is an invitation to allow these realities to deeply trouble one’s own.
I suggest that we must take seriously the possibility of other worlds. By this I do not mean the familiar speculations of the multiverse, or the Many-Worlds hypothesis , introduced by physics to come to terms with the Universe’s ongoing indeterminacy. Rather, it is to take seriously those worlds that physics and modern realism have otherwise dismissed. That is, worlds in which, for instance, the Earth beings of Indigenous peoples are real, the ghosts of Japanese family members are cared for, and where God talks back to evangelical believers who speak with him.
This is also to suggest a return to openly questioning the claims of physics. Where once large public debates took place between physicists and philosophers on the nature of time and the extent to which physics can be said to speak for reality, today, public debates between physics and philosophy are seldom serious. Instead, when similar questions are debated in philosophy journals, they are largely settled in deference to the claims of physics.
Now, there is no doubting the immense achievements of fundamental physics. Further, its speculative wagers may yet be rewarded with time. However, such wagers have left the field unclear where to look for alternatives should it fall short of its own high expectations, and its practitioners, who are believed to be closest to reality, the furthest away from it.
In response, we need a humbler physics that is no less radical in its speculative ambitions but invites contradictory visions into its propositions, not only visions that are subservient to physics’ claims. In this, we would have a more adventurous physics, one that accepts and invites criticism from other practices and disciplines into its current predicament.
More pragmatically, it would allow for the possibility of engaging physics with other practices. Biophysics and climate physics are good examples of this. Unlike cosmology and fundamental physics, they are not premised on the supremacy of one field over the other but understand their limitations, and respective and restricted areas of application.
But we might go even more radical than this, departing from a purely physicalist approach to embrace modes dismissed as mere fantasy, story or ‘immaterial’, to re-engage physics with alternative debates, visions and configurations of reality. This may require that we abandon physics’ privileged place as science’s standard bearer, like we did the philosophers and high priests of old, as the practice with unique access to a deeper reality more fundamental than others.
It may even require that we abandon doing physics altogether, in the attainment of an expanded reality that not only accepts but encourages the possibility of difference and more. Or, as the speculative fiction writer Ursula Le Guin once put it , what we require are ‘the realists of a larger reality’."
How the novel became a laboratory for experimental physics | Aeon Essays,,https://aeon.co/essays/how-the-novel-became-a-laboratory-for-experimental-physics,"In September 1961, Richard Feynman gave the first in a series of lectures on basic physics at the California Institute of Technology. At the start of the first day of class, he described the foundations of his subject to almost 200 aspiring scientists (and more than a few of his colleagues): ‘The principle of science, the definition, almost, is the following: The test of all knowledge is experiment .’ This, he declared, ‘is the sole judge of scientific “truth”.’
Over the next two years, Feynman distilled more than two centuries of knowledge into his lectures. He was a better physicist than he was either a mentor or a man (Feynman was wretched to women and disdainful of his students), yet he remains the greatest teacher most physicists never had. And though he taught his introductory course only once, the magisterial book that derived from it, The Feynman Lectures on Physics (1963-65), still cultivates physicists today.
When I was an aspiring physicist, decades later and more than half a continent away, I too appreciated the basics from The Feynman Lectures on Physics during my first year of college. But it was only during my final year that I began to understand what he had told the class about experiments on the first day. That fall, in a laboratory course required of all students majoring in physics, three experiments became the sole test of my knowledge, too.
With other students, I designed circuits to count flickers of light inside a flask. These were the flashes from a dense liquid struck by antiparticles, whose arrival among cosmic rays signalled the relativity of time and Albert Einstein’s equivalence of energy and substance. Next, we measured the speed of sound in a vat of helium as it cooled from a gas to a liquid. At two degrees above absolute zero, we witnessed helium molecules abruptly huddle into a quantum whole greater than its parts. This placid superfluid then transitioned into a fountain as the molecules charged from their confines, moved as if by collective will. Finally, we communicated all at once with billions upon billions of protons swarming in water. We nudged these particles into resonance, using a radio wave tuned to their spins, as one might excite children to twirl in unison with a song. We then listened for an echo, a collective response to our call, to glean their magnetic resonance – a technique physicians used to image the soft tissue in our aqueous bodies.
We were learning to be the authors of experiments, not just the readers. But even the simplest plots of physics were so bizarre that they were too difficult to recount in words. We learned that, to extract any sense from matter, we had to contrive intricate machines and derive byzantine equations.
Twice a week, after leaving the experiments in that shabby lab, I hustled across campus to study the opposite. In a neo-gothic hall, I learned how to tease meaning from experiments with the immaterial – with prose and the imagination. In a course required of all students in my second major, Spanish Literature, I read the most important works from 16th- and 17th-century Spain. And I entered a whole other world, far from the antiparticles and quantum wholes across t he way.
Here, I learned how the anonymous author who wrote La Vida de Lazarillo de Tormes (1554), The Life of Little Lázaro of Tormes , invented the picaresque form of storytelling through the adventures of an antihero, little Lázaro. This rogue appeared centuries before such characters came to dominate the shows on our screens. Next, we read Don Quixote (1605-15) and realised that its author, Miguel de Cervantes, had not only written the first modern novel, he had also frisked our modern notions of truth and reality. Cervantes had created a character who is more certain of the chivalrous world inside his head than of the actual world outside – the result is a comedy of self-contradiction so original it outwits our expectations even today. Finally, we discovered that, not long after the publication of Don Quixote , Tirso de Molina was the first author to stage the legend of Don Juan, in El Burlador de Sevilla y Convidado de Piedra ( The Trickster of Seville and the Stone Guest ). Molina used his dark comedy to reinvent the anti-romantic figure, satirising the still-impossible ideals of chastity and free will.
I loved those bygone tales. I related to their characters instinctively, even though I had never inhabited their worlds. They moved me. But no matter how authentic they seemed, how true to life, they would always remain figments, unlike the imperceptible particles I detected in the laboratory. I could never empathise with protons, electrons and other particles, but they remained more real than the complex protagonists of Spanish literature.
When I graduated, my two worlds could not have seemed more dissimilar. Fiction wielded an intimate imaginary. Physics plied the unrelatable real. I was not sure I would ever reconcile the two.
Y ou almost certainly believe, as I did, that there is little overlap between the routines and aspirations of fiction writers and physicists. You are, as I was, almost certainly wrong. Both are the purveyors of real and imagined worlds. Both are interrogators of the intangible, posing some of our broodiest questions about existence. Both are persistent experimenters, putting human knowledge to the test.
There are surprisingly deep consonances between the experiments in physics and in fiction, particularly literature written in Spanish. Some of the greatest experiments in the Spanish language, which are some of the greatest experiments in any literature, were either undertaken by former physicists or leased from physical principles. These works, I eventually realised, may even help us understand what it means for experiments to be ‘the test of all knowledge’.
There is no singular method to experiments in the sciences. We do not observe superfluidity as we track ocean currents. Quantum field theory cannot be confirmed through field work. There are simply too many genres of science to generalise all its experiments.
That has never stopped people from trying. Scientists still enumerate a method that has anywhere from three to a dozen steps. Some philosophers generalise experiments further as ‘interventions’: each is a connection between some instrument and an object, visible or not, that humans wish to understand. In their accounting, an experiment is the intermediary between the physical world and human minds. It is the handmaiden to perceptions. At best, it is a programme for discovering ourselves and the impersonal world.
Novelists don’t merely gaze inward to create new forms. They look outward, too
That is not how most people understand experiments in fiction. But experiments in fiction defy simple characterisation as those in science do. There is no unique fictional mode, just as there is no method singular to science. Each experiment is novel, each novel an experiment.
During the 1980s, Gerald Prince, an eminent professor of French literature, did try to define ‘experimental fiction’. The words for ‘experiment’ in each of the Romance languages remain synonyms for the term’s original sense: an experience . But Prince immediately rejected this definition as too generic; surely every novelist wrote from experience. He also rejected the denotation, from Webster’s Collegiate Dictionary , that an experiment is a test or a trial to demonstrate truth – experiments in fiction did more than put truth to the test. Prince even rejected the gimmicky literary experiments of postmodernism and the avant-garde, but he briefly entertained the notion that experiments in fiction could be like those in science. He thought especially of Émile Zola’s empiricism in The Experimental Novel (1880), and he weighed the rigorous work of Raymond Queneau and other writers and mathematicians who were members of the literary collective Oulipo . These authors, at least, adhered to some method, or ‘recipe’ as Prince called it.
But Prince eventually enumerated just three traits common to literary experiments. They pique through form or structure rather than plot. Their concerns are interior to the text, not exterior. They tease or twist language systematically. They are, in other words, self-referential, even recursive. In sum, they are exercises in manipulation and control.
These are not the only experiments that novelists undertake with fiction. Novelists do not merely gaze inward to create new forms or invent new ways of saying. They look outward, too. They direct their experiments toward the greater patterns of the world. They also stretch space and time, and quiz the substance of reality.
Novelists can experiment like physicists do, yet Feynman hinted that the inverse was also true. Scientific experiments required much more than rote method to derive knowledge. As he told his class at Caltech on the first day: ‘Also needed is imagination. ’ Only by experimenting with the possibilities of the world could physicists make guesses about its ‘wonderful, simple, but very strange patterns’.
D uring my final semester of college, I proposed writing an honours thesis with a professor who was an expert on the mingling of reality and fiction in medieval Spanish texts. I was drawn to this mingling but wanted to research a more modern subject – one related to my other major. My professor asked if I might consider why Gabriel García Márquez set so many of the One Hundred Years of Solitude (1967) inside a laboratory, the original site for his magical realism. I demurred. I wanted to do more than reinterpret a lab as the setting for fictional experiments.
I wanted, instead, to understand two Latin American writers who had also studied physics: Nicanor Parra and Ernesto Sábato. I wanted to alloy my two worlds, as I believed they had. My professor agreed to supervise this stubborn idea, but she further recommended the short fictions of Jorge Luis Borges, especially those that alluded to science. I would read Parra’s Poemas y Antipoemas (1954), Sábato’s El Túnel (1948) and Ficciones by Borges (1944).
Parra was born aside the Andes Mountains of central Chile in 1914. Although he wrote poetry as a young man, he would study physics and mathematics at university, both at home and abroad. Beginning in the late 1940s, while learning cosmology at the University of Oxford, Parra developed the irreverent style that would come to define his ‘antipoems’. His ironic works are laden with slang and infused with fantasy; they are a poetry for everyone, in any frame of reference. He would continue to pursue this style after returning to Chile, while teaching physics for 40 years.
A famous compatriot, Roberto Bolaño, once said that Parra writes ‘as if tomorrow he will be electrocuted’. There is a certain gallows humour to Parra’s poems. Angels are commanded to get run over by trains, the names of great loves are forgotten. His ‘Ode to Some Doves’ ridicules those peaceful birds, his poetic ‘Epitaph’ pokes fun at his own appearance. In ‘Self-Portrait’, Parra asks us to look upon him in his usual setting, the classroom. He is a professor of physics who has lost his voice droning on for 40 hours a week. His eyes are ruined, his hair scarce, his nose whitewashed with chalk. He inspires, he says, little more than shame. Gone are the beautiful ideals of his youth; he now hallucinates strange forms and hears unaccountable sounds. ‘Life makes no sense,’ he writes at the end of his collection of antipoems.
Sábato played with time and reality through the unrealities of love and obsession
Parra’s witty antipoems are to traditional poems what antimatter is to matter. In fact, Parra referred to his poems as particles: they are characterised by great energies and speeds because they are so quaint and weightless. Yet in his particulate poems:
Sábato was born three years before Parra, on the flats of Argentina, and he, too, studied physics at home and abroad. In 1938, after completing a dissertation on his experiments with quantum mechanics, he left for the Institut du Radium, founded by Marie Curie in Paris. While experimenting there with radiation, he befriended Surrealists in cafés and began to paint and write. He escaped France during the war and, after studying cosmic rays in the United States, returned home to teach physics. He eventually left science to write a series of essays, Uno y el universo (1945), One and the Universe , before moving into a cabin to finish his first novel, El Túnel ( The Tunnel ) , in 1948. The work was one of the first examples of existentialist fiction, what Jean-Paul Sartre would deem an ‘anti-novel’.
In El Túnel , Sábato played with time and reality through the unrealities of love and obsession. A painter named Juan Pablo Castel is consumed with a married woman, María, across overlapping timelines. He craves her and her inaccessible thoughts so thoroughly that, even after they are together, he invents a pretence to murder her. The story fragments as his mind does, drawing us into his tortured thoughts. Castel is mad with his inability to know María completely – mad with the inaccessibility of the world outside our minds:
In my thesis, I suggested that the literary experiments of Parra and Sábato derived from their physics, but I was still unsure what exactly experiments were.
The previous summer, I had joined a lab to design experiments on the human perception of sound. Surrounded by racks of electronics, I learned to control the pitch and frequency of pure tones, to witness their precise, viridescent waves filling a fluorescent screen. I paid volunteers to don headphones and listen for the slightest variations between two white noises, played for one ear then the other. I verified that each listener had perfect pitch through a battery of tests in an anechoic room. I controlled for as many variables as I could. I still could not account for what the participants heard.
Each time the sound shifted from one ear to the other, the phase between the two white noises shifted randomly but imperceptibly. And, with each shift, listeners reported hearing an illusory pitch, a sound that never registered in our instruments. And all the participants perceived the same fictitious pitch. Although I could neither explain nor understand what the listeners heard, the result was exactly as predicted. I heard it, too.
It was a note from the illusive world inside our heads.
I n physics, reality is not defined by what humans experience. Physicists know how delicate and contrary experiences can be. Reality is, instead, the substratum of objects with properties that are independent of our observations. It is the catalogue of physical possibilities beyond all humanity. Reality is the sturdy net beneath us as we inch the tightrope between birth and death.
Physics is often called a natural science, but that descriptor owes more to the discipline’s history than to current practice. Modern physicists often require as much artifice as fiction writers do. Just as novelists describe artificial worlds that help us understand the real, physicists also require artificial settings – their accelerators, detectors and models – to reveal scientific ‘truth’. When a physicist performs experiments with a cloud chamber, in which the tracks of invisible particles are revealed through a mist, they are using atmospheric conditions that have never been met under our skies.
Physicists do not study the world as they experience it. They do not seek to model what they might observe on a hike. Rather, they fashion simulacra then twist them in a lab. They try to exceed the natural, building clouds from possibilities rather than actualities, rendering wisps of the unseen. They, too, are novelists, crafting narratives of the real from their experiments.
In this library of innumerable possibilities, physicists continue to hunt for one true story
I mean this literally. Physicists even have their own genre of fiction, the thought experiment , in which they reimagine the world’s structure through prose. Erwin Schrödinger once imperilled a non-existent cat while trying to resolve possibilities into certainties. Werner Heisenberg peered through an imaginary microscope to set limits on the certainty of knowledge. Einstein narrated an impossible elevator ride to contemplate space and time.
In this way, physicists fret the nature of their measurements and cast doubts on facile notions of realism. Some quantum physicists go even further. They deny the existence of objects with definite properties, they deny the sturdy net beneath the tightrope we walk between life and death. Without interventions, without experiments in laboratories, the quantum world thus consists of little more than possibilities.
Quantum physicists and string theorists have further liberated their equations from a strict realism by crafting the fictions of other worlds. They have found a space for every story, impossible or not, inside the idea of a multiverse . But in this library of innumerable possibilities, physicists continue to hunt for one true story: a sole account of our reality.
The French writer Stendhal claimed in The Red and the Black (1830) that ‘a novel is a mirror carried along a high road’, which reflects the world as we observe it. He neglected the funhouse inside our heads, the surreal physics of experience and the imaginative limitlessness of the multiverse.
Novelists, too, manipulate imagined worlds and refract our reality through others unlike it. They prolong or compress events, they bridge time and space as wormholes might. They entangle individuals across vast distances, treating them as if they were fundamental particles. They deny the world is merely as we perceive it. The properties of such fictional worlds are not always circumscribed. They can shift, based on the whims and interactions of objects and characters – like the properties of objects in quantum mechanics.
In other words, each novel abides by its own laws, as does each world in the multiverse.
I n physics, we play with what is feasible, regardless of how unimaginable it might be. In fiction, we play with the impossible, however real it seems. I wondered where the two might meet.
While I scuffled to find the physics in texts by Parra and Sábato, I read Ficciones by Borges. This collection of short stories is one of the most bountiful works of the 20th century. In each tale, Borges detailed fantastic impossibilities, stretching characters and environments beyond their temporal, spatial or psychological limits. There were labyrinths of every possible book, a man weighted with perfect memories, a writer who reproduced the novel Don Quixote , word for word, after immersing himself in its world.
I spent my evenings more baffled by these experiments than by the literary works of those former physicists, Parra and Sábato. Borges’s stories were even more confounding than my problem sets in quantum mechanics. In a page or two, frequently less, Borges could elicit paradoxes of time, space and reality that I might spend a lifetime misunderstanding.
I was not the only physicist under his sway. Sábato, too, had read Ficciones , after he quit physics for fiction. He had even borrowed the idea of overlapping timelines from Borges for El Túnel . Although Sábato taught relativity, Borges introduced him to fictional modes of time.
In my undergraduate ignorance, I translated Ficciones into English, not knowing other translations existed, not wanting to know because I wanted Borges to myself. I unravelled the short fictions of Borges, learning how he rarefied his prose – through unnatural infinities and impossibilities passed off as real. I still lost myself in his mirrors, labyrinths and curvatures of space and time.
Through this story, Borges had prefigured the Many-Worlds interpretation of quantum mechanics
Through his work, I entered ‘La biblioteca de Babel’ (1941), ‘The Library of Babel’, in which Borges shelved all the possible texts in every language. The sum was not infinite as described; I calculated the number to be far greater than all the possible atoms in the Universe, yet finite. I later navigated a map as large as the world, a perfect representation of its terrain, in the single-paragraph story ‘Del rigor en la ciencia’ (1946), ‘On Exactitude in Science’. The story begins:
I even walked ‘El jardín de senderos que se bifurcan’ (1941), ‘The Garden of Forking Paths’, a story in which a writer does not have to choose among the countless alternatives for his protagonist. Instead, he writes them all, at every moment, and the character lives them all. At each subsequent moment, still more paths open to the protagonist. She inhabits the everywhere all at once. Through this story, Borges had prefigured the Many-Worlds interpretation of quantum mechanics, which the American physicist Hugh Everett proposed in his doctoral thesis in 1957. When the Argentine physicist Alberto Rojo later asked Borges about this foreshadowing, the author chuckled: ‘How imaginative are physicists!’
I read every Borges story I could, revelling especially in the dreamy fables of El Hacedor (1960), The Maker . At the time, I lived next door to a philosophy student from Mexico who said that, if I liked forking paths, if I liked authors who experimented with reality, I would love Rayuela (1963) by Julio Cortázar, later translated into English as Hopscotch .
Borges and Cortázar were not only compatriots; Borges inspired the younger writer and had published one of his first stories. From the start, they shared the concerns of most experimenters: reality, space and time.
As Cortázar noted in the ‘Table of Instructions’ to his novel, Hopscotch was not one book but many. There were dislocations in space and time, between Paris and Argentina, but the reader was also asked to choose her own adventure, hopping among chapters or following a variation Córtazar had chalked out. Sequence and chronology, causality and locality, need not exist in fiction. Each reader built their own reality, made their own rules. They, too, could explore every possibility for the characters, as Borges described.
In a lecture that Cortázar gave about fictional time while visiting Berkeley in 1980, he would admit his ignorance of relativity, yet he insisted that the elasticity of space and time was the source of the fantastic in all stories. Afterward, a student asked about his references to Heisenberg in Hopscotch. Cortázar explained that, while living in Paris, he had read about the Heisenberg’s uncertainty principle in the science supplement to Le Monde . When he learned that physicists reasoned with the fundamental limits to their knowledge, he was dumbstruck. He told the student: ‘[I]t’s exactly the same process that occurs in certain literature and poetry: just when you reach the limits of expression.’
Uncertainty not only motivated fiction, fiction was the greatest uncertainty. It was the beautiful unknowing that remained whenever we tried to know the physical world. It was what would always remain after our guesses about the ‘wonderful, simple, but very strange patterns’ behind reality. Fiction, in other words, was the sublime complement to science, the other experiments we would need to understand the world.
S ábato once complained: ‘Given my scientific training, no one thought it possible that I might dedicate myself seriously to literature … How was I to defend myself when my best antecedents were in the future?’
His greatest antecedent to come – the true heir to Parra, the beneficiary of Borges and Cortázar – was and would be Agustín Fernández Mallo, a Spanish physicist who inspired a generation of novelists, much as Hopscotch lit one of the matches that ignited what critics later called the ‘boom’ in Latin American literature.
During the mid-1980s, while Mallo was studying physics at the University of Santiago de Compostela, in the north of Spain, a classmate handed him a book with a Möbius strip on its cover: El Hacedor by Borges. Reading this book of poetic impressions, Mallo thought Borges had captured the ineffable logic of physics within fiction; he had written the literary complement to science, the anti-science.
Whenever matter and antimatter meet, they transform into energy and light. Mallo thereafter would write poems alongside the equations in his notebooks, and the two became one. Both compressed elegance into something far-reaching, albeit obscure.
Mallo wanted a new experiment, a scientific poetry equal to humanity
After Mallo graduated, he moved to Palma de Mallorca where, for nearly two decades, he wielded beams of particles against tumours in a hospital basement. During his time underground, he wrote poetic equations and eventually released them to the world in four collections.
Literature had long meddled with time and reality, as relativity and quantum mechanics did. But those sciences prevailed on inhuman scales, the minuscule and the immense. Mallo wanted a new experiment, a scientific poetry equal to humanity. He wanted to mingle the impossible with the real.
In 2003, an editor asked Mallo to explain the relationship between literature and physics. Mallo outlined a theory in his book of essays Postpoesia: Hacia un nuevo paradigma (2009), or Postpoetry: Toward a New Paradigm . His poetry was ‘the next logical step in the line of parallelisms that Nicanor Parra established when he equated poetry up to the 19th century with the Newtonian physics and vanguard poetry with the relativistic and quantum physics of the early 20th century’ (my translation). Mallo was writing a postpoetry built from the patterns and chaos of complex systems rather than the mechanics of inhuman particles.
Literature could be as real and indeterministic as the weather. It could be a semantic web of intimate but random associations, between disparate objects and characters, from real and imagined sources. Order could arise from disorder. A text could be a constellation, a smattering of stars in whose links people located meaning. It could become a superfluid charging from its confines.
D uring the summer of 2004, Mallo undertook his next experiment: a novel. In a newspaper, he had read an article about a cottonwood tree that shaded the loneliest road in Nevada, where people hung their shoes as an impromptu display of humanity in the desert. Around the same time, he found a quote by W B Yeats printed on a sugar packet, which reminded him of a punk song, about a chocolate-hazelnut spread, ‘ Nocilla, ¡Qué merendilla!’ (‘Nocilla, what a snack!’) Convalescing from a broken hip in Thailand, these seemingly unrelated events propelled him into his first novel, Nocilla Dream (2006).
Mallo explained afterward: ‘[S]ome stories and characters have been taken directly from this “collective fiction” we communally refer to as “reality”.’ He linked vignettes about love and physics, about salmon factories, micro-nations, manhole covers, sex workers, and airports, with vague interrelations between characters and scenes. There was also self-reference – a character expounding a theory of literature identical to his own. The character reads ‘On Exactitude in Science’ by Borges every day at noon. Mallo was trying to map the entire world.
After three months, Mallo awoke from his Nocilla Dream . Weeks later, he started a second novel, Nocilla Experience (2008). Months after that, he wrote a third, Nocilla Lab (2010). In the finale to his trilogy, there is a resounding cameo, in graphic form, from Enrique Vila-Matas, the Spanish writer who once asked but could not answer: ‘Does reality really exist?’
In an endnote, about an echo of mathematics from Bolaño’s work in his own, Mallo would describe ‘the hidden threads of a literature that is beyond our control’. In his prose, those threads are thick. Strange parallels and repetitions are so frequent, one is tempted to map them, to represent the world of their making. That would be impossible. Its map would be bigger than the world.
He, too, heard a note from the illusive world inside our heads
Mallo quit physics thereafter to focus on literary experiments. In 2018, he published the novel Trilogía de la Guerra (translated as The Things We’ve Seen ). The former physicist still hunted the interstices between the random and the surreal, a space he believed we all inhabited on the internet. We each read and create more fictions every day, it seemed, in the unreality we curate online than have ever existed.
In the first part of his second trilogy, Mallo follows an author who creates a residency on the abandoned island of San Simón before he begins to hear sounds he cannot account for. The author realises ‘the ringing in my ears had nothing to do with literature, was not a mirror for anything or a representation either, it was just a thing that was happening.’ He, too, heard a note from the illusive world inside our heads.
Other writers have also turned to physics in listening for this sound. Lina Meruane wrote an elegiac novel, Sistema nervioso (2018), or Nervous System , about black holes, family, and the gravity of the past. Jorge Volpi wrote a bestseller about the nature of truth and lies, En busca de Klingsor (1999), or In Search of Klingsor , told through the life of a physicist who resembles Heisenberg. Volpi even has a forthcoming book about the nature of reality in fiction and science. Benjamín Labatut has written several renowned novels about real physicists. These are closer to autobiographies, however, with glosses of well-trodden physics and slight fabrications, rather than heady experiments. None of these authors toys with reality as fluently as Mallo does.
‘The phrase “science fiction” is superfluous,’ Mallo once wrote, ‘because all science is fiction.’ His novels are, indeed, experiments to corroborate his theory of fictional complexity, a map of our world alongside every other, impossible or not. He blends fiction and science, the unreal with reality, the possible with the impossible, to understand all that it means to be human. His prose is a maze of funhouse mirrors, in which we may see ourselves or get lost.
At its worst, Mallo’s fiction resembles a conspiracist’s pegboard, where the associations between pushpins are nothing more than twine. At its best, his fictions cohere to a novel physics. They are a magnetic resonance, what physicists developed to peer inside living bodies, an echo of what lies hidden within.
T he goal of physics is to understand the Universe at every scale, to know the vast but finite potential of all that we and our instruments may observe. The method of physics is to contort what is materially possible until we can shape it no more.
To know the world is to enumerate its possibilities. Physics thus demarcates the impossible, the infinite potential of universes not our own.
Fiction is our laboratory for the impossibilities that exceed our Universe, the infinity that casts limited reality in greater relief. Such impossibilities are also of the world because they are within us, because they move us, because they embolden or cower us. We never experience them directly, not really, not physically, but they enlarge humanity nonetheless. We may be confined by the possible, but we are citizens of the unreal.
Physics asks simple questions: it asks the possible. Fiction asks the hardest from us, the impossible. To know the world, we need both.
Experimentation, in both physics and fiction, is the asking of questions. It is not, however, the answering. No experiment can decide knowledge once and for all. Uncertainty always remains. There can be, in other words, no end to our experiments, no end to our imaginations, in either fiction or physics."
On seeing the laws of nature as a recipe or a news report | Aeon Essays,,https://aeon.co/essays/on-seeing-the-laws-of-nature-as-a-recipe-or-a-news-report,"The Sun rises every day. Water boils at 100°C. Apples fall to the ground. We live in a world in which objects behave the same given the same circumstances. We can imagine living in a different world: a world that constantly changes, a world in which the Sun does not rise every day, a world in which water one day boils at 50°C, and at 120°C another day, a world in which apples sometimes fall from trees and sometimes rise into the sky. Only because we live in a world that displays stable regularities are we able to reliably shape our environment and plan our lives.
We have an intuition that these regularities are due to laws of nature, but we normally do not interrogate what these laws are and how they work in any basic metaphysical sense. Instead, we assume that science not only provides these laws but also elucidates their structure and metaphysical status, even when the answers seem partial at best. In short, we assume that, thanks to science, there is a recipe of sorts for how the laws of nature work. You take the state of the Universe at a given moment – every single fact about every single aspect of it – and combine it with the laws of nature, then assume that these will reveal, or at least determine, the state of the Universe in the moment that comes next.
I refer to this as the layer-cake model of the Universe, which dates back to the 17th-century philosopher René Descartes. Not long after Descartes embraced the idea of a deterministic universe, Isaac Newton presented a mathematical law for gravitation, which gave the concept a powerful quantitative update. The gravitational force on one body at one time is determined by the location of all the bodies in the Universe at that time; the state of the Universe plus the law of gravitation tells you how all bodies will move: a layer-cake model, indeed.
The influence of Descartes and Newton on how we think about laws of nature is immense – and not without justification. It has helped to unify whole fields of physics, including mechanics, gravitation and electromagnetism. It is still so widespread in the scientific community, and it has such a distinguished pedigree, that scientists may not even realise that they subscribe to the layer-cake model at all.
But the uncomfortable truth is that there are many aspects of modern physics that seem to provide counterexamples to the layer-cake model. To date, some of these alternatives have occupied only a rogue niche in physics. But they should be studied more deeply and understood more widely because they pose major challenges to our fundamental understanding of the Universe – how it began, where it is going, and what kind of entity, if any, is driving it.
T he first massive challenge to the layer-cake model, Albert Einstein’s theory of general relativity, appeared in the 20th century . The laws of nature that are core to the theory of general relativity, the Einstein field equations, do not immediately lend themselves to the layer-cake model at all.
The difference can be seen in the structure of the mathematics itself. An equation that adheres to the layer-cake model describes the changes that occur in space in terms of the underlying reasons for these changes. For example, Newton’s equation for his second law of motion describes the acceleration of physical bodies in terms of the underlying forces causing that acceleration. The Einstein field equations, on the other hand, describe the very structure of spacetime as the change agent for moving physical bodies; in fact, most of the solutions to the Einstein field equations yield a spacetime structure that is incompatible with the layer-cake model. When faced with this challenge, physicists do something highly revealing: they specifically search for solutions to the Einstein field equations that comport with the layer-cake model, and they rule out solutions that do not comport with the model as ‘unphysical’ – as artefacts of the mathematics that do not tell us anything about reality, or, at least, not the reality we live in.
Physics has many theories where the future seems to somehow influence the past
In the case of general relativity, there are good reasons for doing this, but in other cases the challenge to the layer-cake model becomes harder to dismiss. In classical mechanics, for example, there is something called the Lagrangian formulation, which holds that, when moving between two separate points, A and B, a physical body will take the most efficient path. This does not look like the layer-cake model because, in order for the physical body to take the path of maximal efficiency, point B, which lies in the future, needs to be determined in advance. It looks, counterintuitively, as if the future is what determines the motion of the body in the past.
As strange as this seems, it turns out that you can derive the familiar Newtonian equations for motion from the Lagrangian formulation. Because of this, scientists often treat the Newtonian version, which comports with the layer-cake theory, as reflecting the true structure of the world. The Lagrangian version is understood to be an interesting and sometimes practical – but never metaphysically accurate – mathematical reformulation.
But the Lagrangian formulation is just the start. Physics has many other theories where the future seems to somehow influence the past. The peculiarities of quantum mechanics have led to the development of so-called retrocausal models . And such midcentury giants of physics as John Archibald Wheeler and Richard Feynman developed a theory of classical electromagnetism that basically says that future charges send light signals into the past.
I do not claim that any of these alternatives to the layer-cake model of the Universe is correct, but they are worthy of deeper study. The door has been opened for an investigation of alternative ways of how laws act in the Universe.
In current philosophy, the layer-cake model has been defended by the philosopher Tim Maudlin , a professor at New York University. In his book The Metaphysics Within Physics (2007), he lists two key metaphysical features: laws are primitive entities, and laws produce the future from the state of the present. In this context, ‘primitive’ means non-reducible to anything else, or standing on its own. Primitive laws thus exist by themselves, and they exist not as concrete objects, like tables or cars, that we can experience and manipulate with our senses, but rather as abstract entities, similar to numbers. An immediate problem arises: how can laws influence any physical object in the world?
In principle, we face a similar issue with legal laws: how can these abstract laws that are passed by Congress influence our behaviour? But the answer is straightforward: once we get notice of a law and understand it, we can choose to abide by it. The fact that we can choose to follow the law means that we have freedom not to follow the law.
Now it is said that the laws of nature do not influence or produce anything in the world
Laws of nature are different. An electron has no freedom not to follow the laws (even if they are indeterministic), and, more importantly, it is utterly mysterious how laws as primitive abstract entities are able to tell the electron what to do.
In order to mitigate this problem of how electrons are able to obey the laws, another conception of laws was proposed by the philosopher David Lewis, which has been dubbed Humeanism about laws, in reminiscence of David Hume.
In An Enquiry Concerning Human Understanding (1748), Hume posed the following problem about the notion of causation. He illustrated the problem with the collision of billiard balls. When billiard ball A hits billiard ball B, which was initially at rest, we observe that billiard ball B moves after the collision; we say that billiard ball A caused billiard ball B to move. This seems to be unproblematic. At least, we know that, due to the causal relation between the two billiard balls, whenever billiard ball A hits billiard B, billiard ball B would move. But how does causation bind the motion of billiard ball A to the change of motion in billiard ball B so that billiard ball B always behaves the same when billiard ball A collides with it? For Hume, this question has no answer. We, as human beings, cannot directly observe this causal binding; all that we can observe is the constant motion of billiard ball A and the successive motion of billiard ball B. And that is all that we can be confident of saying about causation.
Lewis took this epistemic conclusion and turned it into an ontological one. Not only do we not experience how exactly laws influence physical objects, now it is said that the laws of nature do not influence or produce anything in the world. The layer-cake model is utter fiction. Instead, laws of nature effectively describe what is happening in the world. They describe the facts in the world, like a newspaper article reports facts in the world. Therefore, to emphasise the main idea of this proposal, I will call it the newspaper model of laws of nature.
T he newspaper model is probably the most popular theory of laws of nature among professional philosophers, and it attracts a lot of active research right now. It is so attractive because it is metaphysically thin: there are no mysterious, unexplained relations of production as demanded in the layer-cake model. Laws merely summarise the history of physical objects.
The newspaper model, however, faces its own problem. Since there is no causal relation binding objects in the world, there is no reason why billiard ball B ought to move when being hit by billiard ball A. It may just remain at rest or move without being hit or break into parts or just vanish into thin air. Anything goes. If that were the case, the laws of nature would constantly change because they describe changing facts in the world. And still, billiard ball B always behaves the same way, and the laws remain the same too. How does that happen?
The metaphysical thinness has to be bought with Hume’s principle of the uniformity of nature. It is a primitive unexplained fact within the newspaper model that the world always behaves the same way; billiard ball B always moves the same way when being hit by billiard ball A, even if nothing tells billiard ball B to behave so. Lewis reiterated Hume when he wrote that ‘if nature is kind to us, the problem needn’t arise.’ In other words, just as in the layer-cake model, the laws of nature also remain the same over time and keep their structure in the newspaper model.
Common sense would agree. The past determines the present, and the present determines the future
For example, Newton’s laws remain as they were when written down by Newton, whether interpreted as producing the future or as describing the world. You cannot see from the formulation of the law what the metaphysical underpinning is. At least, not without more information.
All scientific laws are compatible with the newspaper model, including Newton’s laws that tell us that the future state of the world can be calculated and deduced from the present state just as the present state was produced from the past. How can the newspaper model support a formulation of a law that looks like the layer-cake model? This is justified by the idea that Newton’s laws are the most efficient description of the world (within the domain of Newtonian physics), balancing simplicity and informativeness. It might be possible to describe the motion of the planets in a different way. For example, you may create a long list with the exact times and the exact spatial coordinates of the planets; such a list would be very informative (more informative than Newton’s laws are), but it would be too complicated. The best balance between simplicity and informativeness to describe the motion of the planets is exactly how Newton formulated his laws.
Not all scientific laws are, in fact, compatible with the layer-cake model, which requires that the past state produced the present state and the present state produces the future. In order for this to make sense, Maudlin adds a third feature: the stipulation of a primitive flow of time independent from the laws. Common sense would agree. The past determines the present, and the present determines the future.
But in physics and philosophy, a primitive flow of time is highly controversial. Some physical laws do not match this structure. The laws of retrocausal models of quantum mechanics (in which the future determines the past), for example, are clearly incompatible with the layer-cake model and with the idea of a primitive flow of time. The laws of special relativity do not fit the layer-cake model either, because they defy an absolute notion of simultaneity, which is part and parcel of Newtonian mechanics.
A s a reaction to this narrow scope of the layer-cake model, the philosopher Eddy Keming Chen and the mathematician Sheldon Goldstein, at the University of California, San Diego and Rutgers University respectively, as well as the philosopher Emily Adlam, at Chapman University, have suggested an alternative. Laws may be primitive, but they nonetheless ‘merely’ constrain the physical possibilities in the world. Call this the straitjacket model of laws of nature. No notion of production and no flow of time is required. All that laws do is to constrain what can happen in the world. In this way, we combine the advantages of the newspaper model with the advantages of the layer-cake model, because we acquire the generality of the newspaper model and a reason for stable regular behaviour from the layer-cake model. Now we have a metaphysical underpinning for retrocausal laws and the laws of special relativity because laws, in the straitjacket model, are primitive and govern the world by constraining what can happen.
Still, the straitjacket model suffers from the same metaphysical issue that plagued the layer-cake model. The layer-cake model was not able to account for how laws produce new states. In a similar vein, the straitjacket model does not specify how laws can constrain what happens in the world. It seems again that abstract laws have to latch on to the real world to tell physical objects how to behave. How laws are able to do so remains unanswered.
We seem to need a metaphysical glue to secure the stable behaviour of our world
The possible implications for any form of law of nature are profound. The layer-cake model seems to be intuitively plausible – the present is determined by the past – but we found out that it requires that laws somehow affect the objects in space and time without being themselves located in space and time.
Since the layer-cake model is too restrictive to capture other formulations of physical laws, like retrocausality and general relativity, the straitjacket model was developed. This model does provide a framework for retrocausality and general relativity, yet it suffers from the same metaphysical problem as the layer-cake model. The newspaper model, on the other hand, tries to introduce laws without any metaphysical baggage, and this seems to be a promising approach. Yet we seem to need a metaphysical glue to secure the stable behaviour of our world.
Given all this, which theory of laws best explains the regularities in our world? If the newspaper model were true, it would be a constant coincidence that the Sun rises every day or that the water in your kettle boils at 100°C, as there is no metaphysical constraint on how objects can behave. In contrast to many of my colleagues, I therefore find the newspaper model pretty unconvincing for explaining stable regularities. The layer-cake model and the straitjacket model fare better in this respect. The advantage of the straitjacket model is that it is general enough to capture unfamiliar laws of nature, like those describing retrocausality. But this virtue comes with a vice: the straitjacket model is so general that any law of nature would fit in.
The metaphysically interesting aspect of nature’s laws is not that they constrain physical possibilities, but how they do that. Even if it is up for debate, the layer-cake model broadly addresses that question best. This works wonderfully with billiard balls. There are conditions where the model just can’t explain how laws of nature produce the future, like retrocausality; but instead of seeking a single new overarching model, perhaps we’d be better off sticking with the layer-cake, after all, and developing a separate tailored account for each type of situation where that model does not fit.
This essay was made possible through the support of a grant to Aeon Media from the John Templeton Foundation. The opinions expressed in this publication are those of the author and do not necessarily reflect the views of the Foundation. Funders to Aeon Media are not involved in editorial decision-making."
A new ‘field theory’ reveals the hidden forces that guide us | Aeon Essays,,https://aeon.co/essays/a-new-field-theory-reveals-the-hidden-forces-that-guide-us,"Why do rocks fall? Before Isaac Newton introduced his revolutionary law of gravity in 1687, many natural scientists and philosophers thought that rocks fell because falling was an essential part of their nature. For Aristotle, seeking the ground was an intrinsic property of rocks. The same principle, he argued, also explained why things like acorns grew into oak trees. According to this explanation, every physical object in the Universe, from rocks to people, moved and changed because it had an internal purpose or goal.
Modern science has rejected this ‘teleological’ way of thinking. In the 17th and 18th centuries , scientists and philosophers began to chip away at Aristotle’s seemingly ‘spooky’ notion of intrinsic causes – spooky because they suggested that rocks and creatures were guided by something not entirely material. For those who rejected these Aristotelean explanations, such as Thomas Hobbes and René Descartes, organisms were simply complex machines animated by mechanisms . ‘Life is but a motion of limbs,’ wrote Hobbes in his Leviathan (1651). ‘For what is the heart, but a spring; and the nerves, but so many strings; and the joints, but so many wheels, giving motion to the whole body.’ The heart does not have the goal of circulating blood. It’s just a spring like any other. For many thinkers at the time, this view had real explanatory benefits because they knew something about how machines worked, including how to fix them. It was in this intellectual environment that Newton developed a powerful mechanical worldview, based on his discovery of gravitational fields. In a Newtonian universe, internal purpose doesn’t cause rocks to fall. They just fall, following a law of nature.
Mechanistic explanations, however, struggled to explain how life develops. How does a grass seed become a blade of grass, in the face of endless disturbances from its environment? Long after the mechanistic revolution, the philosopher Immanuel Kant confronted the stubborn problem of teleology and despaired. In 1790, he wrote in the Critique of Judgment that – as commonly paraphrased – ‘there will never be a Newton for a blade of grass.’ Less than a century later, with the publication of On the Origin of Species (1859), Charles Darwin seemed to crack the problem of biological teleology. Darwin’s ideas about natural selection appeared to explain how organisms, from grass seeds to bats, were able to pursue goals. The directing process was blind variation and the selective retention of favourable variants. Bats who sought moths and had an ever-improved capacity to track and catch them were favoured over those who were less goal directed and therefore had lesser capabilities. Though natural selection seemed to illuminate what Descartes, Hobbes and Kant could not, Darwin’s theory answered only half the problem of teleology. Selection explained where teleological systems like moth-seeking bats come from but didn’t answer how they find their goals.
S o, how do goal-directed entities do it, moment by moment? How does an acorn seek its adult form? How does a homing torpedo find its target? Mechanistic thinking struggles to answer these questions. From a mechanical perspective, these systems look strangely future oriented. A sea turtle, hundreds of miles out to sea, can find the beach where it was born, a location that lies in its future. A developing embryo, without any thought of the future, constructs tissues and organs that it will not need until much later in life. And both do these things persistently: carried off course by a strong current, the sea turtle persistently finds a trajectory back toward its natal beach; despite errors in cell division and gene expression, an embryo is able to make corrections as it grows into its adult form. How is this possible?
Even though mechanistic thinking has failed to solve this teleological problem, it still dominates scientific thought. Today, we invoke mechanism to explain almost everything – including human goal-directed behaviour. To explain the growth of an acorn, we look to mechanisms in its genes. To explain the ocean voyages of a sea turtle, we look to mechanisms in its brain. And to explain our own thoughts and decisions, we focus on neural pathways and brain chemistry to explain decision-making. We explain behaviour in terms of evolutionary needs, such as survival or reproductive success. We may even think of our genes as ‘blueprints’. For some 20th-century thinkers, such as the US psychologist Burrhus Frederic Skinner, human brains are purely mechanistic. Skinner denied that people have goals at all. More recently, the primatologist Robert Sapolsky, based at Stanford University, and others have painted a mechanistic picture of us that denies we have free will.
We seem to have only two ways of explaining it: teleology or mechanism. Both are troublesome. Both are inadequate
And yet, despite centuries of rejection, teleology has not been banished. Most of us still have a deep intuition that there is more to our thinking and action than mere mechanisms. The feeling of being in love isn’t just the mechanical outcome of neurochemistry. We want to believe it is driven by our wants and intentions. Some of us, especially if moved by religious or spiritual impulses, might even see goals in the larger universe: ‘I am here for a purpose,’ you might think to yourself. For many, a world of pure mechanism seems insufficient. And beyond our intuitions about teleology, there are countless areas of science where teleological explanations are commonly deployed, even without any explicit recognition of them. Consider the debate over which parts of a genome are ‘functional’ (ie, they perform roles that are beneficial to an organism) and which are ‘non-functional’ (ie, useless remnants of evolution). The very idea that a gene can either be functional or non-functional implies that certain genes aim towards certain results, or have certain purposes for the organism, while others have no ends and are merely purposeless junk. So, even beyond our intuitions, teleology is so deeply entwined with science that there will be no getting rid of it anytime soon.
So, caught between modern science and our intuitions about teleology, we seem to have only two ways of explaining the apparent goal directedness in some systems: teleology or mechanism. Both are troublesome. Both are inadequate. In recognition of this problem, philosophers of biology and others have, in recent decades, been struggling to find an alternative. We believe we have found it: a third way that reconciles Aristotelian thinking about goal directedness with the mechanistic view of a Newtonian universe. This alternative explains the apparent seeking of all goal-directed entities, from developing acorns and migrating sea turtles to self-driving cars and human intentions. It proposes that a hidden architecture connects these entities. It even explains falling rocks.
We call it ‘field theory’.
T he notion of ‘fields’ was originally developed by physicists such as Newton, Michael Faraday, Richard Feynman and others. In physics, the concept has been used to explain gravity, electromagnetism, and particle interactions in quantum theory. But ﬁelds have also been used in biology to explain the development of living things. In the mid- 20th century , the Austrian biologist Paul Weiss proposed that, within an embryo, large ‘morphogenetic fields’ directed the behaviour of cells inside them. Together, these pioneers in physics and biology showed how objects in the Universe can be directed by often-invisible and large-scale external structures. Our version of ﬁeld theory takes this as its starting point.
So what do fields do? How do they give us goal directedness? To answer this, we need to know something about what it means to seek a goal. Two mid- 20th-century thinkers, the biologist Gerd Sommerhoﬀ and the philosopher of science Ernest Nagel, made a simple observation about goal-directed objects: they all exhibit the same pattern of deviation and correction. When they inevitably deviate from the right trajectory – the right path toward a goal – these objects correct themselves, and direct themselves back toward their goal. A mouse embryo can be split in half at an early stage, and each half will regrow into a fully formed mouse. A person headed out to buy something can be diverted by another errand, but afterward redirect themselves toward the store. Sommerhoff and Nagel called this ability to recover from perturbations ‘persistence’.
The second signature behaviour of goal-directed entities is plasticity, the ability to ﬁnd a trajectory toward a goal from a wide range of starting points. A sea turtle seeking the Florida beach where it was born can begin its journey from anywhere within a wide area, stretching hundreds of miles. A self-driving car can find its destination from almost anywhere. Persistence and plasticity are the common features that all goal-directed entities seem to share. And they point to the central problem of teleology: how do goal-directed entities persistently and plastically find their way toward a target that lies only in their future? How do they know which way to go? After all, the future cannot direct the past. What sort of strange causal chain is at work here?
These fields are not metaphorical. They are real and physical
The answer involves looking away from goal-directed entities, and instead considering what surrounds them. In our view, persistence and plasticity are possible because goal-directed entities – from turtles to self-driving cars – move and change within a larger ﬁeld that envelops and directs them. Sea turtles, for example, are enveloped by Earth’s magnetic field and can use this field to find the beach where they were born. To make this journey, they rely on complex mechanisms in their brains, but also on a larger field that, from a turtle’s perspective, appears everywhere. If a current carries a turtle off course, the field is there to direct it back toward the right beach. Likewise for self-driving cars. Each car is immersed in a microwave field emanating from nearby cell-tower arrays and can use that field to locate its destination from anywhere within range of those towers. If forced to make a detour, the microwave field directs the car back toward its destination.
Our proposal is that fields direct the action of all goal-directed entities. In other words, goal directedness is the result of a particular architecture, a particular arrangement of large fields that contain and guide smaller entities. From this perspective, persistence and plasticity are possible only because a field is present wherever an entity wanders.
In ﬁeld theory, ﬁelds are deﬁned in terms of what the biologist Michael Levin calls ‘nonlocality’. They are structures whose influence extends over a broad area, not localised to any one point. Earth’s magnetic field is present not just locally, where the sea turtle happens to be at one moment, but wherever the turtle could accidentally wander. Our understanding of fields is even broader. It includes atmospheric fields that direct the formation of hurricanes, ecological fields that direct the migration of animal herds, and social fields that, to some extent, guide our wants and intentions. These fields are not metaphorical. They are real and physical. They can be detected, measured, and even manipulated.
T oday, the standard scientific answer for how goal-directed entities work still involves pointing to internal mechanisms, following the tradition that can be traced back to Descartes, Hobbes and Newton. For example, how can we explain the way a homing torpedo, a classic mechanical goal-directed device, seeks its target? Most explanations would turn to internal feedback mechanisms inside the device. This is exactly what cyberneticists did in the mid- 20th century , like the Mexican physician Arturo Rosenblueth. They argued that a homing torpedo uses feedback mechanisms to direct itself, detecting the sound of the target ship and responding when the sound fades by turning in the direction where it is louder. In a similar way, internal mechanisms are also used by contemporary biologists to explain the goal-directed behaviour of organisms.
Consider a dung beetle. When it enters a dung pile, a beetle will sculpt some of the dung into a ball. To escape rivals who might steal the ball, the beetle stands on top of it and rolls it away from the pile in a straight line. If it strays from a straight path, the beetle risks accidentally circling back to the pile, where it will encounter competition again. Anatomical studies have revealed that a complex mechanism in the beetle’s brain is involved in guiding its movements. From this perspective, the dung beetle’s goal-directed behaviour – moving in a straight line away from the pile – can be fully explained by some mechanism buried inside its brain. Such mechanistic approaches have dominated contemporary thinking on goal directedness.
The image of the Milky Way is a ‘light field’ that the beetle can use to orient its movements
The explanatory power of the mechanistic tradition is undeniable. But notice that these explanations of teleological phenomena are incomplete. The feedback mechanisms inside a homing torpedo have no information about the location of the target ship. That information is present only externally, in the ‘sound field’ generated by the target ship. And the mechanisms inside a beetle’s brain have by themselves no information about whether the beetle is moving in a straight line. Instead, beetles rolling their balls away from dung piles are guided by something that is not only external but light years away: the Milky Way galaxy. The image of the Milky Way is a ‘light field’, one that the beetle can use to orient its movements. The beetle’s brain mechanisms are critical parts of the causal chain, but they alone can’t tell a straight line from a very slightly curved one. When it comes to explaining ‘goals’, the mechanistic approach has a serious limitation.
Mechanisms are still important. They explain how goal-directed entities move and change, and how they execute decisions. But internal mechanisms viewed in isolation have no information about external goals. They can’t fully explain how an entity can persistently move toward its goal, even after it deviates. Mechanisms respond and execute; fields guide and direct.
S o far, we have considered relatively simple examples. A more challenging case for field theory involves the development of embryos. To all appearances, embryos seek their adult form guided by internal genes, not an external field. Think of the fruit fly, Drosophila melanogaster , one of the most well-studied animals in scientific research. The mother fruit ﬂy guides the earliest development of her growing embryos, but soon the process seems to proceed almost autonomously, as the embryo partitions itself into segments and then into body regions, with limbs, mouth, and other parts forming later. How does it do it? No information about the overall architecture of these body parts is present in the cells and tissues of the parts themselves, or in each organism’s genes. Once again, the answer requires looking outside .
Guidance is external, but not in the way you might think . It is not external to the entire embryo, but external to each body part. Guidance comes from ‘morphogenetic fields’ that are set up by the embryo itself. It is these fields that supply the cells contained within them with guidance about what to do: where to move, what to secrete, when to divide. These ﬁelds are composed of molecules, produced by genes deep inside an embryo’s cells, but the genes are not the source of guidance. They are just factories. And the molecules they manufacture combine to produce a chemical ﬁeld around the growing body parts, directing their behaviour. This is where the notion of ‘internal’ and ‘external’ becomes trickier. This field is inside the embryo, of course, but is present over a broad area, outside the target cells and tissues, omnipresent and ready to correct them when they inevitably deviate.
Consider a question that still perplexes biologists. Why are your arms pretty much the same length? Genes inside the cells of a developing left arm have, by themselves, no information about the length of the developing right arm. This means that, unless tightly controlled, the cells in one arm might divide a bit faster than those in the other. This kind of variation occurs all the time in the development of organisms. If such variation is possible, then how do our arms grow to the same length? The answer is not yet known. One strong possibility is that some field exists – biochemical or even electrical – which is in touch with both arms, encompassing the cells in each. Such a ﬁeld could persistently guide the growth process toward arms of the same length.
At each level, large fields direct the smaller entities contained within them
The simplified explanation above barely begins to account for the full complexity of fields in goal-directed systems. In embryos, there are multiple fields at the scale of the entire developing organism directing various tissue-level mechanisms inside. In turn, those tissues can also act as fields, directing the cells within them. And these cells in turn can also act as fields, directing various molecule-level mechanisms inside them, and so on. In the most complex systems, multiple levels of entities are nested within multiple fields. This telescoping of levels extends upward as well. Whole organisms are nested within local ecological fields, which in turn are nested within larger ecologies, and so on. What matters in these relationships – what makes goal directedness possible – are the spatial relationships among the nested entities. At each level, large fields direct the smaller entities contained within them.
By now, some might have noticed the teleological elephant in the room: our theory seems to suggest that Aristotle was right to think that falling rocks intend to fall – that they have a downward-seeking goal or purpose. After all, according to field theory, a falling rock is an entity persistently guided downward by an external (gravitational) field. And if teleology requires nothing more than a field directing a contained entity, then field theory would suggest that a falling rock really is teleological. To virtually all contemporary thinking on teleology, this is an outrageous conclusion.
We have two responses. First, not all instances of goal directedness are equal. A falling rock is among the simplest kind of teleological entity imaginable. It is minimally, negligibly, goal directed. Human intentions and purposes are among the most complex. According to field theory, historical and modern thinking on teleology has made an error. Much of this thinking assumes that teleology must be binary, that things are either goal directed or they’re not. We see teleology as something that comes in degrees. Second, allowing a falling rock to be somewhat teleological has the effect of drawing the life sciences and the physical sciences closer together, and we think that is a good thing. The very notion of a partition between them – with the life sciences allowing teleology while the physical sciences do not – would seem to imply there’s something about the life sciences that fails to be purely physical. We think that’s a mistake.
What makes field theory unique is that it is the only modern explanation of goal directedness that locates the source of goal directedness outside of the goal-directed entity. Most other modern theories are mechanistic, and even those that aren’t still point to internal processes or internal organisation, which we argue cannot have the information necessary to direct.
Though we know of no similar approaches to teleology, field theory did not arise in a vacuum. It has deep roots in the work of those studying the properties of nested, hierarchical systems. These studies stretch back almost a century and include research by social scientists (such as Herbert Simon), psychologists (Donald Campbell), biologists (Stanley Salthe) and philosophers (James Feibleman, Ernest Nagel and William Wimsatt). Although not all these thinkers are directly concerned with goal directedness, they explore the ways in which big things can aﬀect little things nested within them, from societies aﬀecting individuals to ecologies aﬀecting species. This research has helped to explain the nature of hierarchical causation, how wholes aﬀect their parts.
F inally, we turn to the most speculative application of field theory: human wants and intentions. If we’re right, then things like human culture and psychology – alongside all goal-directed phenomena – also involve direction by fields. That would mean there needs to be a hierarchical structure to human wanting. And this structure does seem to exist. Looking down, our cells and tissues have the same nested structure as other multicellular organisms. Looking up, we are individuals nested within and directed by small social ecologies (marriages, families, friend groups, etc), which in turn are nested within and directed by larger ones (economic, political and cultural entities), and so on. This is a greatly simplified explanation; even within organisms, the complex nesting of fields is never tidy.
Now consider the laws and legal systems that guide citizens into rough compliance. In this case, the ‘fields’ are the norms, expectations, forms of deterrence, and adjudication and enforcement systems that direct our wants and intentions, and therefore indirectly affect our thinking and actions. Just as Earth’s magnetic field acts on the turtle’s brain, telling it where to turn, the legal system acts on our wants and intentions from above. The same goes for the many economic fields in which we are immersed, guiding our preferences as workers and consumers. And the same also goes for the many social fields that contain us and guide us. These fields arise from partners, friends, families and the countless workplaces, neighbourhood groups, clubs and other social institutions in which we are immersed. The largest fields, like social and economic fields, are extraordinarily nonlocal, directing the wants and intentions of huge numbers of individuals over a large area. From this vantage point, human society emerges as a web of fields. People in a society, like cells in a developing organism, participate in multiple overlapping fields at the same time, which deliver different degrees of (sometimes conflicting) guidance.
But that is not the end of the story. It seems that some purposes or goals originate mostly in our heads. Here we speculate that wants and intentions direct purposeful thinking, speech and action, and must therefore be fields, too, providing both the motivational oomph that gets these processes moving and the directional focus that guides them. My desire for a cup of coffee moves me to think, say and do the things necessary to get one. By ‘wanting’ or ‘intending’ here we are referring to a large category of what might be called affective states, all closely related to emotion, including preferences, cares, feelings, motivations, and so on. This view of the mind, which dates back to the 18th-century philosopher David Hume , posits that our wants direct everything we deliberately think, say and do. Hume called our wants the ‘calm passions’ because, for him, thinking, speaking and acting are purely passive processes, having no goals of their own. We take a similar view: when we deliberately think about, say or do something, it is because some field, some want or intention, has motivated or directed us. Fields, and fields alone, motivate and direct.
Are we merely pawns pushed around by external fields, or are we free to make our own decisions?
To explain how this works, let’s consider a simpler animal. A brief downpour might nudge a squirrel toward wanting to seek shelter. But the desire to seek shelter is the direct cause of the animal’s thinking and motor movements as it heads toward shelter, not the rain – the rain just triggers the desire. Field theory predicts that when squirrel brains are better understood, we will discover that the thought and motor mechanisms involved in seeking shelter lie nested within some larger wanting field that directs them.
The squirrel case is interesting because, like us, squirrels initiate some actions almost entirely on their own, arising less from external triggers and more from internally generated wants and intentions. At any moment, a squirrel might choose to leap from branch to branch just for the exhilaration of near-flight. Field theory speculates that some large-scale pattern of neural activation, the desire for near-flight exhilaration, is a field that acts downwardly on the cognitive and motor centres enveloped by the field, causing the animal to plan, position itself, and leap.
We propose that this same architecture underlies human purposeful behaviour as well. Our deliberate decisions are driven by wants and intentions, which take the form of large fields in our brains that direct our cognitive, speech and motor centres. These fields might consist of large neural circuits. And the goal-directed mechanisms they guide – thinking, speaking and acting – might consist of smaller-scale circuits embedded within them. Like the eddies in a rushing river, the smaller circuits are embedded within the larger flow. Each eddy has its own dynamic, but an eddy’s overall movement is directed by the larger river that envelops it. Likewise, thinking, speaking and acting have their own dynamic, like the capacity of conscious thought to construct narratives, or the capacity of speech mechanisms to retrieve words and formulate sentences. However, their focus, their purposefulness, arises from the wanting or intending fields in which they are embedded. These fields are our motivations.
Our repertoire of wanting fields is enormous, far more diverse than the simple survival and reproductive drives envisioned in some simplistic biological models of intentionality. And they act across a wide range of time scales. An intention to throw a picnic directs a person to make a plan, invite others, collect supplies, travel to a park, and find a suitable spot. A desire for knowledge might direct a person to investigate, sign up for, and ultimately take an online course. A preference for a quieter life might direct someone to prepare for retirement over many years, so they can retire early. The picture is complicated further by the fact that wants are diverse, and sometimes conflicting, even on a single timescale. Owing to the complexity of human existence, we want many things at once. I both want that extra piece of cheesecake (because it’s tasty) and don’t want it (because I’ve eaten too much already) at the same time. I want to stay in school and see the world at the same time. The fields that direct us are interrelated, highly differentiated, and often in conflict.
This is where the question of free will begins to emerge: are we merely pawns pushed around by these external fields, or are we free to make our own decisions?
There is a very old line of thought, which has recently been reintroduced to the popular imagination by Sapolsky, that says we are not free. It says that all thought and action, indeed everything in the Universe, is fully determined by physical laws, and that this determinism is incompatible with free will. But this view, which sees determinism and free will as being at odds, is mistaken. According to a philosophical school called compatibilism , even if the world is perfectly deterministic, freedom is perfectly possible. Field theory is a kind of ‘compatibilist’ explanation of goal directedness.
According to our theory, freedom is direction by the fields within us. There is a temptation to regard direction imposed on us from anywhere as the opposite of freedom, but field theory reminds us that many imposed fields are our own wants and are, therefore, quite literally, parts of us. And when wants originate inside us, they are our wants, and the decisions they motivate are our decisions, regardless of whether they are determined by the external world and the fields that make it up. In this view, freedom is not the total absence of deterministic causation – it would make no sense to be free of your own wants and intentions. In a very real way, your wants and intentions are you, and no one wants to be free of themselves. The freedom we all seek is the freedom to think, say and do what we ourselves deeply want. It is not to be undetermined or free of causes.
W hat evidence exists for our theory of ‘wanting’ fields? The truth is that we are out on a limb, one that is both weak and strong. It is weak because there seems to be little positive evidentiary support, neurologically at least, for the notion that our wants manifest as large fields containing thought and action mechanisms. But the theory is also strong because it is not contradicted by existing research. Much is known in neuroanatomy and neuropsychology about the neural correlates of emotion. Less is known about calmer affective states such as wanting and intending. When it comes to motivations at a molecular level, large-scale neurotransmitter fields involving serotonin or dopamine could be the mediators of our wants and intentions. However, there are other candidates as well, from electromagnetic fields acting over large areas within the brain to neural circuits involving clusters of neurons that are not specific to any one neurotransmitter. Enough mystery remains to support a range of possibilities about how these fields might work.
One of the most valuable aspects of our theory is that it offers empirical guidance. It suggests that researchers hoping to understand human wanting should look for large-scale structures – larger than the thought and action systems they guide. Experiments should seek structures with these systems embedded within them. Of course, field theory, like any theory about the physical world, could turn out to be wrong about how human wanting works. And that, too, is a virtue of the theory. In good scientific fashion, it sets itself up for possible falsification.
It’s possible that our purposes have something deep in common with acorns and dung beetles
Fields are an old idea but, to a world steeped in mechanistic thinking, they offer something new. They expand our explanatory arsenal, supplementing pure mechanism in a way that explains the otherwise unexplained. They help to answer one of the oldest problems in philosophy and science: why do things in the Universe appear to have goals or purposes?
Field theory carries with it a message of unity, bringing together all teleological systems under a shared architecture, revealing a continuity in nature that has long been suspected, at least since Aristotle. Disparate phenomena, from physics to psychology, are unified under a single explanatory framework. The theory raises the possibility that our purposes have something deep in common with other goal-directed systems like acorns and dung beetles, as well as with even simpler ones, like self-driving cars and, yes, even falling rocks.
We acknowledge there are problems to resolve. Fields are often elusive, invisible and intangible. In particular, the fields that guide us as people, the wants our consciousness is bathed in, are poorly understood. We see them only vaguely, from inside. Like gravitational fields, they seem to be everywhere and nowhere in particular. And like gravitational fields, they wield a mysterious power we have yet to fully understand."
Could we learn to love the wind turbine as we did the windmill? | Aeon Essays,,https://aeon.co/essays/could-we-learn-to-love-the-wind-turbine-as-we-did-the-windmill,"Today’s modern wind turbines seem to repel poetic or artistic engagement. It is difficult to imagine a landscape painter portraying their spare lines and uniform rows as icons of a pastoral idyll, as the windmills of the past often were.
Perceptions of modern wind turbines seem worlds away, for example, from how Robert Louis Stevenson described the windmills of England in 1882:
The aspects that struck Stevenson – the motion of windmills, symbols of prosperity, on the horizon – were highlighted a decade before by the French novelist Alphonse Daudet in his depiction of Provençal life:
Yet perceptions of windmills have not been uniformly idyllic. Since they first appeared on the landscape of medieval Europe, windmills represented an imposition of the technological on the pastoral. They were, in the phrase of the wind energy author Paul Gipe, ‘machines in the garden’, straddling the boundary of the agrarian and mechanical. Unlike the static technologies shaping landscapes – from cathedral towers or canals in the past, to power lines, solar panels or rows of genetically modified crops today – windmills are constantly in motion. They refuse to passively disappear into the landscape.
With the spread of modern windfarms, the cultural positioning of wind power remains a contentious issue. But the debate is not new: for centuries, the symbolic nature of windmills – as technological monsters or icons of the idyllic – has been open to question. Understanding this debate can help open new avenues for engagement with today’s wind technology.
T hough European windmills first appeared as early as the 11th or 12th centuries, there are still clues to how this new technology was initially perceived. In Dante’s Inferno , for instance, when the poet reaches the deepest circle of hell, Satan becomes visible through the gloom. As the ominous figure appears, he is described with a metaphor that would have been growing familiar to many of Dante’s original 14th-century readers:
Through the darkness, the devil’s huge limbs are visible like the sails of a windmill pinwheeling on the horizon. To readers whose largest built realities were stable, unmoving church towers or city walls, the ceaseless, arching sweep of a windmill’s arms was no doubt disconcerting to say the least. Later, when Cervantes pictured them as giants in Don Quixote’s imagination, he could gesture to this lingering unease over this most prominent structure on the medieval horizon.
Windmills became integral to communities: structures with names, histories and inhabitants
Besides their visual impression, the first windmills also challenged existing energy infrastructure. Since Roman times, power to mill grain had been confined to water mills and remained the property and protected prerogative of feudal landowners. As the first windmills spread throughout England, they were greeted with resistance by landowners attempting to preserve their ancient rights. Windmills ‘threatened both lucrative old water mill franchises and traditional upper-class privileges,’ recounts the historian Edward Kealey in Harvesting the Air (1987), and ‘offered quick-witted peasants an opportunity to evade manorial regulations, act independently, and become quite prosperous.’ Angry landowners ordered illegal windmills be torn down.
A Windmill near Brighton (1824) by John Constable. Courtesy the V&A Museum, London
Yet eventually this disrupting, disquieting technology became an aspect of the pastoral ideal. Windmills became integral to communities: structures with names, histories and inhabitants. Operating a windmill took skill, care and attention. The miller, who lived in the windmill, was fully occupied with watching the weather, trimming the sails, and keeping the windmill functional from generation to generation – besides providing a vital agricultural role in the community by grinding grain. Ultimately, as the art historian Alison McNeil Kettering has argued , windmills took on the role of ‘cultural signifier’, representing provision and guardianship over a well-run community, becoming familiar icons of ‘pastoral tranquility’ and ‘agrarian idyll’.
As the last generation of millers cared for the last windmills turning in England, their aesthetic value lingered even as their commercial value disappeared. As Stanley Freese wrote in Windmills and Millwrighting (1957):
A s steam replaced wind in Europe, a new type of windmill emerged across the Atlantic. Whereas European windmills were inhabited structures positioned within the community, the unsettled expanse of the western United States saw the windmill altered to run for decades in isolation, pumping water for farmsteads or for cattle stations scattered across hundreds of miles of arid ranchland. Dozens of models represented a Darwinian response to this environmental challenge: self-lubricating mechanisms, designed to track and spill the wind with counterweights and springs, their complex down-gearing transforming rotary motion into a steady back-and-forth pumping action in even the lightest breeze. By 1889, there were more than 70 windmill factories in cities across the American Midwest. At the European windmill’s peak, there were perhaps 100,000 of the structures across Europe; by contrast, there would eventually be more than 6 million windmills in the US.
Lame Deer, Montana, September 1941. Photo by Marion Post Wolcott, Library of Congress
Unlike the graceful European windmill, the new US variety was considered ugly and ungainly. An American architect insisted that American windmills ‘should be condemned’ as offensive to sight: ‘To see these awkward, spider-like structures dancing fandangos before our eyes disturbs the repose and mars the landscape of our otherwise beautiful homes.’ Worse, another author said the countryside was littered with wrecked windmills, casualties of the failure to maintain or lubricate – this last chore a near-weekly requirement for some of the earliest models. Yet despite these initial impressions, the new American windmill (technically, a wind pump) soon became an icon of the Western settlers themselves: independent, self-reliant, steadily facing whatever storms arose.
Manufacturers helped cultivate this view of windmills as totems of the American west
A Kansas City Star reporter writing in 1964 captured the feeling of growing up in the shadow of a farmhouse windmill, an experience common to generations of settlers and their descendants. There was :
Like the European windmill before it, the American windmill was becoming a cultural signifier of a new pastoral ideal.
Courtesy the Library of Congress
In the new commercial context, windmill manufacturers helped cultivate this view of windmills as totems of the American west, presenting them in advertisements and catalogues as part of an idyllic farm landscape. Competition among manufacturers also meant windmill designs became as simple and reliant as possible. Windmills needed to be shipped across the country, assembled hundreds of miles away on the open prairie, and to operate in isolation for years on end. Their buyers needed to own, understand, and service the windmills themselves. The success of this approach is evidenced by the windmills still spinning across the US and the world, with a handful of companies today producing designs that remain unchanged from the 1910s. Like the European windmill, American windmills became pastoral icons, their ceaseless labour – working in the slightest breeze and weathering the harshest storm – a visual metaphor for diligence, independence and patient endeavour.
T oday’s massive wind turbines are larger than past windmills by an order of magnitude. The question facing this latest generation of wind technology is more than whether they will be seen as icons of energy independence and sustainability, or simply another extractive industry ‘replicating the exploitative practices of the infrastructures they would replace,’ as the historian of science Nathan Kapoor put it . Rather, the question is whether the social imaginaries available to previous generations of windmills – the chance to become symbols of provision, community or self-reliance – are available to modern windfarms. It is a question playing out, for instance, in debates between farmers who welcome wind turbines and the income they represent and those – often within the turbines’ shadow – who see them as monstrous technological impositions on the landscape.
What prevents modern wind turbines from the sort of cultural integration that European and American windmills obtained? Part of the answer comes by considering wind technologies in light of work by the philosopher of technology Albert Borgmann. In his classic Technology and the Character of Contemporary Life (1984), Borgmann offers his analysis of ‘device’ as both critique and exemplar of modern technology. According to Jesse S Tatum, a device is a technological artefact designed to make ‘a single commodity highly available while making the mechanism of its procurement recede from view’. Our current technological paradigm is the creation of as many devices as possible, from cars to electronics to infrastructure, to make commodities hyper-convenient and abundant.
The problem with devices is that, by design, they are black boxes. Devices, in Borgmann’s treatment, are inaccessible to understanding or engagement. They demand no skill, disburdening their users while, in Borgmann’s words, resisting ‘appropriation through care, repair, the exercise of skill, and bodily engagement’. Devices, whether kitchen appliances or the electrical systems that supply their energy, neither express their creator nor ‘reveal a region and its particular orientation within nature and culture.’ Writing in 1984, long before the advent of smartphones, Borgmann’s analysis is prescient in highlighting how technological devices provide essential commodities such as information, entertainment, energy and food, while simultaneously keeping the means of their production inaccessible and largely invisible.
Impossible to consume the commodity of ground grain from a windmill without ‘invoking or enacting a context’
Despite the abundance of commodities, our interaction with devices leaves us distracted and dissatisfied as our engagement with the world is reduced to ‘narrow points of contact in labour and consumption’. For Borgmann, the solution is not a return to a pretechnological setting but rather to recentre human practices and flourishing around what he refers to as ‘focal things’. In contrast to a device, a focal thing is ‘inseparable from its context, namely, its world, and from our commerce with the thing and its world, namely, engagement’. Focal things represent locality and craft; they engage body and mind, and that engagement requires skill: ‘The experience of a thing is always and also a bodily and social engagement with the thing’s world.’
Focal things invite users to interact with them, giving rise to what Borgmann calls ‘focal practices’ that make the thing part of the broader culture and social structure of the community. The European windmill, dependent in its operation on the skill and care of the miller, in its construction and maintenance on the knowledge and expertise of carpenters and millwrights, and in its purpose on local agricultural practices, was a quintessential Borgmannian focal thing – a nexus of material culture, social heritage and artistic expression. It was impossible to simply consume the commodity of ground grain from a windmill without ‘invoking or enacting a context’.
Likewise, American windmills, though factory manufactured on a large scale, immediately entered the context of homestead or ranch. They were designed to be open and accessible to users for care and maintenance, and the farmer or rancher took ownership and exercised skill in that maintenance and care – learning the idiosyncrasies of each individual windmill. While providing the essential commodity of water, windmills took on additional symbolic and cultural roles, offering a sense of solace, wellbeing and aesthetic pleasure (as evidenced by their reappearance on smaller scales as lawn ornaments across the Midwest and beyond). Both European and American windmills, according to Borgmann’s paradigm, functioned as focal things – technological artefacts that connected their users and communities to both landscape and wind.
M odern wind turbines are designed to fit the device paradigm, providing the commodity of energy or (to the landowner who rents space for their footprint) money, but they fail in a vital respect. No matter how they are isolated from nearby communities or coastlines, their motion keeps them visible on the horizon, even as the other large-scale energy infrastructure devices (electric lines, telescope poles, cellphone towers) fade from view. Their primary mechanism of transforming wind into energy remains impossible to hide. As the growth of sustainable energy continues, more and larger windfarms will be required, and their visible impact will only increase. The tension between wind turbines as disengaging devices and their obvious presence on the landscape will continue. Previous iterations of windmills, however, were not ultimately accepted by making them invisible but rather by changing how they were perceived. Can something similar happen for modern wind turbines, transforming them from devices to something that’s closer to Borgmann’s focal things, and opening a path to richer cultural and aesthetic engagement?
Wind turbines are currently designed and implemented as devices. At least in part for safety and liability concerns, they are isolated even as they remain in view. Wind turbines are, like Borgmann described high-rise buildings, ‘though imposing … not accessible either to one’s understanding or to one’s engagement.’ But this disengagement is one of the main reasons wind turbines are often viewed with such negativity. As the philosopher Gordon G Brittan Jr expresses it, wind turbines ‘are ubiquitously and anonymously the same, alien objects impressed on a region but in no deep way connected to it. They have nothing to say to us, nothing to express; they conceal rather than reveal.’
On the other hand, these modern windmills have many characteristics of Borgmann’s focal things. Focal things, according to Borgmann, ‘are concrete, tangible, and deep … They engage us in the fullness of our capacities. And they thrive in a technological setting.’ By depth, Borgmann means that all of an object’s physical features are significant, something acutely true of precision-designed wind turbines constructed so that each curve and angle generates as little resistance and as much efficiency as possible. Depth means complexity, and complexity can be an aspect of engagement. Yet much of the complex, elegant design of wind turbines that could make them engaging rather than alien remains physically hidden and corporately protected.
An unused turbine blade propped along a country road allows one to experience its scale and scope
Engagement with focal things need not be physical. Besides the handful of skilled workers who design, construct and maintain the turbines (and whose work itself is a point of possible wider engagement, as shown by the reality TV show Turbine Cowboys ), most people will not be able to physically engage with these artefacts in any practical way. But education and outreach are powerful forms of engagement largely unutilised by the various actors involved in the creation and maintenance of windfarms. This makes sense within the device paradigm: we aren’t usually invited into engagement with our electrical substations. But if it is impossible to ignore them, education and engagement can help us move toward making wind turbines focal things.
Other avenues of engagement could be as simple as suggested routes navigating drivers or cyclists on public roads through wind farms, allowing visitors to intentionally experience them as part of an aesthetic vista. And though no one should be climbing them, there are ways to bring their physicality nearer the observer. Near my own home, for instance, an unused turbine blade propped lengthwise along a country road allows one to experience a sense of the scale and scope of these artefacts. The experience of locality could be integrated with education: information such as how fast the turbines spin or how much energy they generate from moment to moment need not be obscured or accessible only to experts. These physicalities could instead be ways to engage those passing through. This doesn’t mean expensive interpretive centres at each wind farm (though it could); it might be as simple as signage along the roadway. Science communicators can help here to form bridges between the artefacts and the curious public who watches them along the horizon.
Though a more complicated concern, ownership needs to be considered as well. A deep sense of engagement comes about from artefacts that are individually or communally owned. It was this sense of ownership that allowed US windmills their cultural role and that made European windmills a vital part of their communities. This continues today, as individuals and local communities lovingly restore and maintain these earlier windmill iterations, though they are no longer the means of providing the commodities they once did. The current model of off-site ownership of wind turbines is a powerful factor keeping today’s windmills firmly within the device paradigm.
For Borgmann, ‘the dignity and greatness of a thing in its own right’ – and the stately turning turbines along my Midwestern horizons can certainly have this dignity – is what allows focal things and the practices built around them to ‘gather and illuminate the tangible world and our appropriation of it’.
Borgmann’s device paradigm helps make sense of cultural and aesthetic concerns around modern wind farms, and the history of windmills gives hints of how things might be different. Without efforts of engagement, wind turbines remain inscrutable devices, easy to reduce to uniform, monolithic symbols of extractive capitalism. Unless we try to integrate them into local culture as focal things, they will never be symbols of the landscape like windmills of the past."
Why quantum mechanics needs phenomenology | Aeon Essays,,https://aeon.co/essays/why-quantum-mechanics-needs-phenomenology,"In the early 1960s, quantum physics was regarded as one of the most successful theories of all time. It explained a wide range of phenomena to an unprecedented level of accuracy, from the structure of atoms and the formation of chemical bonds, to how lasers and superconductors worked. For some, it was more than just a theory, providing an all-encompassing framework for understanding the micro-world of elementary particles. However, it turned out that the very foundations of that entire framework were built on shaky ground – and the person who noticed wasn’t a physicist but an up-and-coming philosopher.
The debate that resulted not only opened the door to new ways of thinking about those foundations, but also had tucked away within it, overlooked by all the participants at the time, an entirely different philosophical perspective on quantum physics – one that can be traced back to the phenomenological philosopher Edmund Husserl. The impact of that shift in perspective is only now being fully appreciated, offering an entirely novel understanding of quantum mechanics, one that prompts a complete re-evaluation of the relationship between philosophy and science as a whole.
The philosopher who kick-started that debate was Hilary Putnam, who went on to make groundbreaking advances in philosophy of language and philosophy of mind, as well as in computer science, logic and mathematics. In 1961, he responded to a paper offering a resolution of the so-called Einstein-Podolsky-Rosen (EPR) paradox, which appeared to show that the description of reality offered by quantum mechanics could not be complete. In the course of his argument, Putnam pointed out that there was an even more profound problem that lay at the very heart of the theory, as it was standardly understood, and which had to do with one of the most basic of all scientific procedures: measurement.
That problem can be set out as follows. A crucial element in the formalism of quantum mechanics is a mathematical device known as the ‘wave function’. This is typically taken to represent the state of a given system – such as an atom or an electron – as a superposition of all its possible states. So, consider an electron and the property known as ‘spin’. (This is not really the same as the spin put on a ball in a game of baseball or cricket, but the name has stuck.) Spin comes in two forms, labelled ‘up’ and ‘down’, and so when we use the wave function to represent the spin state of our electron as it travels towards our detector, it is as a non-classical superposition of spin ‘up’ and spin ‘down’. However, when we come to measure that spin, the outcome is always one or the other, either ‘up’ or ‘down’, never a superposition of both. How can we account for the transition from that superposition to a definite outcome when we perform a measurement?
This question forms the basis of what came to be known as the ‘measurement problem’. One influential answer emerged from the mind of one of the greatest mathematicians of all time, János (or ‘John’) von Neumann, who was responsible for many important advances, not only in pure mathematics and physics but also in computer design and game theory. He pointed out that when our spin detector interacts with the electron, the state of that combined system of the detector + electron will also be described by quantum theory as a superposition of possible states. And so will the state of the even larger combined system of the observer’s eye and brain + the detector + electron. However far we extend this chain, anything physical that interacts with the system will be described by the theory as a superposition of all the possible states that combined system could occupy, and so the crucial question above will remain unanswered. Hence, von Neumann concluded, it had to be something non-physical that somehow generates the transition from a superposition to the definite state as recorded on the device and noted by the observer – namely, the observer’s consciousness. (It is this argument that is the source of much of the so-called New Age commentary on quantum mechanics about how reality must somehow be observer-dependent, and so on.)
What bothered Putnam was that if we accept von Neumann’s conclusion, then the theory could not be extended to apply to the entire Universe, because that would require an observer existing beyond the physical universe whose consciousness would collapse the superposition of all the Universe’s possible states into one definite one. Either physicists would have to give up the idea that quantum theory was universally applicable, or the standard account of measurement would have to be abandoned.
Putnam’s short paper, published in the journal Philosophy of Science , happened to be read by Henry Margenau, a former physicist turned philosopher of science, who then alerted the Nobel Prize-winning physicist Eugene Wigner. Together they published a response in which they defended von Neumann’s argument and dismissed Putnam’s concern. The debate then went back and forth over several years, the two sides essentially talking past each other until Abner Shimony decisively entered the fray. The holder of two PhDs, one each in philosophy and physics, and a former student of Wigner himself, Shimony subsequently went on to play a leading role in devising the experimental tests of Bell’s theorem (which builds on the EPR result by ruling out certain attempts to supplement quantum mechanics). He weighed in behind Putnam. The central concern was this: just how does consciousness effect this transition from a superposition to a definite state? With no satisfactory answer forthcoming, it appeared that Putnam and Shimony had won the day, clearing the philosophical ground for alternative approaches such as Hugh Everett ’s Many-Worlds interpretation, according to which there is no such transition at all and each element of the superposition is realised as a definite outcome, albeit in a different branch of reality or alternative world.
That debate, as historically important as it was for the further development of the foundations of quantum mechanics, also contained a significant philosophical element that was completely overlooked for many years, and that not only offers an entirely novel response to Putnam and Shimony’s concern, but also opens the door to a fundamentally different understanding of quantum physics. What they didn’t notice was the phenomenological angle.
R ather than drawing on von Neumann’s chain argument as presented in his own text, which was quite technical and had only recently been translated into English, both sides in the debate actually cited core passages from what Wigner referred to as a ‘little book’ by two other physicists, Fritz London and Edmond Bauer. Originally published in French in 1939, La théorie de l’observation en mécanique quantique ( The Theory of Observation in Quantum Mechanics ) formed part of a series of semi-popular expositions of the latest advances in science and technology, covering everything from anthropology to zoology. At just 51 pages long, the pamphlet aimed to set out clearly and accessibly not only the basic framework of the quantum mechanical treatment of measurement, but the role of consciousness in that process. Regarded by both sides as a mere summary of von Neumann’s argument, it was anything but.
Both Bauer and London worked in Paris at the time, the former at the prestigious Collège de France and the latter at the Institut Henri Poincaré. Bauer was an excellent teacher and the first in France to teach the new quantum theory. London, however, was in a different league entirely. He earned his quantum mechanical spurs by showing how the theory could explain chemical bonding, leading his collaborator Walter Heitler to exclaim: ‘Now we can eat chemistry with a spoon!’ London went on to successfully apply the theory to superconductivity with his brother Heinz, and then used it to explain the superfluid behaviour of liquid helium, subsequently publishing a two-volume book on these phenomena that became a classic in the field.
However, London was not just a brilliant physicist. He was also keenly interested in philosophy from a young age. As a student at the University of Munich, he came to the attention of Alexander Pfänder, professor of philosophy and righthand man of Edmund Husserl, the founder of phenomenology. Indeed, London’s thesis on the nature of scientific theories was published in the leading phenomenological journal of the time, the Jahrbuch für Philosophie und Phänomenologische Forschung (the ‘Yearbook for Philosophy and Phenomenological Research’), which was edited by Pfänder himself. And this was no mere youthful fixation; London maintained his interest in phenomenology throughout his career. While in Paris, he had long discussions about physics and philosophy with his friend Aron Gurwitsch, who, like London, had an academic background in both subjects and went on to help establish phenomenology in the United States.
Not just the body of the observer but their consciousness is also correlated with the system under investigation
What is phenomenology? It can be summarised as a fundamental enquiry into the correlations between mental acts or experiences, the objects that these acts or experiences are about, and the contents or (where appropriate) meanings of these acts or experiences. Its primary tool is known as the epoché (from the Greek for ‘suspension’), which requires the phenomenological investigator to ‘bracket off’ the world around us, and suppress the ‘natural attitude’ that blithely takes that world to be objective. The idea is to break the hold that such an attitude has on us so that we may uncover the fundamental epistemological and metaphysical presuppositions underpinning it.
It is important to note that this bracketing off does not mean ‘denying the existence of’. Adopting this manoeuvre does not amount to an endorsement of scepticism, nor should it be understood as leading to solipsism. Instead, by using the epoché , we can hold up to scrutiny both the supposedly objective world and that natural attitude, thereby reorienting our understanding of both. What we then discover is that the relationship between our consciousness and the world should be understood as ‘correlative’, in the sense that both exist in a ‘mutually dependent context of being’, as Maximilian Beck put it in 1928. This is not to say that consciousness and the world should be conceived of as existing independently of one another prior to being related, nor that the former somehow creates the latter. Rather, it is the correlations that constitute both consciousness and the world.
There is a great deal more to phenomenology than this, and indeed not everyone agrees with the correlationist interpretation. But it is this view that underpins London and Bauer’s ‘little book’ on measurement in quantum mechanics, which played such a crucial role in the debate over the role of consciousness in that process. Recall that Margenau and Wigner defended the standard view that consciousness somehow produces a definite observation from a quantum superposition, taking London and Bauer to be simply summarising von Neumann’s argument. Putnam and Shimony, on the other hand, questioned that whole approach, pressing the point that it was unclear how consciousness could actually yield such a result. However, both sides in that debate missed the core point of the ‘little book’. London and Bauer actually went beyond von Neumann in adopting a phenomenological perspective on the issue, according to which consciousness plays a constitutive role via the correlation between the observer and the world. They themselves make it clear how they are departing from the standard approach in the introduction:
What London and Bauer are saying here is that quantum mechanics must be understood as not just a theory like any other – that is, as about the world in some sense – but as a theory of knowledge in itself, insofar as it ‘implies a well-defined theory of the relation between the object and the observer’. This represents a crucial difference from classical physics as it is usually understood. From the perspective of quantum mechanics, the relationship between the observer and the object being observed must now be seen as quite different from that which underpins the previous stance of ‘naive realism’, which is typically adopted with regard to classical mechanics and which holds that objects exist entirely independently of all observation and possess measurable properties, whether these are actually measured or not. That view must now be abandoned. The core of London and Bauer’s text then represents an attempt to articulate the nature of that relationship between the observer and the object or system being measured.
London and Bauer radically depart from von Neumann’s argument at a crucial juncture. In setting out the chain of correlations, from detector + system to observer’s body + detector + system, they do not stop at the consciousness of the observer but also include this in the overall quantum superposition. It is this move that expresses in physical terms the phenomenological idea of the ‘mutually dependent context of being’, so that not just the body of the observer but their consciousness is also correlated, quantum mechanically, with the system under investigation.
H ow do we go from that correlation, manifested through the quantum superposition, to having a definite belief corresponding to our observation of a certain measurement outcome? Here, London and Bauer insist that
In other words, the transition from a superposition to a definite state is not triggered in some mysterious fashion by the consciousness of the observer and, as a result, Putnam and Shimony’s concern regarding how consciousness can cause a definite state to be produced is simply sidestepped. Instead, what we have is a separation of consciousness from the superposition, leading to a ‘new objectivity’, that is, a definite belief on the part of the observer and a definite state attributed to the system.
How can the observer step outside her own perspective and into that of another?
This separation is effected, as London and Bauer explain, via
And, in a typed note inserted by London in his own copy of the manuscript, he wrote:
It is this characteristic and familiar act of reflection that cuts the chain of statistical correlations expressed by quantum theory as a set of nested superpositions, and keeps the twin phenomenological poles of those correlations – namely consciousness and the world – mutually separate. And so, on the one hand, the system is objectified, or ‘made objective’, in the sense of having a definite state attributed to it, and, on the other, the observer acquires a definite belief state through this objectifying act of reflection.
London and Bauer were not unaware of the radical nature of what they were saying. In the final section of their work, they acknowledge that, as a result, it might appear that the idea of scientific objectivity itself was under threat. Indeed, this is a general problem with all such views that deny that states of systems are observer-independent – how can the observer step outside her own perspective and into that of another, and thereby establish what London and Bauer call a ‘community of scientific perception’ about what constitutes the object of the investigation? Their response is to insist that ‘one always has the right to neglect the effect on the apparatus of the “scrutiny” of the observer.’
To understand what they mean here, it is important to realise that the word ‘scrutiny’ in this quote is translated from ‘ regard ’ in the original French text, where the placing of this term between quote marks in the original text itself indicates its significance. Within phenomenology, this ‘regard-to’ is a fundamental reflective act, which, when directed to something, can be understood in terms of consciousness grasping or seizing upon it. When it comes to mental processes, their existence is, then, guaranteed by that ‘regard’. However, although physical objects are likewise brought within the purview of consciousness by the ‘regard’, their existence is not, of course, guaranteed by it (note that phenomenology does not amount to a form of solipsism).
So, when it comes to the measurement apparatus, operated by the physicist in the ‘natural attitude’, we can neglect the effect on it of this ‘regard-to’. And we can further justify our ‘right’ to do so by appeal to what is now known as quantum decoherence. Although there were indications of the core principle behind this as early as 1929, the framework was clearly set out in the early 1970s. The basic idea is that, when a system interacts with the measurement apparatus, the coherence associated with the superposition appears to be lost among the many more physical degrees of freedom offered by the apparatus compared with that of the system. As a result, even though this process does not, in itself, lead to a definite state, as the superposition is still present, the behaviour of the measurement apparatus may be regarded as classical to all intents and purposes. The ‘scrutiny’ or ‘regard’ of the observer can be ignored (unlike the case when we do consider the transition to a definite state) and a collective scientific perception achieved.
I n a lecture given in 1925, shortly before the first papers on the new quantum mechanics appeared, Husserl made it clear that phenomenology needed to be brought down from the abstract heights of philosophical theorising and expressed in concrete terms, stating:
A little more than 10 years later, in The Crisis of European Sciences and Transcendental Philosophy (1936), his final, magisterial, incomplete work, Husserl decried the way in which the mathematisation of ‘material nature’ had led to its conceptualisation as distinct from consciousness. In order for this split to be healed, he argued, there needed to be a fundamental shift back to ‘the universe of the subjective’ via the adoption of the phenomenological stance. Only then can the results of science in general, and physics in particular, be properly grasped and understood.
Merleau-Ponty argued that the observer should not be placed beyond the reach of the wave function
Unfortunately, Husserl died the year before London and Bauer’s ‘little book’ was published, but if he had read it he might have appreciated how they had, in effect, responded to both of his concerns. By couching the relationship between observer and system within a phenomenological framework, they clarified the correlation between constituting subjectivity and constituted objectivity in terms of that specific ‘categorial [form] of worldliness’ represented by quantum mechanics. Furthermore, London and Bauer showed that, by virtue of embodying that correlative relationship between ourselves and the world, quantum mechanics, conceived of phenomenologically, bridges the psychophysical divide and restores within physics, and indeed science as a whole, ‘the universe of the subjective’.
This restoration of the phenomenological nature of London and Bauer’s text – something that was entirely overlooked in the debate between Putnam and Shimony, on one side, and Margenau and Wigner, on the other – is therefore important firstly for illustrating how that particular philosophical movement was entwined with the development of quantum physics, and secondly for situating this ‘little book’ at an early stage in the evolution of a philosophical approach to that theory that has been largely ignored within the philosophy of physics, at least until recently.
This is not to say that other writers in the phenomenological tradition failed to bring quantum mechanics within their philosophical purview. Gurwitsch and Patrick Heelan also emphasised the phenomenological role of human consciousness in the measurement process, again citing London and Bauer. Maurice Merleau-Ponty , one of the most prominent phenomenological thinkers, similarly engaged with quantum theory while in Paris, and likewise argued that the observer should not be placed beyond the reach of the wave function, but must be included in the description of reality offered by physics. He went on to have a significant influence on subsequent writers, including Michel Bitbol who, together with his collaborators, has developed a form of eco-phenomenology that allies the phenomenological stance with an approach to quantum mechanics known as QBism. Initially developed by the physicist Christopher Fuchs, this similarly adopts a first-person approach that takes the concepts of agent and experience as fundamental, and understands the wave function as representing not the state of the system, but that of that agent when it comes to their possible future experiences.
Recent developments such as these have converged in a series of conferences, in turn resulting in two landmark collections, both edited and with useful introductions by Harald Wiltsche and Philipp Berghofer: Phenomenological Approaches to Physics ( 2020 ), which also covers phenomenological approaches to the theory of relativity, and Phenomenology and QBism ( 2024 ).
Within the phenomenological framework, then, one of the central problems of quantum mechanics is resolved or, perhaps better, dissolved, through a subtle but crucial shift to understanding it as a theory of knowledge by virtue of embodying our correlative participation in the world. Whether or not you fully agree with such a philosophical stance, it not only adds a hugely stimulating and potentially fruitful dimension to our understanding of one of the most fundamental constituents of modern physics, but also throws new light on the often-overlooked significance of philosophical reflection in these developments."
"No, Schrödinger’s cat is not alive and dead at the same time | Aeon Essays",,https://aeon.co/essays/no-schrodingers-cat-is-not-alive-and-dead-at-the-same-time,"In 1935, the Austrian physicist Erwin Schrödinger published a rather critical three-part review of what he called the ‘present situation’ in the relatively new theory of quantum mechanics. For the most part, Schrödinger’s review, written in German, is dry and technical, and not the kind of thing that would detain anyone outside the narrow academic world of quantum physics. But in one short paragraph, written with his tongue firmly in his cheek, he gave flight to a fancy that, 90 years later, continues to resonate in popular culture. The paragraph concerned Schrödinger’s eponymous cat. How did an obscure argument about a mathematically complex and rather baffling theory of physics become embedded in public consciousness as an extraordinary exploration of the human psyche? This essay tells the story.
Here’s what Schrödinger wrote (in the English translation by John D Trimmer):
Although this was only a ‘thought’ experiment, the paradox of Schrödinger’s cat was destined to join Pavlov’s dog in science’s bestiary of the bizarre.
T o understand the point Schrödinger was making, we need to do a little unpacking. The nature of Schrödinger’s ‘diabolical device’ is not actually important to his argument. Its purpose is simply to amplify an atomic-scale event – the decay of a radioactive atom – and bring it up to the more familiar scale of a living cat, trapped inside a steel box. The theory that describes objects and events taking place at the scale of atoms and subatomic particles like electrons is quantum mechanics. But in this theory, atoms and subatomic particles are described not as tiny, self-contained objects moving through space. They are instead described in terms of quantum wavefunctions , which capture an utterly weird aspect of their observed behaviour. Under certain circumstances, these particles may also behave like waves .
These contrasting behaviours could not be starker, or more seemingly incompatible. Particles have mass. By their nature, they are ‘here’: they are localised in space and remain localised as they move from here to there. Throw many particles into a small space and, like marbles, they will collide, bouncing off each other in different directions. Waves, on the other hand, are spread out through space – they are ‘non-local’. Squeeze them through a narrow slit and, like waves in the sea passing through a gap in a harbour wall, they will spread out beyond. Physicists call this diffraction. Push a bunch of different waves together and they will merge to form what physicists call a superposition . The peaks and troughs of all the different waves add together. Where peak meets peak, the result is a larger peak. Where trough meets trough, the result is a deeper trough. Where peak meets trough, they are both reduced and, if they happened to be of equal height and depth, they will completely cancel each other out. Physicists call this interference.
By 1935, the mathematical formulation of quantum mechanics was relatively mature, and acknowledged by most physicists as complete. But the theory does not say where all the quantum weirdness is supposed to stop. When applied to the radioactive atom, quantum theory says that, after an hour, the wavefunction of the atom is represented as an equal mix – a superposition – of decayed and undecayed atoms.
So, is the cat dead or alive? We have no way of knowing until we lift the lid of the box, and look
We could choose to stop there, but we know that the interaction between the atom and the diabolical device should also be described by quantum mechanics, at least in its early stages. If we choose to take the theory literally, then extending its equations to include the entire device means that the wavefunction evolves into a superposition of decayed atom and triggered device, and undecayed atom and untriggered device. In his review, Schrödinger coined the term ‘entanglement’ to describe this situation. The radioactive atom becomes entangled with the device.
If we logically extend this entanglement beyond the device to include the cat then, as Schrödinger explained, we arrive at a wavefunction, which is a superposition of decayed atom, triggered device and dead cat, and undecayed atom, untriggered device and live cat. The living cat and the dead cat thus appear ‘mixed or smeared out in equal parts’.
So, is the cat dead or alive? We have no way of knowing until we lift the lid of the box, and look. If we stick with quantum theory as extended to the cat, it is supposedly at the point we lift the lid that the wavefunction ‘collapses’, and we find that the cat is alive, or dead. But there is a small problem with this that has huge consequences. Nowhere in the mathematical formulation of quantum mechanics will you find an equation describing this collapse. We are left to assume it happens.
OK, but can we at least predict the fate of the cat before we lift the lid? Quantum theory says: no, we can’t. According to the accepted interpretation, the superposition of the two possibilities reflects the relative probabilities of getting one or the other. But these probabilities translate into actual outcomes only when the wavefunction is assumed to have collapsed, when the superposition of one possibility and the other transforms into one actuality or the other. It would seem that the act of looking literally kills the cat, or doesn’t.
This is a big deal. This is not the same as tossing a ‘fair’ coin, and getting heads or tails with equal probability. We wouldn’t normally choose to describe the coin as being in a superposition of heads and tails as it spins through the air, though in principle there’s nothing to stop us from doing this. We don’t do this because of course we know that both sides of the coin continue to exist unchanged as we toss it in the air, and as it spins while it falls to the ground. But this is not the way quantum mechanics works. There are now many quantum experiments that demonstrate that assuming objects like atoms or electrons exist in some state before they are observed can give predictions that conflict both with quantum theory and with the results of experiments. We simply can’t do without the superposition, or the probabilities. We need the weirdness.
A lthough the majority of physicists appeared to have accepted the argument that quantum mechanics provides a complete theory of individual quantum objects and events, there were some notable dissenters. Albert Einstein was never comfortable with quantum theory’s implications for the law of cause and effect, and the resort to probabilities, famously declaring that God ‘does not play dice’. Earlier in 1935, Einstein and his Princeton colleagues Boris Podolsky and Nathan Rosen had published a landmark paper arguing that quantum mechanics could not be considered a complete theory. Something profound was missing. Although they disagreed on the details, Einstein and Schrödinger shared common cause, and their correspondence through the summer of 1935 inspired Schrödinger to develop his cat paradox.
Schrödinger understood that under no circumstances could his cat be considered to be both alive and dead at the same time. As far as he was concerned, his paradox exposed the apparent absurdity of quantum theory, not by suggesting that ‘quantum theory says’ that a superposition consisting of a live and dead cat is a real possibility, but by suggesting that what quantum theory doesn’t say can lead to a logical absurdity. Einstein replied: ‘… your cat shows that we are in complete agreement.’
And there the matter rested, for a time. The cat paradox was limited to one paragraph in a lengthy review article, and Schrödinger’s dissent cut little ice with the majority of physicists, including those who spent time pondering on the meaning of quantum theory. It survived in correspondence between Einstein and Schrödinger through to the early 1950s, and resurfaced in 1957, during a conference of physicists and philosophers held in Bristol, England.
The diabolical mechanism now involved electrocuting the cat (or not)
In a discussion featured in the conference proceedings, the American physicist David Bohm resurrected Schrödinger’s cat. By this time, the paradox had evolved and was based on a single photon (a ‘particle’ of light) passing (or not passing) through a half-silvered (or ‘one-way’) mirror. Like the radioactive atom, the photon has a 50/50 chance of passing through the mirror or being reflected by it. Passage of the photon triggers a diabolical mechanism in which the cat is killed with a gun.
The paradox reappeared again in 1965, in an essay by the American philosopher Hilary Putnam titled ‘A Philosopher Looks at Quantum Mechanics’. The photon and half-silvered mirror remain, but the diabolical mechanism now involved electrocuting the cat (or not). Putnam concluded that: ‘ no satisfactory interpretation of quantum mechanics exists today.’
What happened next is rather fascinating. While researching Einstein’s special theory of relativity for a book she was writing sometime in 1972, the American science fiction author Ursula Le Guin came across a reference to Schrödinger’s cat. As the philosopher Robert Crease put it in a 2024 article, she was instantly ‘entranced by the implied uncertainties and appreciated the fantastic nature of Schrödinger’s image.’ We can’t be sure of precise events and timings, as Le Guin read extremely widely but didn’t systematically take notes, but this is the ‘best guess’ of Julie Phillips, who is busy writing an authorised biography of Le Guin to be published in April 2026. Having been asked by her subject to ‘rescue me from the vultures’, Phillips conducted many in-depth interviews with Le Guin before the author died in 2018. They had agreed that the biography would be published posthumously.
In her short story ‘Schrödinger’s Cat’ (1974), Le Guin presents Bohm’s version of the paradox involving the photon, half-silvered mirror and gun. In a dialogue between the nameless narrator and a dog called Rover, Le Guin wrote:
The floodgates opened. From this point onwards, Schrödinger’s cat makes regular appearances in fiction. Not just science fiction, but a broad range of short stories and novels, films, plays, television shows, poems, and music. Developments taking place in physics in the early 1980s simultaneously drove burgeoning interest in popular non-fiction, such as John Gribbin’s In Search of Schrödinger’s Cat (1984).
The cat’s cultural appeal lies in the ‘what if’ questions it provokes. It encourages us to ponder the consequences of our very human choices. What if we choose not to look? If we don’t look, can the cat really be said to exist at all? Our decision to lift the lid is much like encountering a fork in the road. We choose a path. Like the American poet Robert Frost, we may choose the path less travelled by. But what if we had taken the other path? The movie Sliding Doors (1998) delivers two parallel stories, one that unfolds when Helen Quilley (Gwyneth Paltrow) misses her train on the London Underground, and a second that unfolds when she manages to board it. Quilley’s life turns out very differently, depending on whether or not she beats the sliding doors and gets on the train. That such a trivial ‘sliding doors moment’ might profoundly alter the course of our future is deeply unsettling.
There’s more. As Le Guin herself observed in her short story, there appears to be nothing special about the act of lifting the lid, and quantum mechanics is silent on the question of where in the chain of events the weirdness stops. She wrote: ‘But why does opening the box and looking reduce the system back to one probability, either live cat or dead cat? Why don’t we get included in the system when we lift the lid of the box?’ Could we be just like the cat, but trapped in a much bigger box that we call reality? If we are, who is doing the looking? And what will happen when they lift the lid?
If the act of looking inside the box doesn’t collapse the wavefunction of the system, then logically the observer must in turn become entangled in the superposition. ‘[T]here we would be,’ Le Guin wrote, ‘looking at a live cat, and … looking at a dead cat.’ If you are the one doing the looking, there would now be another superposition involving two versions of you.
A t this point, we might be tempted to reach for an altogether different interpretation of quantum mechanics. If the mathematics doesn’t account for the collapse of the wavefunction, why assume it happens at all? Why not suppose that, as Le Guin suggested, you become entangled with the system when you lift the lid? As nobody has ever experienced the eerie sensation of co-existing with multiple versions of themselves, all witnessing different events, we could further suppose that the act of lifting the lid ‘splits’ the Universe into two parallel versions. In one universe, one version of you observed a dead cat. In another universe, another version of you observed a live cat. There is no eerie sensation because these different universes have diverged, and you are completely unaware of the other parallel versions of yourself.
This is the so-called ‘ Many Worlds ’ interpretation, proposed in 1957 by the American physicist Hugh Everett III. It offers us a multiverse of parallel possibilities. The multiverse allows a much broader and more sophisticated range of ‘what ifs’ beyond the binary alive/dead-type questions relating to a sliding doors moment. What if the effects of your choices accumulate over time and conspire to change not just your future circumstances, but your entire personality?
In a multiverse of possibilities, is there a multiplicity of very different versions of you all behaving very differently and living different lives? Perhaps in one of these universes you are humane and kindly but homeless, reduced to begging on street corners. But in another you are an unempathetic tech billionaire, threatening to undermine the accepted world order. Such questions are explored to great effect in Blake Crouch’s novel Dark Matter (2016), which was adapted for television and broadcast last year on Apple TV+.
The Schrödinger’s cat of popular culture feeds our innately human desire for mystery and gives licence for daring flights of imagination that help us to explore what makes us ‘us’. And, remarkably, it claims to do these things in the name of science, because this is what ‘quantum theory says’. Who thought physics could be so much fun?
This doesn’t mean that the cat is literally alive and dead at the same time
Alas, most physicists adopt a more sober perspective. In the 1920s and ’30s, the founders of quantum mechanics nagged away at these problems of interpretation and arrived at solutions that many deemed satisfactory, though some (like Einstein and Schrödinger) deemed them deeply unsatisfactory.
What does the superposition and, more broadly, the quantum wavefunction, actually represent? One view, most closely associated with the Danish physicist Niels Bohr and known generally as the ‘ Copenhagen interpretation ’, is that these are merely calculation devices not to be taken literally. They are not real: they are purely symbolic. The superposition simply represents what we know about the cat-in-a-box system, and we use the equations of quantum mechanics to calculate the probabilities for various expected outcomes.
So, when we talk about the cat being in a superposition of life and death, this doesn’t mean that the cat is literally alive and dead at the same time. In truth, we don’t know what the state of the cat really is, nor how to describe its real physical situation, because we can’t say for certain when the radioactive atom will decay, or whether the photon will be transmitted or reflected. But if we represent this system as a superposition, we know we will make predictions that will prove to be consistent with experiment. Most physicists, at least those who can be bothered to think about these things, adopt this view. This might be why they’re not often invited to parties .
It follows from this that it doesn’t matter precisely where in the chain of events we declare that the weirdness stops. It doesn’t matter where we place a ‘Heisenberg cut’, named for the German physicist Werner Heisenberg (of uncertainty principle fame), the point at which we stop using quantum mechanics and switch to more familiar theories of physics published more than 300 years ago by Isaac Newton. This is the point at which we assume the wavefunction collapses, and we replace the and of the quantum superposition with the or of actual outcomes.
Heisenberg was a member of Bohr’s group, although in some of his pronouncements he diverged substantially from Bohr’s philosophy. As far as Heisenberg was concerned, it does not matter where we choose to place the cut. But a cat is not a radioactive atom, nor a photon. It clearly does not belong in the quantum realm, and the equations of quantum mechanics, even interpreted symbolically, should not apply to it. Bohr preferred to place the cut at the point of the ‘irreversible act of amplification’ associated with the early stages of the diabolical device. That we can’t be specific about precisely where or when in this process the weirdness stops doesn’t invalidate the conclusion that this happens long before we get to the cat.
Schrödinger ends his 1935 review with the observation:
We are faced with a choice. We can recognise that quantum mechanics – with all its weirdness – is a purely symbolic framework for predicting the probabilistic outcomes of our experiments. It is indeed a calculational trick, not to be taken literally, which allows us some ability to get a handle on an otherwise unfathomable atomic and subatomic world.
Or we can recognise (with Einstein and Schrödinger) that quantum theory is at the very least incomplete, and deeply unsatisfactory. A theory capable of fathoming the atomic and subatomic world ought to be possible, if only we have the will to look for it, and the wit to find it.
This is the fork in the road. Which path will you take?"
"Why identity, morality and faith splinter in the multiverse | Aeon Essays",,https://aeon.co/essays/why-identity-morality-and-faith-splinter-in-the-multiverse,"Recently, I was caught on the horns of a dilemma. I had a decision to make and, either way, I knew my life would follow a different track. On one path, I accept a job offer: it’s an incredible opportunity, but means relocating hundreds of miles away, with no social network. On the other, I stay in Oxford where I’d lived for a decade: less adventure, but close to my friends and family. Both options had upsides and downsides, so I wished that I could take the job and turn it down, somehow living each life in parallel.
Well… there was potentially a way to make this happen. I could have my cake and eat it too.
This will seem odd at first, but bear with me. There are smartphone apps that can help you decide between two options by harnessing the unpredictable quirks of quantum mechanics. But this is no ordinary coin toss, where randomness decides your fate. Instead, it guarantees that both choices become realities.
You open the app and request a measurement of a photon, which forces it to occupy a binary state, such as ‘spin up’ or ‘spin down’. In my case, ‘spin up’ meant accept the job and ‘spin down’ meant decline. You will see only one result but, in theory, another you will see the opposite, in a different universe. From that moment, two versions of you co-exist, living in parallel.
It’s inspired by the ‘Many-Worlds’ interpretation of quantum mechanics, first proposed by the physicist Hugh Everett III in his doctoral dissertation in the 1950s. He argued that our Universe branches into multiple worlds every time a quantum event takes place – and thousands happen every second. While this idea seems fantastical, a growing number of scientists and philosophers think this is how our world really works. In fact, if the Many-Worlds interpretation of quantum mechanics is true, then the splitting of worlds is not only possible, it is ubiquitous.
As a philosopher of religion, I am interested in how this mind-boggling scientific theory might force us to reexamine even our most deeply held beliefs. In fact, I believe that the Many-Worlds interpretation of quantum mechanics encourages us to radically reconceptualise our understanding of ourselves. Perhaps I am not a single, unique, enduring subject. Perhaps I am actually like a branching tree, or a splitting amoeba, with many almost identical copies living slightly different lives across a vast and ever-growing multiverse. I also believe that this picture encourages us to rethink our ideas about moral responsibility, and what religion tells us about God – maybe, even, abandon the traditional idea of God altogether.
T he year 2025 will mark the centenary of Werner Heisenberg’s development of matrix mechanics, the first formalisation of quantum mechanics into a coherent physical theory. You might think that after 100 years and such staggering success, there would be consensus about how to interpret what the theory means for the wider world. You’d be wrong.
For much of the 20th century, the so-called Copenhagen interpretation of Heisenberg, Niels Bohr and others ruled the day. In recent years, however, a growing number of people are questioning it, dubbing it the ‘ shut up and calculate ’ approach because of its unwillingness to ask deeper questions about what all this is supposed to mean. The appetite for something more metaphysically satisfying is growing, and those working in this area are rising to the challenge.
The Everett, or Many-Worlds, interpretation is now a powerful contender. It builds on the fact that quantum objects exist in superpositions: in all possible states at once. As Erwin Schrödinger observed in the 1930s, hypothetically this could mean you could have a cat in a box that is alive and dead simultaneously. This seems impossible – and, for Schrödinger himself, that was the point. But the Many-Worlds interpretation solves the problem by claiming that the cat is not both alive and dead; rather, it involves multiple worlds , some of which contain alive cats and some of which contain dead cats. In other words, each possible outcome actually happens. Whichever state of affairs we observe when we open the box indicates which world we are in, but the other worlds are equally real.
That’s all very well, but what should we make of this in day-to-day experience? Here physics falls short. For starters, if we live in a universe where there are multiple versions of you, thorny questions are raised about whether these versions of you can be considered the exact same person. For most people, the idea that they continue to exist from birth to death is an obvious fact that requires no further contemplation, but philosophers have spilled much ink trying to define what makes ‘you’ persist through time. Some have argued it’s about psychological continuity – you have the same mind and memories as your past self. Others propose that it’s about owning the same body. Many religious believers, meanwhile, would point to the existence of a single, indivisible soul.
If you can theoretically survive teleportation, why should an extra version of you mean you have died?
A few decades ago, the philosopher Derek Parfit stress-tested these assumptions by exploring the concept of ‘fission’ in thought experiments, inspired by cell division. He argued that identity is a one-one relation, meaning that you cannot be identical with multiple people. This means that if you were to split into two identical copies, then you would cease to exist.
It might be hard to imagine why at first. If you were destroyed but a copy of your body and mind appeared in their place a fraction of a second later, you probably wouldn’t notice the difference. From your internal perspective and from the perspective of those who know and love you, you would continue to exist. In fact, it is this kind of disappearing and reappearing that would be involved in teleportation , if that were ever to become possible. So if you can theoretically survive teleportation, why should an extra version of you appearing mean that you have died? Surely there being more of you can’t mean that you have ceased to exist altogether?
When tackling this problem, a philosopher might first reach for the principle of ‘the identity of indiscernibles’. According to this principle, if two objects are in fact one and the same identical object, they must have all the same properties. For example, the morning star and the evening star are identical because both names refer to the same thing: the planet Venus.
Another principle is ‘the transitivity of numerical identity’. This picks up on something intuitively obvious – the relation of ‘being identical with’ transfers. If the current British king is identical with Elizabeth II’s eldest son, and Elizabeth II’s eldest son is identical with Charles Mountbatten-Windsor, then the current king is identical with Charles Mountbatten-Windsor.
Let’s apply these principles to Parfit’s fission. Suppose someone, let’s call her Alice, splits into two identical copies (we’ll call them Lefty and Righty). Is the pre-fission person, Alice, identical with Lefty, Righty, both, or neither? If both Lefty and Righty share Alice’s memories and feel like Alice from the inside, then they have psychological continuity with Alice. According to this way of understanding identity, they are both identical with Alice. But the problem is, they are not identical with each other. They occupy different spatial locations, have distinct streams of consciousness, and so by any measure they are different persons.
To return to our above principles, Lefty and Righty are discernible, so they are not identical. That means that Alice cannot be identical to either of them without violating the transitivity of identity principle. If Alice is not identical with either Lefty or Righty, and Lefty and Righty are all that remain after Alice has split, then Alice does not exist anymore. It is because of this that some philosophers believe that fission means death.
This all might sound very abstract and hypothetical, but if the Many-Worlds interpretation is correct, then Parfit’s idea of fission was part of reality long before philosophers existed to contemplate it. As the physicist Sean Carroll put it in his book Something Deeply Hidden (2019): ‘the life-span of a person should be thought of as a branching tree, with multiple individuals at any one time, rather than as a single trajectory – much like a splitting amoeba.’ What are we to make of the idea that the human person ought to be thought of like a phylogenetic tree, a set of descendants branching off from each other forever but sharing a common ancestor? The philosophical implications for our understanding of personal identity are mind-boggling.
A n additional thorny problem raised by a universe of many worlds is that of moral responsibility . Most ordinary people’s moral intuitions about right action – whether some action was freely made, whether it accords with shared moral principles, and whether a person can be held responsible for it – were formed under the assumption that we live in a singular universe. But if Everett was right and we are living in a quantum multiverse, we might need a rethink.
When we say an individual is morally responsible for a crime or a kindness, for instance, we are saying that they possess capacities such as volition, control over their actions and awareness of consequences. The individual deserves blame or condemnation if what they have done is bad, and praise and admiration if what they have done is good.
Whether we realise it or not, our judgments about moral responsibility rely upon certain conditions. These are:
To understand why these conditions matter, consider two versions of a single event: my elderly grandmother is knocked to the floor and injures herself at a family birthday party. In one version, I’ve slipped on a banana skin and fallen into her. In the other, I’ve deliberately pushed her in rage because she ate the last piece of birthday cake.
Across the multiverse, everything that can happen does happen; each branch is inevitable
In each case, I am causally responsible for my grandmother’s injuries. Only in the second case, however, am I morally responsible. If it’s an accident, I lack volition, or free and deliberate action in accordance with my will.
The problem is, Many-Worlds is a deterministic theory – and determinism is considered by many, though not all, philosophers to be incompatible with genuine freedom . Across the multiverse, everything that can happen does happen; each branch is inevitable. If that’s the case, even if we feel like we have the freedom to choose what actions we take, this may in fact be an illusion. We wouldn’t think me morally responsible for pushing over my grandmother if someone held a gun to my head and threatened to kill me if I didn’t. Similarly, if all my actions are determined by physical forces outside my control – like the laws of quantum mechanics – then it seems pretty unjust to punish me for them.
Alongside freedom, personal identity is also essential to moral responsibility. I cannot be held responsible for something unless I am the one who does it, or the one who forces someone else to do it. If someone who looks very much like me, such as my identical twin, pushed over our grandmother, it would be deeply immoral to punish me just because I look identical to the true culprit. In order for the responsibility to be mine , the action must have been committed by me. Because of this, it is of the utmost importance to work out whether the other versions of me, including the past versions of me whom I seem to remember being, can be firmly established as being myself and not just people who look rather like me. If we cannot do this, then our ordinary notion of moral responsibility and all that comes with it – holding people accountable, punishing those who have done wrong, trying to uphold moral standards in society – slips through our fingers.
T he Many-Worlds interpretation also raises conundrums for religious believers committed to the idea of an all-powerful creator. In particular, it threatens to amplify the ‘ problem of evil ’: the oldest and most frequently cited argument against the existence of an omniscient, omnipotent and benevolent god. In broad brushstrokes: if God is all-seeing, God should be aware of all the evil and suffering that occurs. If God is all-powerful, then God should be able to stop all evil and suffering. If God is all-loving and perfectly good, God should want to exercise these powers to end evil and prevent suffering. And yet we see suffering and pain all around us. The problem of evil exists whether we are living in a multiverse or not – but I believe the problem is far worse if Everett was right about how to interpret the quantum equations.
As we have already seen, the Many-Worlds interpretation claims that there are multiple versions of me, each of whom shares parts of my past but not my present, nor my future. Out of all these people, there must be at least one who is living the worst possible version of my life, and there will be countless more living very bad lives. In other words, there is much more suffering in the Everettian multiverse than there is in a singular universe. What’s more, suffering will be in each of our futures, even if we don’t know which future and which version of us will have to endure it.
When I present this problem to religious believers, many want to turn it on its head. While there is much more pain in Many-Worlds, they argue, there is a correspondingly greater amount of good. For every suffering version of me, there is at least one living the best version of my life, and many more living good ones. On balance, then, the distribution of goods and evils is not so different to the world described by non-Everettian physics, and the problem of evil is no greater than it was before. Perhaps belief in God is not in that much trouble after all.
A world that allows a child to suffer for the benefit of someone else is a world we should not want to live in
I think this response fails, and my reason is based upon a common distinction theologians make between two equally important descriptions of God. That is, the distinction between God as a provider of global goods on the one hand, and as a loving parent of individual persons on the other. God isn’t just supposed to be a powerful but largely absent creator who set the world into motion and then stepped back without caring how it unfolded. Fewer people would worship a detached, impersonal force who has no interest in the small, intimate details of ordinary human lives. God is also supposed to care deeply and personally about each and every creature. Their suffering matters. If religious believers did not think this, then prayer would be pointless and worship worthless.
God should want to stop the extra suffering of the many versions of me living terrible lives, even if it meant that there wouldn’t be a corresponding number of Emilys living happy lives. Why? Because joy and suffering are not morally equal. If you were forced to choose between curtailing the joy, pleasure or happiness of one person to stop some other person suffering, you know that stopping suffering must always take precedence. That’s why we punish violent sexual crimes – the pain and the violation of the victim is much more morally important than any predator’s pleasure.
One might see how a God could choose to design a universe that vastly increases the number of versions of each person who are able to enjoy virtuous, joyful and fulfilling lives. Yet as a parent who loves each individual creature, it seems inconceivable that God would create a reality in which at least one version of every single person is living the worst iteration possible of their life. In the oft-referenced words of Ivan Karamazov, a world that allows an innocent child to suffer greatly for the benefit of someone or something else is a world that we should not want to live in.
Even if we live in a singular universe, the problem of evil is the prevailing reason that I cannot accept that God exists. If there are many worlds, with untold suffering far beyond what I can imagine, then it only deepens my conviction.
S o, if the Everettian multiverse is reality, we exist as continually branching selves like splitting amoeba, our ideas of morality are turned upside down, and there are compelling arguments for why there is no God overseeing it all.
We are also left with something of an identity crisis: it’s not even clear if we ‘survive’ from moment to moment, if branching is a kind of a death. How do I make sense of that personally? One solution I am currently exploring is the idea that who we are is determined by a narrative thread of our own weaving. On this view, who I am is no more nor less than my own internal self-conception, shaped by memory, desire, emotion, experience and embodiment. On this view, it does not matter whether there are copies of me or not, and I should not care which philosophical principles these copies might violate. All that matters is that who I am is decided by me – it is subjective , not objective.
What this means is that instead of human beings having a core essence – something like an eternal and indivisible soul – we are collections of stories told and retold by us and those who love us. We are rivers, ever fluid, whose banks are shaped by sediments of stories stored in our depths.
In the Everettian multiverse, we have an infinite number of futures laid out before us
The late philosopher Daniel Dennett held a similar view. According to him, the self is like a centre of gravity, a useful fiction that theoretical physicists and ordinary people alike construct to make sense of the world. For Dennett, all we have are evolving identities tied together by an autobiographical thread that is ever in flux. The self is an abstract construct; it does not really exist.
It would also mean that, if you are ever caught in a seemingly impossible dilemma, there is a way for you to have your cake and eat it too. I decided not to accept my job offer in this universe, but I like to think that another Emily did. In the many worlds of the Everettian multiverse, we have an infinite number of futures laid out before us. No more Sophie’s choice-type decisions need keep you up at night, instead you can flip a quantum coin and live both lives (even if you won’t ever know how the other future works out). Although we know that some of our futures contain suffering, perhaps we can take comfort in the idea that somewhere out there at least one version of us is living the best life possible.
Finally, it might encourage us to abandon religious ideas written in different historical contexts and shaped by different metaphysical understandings of the Universe. Perhaps there is no God. Instead of this leaving us with a cold and meaningless cosmos, I think it gives us radical autonomy to create our own systems of meaning. What matters, what is good and valuable, is up to us to decide.
Whatever solutions we eventually land on, what’s clear is that this radical and mind-bendingly weird interpretation of quantum mechanics raises big questions about ourselves, the Universe, and the existence of God. I, for one, can’t wait to see where the quantum physics takes us. Wherever we end up, it’ll be a wild ride."
Going deep underground to reveal the mysteries of the neutrino | Aeon Essays,,https://aeon.co/essays/going-deep-underground-to-reveal-the-mysteries-of-the-neutrino,"The neutrino is a particle that almost isn’t here. It has no charge, almost no mass, and not even a fixed identity: it comes in three subtly different forms, and any given particle constantly shifts between them. About 500 trillion neutrinos fly into your body every second, and then they fly right back out without a trace. Neutrinos treat the rest of the Universe with indifference. ‘If you sent a neutrino at a lead wall that was a light-year thick, there’s a 50 per cent chance that neutrino would go all the way through without interacting,’ says Chris Mossey, project director of the Deep Underground Neutrino Experiment (DUNE) at Fermilab in Batavia, Illinois.
Neutrinos may be indifferent to us, but we are hardly indifferent to them.
For nearly seven decades, physicists have crafted increasingly elaborate machines to outwit nature and to detect these seemingly undetectable particles. DUNE is the latest and greatest of these devices: a $3.2 billion, 1,300-kilometre-long set of experiments designed to bring neutrinos into view like never before. Mossey, a retired US Navy rear admiral, is part of a global team of more than a thousand researchers working full-tilt to get DUNE up and running by 2031.
The path of neutrinos in the Deep Underground Neutrino Experiment. A proton beam is produced in Fermilab’s accelerator complex, hits a target and produces a neutrino beam that travels through a particle detector, travelling 800 miles to the detectors at Sanford Underground Research Facility. Courtesy Fermilab
On the face of it, that operation seems like a crazy level of effort to expend on chasing ghosts. The obsession with neutrinos makes sense when you understand what they represent, however. ‘I find myself often explaining to my colleagues: “What’s a neutrino?”’ says Mossey, whose background is not in physics but in supervising huge construction projects for the US Navy. He happily repeats his own neutrino education. ‘The science at DUNE really is amazing. It goes after some of the most fundamental questions that we seek to understand about the Universe: why does matter exist? These are Nobel prize-winning questions if they can be answered.’
According to our current understanding of physics, the Universe was born in an unimaginably hot, intense bath of energy: the Big Bang. That energy should have given rise to precisely equal amounts of matter and antimatter, which should have perfectly annihilated each other. The net result, then, should have been a whole lot of nothingness. Instead, something remarkable happened. Particles of matter outnumbered particles of antimatter, by about one part in 10 billion. That minuscule imbalance made all the difference. The extra particles of matter were enough to create all the galaxies, stars, planets and people we see around us.
Much as the neutrino almost isn’t here, we almost weren’t here, either. We are the cosmic leftovers, the happy result of an unknown kink in the laws of physics.
The peculiar attributes of the neutrino, most notably a protean ability to shapeshift into different versions of itself, could expose the crucial asymmetry that allowed matter to dominate over antimatter. Simply discovering why and how that imbalance occurred would be a breakthrough, but neutrinos may be more than passive messengers about the earliest moments in the life of the Universe. According to some theories, neutrinos actively participated in the genesis of matter, and hence all that followed from the Big Bang. So there is a lot riding on these little particles, and on the giant machines being built to study them.
‘We’d like to know why we’re here – why we exist,’ says Sam Zeller, deputy project director for DUNE at Fermilab. ‘There’s a lot of hope that the results at DUNE will give a definitive answer.’
P hysicists have a long history of pinning their grand hopes on the neutrino. In fact, they’ve been doing it since long before anyone knew that these particles even existed.
Starting in 1914, the English physicist James Chadwick began noticing that certain types of radioactive atoms, such as carbon-14, seemed to be misbehaving. During a type of spontaneous nuclear reaction called beta decay, the nuclei of these atoms shoot out high-speed electrons. In theory, the emitted electrons should always emerge the exact same way, carrying away precisely as much energy as is lost from the nucleus. In reality, the electrons always fell short. Even worse, they fell short in an inconsistent way – sometimes by a little, sometimes by a lot.
James Chadwick in the early 1930s. Courtesy the Linda Hall Library
Chadwick’s discrepancy carried a disturbing implication: the radioactive atoms seemed to be violating conservation of energy – perhaps the most fundamental law in all of physics, stating that energy cannot be created or destroyed. It was such a shocking result that the renowned Danish physicist Niels Bohr suggested the familiar laws of physics might no longer hold at subatomic scales. Maybe conservation of energy wasn’t a hard-and-fast rule in quantum physics, but more of a statistical average. In this view, a little energy can be destroyed over here or created over there, as long as the overall ledger balances out.
Many of Bohr’s colleagues simply held out hope that Chadwick had made a measurement error. By the time Chadwick published an exhaustive investigation of beta decay in 1922, though, the discrepancy became undeniable and the search for an explanation more urgent.
They rejected the paper, as ‘it contained speculations too remote from reality to be of interest to the reader’
Finally, in December 1930, the Austrian physicist and quantum pioneer Wolfgang Pauli offered what he called a ‘desperate remedy’ to rescue the law of conservation of energy. His idea was so bizarre that he presented it as a whimsical letter written to a group of nuclear physicists meeting in Tübingen, Germany, addressed to ‘ Liebe Radioaktive Damen und Herren ’ (‘Dear radioactive ladies and gentlemen’). What if, Pauli suggested, an invisible, uncharged particle appears out of nowhere at the moment of beta decay? And what if that wondrous, hypothetical particle then carries away exactly the amount of energy needed to make everything balance out?
At the time, physicists knew of only two subatomic particles, the proton and the electron, and conjuring up a completely new, undetectable particle to explain away a peculiar observation seemed just as outrageous as discarding the principle of conservation of energy. One of the few people to take Pauli’s idea seriously right off the bat was Enrico Fermi, the Italian-born physicist who later gained fame as the creator of the first nuclear reactor. Fermi developed a more comprehensive theoretical description of Pauli’s particle, and in 1934 sent a paper describing his ideas to the journal Nature . The editors rejected the paper, reportedly because ‘it contained speculations too remote from reality to be of interest to the reader’.
But the neutrino proved stubbornly persistent. Continued experiments showed that the electrons emitted during beta decay followed a specific energy pattern, consistent with the presence of an unseen particle rather than with an unconstrained violation of conservation of energy. In 1938, The New York Times declared that the neutrino is ‘no mere hypothesis’, capturing the new consensus view in physics. Even then, Pauli realised that his particle would be extremely difficult to detect – so difficult, that he bet a case of Champagne to anyone who could do the job.
Pauli’s Champagne remained on ice for nearly two decades until the physicists Frederick Reines and Clyde Cowan, Jr, of Los Alamos National Laboratory, became determined to track down the neutrino once and for all (‘Because everybody said you couldn’t do it,’ in Reines’s words). In the early 1950s, Reines and Cowan realised that the best places to look for neutrinos would be where there are a tremendous number of neutrino-spewing nuclear reactions: close to an atomic bomb or a nuclear reactor. They briefly considered placing neutrino detectors next to a bomb test at Los Alamos before switching to a steadier and less lethal source: a US military nuclear reactor at the Savannah River Plant in South Carolina.
For their search, which they nicknamed ‘Project Poltergeist’, Reines and Cowan set up a 1,400-litre detector filled with water and cadmium chloride. On the highly, very, extremely rare occasions when a neutrino would collide with a cadmium atom, it would emit a detectable gamma ray. Using this setup, the determined duo finally found the definitive signal of neutrinos fleeing the Savannah River reactor. On 14 June 1956, they sent a telegram to Pauli: ‘We are happy to inform you that we have definitely detected neutrinos from fission fragments.’ Pauli wrote back: ‘Everything comes to him who knows how to wait.’
And so the neutrino went from a theory to a tangible object.
N ow that neutrinos were detectable (albeit barely) in a controlled experiment, another obsessive physicist wanted to study them in the wild. He sensibly set his sights on the largest neutrino-spurting nuclear reactor in the neighbourhood, the Sun. Starting in 1965, the physicist Raymond Davis, Jr persuaded the powers at Brookhaven National Lab to fund the world’s first neutrino telescope. It was a ‘telescope’ only in the loosest sense of the term, since conventional mirrors and lenses are irrelevant to neutrinos. For his detector, Davis used 100,000 gallons of chlorine-rich drycleaning fluid, which he pumped into a cavern of the former Homestake Gold Mine in Lead, South Dakota – the same site, not coincidentally, where the DUNE experiment is currently taking shape.
The vast majority of solar neutrinos flew right through the drycleaning fluid, the same way that they fly through everything else. As with the Reines and Cowan experiment, Davis was relying on sheer numbers to help him. All he needed was a few passing neutrinos to crash into a few of the chlorine atoms among the billion billion billion of them in his tank. When that happened, the chlorine atoms would transform into argon atoms, which Davis could then detect and count. His painstaking effort worked, but only sort of: he got his expected argon atoms, but the resulting numbers seemed all wrong. Davis kept refining and calibrating his experiment until, in 1975, it became clear that the observed flux of neutrinos from the Sun was just one-third of the expected value. The other two-thirds of the particles were AWOL.
Either physicists had misunderstood how the Sun shines, Davis concluded, or they had underestimated how tricky neutrinos are.
Arthur McDonald, an astrophysicist at Queen’s University in Canada, decided to bet big on neutrinos being tricky, and helped design an experiment to prove it. As with all things neutrino, McDonald’s proposed project was a massive undertaking. In the 1980s, he and his colleagues approached the Canadian government with an outrageous request: ‘Do you think we could borrow 4,000 tons of heavy water to do a measurement of neutrinos from the Sun?’ he recalls. ‘At that point, it was about 1.2 billion bucks worth.’
What if neutrinos could change their identities on the fly, oscillating from one form to another?
McDonald was inspired by recent discoveries indicating that the neutrino is not just a single type of particle, but rather a whole family. By the mid-1970s, physicists had deduced that neutrinos come in at least three distinct varieties , or flavours: electron-neutrinos, muon-neutrinos , and tau-neutrinos. Nobody could explain exactly how one kind of ghost particle could have three different ghostly identities, but theory implied that each type of neutrino paired uniquely with one corresponding fundamental particle: the electron, muon, and tau particles.
That number three caught physicists’ attention, suggesting an explanation for the puzzling shortage of neutrino detections in Davis’s experiment. Perhaps the other two-thirds of the expected neutrinos weren’t truly missing, but merely hiding. Physics models indicated that the Sun should produce one flavour of neutrino only – the electron variety – and that’s the only flavour that Davis’s experiment could detect.
But what if, some theorists suggested, neutrinos could change their identities on the fly, oscillating from one form to another? This idea, called the MSW (Mikheyev-Smirnov-Wolfenstein) effect, did not jibe with the behaviour of any other type of particle, but it did have an appealing logic. With three options to choose from, just a third of the solar neutrinos would retain their electron-neutrino identities by the time they reached Earth. And only that third would register in Davis’s detector, exactly matching what he found.
McDonald had to prove that neutrinos really did oscillate in this duplicitous (or rather, tri -plicitous) way. To his amazement and delight, the Canadian government came back with an offer of 1,000 tons of heavy water, enough to get his experiment started and to establish what became the Sudbury Neutrino Observatory in Ontario. Unlike the chlorine atoms in the Davis experiment, heavy water can interact with all three flavours of neutrinos, which allowed McDonald to keep track of the particles, no matter which guise they adopted. ‘In 2001, 2002, we were able to show that neutrinos from the Sun changed from one of the flavours to the other,’ he says. Thirteen years later, McDonald shared the 2015 Nobel prize for his work.
The discovery of neutrino oscillations was another seemingly arcane physics observation that turned out to have profound implications. Oscillations are possible only if some neutrinos have mass, which came as a shock to theorists: ‘The Standard Model [the prevailing theory of fundamental forces and particles, formalised during the 1970s] didn’t predict neutrinos having mass in any way,’ says Alexandre Sousa of the University of Cincinnati, who wields the cool title of Beyond the Standard Model Physics Co-Convener for the DUNE experiment. Where that mass comes from, and what it says about the nature of mass in general, remain active and contentious topics of research.
More important, neutrino oscillations revealed that neutrinos could be used as sensitive probes of natural symmetries – laws of behaviour that remain constant even when you switch one basic condition. For instance, the laws of physics appear the same when you look in a mirror (light is still light, objects still fall down, and so on). The same mirror-image consistency generally holds true for individual particles, a property called ‘parity symmetry’. Particles of matter and particles of antimatter should also behave alike except for the reversal of their electric charges, a property called ‘charge symmetry’. For much of the past century, physicists have heavily relied on symmetries like these to make sense of the rules governing reality.
Neutrinos don’t obey those symmetry rules, however. In particular, they violate the combination of charge and parity symmetry – a combo called, naturally, ‘CP symmetry’. The manner in which neutrinos shift from one flavour to another depends sensitively on exactly how, and how much, they violate CP symmetry. And CP symmetry just so happens to influence the balance between matter and antimatter. The only way to get a universe full of matter (and the good things that come with it, from galaxies to people) is by violating this specific symmetry. Find the breakdown in CP symmetry within the Universe, then, and you might be able to figure out where all this stuff came from.
With neutrinos, ‘you can measure a value for this CP violation,’ Sousa says. ‘If that value is large, then in theory it could explain everything .’ That is where DUNE comes in.
B efitting its grand size and budget, DUNE is a multilayered physics experiment, designed to scrutinise every aspect of neutrino behaviour with unprecedented precision. But above all, it is intended to investigate that vexing question of cosmic origins: why is there something rather than nothing? Our best hope for getting a meaningful answer is to study neutrino oscillations at a level of precision never before possible. To that end, DUNE will incorporate techniques and technologies from all the neutrino experiments that have come before, and vastly expand on them.
The engineering required to make DUNE work is comically out of proportion to the infinitesimal particles it will study. Mossey chuckles in disbelief at the scale of the project: neutrino detectors that measure five stories tall and 200 feet long, sheltered a mile underground, containing nearly 70,000 tons of cryogenic liquid argon. In February 2024, Mossey’s team completed the excavation of the primary workspace for DUNE, three enormous caverns to house those enormous detectors. That task alone required excavating 800,000 tons of rock. ‘And all this had to be done through a mineshaft that’s five feet wide,’ he says. ‘Think of the hoist, think of the power, think of the ventilation.’
Shimmering liquid argon inside a 35-ton-capacity prototype cryostat used for early tests of LBNF/DUNE. Courtesy Fermilab/Reidar Hahn
When DUNE fully activates in 2031, it will generate ‘the most intense neutrino beam in history’, Zeller says, a 1.2-megawatt neutrino blast designed to expose those ghost particles once and for all. Creating that beam will entail a whole other, marvellously elaborate set of engineering tricks designed to manipulate particles rather than rocks – a quantum Rube Goldberg machine.
Each round of science at DUNE begins at Fermilab – the long-running particle physics facility outside Chicago – where a new particle accelerator, called the Proton Improvement Plan II (PIP-II), shoots out a beam of high-energy protons. Forty-one huge magnets, mostly made at the Bhabha Atomic Research Centre in India, focus and align the protons so that they hit a carefully crafted, 1.5-metre-long cylindrical graphite target. The bombarded target unleashes a spray of particles called pions and kaons. Those particles, in turn, are guided by magnets through a 200-metre-long, helium-filled tunnel, where they decay to produce muon neutrinos. That last step, at last, creates an intense neutrino searchlight that is aimed toward the main detectors in South Dakota.
As far as neutrinos are concerned, our planet is transparent
The DUNE searchlight will fire off neutrinos in pulses lasting just a millisecond or so, once every few seconds, over and over, for a decade or more. ‘They come in short bursts so that you can clearly identify them,’ Zeller says. Otherwise, DUNE’s sensitive detectors could confuse human-made neutrinos with the random ones that are flying around all the time from the rest of the Universe.
At the very beginning of their journey, just 574 metres from the source at Fermilab, the neutrinos get their first physical exam as they pass through a small experiment called the Near detector. Small is a relative term, in this case: the Near detector consists of a movable, 300-ton liquid argon neutrino collector located some 200 feet underground. A core part of its job is to measure the properties of the outgoing beam in support of the main DUNE experiment, but the Near detector will also perform scientific measurements of its own, mostly searching for a possible fourth type of neutrino called a sterile neutrino.
After fleeing the Near detector, the neutrinos keep going at nearly the speed of light, unwavering in their path, straight through Earth. As far as neutrinos are concerned, our planet is transparent. (‘We had a reporter from a tunnelling magazine who was very disappointed that we don’t need to make an 800-mile-long tunnel from Fermilab,’ Mossey says.) About 1 / 250 th of a second later, the particles reach their primary destination: DUNE’s humongous Far detector at the Sanford Underground Research Facility (SURF), located 1.5 kilometres beneath South Dakota, right next to the site of Davis’s old solar-neutrino experiment.
When a passing neutrino beats the astronomical odds and collides with an argon atom in DUNE’s detectors, it sets off a chain reaction. As with all things neutrino, the process is hardly straightforward. The neutrino stimulates the emission of one (or more) charged particle, typically a muon; those particles, in turn, knock loose a series of electrons from neighbouring argon atoms. Then a network of digital sensors detects the electrons, noting their precise energies and locations, to reconstruct the details of the original neutrino culprit in glorious 3D. In the end, humans will be able to ‘see’ neutrinos like never before.
For neutrino researchers who are used to waiting days or months for a single detection, DUNE will be a revelation, delivering perhaps 50 hits a millisecond. ‘It’s something of the likes we’ve not had available to us. You see all the different particle tracks originating from a neutrino colliding with an argon nucleus,’ Zeller says. Researchers will log each event and tag it: what type of neutrino was it; what was its energy; what were its properties?
DUNE will also create a physics community like none before. Davis operated nearly as a one-man band. The DUNE experiment will be more like a neutrino Woodstock. Mary Bishai of Brookhaven National Lab, who started working on DUNE before it even had its name and who now serves as an official spokesperson for the project, has been amazed – and slightly overwhelmed – to watch a sprawling collaboration grow around it. ‘You have 1,400 people, and everybody answers to a different university or lab. It’s a lot of interacting physicists, like herding cats,’ she says wryly. ‘What keeps us working together is the love of the science.’
A fter all the tunnelling, particle smashing and beam shooting is accomplished, the actual data output at the end of DUNE will essentially be an interference pattern of neutrino interactions – like waves rippling into each other on a lake, their crests adding to or cancelling each other – but constructed from swarms of invisible particles. The three flavours of neutrino will oscillate and interact with each other, creating analogous patterns of neutrino type and intensity. By adjusting DUNE’s beam and detectors, researchers will gradually map those patterns and build up the most complete picture ever of how neutrinos behave.
‘You create muon neutrinos at Fermilab. Then 5 per cent of them are going to oscillate into electron neutrinos, and the rest oscillate into tau neutrinos’ by the time they reach the Far detector, Sousa says. The resulting oscillation pattern, called ‘the mixing angle’, is between the three different neutrino identities. ‘DUNE will measure that mixing angle, and all the physics linked to it,’ Sousa explains. The mixing angle is a set of numbers that indicate how far nature deviates from a simple, mathematically idealised form of symmetry – the kind of sad symmetry that would have led to an empty, barren reality.
While DUNE is busy looking for oscillating neutrinos, it will be taking on many other physics jobs as well. It will search for hypothetical, heavy neutrinos that could explain the unseen dark matter that appears to outweigh all the visible matter in the Universe. These ‘sterile’ neutrinos, if they exist, must be even more inert than the regular kind; they could hide an entire shadow realm of previously unseen physics. The DUNE detectors will function as an ultra-sensitive neutrino telescope, a greatly enhanced version of the experiment Davis began at the same site in the 1960s. If a supernova blows up anywhere in our galaxy, scientists will know it, and will get a unique look into the heart of a dying star – a view that only neutrinos can provide. ‘We would learn so much,’ Sousa says.
DUNE seems like a modern cathedral – a place of congregation where people seek enlightenment
By the end of the DUNE run, sometime around 2040, we should learn how the three known types of neutrinos behave, how they interact, and what masses they have. We may learn the identity of the dark matter that binds together distant galaxies. Above all, we should have a true measure of mixing angles and CP violation, which will tell us if we’ve found the one-part-in- 10-billion imbalance that explains why we are here.
Particle physicists will be looking very closely at the exact values of the measured mixing angle for even deeper clues about our origin: not just why matter exists, but where it all came from. If neutrinos display just the right level of imbalance, that finding would support a wild theory called the ‘seesaw mechanism’, which posits that the neutrinos we see today are the meek descendants of ultramassive neutrinos that existed right after the Big Bang. Those bruisers would have been a quadrillion times as massive as a proton; as they decayed, they could have become the direct source of all the atoms that we see today. If so, we are all the progeny of neutrinos.
There would also be the philosophical and sociological satisfaction of drawing together theorists, technicians and hands-on particle physicists from more than 35 countries, working together for more than two decades, to solve an impractical but profoundly personal question: why is there something rather than nothing? ‘How much better a job could you possibly have than to answer a question that absolutely no one on the planet knows the answer to?’ Zeller asks. In that light, DUNE seems less like a piece of machinery than a modern cathedral – a place of congregation where people seek enlightenment, inspired by a shared goal that transcends individuals and that spans generations.
Mossey places the mystery of the neutrino on an even longer evolutionary timescale. ‘You’ve got 65 billion of them going through every square centimetre of your body every second, all your life, going back a 100,000 years to when the first humans walked the Earth,’ he says. ‘Something that ubiquitous has got to have an important role in the Universe, and somehow we’ve got to figure out what it is.’
This Essay was made possible through the support of a grant to Aeon Media from the John Templeton Foundation. The opinions expressed in this publication are those of the author and do not necessarily reflect the views of the Foundation. Funders to Aeon Media are not involved in editorial decision-making."
How Soviet communist philosophy shaped postwar quantum theory | Aeon Essays,,https://aeon.co/essays/how-soviet-communist-philosophy-shaped-postwar-quantum-theory,"The quantum revolution in physics played out over a period of 22 years, from 1905 to 1927. When it was done, the new theory of quantum mechanics had completely undermined the basis for our understanding of the material world. The familiar and intuitively appealing description of an atom as a tiny solar system, with electrons orbiting the atomic nucleus, was no longer satisfactory. The electron had instead become a phantom. Physicists discovered that in one kind of experiment, electrons behave like regular particles – as small, concentrated bits of matter. In another kind of experiment, electrons behave like waves. No experiment can be devised to show both types of behaviour at the same time. Quantum mechanics is unable to tell us what an electron is .
More unpalatable consequences ensued. The uncertainty principle placed fundamental limits on what we can hope to discover about the properties of quantum ‘wave-particles’. Quantum mechanics also broke the sacred link between cause and effect, wreaking havoc on determinism, reducing scientific prediction to a matter of probability – to a roll of the dice. We could no longer say: when we do this , that will definitely happen. We could say only: when we do this, that will happen with a certain probability.
As the founders of the theory argued about what it meant, the views of the Danish physicist Niels Bohr began to dominate. He concluded that we have no choice but to describe our experiments and their results using seemingly contradictory, but nevertheless complementary, concepts of waves and particles borrowed from classical (pre-quantum) physics. This is Bohr’s principle of ‘complementarity’. He argued that there is no contradiction because, in the context of the quantum world, our use of these concepts is purely symbolic. We reach for whichever description – waves or particles – best serves the situation at hand, and we should not take the theory too literally. It has no meaning beyond its ability to connect our experiences of the quantum world as they are projected to us by the classical instruments we use to study it.
Bohr emphasised that complementarity did not deny the existence of an objective quantum reality lying beneath the phenomena. But it did deny that we can discover anything meaningful about this. Alas, despite his strenuous efforts to exercise care in his use of language, Bohr could be notoriously vague and more than occasionally incomprehensible. Pronouncements were delivered in tortured ‘Bohrish’. It is said of his last recorded lecture that it took a team of linguists a week to discover the language he was speaking. And physicists of Bohr’s school, most notably the German theorist Werner Heisenberg, were guilty of using language that, though less tortured, was frequently less cautious.
It was all too easy to interpret some of Heisenberg’s pronouncements as a return to radical subjectivism, to the notion that our knowledge of the world is conjured only in the mind without reference to a real external world. It did not help that Bohr and physicists of Bohr’s school sought to shoehorn complementarity into other domains of enquiry, such as biology and psychology, and attempted to use it to resolve age-old conundrums concerning free will and the nature of life. Such efforts garnered little support from the wider scientific community and attracted plenty of opprobrium.
Albert Einstein famously pushed back, declaring that, unlike quantum mechanics, God does not play dice . He argued that, while quantum mechanics was undoubtedly powerful, it was in some measure incomplete.
In 1927, Bohr and Einstein commenced a lively debate. Einstein was joined in dissent by the Austrian physicist Erwin Schrödinger, who devised the conundrum of ‘Schrödinger’s cat’ to highlight the seemingly absurd implications of quantum mechanics. But although both Einstein and Schrödinger remained strident critics, they offered no counter-interpretation of their own. Despite their misgivings, there was simply no consensus on a viable alternative to complementarity.
C omplementarity also fell foul of the principal political ideologies that, in different ways, dominated human affairs from the early 1930s, through the Second World War, to the Cold War that followed. Both Bohr and Einstein were of Jewish descent and, to Nazi ideologues, complementarity and relativity theory were poisonous Jewish abstractions, at odds with the nationalistic programme of Deutsche Physik , or ‘Aryan physics’. But the proponents of Deutsche Physik failed to secure the backing of the Nazi leadership, and any threat to complementarity from Nazi ideology disappeared with the war’s ending. Much more enduring were the objections of Soviet communist philosophers who argued that complementarity was at odds with the official Marxist doctrine of ‘dialectical materialism’.
Vladimir Lenin, who had led the Bolshevik Party in the October Revolution of 1917, was a dogmatic advocate of the materialist worldview expounded by the German philosophers Karl Marx and Friedrich Engels, authors of The Communist Manifesto , first published in 1848. The world according to Marxism consists of objectively existing matter in constant motion, bound by laws. Such laws govern different levels of existence that we attempt to describe through different scientific disciplines that are not necessarily reducible one to another. For example, sociology – regarded as an empirical science – is not reducible to physics and is therefore bound by its own laws of human social and economic behaviour.
Marx and Engels observed that such behaviour breeds functional contradictions within an organised society. To survive, people submit to exploitative relationships with the means of economic production and those who own them. Distinct classes emerge: masters and their slaves, lords and their serfs, business owners (the bourgeoisie) and their low-wage workers (the proletariat).
It was not enough just to interpret the world, Marx claimed. Philosophers must also seek to change it
These functional contradictions are ultimately resolved through inevitable class struggle resulting in irreversible changes in social organisation and the means of production. The classical antiquity of Greece and Rome had given way to feudalism. Feudalism had given way to capitalism. And capitalism was destined to give way to socialism and communism, to the utopia of a classless society. But the necessary changes in social organisation would not happen by themselves. The path led first through socialism and the ‘dictatorship of the proletariat’, supported by an autocratic state that would eventually no longer be needed when the communist utopia was realised. For Lenin, the ends justified the means, which included the violent repression of bourgeois capitalist and counter-revolutionary forces.
In Marxist philosophy, the method of studying and apprehending both social and physical phenomena is dialectical, and the interpretation of natural phenomena is firmly materialistic. It was not enough just to interpret the world, Marx claimed. Philosophers must also seek to change it, and this could not be done in a world built only from perceptions and ideas. Any philosophy that sought to disconnect us from material reality, by reducing the world to mere sensation and experience, posed a threat to Marxism.
In Materialism and Empirio-Criticism (1909), Lenin had berated the physicist Ernst Mach and his Russian followers, and the German philosopher Richard Avenarius, who had formulated the positivist doctrine of empirio-criticism. The philosophy of positivism was anathema, as it sought to reduce knowledge of the world to sensory experience. Lenin argued that such thinking led only to a subjective idealism, or even solipsism. To him, this was just so much ‘gibberish’.
Complementarity looked just like the kind of positivist gibberish that Lenin had sought to annihilate. A reality accessible only in the form of quantum probabilities did not suit the needs of the official philosophy of Soviet communists. It appeared to undermine orthodox materialism. Nevertheless, an influential group of Soviet physicists, including Vladimir Fock, Lev Landau, Igor Tamm and Matvei Bronstein, promoted Bohr’s views and for a time represented the ‘Russian branch’ of Bohr’s school. This was not without some risk. Communist Party philosophers sought their dismissal, to no avail, largely because they could not agree on the issues among themselves.
T he situation in the Soviet Union changed dramatically a few years later. As his health declined, Lenin had tried to remove the Communist Party’s general secretary, Joseph Stalin, whom he deemed unfit for the role. But Stalin had been quietly consolidating his position and had placed loyalists in key administrative posts. After a brief power struggle following Lenin’s death in 1924, Stalin became supreme leader. In 1937-38, he tightened his grip by unleashing a reign of terror, known as the Great Purge, in which many of the old Bolsheviks who had fought alongside Lenin in 1917 were executed. Although the total death toll is difficult to determine, a figure of 1 million is not unreasonable. Physicists were not exempt. Bronstein was arrested, accused of terrorism offences, and executed in February 1938.
Stalin put his own stamp on the political ideology of Soviet communists in his short text titled Dialectical and Historical Materialism (1938), a formulation of Marxist philosophy that would be adopted as the official Communist Party line. Those intellectuals who resisted the official doctrine now faced real risks of losing more than just their jobs.
An outspoken commitment to complementarity became positively dangerous
The distractions of the Second World War meant that little changed for physicists until Andrei Zhdanov, the Party’s philosopher and propagandist-in-chief, who was thought by many to be Stalin’s successor-in-waiting, specifically targeted the interpretation of quantum mechanics in a speech delivered in June 1947. ‘The Kantian vagaries of modern bourgeois atomic physicists,’ he proclaimed, ‘lead them to inferences about the electron’s possessing “free will”, to attempts to describe matter as only a certain conjunction of waves, and to other devilish tricks.’ This was the beginning, writes the historian Loren Graham, ‘of the most intense ideological campaign in the history of Soviet scholarship’. An outspoken commitment to complementarity became positively dangerous.
Soviet physicists scrambled to defensible positions. Fock retreated from complementarity as an objective law of nature, and criticised Bohr for his vagueness. Others sought ways to ‘materialise’ quantum mechanics. Dmitry Blokhintsev, a student of Tamm’s, favoured a statistical interpretation based on the collective properties of an ‘ensemble’ of real particles. In such an interpretation we are obliged to deal with probabilities simply because we are ignorant of the properties and behaviours of the individual material particles that make up the ensemble. Einstein had used this conception in the opening salvo of his debate with Bohr in 1927. Yakov Terletsky who, like Tamm, had studied under the Soviet physicist Leonid Mandelstam, favoured a ‘pilot-wave’ interpretation of the kind that had initially been promoted by the French physicist Louis de Broglie before it was shot down by Bohr’s school in 1927. In this interpretation, a real wave field guides real particles, and probabilities again arise because we are ignorant of the details.
A s the 1930s progressed towards world war, many Western intellectuals had embraced communism as the only perceived alternative to the looming threat of Nazism. Numbered among the small group of Jewish communist physicists gathered around J Robert Oppenheimer at the University of California, Berkeley was David Bohm. As Oppenheimer began to recruit a team of theorists to work on the physics of the atomic bomb at the newly established Los Alamos National Laboratory in early 1943, Bohm was high on his list. But Bohm’s communist affiliations led the director of the Manhattan Project, Leslie Groves, to deny him the security clearance necessary to join the project.
Bohm was left behind at Berkeley and joined with his fellow communist and close friend Joseph Weinberg in teaching the absent Oppenheimer’s course on quantum mechanics. His long discussions with Weinberg, who argued that complementarity was itself a form of dialectic and so not in conflict with Marxist philosophy, encouraged him to accept Bohr’s arguments, although he was not free of doubt. In his textbook Quantum Theory (1951), derived in part from his experiences teaching Oppenheimer’s course, Bohm broadly adhered to Bohr’s views.
Bohm had by this time moved to Princeton University in New Jersey. Einstein, who in 1933 had fled from Nazi Germany to Princeton’s Institute for Advanced Study, asked to meet with him sometime in the spring of 1951. The meeting re-awakened the Marxist materialist in Bohm. As Einstein explained the basis for his own misgivings, Bohm’s doubts returned. ‘This encounter with Einstein had a strong effect on the direction of my research,’ he later wrote, ‘because I then became seriously interested in whether a deterministic extension of the quantum theory could be found.’ Was there, after all, a more materialistic alternative to complementarity? ‘My discussions with Einstein … encouraged me to look again.’ Although there is no documented evidence to support it, Bohm later claimed he had also been influenced ‘probably by Blokhintsev or some other Russian theorist like Terletsky’.
Bohm’s theory sought to restore causality and determinism to the quantum world
But Bohm’s relationship with Weinberg had by now returned to haunt him. In March 1943, Weinberg had been caught betraying atomic secrets by an illegal FBI bug planted in the home of Steve Nelson, a key figure in the Communist Party apparatus in the San Francisco Bay Area. This evidence was inadmissible in court. In an attempt to expose Weinberg’s betrayal, in May 1949 Bohm had been called to testify to the House Un-American Activities Committee, set up by the House of Representatives to investigate communist subversion in the US. He pleaded the Fifth Amendment, a standard means of avoiding self-incrimination, which only raised more suspicion.
Bohm was arrested, then brought to trial in May 1951. He was acquitted (as was Weinberg a couple of years later). Now caught in the anti-communist hysteria whipped up by Joseph McCarthy, Bohm lost his position at Princeton. Only Einstein tried to help, offering to bring him to the Institute. But its new director – Oppenheimer, now lauded as the ‘father of the atomic bomb’ and increasingly haunted by the FBI’s interest in his own Leftist past – vetoed Bohm’s appointment. Bohm left the US for exile in Brazil, from where he published two papers setting out what was, in effect, a re-discovery of de Broglie’s pilot-wave theory. The theory sought to restore causality and determinism to the quantum world and was firmly materialist. Oppenheimer rejected Bohm’s efforts as ‘juvenile deviationism’. Einstein, who had once toyed with a similar approach and might have been expected to be sympathetic, declared it ‘too cheap’.
Under a barrage of criticism, Bohm gained support from the French physicist Jean-Pierre Vigier, then assistant to de Broglie in Paris. He was just what Bohm needed: a resourceful theorist, a man of action, a hero of the French Resistance during the war, and a friend of the president of the Democratic Republic of Vietnam, Ho Chi Minh. Invited to join Einstein in Princeton, Vigier’s communist associations had led the Department of State to forbid his entry into the US. He worked with Bohm on another variation of the pilot-wave theory and persuaded de Broglie to rekindle his interest in it, sounding alarm bells among the Bohr faithful: ‘Catholics and communists in France are uniting against complementarity!’
B ut Bohm’s mission to restore materiality to quantum mechanics amounted to more than demonstrating the possibility of a deterministic alternative. In 1935, working with his Princeton colleagues Boris Podolsky and Nathan Rosen, Einstein had set up a stubborn challenge, a last throw of the dice in his debate with Bohr. In the Einstein-Podolsky-Rosen (EPR) thought experiment, a pair of quantum particles interact and move apart, to the left and right, their properties correlated by some physical law. Schrödinger invented the term ‘ entanglement ’ to describe their situation. For simplicity, we assume that the particles can have properties ‘up’ and ‘down’, each with a 50 per cent probability.
We have no way of knowing in advance what results we’re going to get for each particle. But if the particle on the left is found to be ‘up’, the correlated particle on the right must be ‘down’, and vice versa. Now, according to quantum mechanics, the entangled particles are mysteriously bound together no matter how far apart they get, and the correlation persists. Suppose the particles move so far apart that any message or influence sent from one cannot get to the other even if it travels at the speed of light. How then does the particle on the right ‘know’ what result we obtained for the particle on the left, so that it can correlate itself?
We could assume that when they are sufficiently far apart the particles can be considered separate and distinct, or ‘locally real’. But this conflicts with Einstein’s special theory of relativity, which forbids messages or influences from travelling faster than light, as Einstein himself explained: ‘One can escape from this conclusion only by either assuming that the measurement of [the particle on the left] (telepathically) changes the real situation of [the particle on the right] or by denying independent real situations as such to things which are spatially separated from each other . Both alternatives appear to me entirely unacceptable.’ (Emphasis added.) Particles that do not exist independently of each other are said to be ‘nonlocal’.
A prospective Soviet spy codenamed ‘Quantum’ attended a meeting at the Soviet embassy in Washington, DC
Einstein was known for his pacifist and Leftist inclinations. Podolsky was Russian-born, and Rosen was a first-generation descendant of Russian émigrés. Both of Einstein’s assistants were sympathetic to the Soviet cause. Six months after the publication of the EPR paper, Rosen asked Einstein to recommend him for a job in the Soviet Union. Einstein wrote to the chairman of the Council of People’s Commissars, Vyacheslav Molotov, praising Rosen for his talents as a physicist. Rosen was at first delighted with his new home, and soon he had a son. ‘I hope,’ Einstein wrote in congratulation, ‘that he too can help in furthering the great cultural mission that the new Russia has undertaken with such energy.’ But by October 1938 Rosen was back in the US, having discovered that his research did not prosper in the people’s paradise.
Podolsky had earned his PhD at the California Institute of Technology and had returned to the Soviet Union in 1931 to work with Fock and Landau (and the visiting English theorist Paul Dirac) at the Ukrainian Institute of Physics and Technology in Kharkiv. From there, he joined Einstein at the Institute in Princeton in 1933. Ten years later, a prospective atomic spy assigned the codename ‘Quantum’ by Soviet intelligence attended a meeting at the Soviet embassy in Washington, DC and spoke with a high-ranking diplomat. Quantum was seeking an opportunity to join the Soviet effort to build an atomic bomb, and offered information on a technique for separating quantities of the fissile isotope uranium-235 . He was paid $300 for his trouble. In Russian Foreign Intelligence Service (SVR) files made public in 2009, Quantum was revealed to be Podolsky.
B ohm examined the EPR experiment in considerable detail. He developed an alternative that offered the prospect of translation from a thought experiment into a real one. With the Israeli physicist Yakir Aharonov, in 1957 he sought to demonstrate that real experiments had in fact already been done (in 1950), concluding that they did indeed deny independent real situations to the separated particles, such that these cannot be considered locally real.
This was far from the end of the matter. Befuddled in his turn by Bohrian vagueness and inspired by Bohm, the Irish physicist John Bell also pushed back against complementarity and in 1964 built on Bohm’s version of EPR to develop his theorem and inequality. The experiments of 1950 had not gone far enough. Further experiments to test Bell’s inequality in 1972 and in 1981-82 demonstrated entanglement and nonlocality with few grounds for doubt.
It began to dawn on the wider scientific community that entanglement and nonlocality were real phenomena, leading to speculations on the possibility of building a quantum computer, and on the use of entangled particles in a system of quantum cryptography. The 2022 Nobel Prize in Physics was awarded to the three experimentalists who had done most to expose the reality of entanglement and its promise of ‘a new kind of quantum technology’. The projected value of the quantum computing industry is estimated to be somewhere between $9 billion and $93 billion by 2040. I doubt there is any other example in history of such a high-value industry constructed on a physical principle that nobody understands.
Marxism powered many objections to Bohr’s complementarity, and so helped to shape the development of postwar quantum mechanics. Soviet physicist-philosophers lent their support by finding positivist tendencies in Bohr’s teaching in conflict with dialectical materialism. Some sought an alternative materialistic interpretation. Podolsky and Rosen both admired the Soviet Union and in different ways sought to contribute to its mission. Bohm laboured at a time when there was little appetite for what many physicists judged to be philosophical, and therefore irrelevant, foundational questions. It says much about Bohm’s commitment that he resisted the temptation to leave such questions to play out in the theatre of the mind. The Marxist in Bohm sought not only to show that a materialistic alternative was possible, but also to find a way to bring the arguments into the real world of the laboratory.
It was not enough just to interpret the world. Bohm also sought to change it.
This essay is dedicated to the memory of my colleague, co-author and friend, John Heilbron, who died on 5 November 2023."
Why the empty atom picture misunderstands quantum theory | Aeon Essays,,https://aeon.co/essays/why-the-empty-atom-picture-misunderstands-quantum-theory,"The camera zooms in on the person’s arm to reveal the cells, then a cell nucleus. A DNA strand grows on the screen. The camera focuses on a single atom within the strand, dives into a frenetic cloud of rocketing particles, crosses it, and leaves us in oppressive darkness. An initially imperceptible tiny dot grows smoothly, revealing the atomic nucleus. The narrator lectures that the nucleus of an atom is tens of thousands of times smaller than the atom itself, and poetically concludes that we are made from emptiness.
How often have you seen such a scene or read something equivalent to it in popular science? I am sure plenty, if you are fans of this genre like me. However, the narrative is wrong. Atomic nuclei in a molecule are not tiny dots, and there are no empty spaces within the atom.
The empty atom picture is likely the most repeated mistake in popular science. It is unclear who created this myth, but it is sure that Carl Sagan, in his classic TV series Cosmos (1980), was crucial in popularising it. After wondering how small the nuclei are compared with the atom, Sagan concluded that
I still remember how deeply these words spoke to me when I heard them as a kid in the early 1980s. Today, as a professional theoretical chemist, I know that Sagan’s statements failed to recognise some fundamental features of atoms and molecules.
Yet his reasoning is still influential. While preparing this essay, I ran a poll on Twitter asking whether people agreed with Sagan’s quote above. Of the 180 voters, 43 per cent answered that they mostly agreed, and 27 per cent fully agreed. Google ‘atoms empty space’, and you will find tens of essays, blog posts and YouTube videos concluding that atoms are 99.9 per cent empty space. To be fair, you will also find a reasonable share of articles debunking the idea.
Misconceptions feeding the idea of the empty atom can be dismantled by carefully interpreting quantum theory, which describes the physics of molecules, atoms and subatomic particles. According to quantum theory, the building blocks of matter – like electrons, nuclei and the molecules they form – can be portrayed either as waves or particles. Leave them to evolve by themselves without human interference, and they act like delocalised waves in the shape of continuous clouds. On the other hand, when we attempt to observe these systems, they appear to be localised particles, something like bullets in the classical realm. But accepting the quantum predictions that nuclei and electrons fill space as continuous clouds has a daring conceptual price: it implies that these particles do not vibrate, spin or orbit. They inhabit a motionless microcosmos where time only occasionally plays a role.
Most problems surrounding the description of the submolecular world come from frustrated attempts to reconcile conflicting pictures of waves and particles, leaving us with inconsistent chimeras such as particle-like nuclei surrounded by wave-like electrons. This image doesn’t capture quantum theory’s predictions. To compensate, our conceptual reconstruction of matter at the submolecular level should consistently describe how nuclei and electrons behave when not observed – like the proverbial sound of a tree falling in the forest without anyone around.
H ere’s a primer on how to think of the fundamental components of matter: a molecule is a stable collection of nuclei and electrons. If the collection contains a single nucleus, it is called an atom. Electrons are elementary particles with no internal structure and a negative electric charge. On the other hand, each nucleus is a combined system composed of several protons and a roughly equal number of neutrons. Each proton and neutron is 1,836 times more massive than an electron. The proton has a positive charge of the same magnitude as an electron’s negative charge, while neutrons, as their name hints, have no electric charge. Usually, but not necessarily, the total number of protons in a molecule equals the number of electrons, making molecules electrically neutral.
The interior of the protons and neutrons is likely the most complex place in the Universe. I like to consider each of them a hot soup of three permanent elementary particles known as quarks boiling along inside, with an uncountable number of virtual quarks popping into existence and disappearing almost immediately. Other elementary particles called gluons hold the soup within a pot of 0.9 femtometres radius. (A femtometre, abbreviated fm, is a convenient scale that measures systems tens of thousands of times smaller than an atom. Corresponding to 10 ‑15 m, we must juxtapose 1 trillion femtometres to make one millimetre.)
Instead of localised bullets in empty space, matter delocalises into continuous quantum clouds
Particles with the same electric charge sign repel each other. So additional interactions are required to hold protons close-packed in the nucleus. These interactions arise from quark and antiquark pairs called pions that constantly spill out of each proton and neutron to be absorbed by another such particle nearby. The energy exchanged in this transfer is big enough to compensate for the electric repulsion between protons and, thus, bind together protons and neutrons, storing the immense energy that may be released in nuclear fission processes.
However, the extremely short lifetime of the pions limits how far protons and neutrons may be from each other, curbing the nucleus size to a 1 to 10 fm radius. Thus, from a particle perspective, the nucleus is tiny compared with an atom. A nitrogen nucleus, composed of seven protons and seven neutrons, has a radius of about 3 fm. In contrast, nitrogen’s atomic radius is 179,000 fm. At the scale of atoms and molecules, nuclei are no more than heavy, point-like positive charges without any apparent internal structure. So are the electrons: they are just light, point-like negative charges.
If atoms and molecules remained a collection of point-like particles, they would be mostly empty space. But at their size scale, they must be described by quantum theory. And this theory predicts that the wave-like picture predominates until a measurement disturbs it. Instead of localised bullets in empty space, matter delocalises into continuous quantum clouds.
M atter is fundamentally quantum. Molecules cannot be assembled under the rules of classical physics. The classical electrical interactions between nuclei and electrons are insufficient to build a stable molecule. Due to the electric attraction of charges of opposite signs, the negatively charged electrons would quickly spiral toward the positively charged nuclei and glue to them. The resulting combined particles with no net charge would fly apart, preventing any molecule from forming.
Two quantum properties avoid this bleak fate.
The first property arises from the Heisenberg uncertainty principle , which holds that a quantum particle cannot simultaneously be at a precise position and also have zero speed. This implies that an electron cannot glue to a nucleus because both particles would be in a well-defined place and at rest to each other – defying a central rule of the quantum world.
The second quantum property is the Pauli exclusion principle. The fundamental components of matter are split into two types, bosons and fermions. The gluons inside the proton are examples of bosons. We can have as many of them as we want, sharing the same position simultaneously. On the other hand, fermions – such as electrons, quarks, protons and neutrons – obey a much more restrictive rule named the Pauli exclusion principle: no two identical fermions can simultaneously occupy the same space and have the same spin (a quantum property analogous to a classical rotation of a particle about its axis).
In the quantum world, the wave function represents more than a mere lack of knowledge
With all those effects encoded into the Schrödinger equation, the master equation of quantum theory, it predicts that our point-like nuclei and electrons must, in fact, behave like waves. They delocalise in quantum clouds much bigger than their particle-picture size to satisfy the Heisenberg uncertainty principle, with electrons shaped into different clouds to satisfy the Pauli exclusion principle. The lighter the particles are, the bigger the delocalisation. Thus, a single electron cloud may spread over multiple nuclei, forming a chemical bond and stabilising the molecule.
Take an ammonia molecule, NH 3 , illustrated below. The small blue smudge in the middle is the nitrogen nucleus cloud, while the three green blobs are the proton (hydrogen nuclei) clouds. The 10 electrons of the ammonia molecule delocalise into the fat yellow cloud, tying the party together.
Figure 1: Electronic and nuclear quantum clouds in an ammonia molecule. The yellow cloud represents the 10 electrons in this molecule. The small blue cloud is the nitrogen nucleus, while the three green clouds indicate each hydrogen nucleus. Electronic points in front of the nuclei were made transparent so as not to hide the nuclear clouds. Technical details are explained in Toldo et al , 2023 . Courtesy the author
A particle-like nitrogen nucleus has a 3 fm radius. However, in the ammonia molecule, the nitrogen nucleus grows to a respectable 3,000 fm radius due to delocalisation. The delocalisation of the hydrogen nuclei is even more impressive. They grow from a radius of 0.9 fm when seen as particles to clouds of about 23,000 fm. But the electrons take the cake. Due to their tiny mass, they grow from particles much smaller than a nucleus into a cloud that defines the molecular volume.
Nuclei and electrons, however, are not atomic giants. If the nitrogen nucleus is measured (for instance, by throwing fast electrons against it and observing them bounce back), the nuclear cloud would immediately collapse into the initial 3 fm dot. The same is true for each electron.
Indeed, quantum theory prescribes a precise relationship between the wave and particle pictures. The clouds of the wave picture are mathematically described by a wave function, essentially an equation that attributes an intensity to every point in space and how these intensities change with time. The wave function is analogous to mathematical functions describing conventional sound or water waves, but with the peculiarity that it has an imaginary-number component , which is negative when squared.
The square of the wave function modulus (a mathematical operation that always yields positive numbers) gives the probability of finding the particle at each point in space if we attempt to observe it. The denser the cloud, the bigger the odds of observing the particle there. Thus, if we try to measure the point-like nitrogen nucleus, we are sure that it will be somewhere in the region of the delocalised nitrogen nucleus cloud, the blue smudge in the figure.
However, interpreting the quantum cloud as probability does not mean it is just a measure of a lack of knowledge about the system. If I left my keys in one of my jacket’s two pockets, but I am unsure which one, I may write a probability function with a 50 per cent value at each pocket and zero value at every other point of my office. This function obviously does not imply that my keys are delocalised over the two pockets. It just states my ignorance, which can be easily fixed by checking the jacket.
In the quantum world, the wave function represents more than a mere lack of knowledge. Delocalised systems – like nuclear and electronic clouds – cause phenomena that localised particles cannot explain. The existence of chemical bonds forming molecules is a direct example of the effect of electronic delocalisation. In the case of nuclear delocalisation, one of its main effects is to boost the chances of a hydrogen nucleus (a single proton) flowing from one molecule to another nearby. This kind of enhanced proton transfer has dramatic biological consequences, like increasing the acidity of specific enzymes compared with how acidic they would be if hydrogen nuclei behaved as particles.
A lthough electron clouds are commonly depicted in popular science and chemistry, delocalisation of the nucleus is often interpreted as vibrations and rotations. But these are only classical, albeit helpful, analogies. From a quantum perspective and for conceptual consistency, nuclei should be depicted on the same footing as electrons, as clouds as well.
Yet another misconception is that atoms are empty because their mass is in their nucleus. The atomic mass is indeed highly localised. In an ammonia molecule, 82 per cent of the mass is in the blue smudge of the nitrogen nucleus shown in Figure 1 above. If we add the masses of the three green proton clouds, they account for 99.97 per cent of the total. Thus, the big yellow cloud of the electrons carries only 0.03 per cent of the mass.
The association between this mass concentration and the idea that atoms are empty stems from a flawed view that mass is the property of matter that fills a space. However, this concept does not hold up to close inspection, not even in our human-scale world. When we pile objects on top of each other, what keeps them separated is not their masses but the electric repulsion between the outmost electrons at their touching molecules. (The electrons cannot collapse under pressure due to the Heisenberg uncertainty and Pauli exclusion principles.) Therefore, the electron’s electric charge ultimately fills the space.
Anyone taking Chemistry 101 is likely to be faced with diagrams of electrons orbiting in shells
In atoms and molecules, electrons are everywhere! Look how the yellow cloud permeates the entire molecular volume in Figure 1. Thus, when we see that atoms and molecules are packed with electrons, the only reasonable conclusion is that they are filled with matter, not the opposite.
Despite all this, anyone taking Chemistry 101 is likely to be faced with diagrams of electrons orbiting in shells, like concentric and separated layers with empty space between them. The idea that these diagrams represent physical reality is a third common misconception. Electrons do not literally orbit around the atomic nucleus in the shape of these shells.
In atoms and molecules, electrons must have specific energies, each energy associated with a particular cloud shape. Consider, for example, an atom with a single electron. In the lowest possible energy, the ground energy level, this electron delocalises into a spherical cloud, dense at the centre of the atom and gradually fading out. The single-electron wave functions describing these clouds are called orbitals.
At higher energy levels, the single electron delocalises into more complex clouds with nested spheres, multiple blobs or even doughnut shapes. Thus, when speaking of atoms and molecules, electrons are not little particles chaotically rocketing around the nuclei until they become a fuzzy cloud, as often depicted. And electrons are not in the orbitals, nor do they populate them. Electrons are the orbitals. They are delocalised clouds.
W ith multiple electrons, which have been terra incognita in popular science, things get much more complicated. This is hardly a surprise since even professional theoretical chemists are uncomfortable describing them, despite their exceptional competence in predicting the properties of multi-electron systems.
Like ill-fitting clothes, chemistry vernacular is filled with awkward analogies and descriptions. Chemists may say that an electron occupies or populates an orbital as if orbitals were pre-existing places where electrons are put. Chemists often draw diagrams where orbitals are represented as short horizontal lines and electrons as small vertical arrows on those lines, like objects on shelves. All these verbal and visual metaphors fail to translate what quantum theory tells us about atoms and molecules.
When dealing with multi-electron systems (encompassing virtually all molecules), quantum theory no longer distinguishes between each electron; they are all described by a single wave function, a single cloud. Nevertheless, single electron orbitals are still a valid approximation that chemists constantly use to rationalise chemical reactions. The multi-electron wave function resembles a composition of these individual clouds overlapping within the volume defining the molecule. They feel each other; they recombine into new shapes; some bulge and others shrink; the clouds skew, stretch and twist until they comfortably adapt, occupying every available space. It may look like a messy sock drawer.
For a fraction of a picosecond, the tempest rages and reshapes the molecular landscape until stillness is restored
A molecule is a static object without any internal motion. The quantum clouds of all nuclei and electrons remain absolutely still for a molecule with a well-defined energy. Time is irrelevant. Quantum theory does not predict vibrating nuclei or orbiting and spinning electrons; those dynamic features are classical analogues to intrinsic quantum properties. Angular momentum, for instance, which in classical physics quantifies rotational speed, manifests as blobs in the wave function. The more numerous the blobs, the bigger the angular momentum, even though nothing rotates.
Time, however, comes into play when a molecule collides with another one, triggering a chemical reaction. Then, a storm strikes. The quantum steadiness bursts when the sections of the electronic cloud pour from one molecule upon another. The clouds mix, reshape, merge, and split. The nuclear clouds rearrange to accommodate themselves within the new electronic configuration, sometimes even migrating between molecules. For a fraction of a picosecond (10 -12 seconds or a billionth of a millisecond), the tempest rages and reshapes the molecular landscape until stillness is restored in the newly formed compounds.
In the Flammarion engraving (Figure 2 below), a person at the edge of Earth dares to look beyond the firmament dome to uncover the marvellous machinery of clouds controlling the heavens. They could well be looking at a molecule instead. Then, this non-disturbing observer would find that nuclei and electrons are majestic, stable, structured, closed-packed clouds, driving every aspect of matter as we know it.
Figure 2: Wood engraving from L’atmosphère: météorologie populaire (1888) by Camille Flammarion. Courtesy Wikipedia
M y criticism of the empty atom picture isn’t meant to shame people’s previous attempts to describe atoms and molecules to the public. On the contrary, I applaud their effort in this challenging enterprise. Our common language, intuitions and even basic reasoning processes are not adapted to face quantum theory, this alien world of strangeness surrounded by quirky landscapes we mostly cannot make sense of.
And there is so much we do not understand. We have yet to learn how to reconcile the dual wave-like and particle-like behaviour of matter. We do not even know whether wave functions have objective reality. Our brains melt, facing the multiple potential interpretations of quantum theory to the point that outstanding scientists seemingly gave up hope that we may reach a scientific consensus. We turn a blind eye to the dirty tricks we carry from the conceptual construction of quantum theory to the actual predictions.
The account of the quantum molecular world I presented is on comfortably safe grounds
We could conform to the unsatisfying ‘Shut up and calculate!’ attitude that has accompanied the increasingly weird predictions of quantum theory, which enabled the outstanding technological advancements of the past 100 years, from lasers to microprocessors. However, we do not want to make only useful predictions. Our ultimate goal is to tell stories about our Universe. Thus, we calculate but do not shut up . Generations of scientists and science popularisers do their best to translate all this strangeness into friendly metaphors of a theoretical body still full of mystery. We build new mental images of the quantum world one step at a time, even under the risk of tripping up here and there.
The account of the quantum molecular world I presented is on comfortably safe grounds. It is based on a quantum theory domain that is highly consensual among specialists. It is the town square of what the Nobel laureate Frank Wilczek called the Core Theory, the physics framework describing fundamental particles, their interactions and Albert Einstein’s general relativity. Physicists are so confident about this core’s stability that they believe it should persist within any new theories of matter developed in the future.
Breathing this confidence and realising we are not made of empty space may be a soothing thought.
This Essay was made possible through the support of a grant to Aeon+Psyche from the John Templeton Foundation. The opinions expressed in this publication are those of the author and do not necessarily reflect the views of the Foundation. Funders to Aeon+Psyche are not involved in editorial decision-making."
"Our simple, magic-free recipe for quantum entanglement | Aeon Essays",,https://aeon.co/essays/our-simple-magic-free-recipe-for-quantum-entanglement,"Almost a century ago, physics produced a problem child, astonishingly successful yet profoundly puzzling. Now, just in time for its 100th birthday, we think we’ve found a simple diagnosis of its central eccentricity.
This weird wunderkind was ‘quantum mechanics’ (QM), a new theory of how matter and light behave at the submicroscopic level. Through the 1920s, QM’s components were assembled by physicists such as Werner Heisenberg and Erwin Schrödinger. Alongside Albert Einstein’s relativity theory, it became one of the two great pillars of modern physics.
The pioneers of QM realised that the new world they had discovered was very strange indeed, compared with the classical (pre-quantum) physics they had all learned at school. These days, this strangeness is familiar to physicists, and increasingly useful for technologies such as quantum computing.
The strangeness has a name – it’s called entanglement – but it is still poorly understood. Why does the quantum world behave this strange way? We think we’ve solved a central piece of this puzzle.
E ntanglement was first clearly described, and named, in 1935 , by the Austrian physicist Erwin Schrödinger. He pointed out that, after two quantum particles interacted, they could no longer be considered independent of each other, as classical physics would have allowed. As the contemporary US physicist Leonard Susskind puts it in the preface to Quantum Mechanics: The Theoretical Minimum (2014), ‘one can know everything about a system and nothing about its individual parts.’
Here’s a simple analogy. If we want to give a complete description of the present state of a two-handed poker game, for example, we just give a description of the two five-card hands. What could be more obvious? But in QM, for some reason, the obvious thing doesn’t work.
Schrödinger said that, in general, the quantum description of the two particles is ‘entangled’, and the name stuck. As he puts it: ‘When two separated bodies that each are maximally known come to interact, and then separate again, then such an entanglement of knowledge often happens.’
The full weirdness of entanglement wasn’t immediately obvious
Schrödinger concluded elsewhere that entanglement is not ‘ one but rather the characteristic trait of quantum mechanics.’ Many physicists now agree. Susskind says it is ‘the essential fact of quantum mechanics’, while in his Lectures on Quantum Mechanics (2013), Steven Weinberg writes that it is ‘perhaps its weirdest feature’.
The full weirdness of entanglement wasn’t immediately obvious, and Schrödinger himself didn’t quite live to see it. For him, its strangeness was the prohibition it imposed on describing a two-particle system by its parts. He thought that this had important consequences, especially because it debunked what had become the orthodox view of what QM is telling us about the microworld.
This orthodox view was the so-called Copenhagen Interpretation, proposed by the Danish physicist Niels Bohr. Bohr argued that it was nonsense to think of quantum systems as having definite properties, before they were measured. Like Einstein before him, Schrödinger thought that entanglement proved Bohr wrong.
T o grasp the Einstein-Schrödinger argument, consider the two poker hands, now with some of the cards face down, hidden from view. The state of this game can no longer be described in terms of the known cards (the ones turned face up). At least superficially, this looks like entanglement: a full quantum system can’t be described in terms of what’s known about its pieces.
Moreover, when an additional card on one side is revealed, it changes our knowledge about the other hand. If the queen of hearts turns up in the hand on the left, say, then we know that it is not one of the hidden cards in the hand on the right. The same is true for entangled particles. Observing one gives us new knowledge about the other, even if it is a long way away.
Einstein and Schrödinger argued that this meant that something is hidden inside these quantum systems prior to measurement – something not fully described by QM, and disallowed by Bohr’s view. They argued that, if measuring a nearby particle teaches us a new fact about a remote particle, this new fact must have existed already, even though the best QM description didn’t include it.
The alternative would be that the nearby measurement was changing the remote particle in some way. Schrödinger thought that this was absurd: ‘measurements on separated systems cannot affect one another directly, that would be magic [our emphasis].’
‘It provides a gentle pillow for the true believer from which he cannot very easily be aroused’
Schrödinger died in Vienna in 1961. Just three years later, the Northern Irish physicist John Stewart Bell argued that, if the predictions of QM are correct, then Schrödinger’s magic actually happens. When we have entangled particles, measurements on one of them can have a subtle effect on the other one, even though they might in principle be light years apart.
Bell called this magic nonlocality. These days it is often linked to Einstein’s phrase ‘spooky action at a distance’, though Einstein, too, didn’t live to see Bell’s result. (When Einstein complained about spooky action at a distance, in a 1947 letter to the physicist Max Born, he had in mind a different weird feature of the orthodox interpretation of QM.)
The importance of Bell’s argument took some time to sink in. The field had to first shake off some of what Einstein in 1928, writing to Schrödinger, called the ‘Heisenberg-Bohr tranquilising philosophy … so delicately contrived that, for the time being, it provides a gentle pillow for the true believer from which he cannot very easily be aroused.’
But gradually, in the second half of the quantum century, entanglement became one of the major concerns of the field. It is now absolutely central, theoretically, experimentally and, increasingly, technologically. Entanglement is what makes quantum computers different from their classical cousins, for example. A major motivation for this shift was Bell’s work. As the physicist Krister Shalm put it to Quanta Magazine in 2021: ‘The quantum revolution that’s happening now, and all these quantum technologies – that’s 100 per cent thanks to Bell’s theorem.’
B ell had argued that, if the QM predictions were correct, then nonlocality was unavoidable. But were the predictions correct? Answering that question required some very subtle and difficult experiments, involving two-particle systems similar to those that Schrödinger had discussed in 1935. Since they were inspired by Bell’s work, they came to be called ‘Bell experiments’.
Most Bell experiments use photons, the fundamental quantum components of light. Pairs of photons are produced together, with their properties entangled in the way that Schrödinger had described. Each photon is sent to one of two physicists, conventionally called Alice and Bob. Alice and Bob each choose one of several available measurements – this is called choosing a measurement setting .
Each measurement produces an outcome, which might be a 1 or a 0, depending on which way the photon emerges from the measuring device. Each run of the experiment thus produces four numbers: the two settings and the two outcomes. Repeated over and over, the experiment generates a long table of results, with these four numbers in each row.
Bell realised that these experimental results, as predicted by QM, looked quite strange. So strange, in fact, that with just a few additional assumptions, he could prove that the results were impossible . The primary assumption was that Schrödinger’s magic was not allowed – Bell called this assumption locality. So if QM’s predictions were correct after all, that would be bad news for locality (and good news for magic).
‘This is the real problem with quantum theory: the apparently essential conflict [with] fundamental relativity’
It took several decades, but we now know that QM is indeed correct. Some of the most convincing Bell experiments were conducted as recently as 2015 . In 2022, nicely timed for the decade of quantum centenaries, the Nobel Prize in Physics was awarded to three pioneers of these experiments: Alain Aspect, John Clauser and Anton Zeilinger. As the Nobel citation put it, the prize recognised their ‘experiments with entangled photons, establishing the violation of Bell inequalities and pioneering quantum information science.’
Combined with these experiments, Bell’s analysis seems to imply the kind of magical action at a distance that Einstein and Schrödinger considered absurd. One reason for thinking that it would be absurd was that it would seem to clash with a core principle of Einstein’s own theory of relativity – that nothing could go faster than light.
Bell was well aware of this tension, saying in 1984 that there was ‘an apparent incompatibility, at the deepest level’, between QM and relativity. ‘For me then,’ he said, ‘this is the real problem with quantum theory: the apparently essential conflict [with] fundamental relativity.’ Forty years later, this conflict has not been resolved.
The work of Aspect, Clauser and Zeilinger and many others certainly confirms that entanglement is real. As Aspect himself put it in his speech at the Nobel Prize banquet: ‘Entanglement is confirmed in its strangest aspects.’ But the experiments don’t tell us what entanglement is, or where it comes from. In that sense, entanglement remains as mysterious as ever. Why is the world put together in this weird way?
O ur research suggests a surprisingly simple answer. Our recipe for producing entanglement uses just four ingredients. All of these ingredients are available off the shelf (although admittedly, in one case, from a remote corner of the shelf). As far as we know, it has not previously been noticed that they can be combined in this way, to throw new light on the weirdest feature of the quantum world.
Let’s start with the main ingredient. Called collider bias, it is well known to scientists who use statistics in fields such as sociology, psychology and medicine. One of the first writers to describe it clearly was Joseph Berkson, a Mayo Clinic physicist, physician and statistician. In the 1940s, Berkson noted an important source of error in statistical reasoning used in medicine. In some circumstances, the selection of a sample of patients produces misleading correlations between their medical conditions.
Simplifying Berkson’s own example, imagine that all the patients admitted to hospital Ward C have similar symptoms, caused by one of two rare infections, Virus A or Virus B. Ward C specialises in treating those symptoms, so all its patients have at least one of these diseases. A few may have both, but everyone on the ward who doesn’t have Virus A is certain to have Virus B, and vice versa.
Taken at face value, these correlations might suggest that avoiding one virus causes infection with the other one. But Berkson pointed out that this apparent causal connection isn’t real. It is an artefact of the way the sample has been selected. The patients on Ward C are a very biased sample. In the general population, having a vaccine for Virus A won’t make you more likely to catch Virus B.
Figure 1: a simple collider
This means that if a patient on Ward C with Virus A says to himself: ‘I’m on Ward C, so, if I hadn’t caught Virus A, I would have caught Virus B,’ then he’s making a mistake. If he hadn’t caught Virus A then (most likely) he wouldn’t have either virus, and he wouldn’t have been admitted to the ward.
It may look like these causes are influencing one another, but they are not
This statistical effect is now called Berkson’s bias, or collider bias. The term collider comes from causal modelling, the science of inferring causes from statistical data. Causal modellers use diagrams called directed acyclic graphs (DAGs), made up of nodes linked by arrows. The nodes represent events or states of affairs, and the arrows represent causal connections between those events. When an event has two independent contributing causes, it is shown in a DAG as a node where two arrows ‘collide’. This is shown in Figure 1 above, where being admitted to Ward C has two contributing causes, from the two kinds of virus infection.
If we just look at a sample of cases in which the event at a collider happens, we’ll often see a correlation between the two independent causes. It may look like these causes are influencing one another, but they are not. It is a selection artefact, as causal modellers say. That’s collider bias. The correlation stems from the way in which the event at the collider depends on the two causes – in our simple example, it needed one cause or the other.
We want to take collider bias in the direction of physics – ultimately, in the direction of the experiments for which Aspect, Clauser and Zeilinger won their Nobel Prize. We want to propose an explanation for what may be going on in those experiments, and other cases of quantum entanglement.
We’ll get there via a series of toy examples. For the first of them, imagine that two physicists, Alice and Bob, play Rock, Paper, Scissors. For anyone who doesn’t know the rules of this game, at every turn, Alice and Bob each choose one of these three options, and send their calls to a third observer, Charlie. As in the usual version of the game, rock beats scissors, scissors beats paper, and paper beats rock. Charlie makes a list of the results: Alice wins, Bob wins, or it’s a draw.
Suppose that Charlie likes Alice and dislikes Bob. He therefore follows the policy of throwing away most of the results when Bob wins. In the remaining ‘official’ results, Alice wins a lot more often than Bob. The correlation looks the way it would if Alice actually had some influence over Bob’s choice – as though Alice choosing scissors makes it a lot less likely that Bob will choose rock, and so on. If Alice and Bob are far apart, this could look like Schrödinger’s magic. But there’s no real Alice-to-Bob causation involved. It is just collider bias at work. Given Charlie’s policy, the event at the collider – whether he retains or throws away the result – is influenced both by Alice’s choice and by Bob’s choice, giving us the same kind of converging arrows as in Figure 1 above.
Suppose that in a particular round of the game Alice chooses paper and Bob chooses rock. As in the medical case, Alice would be making a mistake if she says: ‘If I had chosen scissors instead, Bob would probably not have chosen rock.’ The right thing for her to say is: ‘If I had chosen scissors, then Charlie would probably have discarded the result – so my choice may have made a difference to Charlie’s decision, but it didn’t make a difference to Bob’s choice.’
N ow to our second ingredient. It is the least familiar of all, although it, too, is already on the shelf, if you know where to look. It doesn’t have an established name, outside of our own work. We call it constraining a collider. We’ll use the Rock, Paper, Scissors game to explain what it is.
In the version of the game just described, Charlie could favour Alice only by discarding some results. Let’s see what happens if we rig the game in Alice’s favour, without throwing any results away. In our world, this isn’t going to happen naturally so, for now, let’s imagine it happening supernaturally. Suppose God also likes Alice more than Bob, so he tweaks reality to give her an advantage. Perhaps he arranges things so she never loses when she plays the game on Sundays.
How does God do it? It doesn’t matter for our story, which doesn’t need to be realistic at this point, but here’s one possibility. In a so-called ‘deterministic’ universe, everything that happens is determined by the initial conditions at the very beginning of time. If God gets to choose the initial conditions, and (relying on his divine foreknowledge) knows exactly what follows from them, he can simply choose the initial conditions so that Alice never loses on Sundays.
Readers who prefer a God-free version could imagine that Alice and Bob live in a simulation, and that the artificial superintelligence (ASI) that runs the simulation favours Alice on Sundays. Some serious thinkers have suggested that we ourselves may live in a simulation, so it would be hasty to say that this version is inconceivable.
On Sunday Alice can’t lose, so if she had chosen scissors, Bob could not have chosen rock
Now we can explain our terminology. In a case like this, we say that God (or the ASI) constrains the collider – just on Sundays, in this version of the story. A collider is constrained if something prevents some of the possibilities that would normally be allowed (such as Bob winning, in our example).
To see what difference this makes, think about a round of the game where Alice chooses paper and Bob chooses rock. Is Alice still making a mistake if she says: ‘If I had chosen scissors instead, Bob would not have chosen rock’?
It now depends what day of the week it is. This is still a mistake on Monday through to Saturday. On those days, the right thing for Alice to say is: ‘If I had chosen scissors, Bob would still have chosen rock (and I would have lost).’ But Sunday is different. On Sunday Alice can’t lose, so if she had chosen scissors, Bob could not have chosen rock.
Let’s suppose that Alice knows that the game works this way. Perhaps she figured it out after years of experiments, and now makes a comfortable living as a gambler, working one day a week. From her point of view, it looks like she can control Bob’s choices (though only on Sundays). By choosing scissors, she can prevent Bob from choosing rock, and so on.
With a constrained collider, then, we would have something that looks a lot like causation across the collider, from one of the pair of incoming causes to the other. True, it would be a very strange kind of causality. For one thing, it would work the other way, too, from Bob to Alice (though less happily, from his point of view). By choosing rock on a Sunday, Bob could prevent Alice from choosing scissors, and so on.
For our purposes, it isn’t going to matter whether this would be real causality, or even whether the question makes sense. Could we still speak of both Alice and Bob as making free choices, for example, if the choices are linked in this way?
We think that entanglement itself is connection across a constrained collider
We take the following lesson from the example above: if natural causes constrained a collider, we should expect to find a new kind of dependence between the normally independent causes that feed into that collider. We call this new kind of relation connection across a constrained collider (CCC).
As we said, we invented the term ‘constrained collider’. As far as we know, the idea hasn’t been explicitly discussed before, in physics or in causal modelling. But it is already on the shelf, in the sense that there’s at least one place in physics where what we’re calling CCC has actually been proposed: it has been suggested as a key for solving the so-called black hole information paradox by the physicists Juan Maldacena and Gary Horowitz.
The background here is that Stephen Hawking discovered a process now called Hawking radiation , by which all black holes eventually evaporate away to nothing. He thought initially that this process would be random, preventing the escape of information that had fallen into the black hole in the first place. Some physicists disagreed, and in 1997, with Kip Thorne and John Preskill, Hawking made a public bet on the matter. Hawking and Thorne took one side (against the escape of information), and Preskill the other. (Hawking eventually conceded that Preskill had won.)
In 2004, Maldacena and Horowitz proposed a new way for information to escape from a black hole. In our new terminology, they suggested that a collider inside the black hole is constrained by a special ‘final state boundary condition’ at that point. They suggest that this creates a zig-zag causal path through time, along which information can escape from a black hole. In our terms, that would be a connection across the constrained collider.
Discussing the Maldacena-Horowitz hypothesis in 2021, the Cambridge physicist Malcolm Perry said :
Our proposal is that ‘such pathologies’ are exactly what’s been bothering us in QM, ever since 1935. We think that entanglement itself is connection across a constrained collider. To explain how that can be the case, and to introduce our two remaining ingredients, we need to get closer to the physics of the quantum world.
A s noted, many Bell experiments have now confirmed the strange correlations, predicted by QM showing the quantum world is unavoidably nonlocal. Given that these so-called Bell correlations were important enough to win Nobel Prizes, readers may be surprised to learn that they can easily be reproduced in a version of our Rock, Paper, Scissors game. The only change we need is to have Alice and Bob each flip a coin before they make their choice.
In this variant – let’s call it quantum Rock, Paper, Scissors – Alice and Bob each send two pieces of information to Charlie: their choice of rock, paper or scissors, and the result of their coin flip. So Charlie gets four values, two choices and two coin outcomes. This is precisely the same amount of information generated in each run of a Bell experiment.
In quantum Rock, Paper, Scissors, it is very easy for Charlie to set up a filter, keeping some results and throwing away others, to make sure that the set of results he keeps satisfies the Bell correlations. By using the right filter, Charlie can ensure that the selected results look exactly like the data generated in real Bell experiments. To match one kind of Bell experiment, for example, Charlie’s filter specifies that, when the settings are the same, the two outcomes must be different; and that, when the settings are different, the outcomes are the same 75 per cent of the time.
This doesn’t mean that there is any sort of strange nonlocal magic in quantum Rock, Paper, Scissors, of course. As in the earlier version, the correlations are simply a selection artefact, a result of collider bias.
There is one big difference between quantum Rock, Paper, Scissors and real Bell experiments
We could reintroduce God or an ASI at this point, to add a constrained collider to quantum Rock, Paper, Scissors. There would be one interesting difference from the original game. In that case, the effect of the constraint was to give Alice and Bob control over each other’s choices, making it hard to maintain that they both had freedom to choose. In quantum Rock, Paper, Scissors, as in the analogous real Bell experiments, that problem goes away: Alice and Bob each get some influence over the result of the other’s coin toss, but we can still treat both of their own choices as completely free.
There is one big difference between quantum Rock, Paper, Scissors and real Bell experiments, however, that we haven’t yet mentioned. In quantum Rock, Paper, Scissors, Alice and Bob send their choices to Charlie after they are made. In a spacetime diagram with time running up the vertical axis, the structure looks like an upside-down letter V – see the left-hand side of Figure 2 below. We’ll say that cases like this are ‘∧-shaped’.
In real Bell experiments, Alice and Bob receive their particles from the source, which emits them earlier in time. So the structure looks like ∨, as in the right-hand side of Figure 2 – we’ll say that they are ‘∨-shaped’.
Figure 2: the difference between ∧-shaped and ∨-shaped experiments
Can we flip quantum Rock, Paper, Scissors to make it ∨-shaped as well? It might look easy. We can have Charlie toss the two coins and send them to Alice and Bob, so that the results (heads or tails) become Alice and Bob’s measurement outcomes.
But if that’s all we do, Charlie won’t know what choices Alice and Bob are going to make when he sends out the coins. That means there’s no way for him to put bias into the results, in the way that he could in the ∧-shaped case. There’s no way that Charlie can produce the Bell correlations, in other words.
But suppose we let Charlie know in advance what choices Alice and Bob are going to make – we give him a crystal ball, say. Then it is very easy for him to manage the coins so that the net results, gathered over many plays of the game, satisfy the Bell correlations. The trick is for Charlie to toss one coin, and then choose the result for the other coin based on a rule that takes into account Alice and Bob’s future choices. The rule he needs is the same as in the ∧-shaped version of the game. When Alice and Bob’s settings are the same, he sends them different coin results; when the settings are different, he sends the same coin results 75 per cent of the time.
Let’s ask the same question we did about the ∧-shaped version. Does the new ∨-shaped case involve some kind of nonlocal magic from Alice to Bob, and vice versa?
We hope that readers will be inclined to say ‘No’ to this question. After all, the basic causal structure of the new ∨-shaped version is something like Figure 3 below. Thanks to Charlie’s crystal ball and the preset rules, Alice’s and Bob’s choices both influence Charlie’s outcomes, in every case. This means that Charlie’s selection procedure is a collider, and we have to be on our guard for collider bias.
Figure 3: a past collider
For this reason, attentive readers might suspect that collider bias plays the same role in explaining the results of the new ∨-shaped quantum Rock, Paper, Scissors as it did in the ∧-shaped case. But there’s one very big difference between these two cases – which brings us to our third ingredient – something we call ‘initial control’.
I n the ∧-shaped version of quantum Rock, Paper, Scissors, Charlie had to throw away results he didn’t want. But in the ∨-shaped case, he gets to choose the results in light of what he learns from the crystal ball. He’s arranging the coins in exactly the pattern he wants, not achieving the pattern by discarding a lot of cases that don’t fit. In this case, then, Charlie himself can constrain the collider, no gods or ASI needed.
What Charlie needs to do this is an ordinary ability we take for granted, to control the so-called ‘initial conditions’ – the way things are set up at the beginning of the experiment. This familiar ability is our third ingredient. Let’s call it initial control.
Perhaps we shouldn’t take initial control for granted. It is actually a remarkable ability, one that depends on the fact that we live in a place where abundant energy can be harnessed by creatures like us to do work. Living on a cool planet next to a hot star is much like living at the base of a giant waterfall. It’s easy to harness the passing flow of energy, just as life on Earth has been doing for billions of years.
Like all the complicated ways in which terrestrial creatures control their environment, the ability of human scientists to control experiments depends on harnessing this energy flow. But, like the natural flow of heat between different-temperature objects, it works only one way. We have much more control over the initial conditions of experiments than over their final conditions. It’s easy to arrange the balls on a pool table into precise positions before the initial break, for example, but virtually impossible to play the game so that they all end up in those positions.
It doesn’t have the relativity-challenging character normally associated with Schrödinger’s magic
The combination of the collider structure in Figure 3 above and the constraint provided by initial control gives us CCC – connection across the collider. If we are happy to use causal language, we can say that it gives us the kind of zig-zag causal connection shown in Figure 4 below. There’s also a zig-zag path from Bob’s choice to Alice’s outcome, of course.
Figure 4: the Parisian Zig Zag
But does ∨-shaped quantum Rock, Paper, Scissors involve some kind of nonlocal magic from Alice to Bob, and vice versa? At this point, we need to be careful about what we mean by nonlocality. As we have just seen, there is indeed some influence, or connection, from Alice to Bob, and vice versa – it is CCC. Since they are at a distance from each other, and a direct connection might need to be faster than light, we might still want to call it nonlocality. (One of last year’s Nobel laureates told us he thought such a zig zag should still count as a nonlocal effect.)
However, the connection between Alice and Bob is indirect, and depends entirely on processes that don’t themselves require anything faster than light. So, whatever we call it, it doesn’t have the relativity-challenging character normally associated with Schrödinger’s magic. And it is not very mysterious: we know exactly what it is, namely, connection across a constrained collider.
The crystal balls were magic, of course, but, once we gave ourselves those, the explanation of the connection between Alice and Bob is straightforward. Imagine if something like this could explain the results of real Bell experiments – that would be a nail in the coffin of the quantum spooks.
To make this work, we need our final ingredient. It is retrocausality, the idea that causality might work backwards in time, from future to past. In ∨-shaped quantum Rock, Paper, Scissors, we gave Charlie a crystal ball, to allow causation to work backwards – in other words, to allow Alice and Bob’s choices to feed into the rule Charlie uses to select the measurement outcomes.
This zig-zag path would avoid the kind of faster-than-light magic that Einstein and Schrödinger objected to
In the real world, of course, we don’t find magical crystal balls on any actual shelf. In the quantum world, however, retrocausality is an old and familiar idea. In that sense, it is certainly available off the shelf. It was first proposed in the late 1940s by the Parisian physicist Olivier Costa de Beauregard. He was a graduate student of the French physicist Louis de Broglie, another of the 1920s pioneers. In his own PhD thesis in 1924, de Broglie had proposed that all particles can behave like waves. Just five years later, after experiments had confirmed it, this won him the Nobel Prize.
Costa de Beauregard spotted a loophole in the Einstein-Schrödinger argument from 1935. Schrödinger had said that ‘measurements on separated systems cannot affect one another directly, that would be magic’. Costa de Beauregard pointed out that they might affect each other indirectly, via the kind of zig-zag path shown in Figure 4 above. (That’s why we called it the Parisian Zig Zag.)
This zig-zag path would avoid the kind of faster-than-light magic that Einstein and Schrödinger objected to. But it would still undermine the Einstein-Schrödinger argument against Bohr. If the reality on Bob’s side of the experiment can depend on Alice’s choice of measurement, we’re not entitled to assume that it would have been there anyway, even if Alice had done something else.
Later, after Bell’s work in the 1960s, Costa de Beauregard proposed that the zig zag could explain the strange Bell correlations, without relativity-threatening nonlocality.
R etrocausality remained a niche idea in QM for many years, though it has long had some distinguished proponents. In the 1950s, one of them, at least briefly, was the British physicist Dennis Sciama, who taught an astonishing generation of physicists, including Hawking. Sir Roger Penrose, himself a recent Nobel laureate, has long been sympathetic to the idea, as he argued in his chapter for the collection Consciousness and Quantum Mechanics (2022), edited by Shan Gao. There’s a story from the 1990s of Penrose drawing a zig zag at a quantum workshop at the Royal Society in London, and joking: ‘I can get away with proposing this kind of thing, because I’m already a Fellow here.’ (Now that he has a Nobel Prize, it is even easier, presumably!)
More recently, we ourselves have written about the advantages of retrocausal approaches to QM, both in avoiding action at a distance, and in respecting ‘time-symmetry’, the principle that the microworld doesn’t care about the distinction between past and future. But an additional striking advantage of retrocausality seems to have been missed. It suggests a simple mechanism for ‘ the characteristic trait of quantum mechanics’ (Schrödinger), ‘its weirdest feature’ (Weinberg) – in other words, for the strange connections between separated systems called quantum entanglement.
Starting with retrocausality, our recipe goes like this, in four easy steps:
Taken together, these steps suggest a simple explanation for the Parisian Zig Zag, and the strange connections in the quantum world required by entanglement: it is connection across constrained colliders, where the colliders result from retrocausality and the constraints from ordinary initial control of experimental setups.
We don’t mean that it is a trivial step from ∨-shaped quantum Rock, Paper, Scissors to real Bell experiments. But this toy example demonstrates that the combination of retrocausality and initial control can give rise to a connection between separated systems that looks very similar to entanglement. In our view, this is such a striking fact – and entanglement is otherwise such a strange and mysterious beast – that we propose the following hypothesis:
If this hypothesis turns out to be true, then in place of Schrödinger’s magic we’ll get something that works like Costa de Beauregard’s zig zag. That’s just what connection across a constrained collider does: it makes a zig zag from two converging arrows.
It will still be true that QM gives us a new kind of connection between the properties of distant systems. Bell experiments provide very convincing evidence that quantum entanglement is a real phenomenon. But it would no longer look mysterious – any world that combines retrocausality and initial control would be expected to look like this.
QM has been built on the idea that there are limits to what it is possible to know about physical reality
Finally, a note for readers who are worried that the cure is worse than the disease – that retrocausality opens the door to a menagerie of paradoxes and problems. Well spotted! For one thing, the crystal balls give Charlie options much like those of the famous time-traveller, meeting his own grandfather long before his parents met. What’s to stop him from interfering with the course of history, say by bribing Bob to make a different choice than the one shown in the crystal ball? (In the causal loop literature, this is called ‘bilking’.)
Also – less dramatic, maybe, but especially interesting in comparison to QM – the crystal balls allow Alice and Bob to send messages to Charlie, and hence potentially, with his help, to signal to each other. This isn’t possible in real Bell experiments, where Alice and Bob can’t signal to each other, despite having some influence on each other’s measurement outcomes. So isn’t this bad news for retrocausality?
These are good objections, but it is easy to modify the ∨-shaped quantum Rock, Paper, Scissors game to avoid them. We just need to split Charlie’s functions into two parts. Most of what he does gets replaced by a simple algorithm, inside a black box, that takes in information about the two future measurement settings, and spits out the two measurement outcomes.
Charlie himself can’t see inside the black box, and doesn’t have access to the future settings. But he still has a vital job to do. The box has a knob on the front, with a small number of options. Charlie controls that knob, and if he wants the device to produce the Bell correlations, he needs to choose the right option. In the terminology of QM, that’s called ‘preparing the initial state’.
If that’s all that Charlie does, and the quantum black box takes care of the rest, the door to the menagerie is closed. Alice and Bob can no longer signal to Charlie, or to each other. Everything works as in orthodox QM, except that we now have the prospect of an explanation for entanglement.
This means that if nature wants retrocausality without signalling into the past, and the paradoxes it would lead to, it needs black boxes – places in nature where observers like Charlie can’t see the whole story. In normal circumstances, such black boxes would seem like another kind of magic. Charlie is a clever guy, after all. What’s to stop him from taking a peek inside?
The answer, in the quantum case, is Heisenberg’s uncertainty principle, from 1927. Ever since then, QM has been built on the idea that there are limits to what it is possible to know about physical reality. This is just the veil of ignorance we need, to allow retrocausality in QM without threatening anybody’s grandparents. As Adam Becker put it in the New Scientist in 2018:
It may seem just too convenient, that one curious feature of quantum theory allows a paradox-free version of another curious feature. But in the real world, every piece of stage magic has a coherent explanation underneath. Often that explanation combines various components in surprising ways: stage magic wouldn’t be magic if it was obvious how it worked.
We’ve seen that quantum entanglement looked like magic, by the standards of some of the pioneers who discovered it. It still looks very strange, even to the physicists who have just won Nobel Prizes for proving that it is real. Any coherent explanation of it seems likely to combine some unexpected elements, and to require a careful analysis of how causes interact with each other, down at the level where we can’t see all the effects. The biggest surprise, in our view, is how few ingredients the explanation seems to need – and how simple the recipe is for putting them together."
Multiple worlds has been given artistic impetus by physics | Aeon Essays,,https://aeon.co/essays/multiple-worlds-has-been-given-artistic-impetus-by-physics,"When I was in my mid-30s, I was faced with a difficult decision. It had repercussions for years, and at times the choice I made filled me with regret. I had two job offers. One was to work at a very large physics experiment on the West Coast of the United States called the National Ignition Facility (NIF). Last year, they achieved a nuclear fusion breakthrough. The other offer was to take a job at a university research institute. I agonised over the choice for weeks. ­There were pros and cons in both directions. I reached out to a mentor from graduate school, a physicist I respected, and asked him to help me choose. He told me to take the university job, and so I did.
In the years to come, whenever my work seemed dull and uninspiring, or the vagaries of funding forced me down an unwelcome path, or – worse – the NIF was in the news, my mind would turn back to that moment and ask: ‘What if?’ Imagine if I were at that other job in that other state thousands of miles away. Imagine a different life that I would never live.
Then again, perhaps I had dodged a bullet, who knows?
Every life contains pain. Even the perfect life, the life where you have everything you want, hides its own unique struggles. Writing in The Genealogy of Morals (1887), Friedrich Nietzsche said: ‘Man, the bravest animal and most prone to suffer, does not deny suffering as such: he wills it, he even seeks it out, provided he is shown a meaning for it, a purpose of suffering.’ A life apparently perfect but devoid of meaning, no matter how comfortable, is a kind of hell.
In our search for meaning, we fantasise about the roads not taken, and these alternative lives take on a reality of their own, and, perhaps, they are real. In his novel The Midnight Library (2020), Matt Haig explores this concept. In it, a woman named Nora Seed is given the chance to live the lives she would have lived had she made different choices. Each life is a book in an infinite library. Opening the book takes her to live in that other world for as long as she feels comfortable there. Each possible world becomes a reality.
F or centuries, philosophers have dreamed of possible worlds. But only with the advent of quantum physics and the need to interpret its counterintuitive predictions did it appear that these possibilities might be real. Introduced in the 1950s by a graduate student, Hugh Everett , to little fanfare, and promoted in the 1970s by the physicist Bryce DeWitt, the ‘many-worlds’ interpretation of physics has captured the public imagination and flowered a burst of art and culture. Born out of a need to interpret the behaviour of the smallest building blocks of our Universe, quantum physics has powered a cultural conversation from the depths of academic philosophy and science, to the pinnacle of Hollywood’s elite.
The modern concept of possible worlds is attributed to the German polymath, co-inventor of calculus, and rival to Isaac Newton, Gottfried Wilhelm Leibniz , in his work Theodicy: Essays on the Goodness of God, the Freedom of Man, and the Origin of Evil (1710). The phrase ‘best of all possible worlds’ comes from this work and refers to Leibniz’s attempt to solve the problem of evil by proposing that ours is the best possible world. In other words, any other possible world would contain more evil.
‘Could Socrates have been an alligator?’ Yes. His being a human is not necessary but contingent
Leibniz drew on the work of the 16th-century Spanish Jesuit priest Luis de Molina, who posited that God contains ‘middle knowledge’, the knowledge of what a person would do if placed in a given situation. In any given possible world, a person’s actions are fixed but, from one world to another, they may act differently because of changes in their life circumstances. Hence, God gives us a kind of free will, which is essential to holding us responsible for our actions but, by his middle knowledge, places us in the best possible world for the greatest number of people; in this world, our choices are predetermined. Molina’s theology proposes that even God requires some people to damn themselves to save others.
The contemporary American analytic philosopher Alvin Plantinga drew on Leibniz’s theological ideas to produce his seminal work on possible worlds, The Nature of Necessity (1974). As in Haig’s novel, Plantinga conceives of a library of books, each corresponding to a possible world. There, he defines a book on a world as everything that is true, including everything necessary (meaning true across all worlds) and everything that is contingent (meaning true only in some worlds). Each world has one, and only one, book of true things.
Plantinga illustrates the difference between necessary and contingent truths in this way: ‘Could Socrates have been an alligator?’ Yes. There may be a possible world where Socrates wakes up, as in Franz Kafka’s novella The Metamorphosis (1915), to find his body to be that of an alligator. Thus, Socrates being a human being is not necessary but contingent. It is not true in every book in the library. On the other hand, mathematical implications like 1 + 1 = 2 and logical proofs are true in all worlds. They are necessary.
D espite considering many possible worlds, like Leibniz and Molina, Plantinga asserts that there is only one real world. For him, alternative worlds are useful for philosophers to think about but do not actually exist.
The many-worlds interpretation (MWI) of quantum physics, on the other hand, says that all possible worlds exist, and the one we live in is no different from any of the others. According to one form of this belief, somewhere out there is an exact duplicate of you, your house, your family, but one small detail is different, perhaps something as tiny as a stray photon that went left instead of right, or maybe something big like you have a different significant other. Maybe a stray cosmic ray hit your DNA before you were born, and you have red hair instead of brown, or you developed a serious birth defect. Maybe you don’t exist at all.
To the layperson, the idea of all these worlds existing out there might seem disturbing because it takes away from our own personal uniqueness. To philosophers like Plantinga, it is disturbing because it takes away from the uniqueness of truth.
A good example is Schrödinger’s cat. In this classic thought experiment, a cat is placed in a box and the lid closed. Say I also put in the box a semi-reflective mirror that has a 50 per cent chance of letting light through, and a 50 per cent chance of deflecting light. Behind the mirror is Detector D (for ‘Death’), which can detect even a single photon of light and, if it does, it sends a signal that opens the lid of a vial of poison, filling the box with poison gas and killing the cat. Next to the mirror is Detector L (for ‘Life’), not hooked up to any poison. An automatic emitter inside the box is programmed to fire a single photon at the mirror at a certain time. We don’t know which detector it will hit because it is random. Once it does, we wait a minute to ensure that the poison has had its effect.
Both are still possible – a single world containing two contradictory facts
If the box is completely sealed and impenetrable by anything external, we won’t know what happened inside until we open it.
All this seems very ordinary until I take the quantum nature of light into account. A quantum particle, experimental science has shown, can be in two states at once until it is measured. Thus, when the photon is fired at the mirror, it does not go through or deflect. Rather, it enters a state where, having gone through and having been deflected are both still possible – a single world containing two contradictory facts. These facts are, hypothetically, passed on to the cat, although nothing as large and complex as a warm-blooded animal could be put into such a state in practice.
We know this is true for particles because of what physicists call the double-slit experiment . In it, a single beam of light is sent through two slits in a barrier to a screen on the other side. Even though the light originates as a straight beam, after it passes through the two slits, it emerges as two interfering waves hitting the screen together. This looks like alternating bars of light and dark.
We want to know if light is made of particles or a continuous wave. To do so, we fire the smallest amount of light we can, which are little packets called photons, at the double slit. We hypothesise that if these appear at individual points, then photons are particles; but if they appear spread across the screen, then photons are waves. We begin the experiment and see immediately that the photons appear at individual points on the screen: score 1 for particle hypothesis. If we continue firing photons, however, we find that the dots appear in the same alternating light and dark bars as if the photons were interfering with each other. Score 1 for the wave hypothesis.
The reason this happens is because, when the photon goes through the barrier, it enters what physicists term a superposition where it has, in a sense, passed through both slits at the same time, like a wave, but arrived at one point on the panel, like a particle. This is called wave-particle duality.
In standard interpretations of quantum physics, we do not say that the photon has passed through both slits at the same time; rather, we say that its wavefunction – a kind of probability field – has passed through both slits at the same time. That wavefunction then ‘collapses’ or vanishes, leaving the one photon on the panel. This resolves the contradiction neatly because we can assert that ‘the photon entered the left slit’ and ‘the photon entered the right slit’ are never simultaneously true. Rather, we say the wavefunction passed through the slits and collapsed into the photon’s position on the screen.
According to the MWI of quantum physics, however, the entire wavefunction is a spectrum of alternative realities coexisting. These worlds are all connected and the photons in them interact weakly before they are measured – but the very act of measurement causes them to either split apart or appear to do so. When that split happens, copies of you and the rest of the Universe split apart as well.
T he MWI is controversial and is itself subject to interpretation depending on whether you believe there is a quantum mechanism for world splitting, or if it is simply how human beings experience quantum phenomena.
Real or not, possible worlds explain strange quantum paradoxes. For example, in the double-slit experiment, if I place a detector in front of each slit, it will detect only a single photon going through one or the other. Never both. If I take the detectors away, I get the interference pattern as if the photon went through both slits. This creates a paradox. Why can it be one way when I measure, and another when I don’t?
This doesn’t happen in classical physics. If I shoot an arrow at a bullseye, I can be absolutely certain that the arrow will follow a single trajectory from my bow to the target, whether I watch it fly or not. If I don’t watch it but imagine a world where I did, that is called a counterfactual world. In classical physics, counterfactual worlds and real worlds are always the same but in quantum mechanics they are not. The world is really different if I look at a particle flying through space versus if I do not.
Physicists knew this to be true in the 1920s, but it took more than 60 years before anyone proposed a way to split the difference between looking and not looking. In 1988, the physicists Yakir Aharonov, David Albert and Lev Vaidman introduced such a method, called ‘weak measurements’. These measurements collect some information about particles and, over the course of many, many measurements, can give us statistical information that helps us understand what is going on inside a quantum superposition.
We are more like two-dimensional beings in a 3D world , perceiving only our little slice
Weak measurements let us detect traces of particles even when they are not present . If there is a trace of a particle, that means it had some measurement effect but was not necessarily there in any real sense. This is what researchers see during the double-slit experiment. A particle has a trace from both slits because of the pattern on the screen but has no presence in either. If a particle is present, that would be ascertained through a strong measurement where it is localised, literally appearing on a detector screen.
The MWI interprets trace and presence in a unique way. A trace is when particles in different worlds have not been measured strongly enough to stop interacting, so the worlds are not ‘split’. When the worlds cease interacting (split), then trace becomes presence.
Real-world studies of ‘weak measurements’ have been designed with atoms, photons and other elements of the quantum world. For example, a lens can deflect photons in a laser slightly and cause them to interfere differently with another beam of photons than if the lens is not present. You can imagine, therefore, if you were to put lenses in front of the slits, they would have a measurable effect but, if the deflection is very slight, it would not be enough to collapse the wavefunction or split the worlds. Using that fact, you can construct experiments that allow you to see traces without presence.
Real experiments measure bizarre effects inside superpositions. For example, experiments with both photons and atoms have been done that show that sometimes a particle duplicates so that it can be in two places at once but each with 100 per cent probability, not the 50 per cent probability of the double slit. The particle will compensate by spawning a ‘negative’ copy of itself, also with 100 per cent probability, somewhere else, so that the total still adds up to one.
These results are counterintuitive unless you believe the wavefunction is a real thing, in which case the particle is a wavefunction that has 100 per cent probability peaks in two spots and a (-100 per cent) trough in another.
For this reason, some flavours of the MWI, such as Vaidman’s, maintain the primacy of the wavefunction over the concept of having multiple ‘copies’ of the world that split. In other words, the multiverse isn’t many worlds but one world, and we are more like two-dimensional beings in a 3D world, perceiving only our little slice. Worlds are like pieces in a jigsaw puzzle, fitting together in a commonsense way when together, but defying intuition when left apart.
T his suggests that our lives too might be a jigsaw puzzle. Perhaps they make sense only when we look at them across a multiverse of possible lives and, if we could only talk to those other copies of ourselves, we could understand our experiences. Consider that, when we imagine ourselves in other possible worlds, we don’t just want to know how our alternative selves are getting along. We want to know what they would think of us, what it would be like to speak to them, and we want to know what it might be like to live in those other worlds that those other selves inhabit. More than that, we want to resolve the uncertainty we have in our own past decisions by asking them: ‘How did it work out?’ The only way to do that is to uncover the looking glass and glance through.
One means of connecting with our alternative selves is through literature, film and the arts. The MWI first appeared in Michael Moorcock’s novella The Sundered Worlds (1962), a space opera that ranges across a vast multiverse. In this Star Wars -like action novel, the hero Renark von Bek undertakes to save the multiverse from Armageddon. This novel also hosted some of the earliest uses of virtual reality, computer tablets, digital displays and, of course, quantum physics, and it also launched Moorcock’s long career.
Since then, numerous novels, movies and TV shows have made use of the concept, including children’s fiction. The first book about a parallel universe that I recall reading was the children’s book The Double Disappearance of Walter Fozbek (1980) by Steve Senn, about a boy who somehow swaps places with his dinosaur counterpart in a world where people are all dinosaurs. As a child, I was blown away by this idea of parallel worlds, and that remained my favourite book for many years.
A rupture opens a doorway, a necessary trope for reaching our parallel selves
The idea has captured the movies, too. Among the many multiverse films are those in the Back to the Future trilogy (1985-90), about what happens when we go back in time, change the past, and find the future is another world entirely. There’s also Spider-Man: Into the Spider-Verse (2018), a computer-animated smash hit about a high-school student, Miles Morales, who becomes a Black Spider-Man in his own universe and teams up with Spider-people (men, women, and even Spider-Ham, a pig) from other universes to defeat his nemesis Kingpin. Also, Doctor Strange in the Multiverse of Madness (2022), a Marvel Universe battle between good and evil in parallel worlds; and the Academy Awards Best Picture winner, Everything Everywhere All at Once (2022), about a heroine who learns that she can draw skills and powers from her alternative selves to battle villains who threaten the world.
In each work, a rupture opens a doorway, a necessary trope for reaching our parallel selves. Yet the MWI actually tells us that worlds are generally unreachable. The work on weak measurements means that worlds can diverge without completely disconnecting. A better device might be a hidden passage that already exists, more like the wardrobe portal in C S Lewis’s Chronicles of Narnia series (1950-56) than a dangerous rip in space and time. I have yet to read a story where the plot revolved around keeping worlds from separating rather than worlds accidentally and catastrophically merging, but that might be more realistic.
In some cases, the literary purpose of the multiverse is not so much to connect parallel worlds as to tell different stories with the same characters. Star Trek , for example, depended on the multiverse for its James T Kirk reboot movies (2009-16), allowing the director J J Abrams to skirt around canon and change details to reimagine the young Kirk and his adventures on the USS Enterprise.
Using the multiverse to reboot Spider-Man in the movie Spider-Man: No Way Home (2021), MWI explains how the different actors – Tobey Maguire, Andrew Garfield and Tom Holland – who have played Spider-Man over the years might all exist simultaneously in different universes, and how they might meet up to fight as a team. The multiverse is not only a fun way to have all three actors appear in the movie but also a means of exploring how their characters differ and what they thought of the choices they made and the challenges they each faced, both similar and unique.
T he multiverse has also opened up new ways of looking at the human condition. One of the most fascinating areas where culture, philosophy and possible worlds collide is in the work of Robert Lanza on biocentrism, which is a philosophical approach to physics through the lens of living beings. Lanza, a professional biologist, proposes that the Universe arises directly from an individual’s conscious observation of it. He hypothesises that, for this reason, a conscious being cannot cease to be conscious. This leads to the potential fact that it is impossible to be dead. Instead, one’s consciousness simply splits off, by quantum processes, into worlds where that consciousness can continue to exist. Every wavefunction collapse or world splitting leaves us in a world where we remain alive.
Another novel, The Doors of Eden (2020) by Adrian Tchaikovsky, explores parallel worlds through the phenomenon of branching evolution. For each parallel Earth in the story, a different species dominates, having continued on, rather than suffering extinction. For instance, the author imagines what a society of trilobites might look like. As in many multiverse stories, reality collapses and the different worlds bleed into one another. The book contains many detailed and imaginative scenarios about speculative evolution, and, from an MWI perspective, it is perfectly reasonable to imagine many different potential evolutionary outcomes, since evolution is highly dependent on randomness, including quantum variations in cosmic rays striking DNA.
Even the art world has taken notice of the multiverse. In response to the COVID-19 pandemic, the Burning Man in the Multiverse experience in 2020 showcased the multiverse with immersive visual styles in a virtual event. In this project, eight teams developed different virtual universes, with a unique Burning Man in each. You could traverse the Burning Man Playa – the dry lake bed where it normally takes place at Black Rock City – in virtual reality as an avatar, explore art and sculpture created within a virtual world, and imagine the parallel realities of the annual festival itself.
What greater despair than to believe you are living the wrong life?
The most powerful reason why the multiverse has infiltrated culture is because people are storytellers. Research shows that this tendency is universal and appears in early childhood. It is written in our DNA. Implicit in storytelling is the modification of details such that one possible world becomes another. Such narratives are essential to how our species has understood the world for millennia. Meta-stories containing conflicting possible worlds simultaneously become not only plausible but essential to how we interpret our perceptions: personal, nonlinear and qualitative, rather than objective, linear and quantitative.
The human mind even creates its own multiverses through dreams, where alternative realities appear. Who hasn’t dreamed of a loved one acting in ways they never would, or living in a house that they’ve never seen before? Fundamentally, the human mind has evolved to imagine multiple possible futures branching out from the present. Whether this is actually the case is an open question that physics still must resolve, if it ever can.
While the many-worlds interpretation has at times been overused, the pervasiveness of the multiverse in culture is a shift with benefits. There is more than one way to see the world, and every conscious mind may create its own version of reality. In a world awash with data, hard facts have become difficult to come by, and everyone needs to have their minds open to the possibilities that what they believe or have been told is only one of many possible worlds.
On the other hand, when we start longing to live in one of those alternative realities, it can make us desperately unhappy. This is the curse of imagining all these branching pathways in our lives. As the American novelist James Branch Cabell wrote in The Silver Stallion (1926): ‘The optimist proclaims that we live in the best of all possible worlds; and the pessimist fears this is true.’ What greater despair than to believe you are living the wrong life? Yet, how can we claim a life is wrong? A life full of suffering is not a meaningless one as Nietzsche points out.
As Nora understands at the end of Haig’s The Midnight Library :
This Essay was made possible through the support of a grant to Aeon+Psyche from the John Templeton Foundation. The opinions expressed in this publication are those of the author and do not necessarily reflect the views of the Foundation. Funders to Aeon+Psyche are not involved in editorial decision-making."
Monist philosophy and quantum physics agree that all is One | Aeon Essays,,https://aeon.co/essays/monist-philosophy-and-quantum-physics-agree-that-all-is-one,"‘From all things One and from One all things,’ wrote the Greek philosopher Heraclitus some 2,500 years ago. He was describing monism, the ancient idea that all is one – that, fundamentally, everything we see or experience is an aspect of one unified whole. Heraclitus wasn’t the first, nor the last, to advocate the idea. The ancient Egyptians believed in an all-encompassing but elusive unity symbolised by the goddess Isis, often portrayed with a veil and worshipped as ‘all that has been and is and shall be’ and the ‘mother and father of all things’.
This worldview also follows in straightforward fashion from the findings of quantum mechanics (QM), the uncanny physics of subatomic particles that departs from the classical physics of Isaac Newton and experience in the everyday world. QM, which holds that all matter and energy exist as interchangeable waves and particles, has delivered computers, smartphones, nuclear energy, laser scanners and arguably the best-confirmed theory in the entirety of science. We need the mathematics underlying QM to make sense of matter, space and time. Two processes of quantum physics lead directly to the notion of an interconnected universe and a monistic foundation to nature overall: ‘entanglement’, nature’s way of integrating parts into a whole, and the topic of the 2022 Nobel Prize in Physics; and ‘decoherence’, caused by the loss of quantum information, and the reason why we experience so little quantum weirdness in our daily lives.
Yet, despite the throughline in philosophy and physics, the majority of Western thinkers and scientists have long rejected the idea that reality is literally unified, or nature and the Universe a system of one. From judges in the Inquisition (1184-1834) to quantum physicists today, the thought that a single system underlies everything has been too odd to believe. In fact, though philosophers have been proposing monism for thousands of years, and QM is, after all, an experimental science, Western culture has regularly lashed out against the concept and punished those promoting the idea.
I t wasn’t always that way. In ancient times, the concept of monism held more weight in the popular mind. Philosophers in the school of Pythagoras ( c 570- 490 BCE), renowned for his alleged discovery of the geometrical relation among the three sides of a right triangle, identified the number one as the centre of the Universe. Heraclitus’ contemporary Parmenides ( c 520- 460 BCE) believed in reality as a timeless ‘one, that is and that is not not to be’. And Plato, arguably the most influential philosopher ever, is said to have taught monism as a secret doctrine at his academy, to be disseminated only orally. Indeed, monism later evolved into a trademark of his school, and Neoplatonists such as Plotinus ( c 205- 270 CE) wrote about ‘the one’ that is ‘all things’ and ‘being’s generator’. Around the same time, mystery cults popular in late antiquity advocated a hidden unity behind the many gods of the Greco-Roman polytheistic pantheon, and understood the different deities as representations of the various facets of a single, unified reality.
Later on, philosophical ideas derived from Plato’s monistic instincts competed with Christianity to become the dominant worldview of the Roman Empire. Christianity prevailed.
Even then, Christianity adopted Platonic ideas by identifying the monistic ‘One’ with God. But Christianity drew also on dualistic philosophies such as Manichaeism, which advocated a world caught in an epic struggle between good and evil. This is how concepts such as God and devil, heaven and hell, or angels and demons received their prominent role among Christian beliefs. At the same time, the monistic influences were pushed into an otherworldly beyond. The Christian God was understood as different from the natural world that he governs from outside.
A student who claimed that ‘God, the world, and nature, are but one thing’ was hanged for blasphemy
With the Christian Church rising to political power and the fall of the Roman Empire, much of antiquity’s culture and philosophy got lost, and monism got suppressed as a heresy. If ‘all is One’, God gets conflated with the world, and medieval theology understood that as atheism or a devaluation of God.
When in 855 John Scotus Eriugena, a medieval philosopher at the court of the Frankish emperor Charles the Bald, described God as an ‘indivisible unity’ holding together ‘all things’, he got condemned and his books forbidden. Sure, these monistic ideas inspired philosophers, but theologians saw them as an intrusion into the realm of religion. By the 13th century, a group of scholars in Paris had resorted to the stance that there exists a double truth: that what is right in natural philosophy may be wrong at the same time in theology, and vice versa.
These conflicts framed the relationship between religion and the developing sciences. After Nicolaus Copernicus advocated a heliocentric model of the planetary system in 1543, proposing that Earth and planets revolved around the Sun, instead of the Universe around Earth, his book was suspended by the Inquisition in 1616; for more than 200 years, it was allowed to be published only in editions that stressed it presented just a mathematical model but no statement about reality. That same year, Galileo Galilei was warned by the cardinal Robert Bellarmine, an inquisitor and one of the judges who had condemned Giordano Bruno to be burnt at the stake, to teach the heliocentric model not as truth but only as a hypothesis.
In 1600, Bruno, an early advocate of the Copernican model, was burned alive in Rome. Among his heresies was his monistic philosophy, affirming that ‘the whole is one’ and that ‘Nature … is none other than God in things’. In 1619, Lucilio Vanini, who had preached a religion of nature where a leaf of grass was proof of God, got his tongue cut out and was strangled at the stake, his body burned in Toulouse. And in 1697, Thomas Aikenhead, a student who claimed that ‘God, the world, and nature, are but one thing’, was hanged for blasphemy in Edinburgh.
Science in those early days often emerged as a sort of ‘soft monism’. Johannes Kepler, who discovered that Earth and the other planets revolve around the Sun in elliptical orbits, tried to understand nature in terms of harmonies and symmetries. Bruno’s influence and the ideas of monism directly inspired his efforts to develop a unified theory and find harmonic, beautiful patterns in the natural world.
The monist influence was even more apparent in the work of Newton, best known for his theory of gravity. One of Newton’s most important accomplishments was the insight that gravity acts universally on all bodies on Earth and elsewhere in the Universe. He explicitly compared this feature with the idea of an all-encompassing divinity that he adopted from the Cambridge Platonist Ralph Cudworth. ‘One and the same divinity [exercises] its powers in all bodies whatsoever,’ Newton wrote.
Michael Faraday, who proposed force fields were permeating the Universe, made significant steps toward the unification of electricity and magnetism – a monistic point of view, indeed.
Albert Einstein, who gave us such concepts as the curved universe and space-time, believed that the separation of humans from the rest of the Universe was essentially an optical delusion of consciousness.
Monism has resurfaced again and again by inspiring humanity’s greatest creations and creators across the arts. Mozart’s opera The Magic Flute (1791) included a eulogy of Isis. Beethoven kept the quote ‘I am all that is, that has been and will be, and no mortal has ever lifted my veil,’ attributed to Isis, in a frame on his desk. The Romantic poets from Goethe to Coleridge to Wordsworth describe the longing for a reconciliation of ego and the world within nature.
Despite all this, the hard line of the Church stuck: monism could influence science and inspire our greatest art, but the idea that it quite literally described nature was rejected by the overwhelming majority through the years. To the present day, we tend to believe that monism and nature, or monism and science, don’t belong together; that the hypothesis of ‘all is One’ simply isn’t proper science at all.
I f anything should convince us to change our mind, it is the experimental science of quantum mechanics and its underlying mathematics. One famous feature of QM is that there is no strict separation between particles and waves. What had been considered as a particle before, for example an electron, can sometimes behave as a wave, while waves (such as, for example, light) can absorb and emit energy in discrete portions, understood as particle-like quanta. In contrast to a particle though, a wave doesn’t exist in a specific place. It stretches out over the surface of a pond or the expanse of the Universe; it is ‘non-local’, in physics lingo. A quantum object described as a wave exists in several places simultaneously – until it gets measured. In that instant, the object seems to collapse into one of its potential locations.
This leads to the weirdest aspect of QM – entanglement, a property of quantum systems made up of two or more particles. According to the quantum pioneer Erwin Schrödinger writing in 1935, entanglement is ‘the characteristic trait of quantum mechanics, the one that enforces its entire departure from classical lines of thought’.
Consider observing a wave pattern on your pond that you know results from two ripples combined, such as two stones dropped into the water. Just by looking at the water surface, you won’t be able to tell what these individual ripples were to start. For instance, the pattern could have arisen from two stones causing two equal swells in the water, or from a small stone causing a third of the swell and a larger stone creating two-thirds.
Taking this logic at face value, nothing we see really exists; there are no particles or physicists or cats or dogs
The same is true for entangled quantum systems: you may know the complete system perfectly well but at the same time know nothing about its constituents until we pin them down by experiment, measuring them. In such an experiment, the very act of measurement would destroy the original whole.
It was Schrödinger who clearly summarised what entanglement means:
Entanglement is QM’s way of integrating parts into a whole and, when you apply entanglement to the entire Universe, you end up with Heraclitus’ tenet ‘From all things One’. Taking this logic at face value, nothing we see around us really exists; there are no particles or physicists or cats or dogs. The only thing that truly exists is the Universe as a whole.
Yet, while this logic is easy to follow, the conclusion seems bizarre, and is far from a general consensus, even among physicists. In fact, it sparked a controversy that can be traced back to the early history of QM when, in 1927, Niels Bohr and Werner Heisenberg realised that one can never experience both the particle and wave aspects of a quantum object at the same time. Heisenberg’s friend and collaborator Wolfgang Pauli tried to illustrate this finding by saying one could look at nature with two different eyes, seeing either particles or waves, but if the observer tried to open both eyes together, they would go astray. This seemed to suggest that reality is fundamentally unobservable, just like the veiled Egyptian goddess Isis. But physics is an experimental science. As a consequence, physicists aren’t easily convinced about the existence of a hidden quantum reality, even if it may unify the experiences of things such as particles or waves.
Bohr, Heisenberg and Pauli at least remained unconvinced. When they tried to make sense out of quantum mechanics, they came to the conclusion that what we see is real and that there is no underlying, more fundamental quantum reality hiding behind. According to this ‘Copenhagen interpretation’, QM doesn’t describe a deeper reality but merely our incomplete knowledge of nature.
Schrödinger’s theoretical wave function, the mathematical expression that describes the different probabilities a quantum object has of being in a given state or location, wasn’t accepted as a model of nature but understood as merely a tool to predict what our measurement devices would register. ‘There is no quantum world,’ Bohr reportedly affirmed. For many years to come, this view became the orthodox interpretation about what QM meant.
From the time Schrödinger’s paper on entanglement was published in 1935, physicists could have adopted a monistic interpretation of QM, or at least have accepted it as a major contender for Bohr’s instrumental interpretation that QM was merely a tool. Yet it appears as if Heisenberg and Bohr, as soon as they had discovered this strange, new quantum reality that was underlying our everyday world and unifying everything in the Universe, shied away from setting out to explore this uncharted territory. Instead, they decided to declare it nonexistent.
This reaction is even more baffling since physicists, of all people, weren’t completely unaware of the monistic implications of QM. For example, when in 1947 Bohr received the Order of the Elephant, Denmark’s highest honour, he designed his own coat of arms that featured a yin and yang symbol, the pictorial representation of the monistic Taoist philosophy that seemingly opposite forces in nature are actually complementary pieces of a fundamental whole on a deeper level of understanding. In a similar spirit, Heisenberg titled his autobiography Der Teil und das Ganze (1969), or ‘The Part and the Whole’.
More concretely, the physicist David Bohm wrote in his popular textbook Quantum Theory (1951) that QM requires that we ‘give up the idea that the world can correctly be analysed into distinct parts’ and ‘replace it with the assumption that the entire universe is basically a single, indivisible unit’. By the 1970s, Fritjof Capra’s bestseller The Tao of Physics (1975) was comparing quantum physics with East Asian spirituality. So why were the monistic implications of quantum physics not taken seriously? Why was quantum physics an apt mathematical model, but considered insufficient to describe the contours of nature itself?
T here are many reasons why this didn’t happen.
For one thing, despite the monistic inclinations of visionaries like Newton and Kepler, the notion that ‘all is One’ usually isn’t understood as a meaningful statement in science. This ‘One’ isn’t directly observable, and science is an experimental endeavour. But more than that, the Western mind was inclined to restrict science to problem-solving while reserving the absolute and final answers for religion. The mindset has been internalised to this day, even by people who aren’t necessarily religious themselves.
What’s more, it didn’t really seem to matter what the quantum-mechanical wave function implied. The formulas and predictions of quantum physics worked perfectly well and could be applied successfully to the various emerging research fields in nuclear, particle and solid-state physics, irrespective of what one believed about its underlying reality. Moreover, for many years, no one truly understood what happened during a quantum measurement and how quantum mechanics was related to our everyday experience in a world made of large objects existing in definite shapes and places.
This situation changed only around 1970 when the physicist Heinz-Dieter Zeh in Germany discovered a process known as ‘decoherence’, which is important to virtually any branch of modern physics. Decoherence protects our daily-life experience from too much quantum weirdness. And it realises the last part of Heraclitus’ tenet: ‘from all things One’.
It is as if decoherence opens a zipper between parallel universes
Decoherence happens when a quantum object interacts with its environment – for instance, when a particle like an electron, a human observer or measurement device, and the environment get entangled. If the quantum object is a particle existing in two different locations (possible if it takes the form of a wave) each of them is linked to a corresponding state of the measurement device recording the particle in the respective position.
While these possible realities are superposed in the entangled whole, they unravel from the perspective of the observer who doesn’t know the exact state of the environment, which arguably is the entire rest of the Universe. It is as if you observe your garden through a partitioned window: nature looks divided into separate pieces, but this is an artefact of your perspective.
From the observer’s perspective immersed in their own reality (called the ‘frog perspective’ by the cosmologist Max Tegmark) the measurement device might describe two realities based on mathematical probabilities in the wave function – the particle could be located at position A with a measurement device observing this location, or the particle could be found at position B with another device recording this position.
Zeh’s discovery endorsed a controversial view of quantum mechanics, proposed by the physicist Hugh Everett, that became famous under the misleading label ‘many-worlds interpretation’. According to Everett, quantum measurements don’t have only a single outcome. Instead, all outcomes allowed in quantum mechanics are realised, albeit in parallel realities. It is as if decoherence opens a zipper between parallel universes. On a more fundamental level though, Everett’s interpretation doesn’t describe many classical worlds but rather a single quantum universe, governed by a universal wave function. If a hypothetical observer could see the entire Universe from the outside with all its possibilities revealed, the cosmos would manifest as a single quantum object. That, metaphorically speaking, would be the ‘bird perspective’, Tegmark says.
As remarkable as Everett’s and Zeh’s conclusions were, they weren’t appreciated by their physicist peers. Instead, for decades any deeper enquiry in the foundations of quantum mechanics was discouraged, and anyone who dared to question Bohr’s orthodox interpretation encountered a toxic blend of hostility and dogmatic pragmatism. The attitude was fittingly summarised in 1989 by the physicist David Mermin as ‘Shut up and calculate!’ The motto reflected the pressure on 20th-century students to adopt QM as a tool instead of wasting their time with metaphysical pondering or any effort to find its expression in reality.
John Clauser, one of the recipients of the 2022 Nobel Prize in Physics for his work on quantum entanglement, described how ‘a very powerful … stigma began to develop within the physics community towards anyone who sacrilegiously was critical of quantum theory’s fundamentals’. Léon Rosenfeld, a close collaborator of Bohr’s, characterised Everett as ‘undescribably [sic!] stupid’ and claimed he ‘could not understand the simplest things in quantum mechanics’. Around the same time, Zeh who discovered decoherence was informed by his advisor, a Nobel Prize winner, ‘that any further activities on this subject would end [his] academic career!’ Zeh stressed the parallels between the Inquisition’s conservative stance and the dogmatic antirealism of many physicists today:
Thus, even after decoherence had explained how our everyday experience can follow from a monistic quantum reality, the idea remained the outsider view of a small group of renegade physicists. And, in fact, for most of us, the notion of an all-encompassing ‘One’ doesn’t feel like proper science. It comes with a scent of New Age bullshit.
B ut why does this idea sound so bizarre to us? To understand this bias, we have to leave quantum mechanics for a moment and look back to how monism evolved in Europe over the past 800 years. It turns out, the controversy about how to interpret QM is part of the larger story – the conflict about who was entitled to define the foundation of reality: religion, or science?
According to Everett and Zeh, the fundamental description of the Universe is a single entangled state, described by a universal wave function. Everything we experience in our daily lives emerges from this fundamental quantum reality.
If this is correct, it implies that the traditional approach of physics to understand things in terms of constituents doesn’t work anymore. If physicists explain how everyday objects such as chairs, tables and books are made of atoms, atoms are composed of atomic nuclei and electrons, atomic nuclei contain protons and neutrons, and protons and neutrons consist of quarks, they ignore that these particles aren’t fundamental but just abstractions from the fundamental whole.
If there exists but a single thing in the Universe, then space doesn’t make sense any more
Instead, the most fundamental description of the Universe has to start with the Universe itself, understood as an entangled quantum object. Indeed, the 2022 Nobel Prize in Physics was awarded for experiments that probe correlations between particles separated by large distances yet connected to each other based on entanglement.
This view also requires us to rethink our notion of space and time. If there exists but a single thing in the Universe, then space, often understood as the relative order of things, doesn’t make sense any more. Nor is it easy to imagine this single object evolving in time. Accordingly, the Wheeler-DeWitt equation, describing the quantum mechanical wave function of the Universe and the starting point for much of Stephen Hawking’s work on cosmology, describes a timeless universe.
Entanglement also plays a crucial role in the most advanced approaches to quantum computing and the search for a theory of quantum gravity, in which entanglement creates connections between distant regions of space-time. Just a few weeks before the new Nobel laureates were honoured in Stockholm in 2022, a different team of distinguished scientists had a paper published in Nature that described a process on Google’s quantum computer that could be interpreted as some kind of wormhole, a tunnel connecting far-away regions in space. Although the wormhole realised in this recent experiment exists only in a two-dimensional toy universe, it hints at an intimate relationship between quantum entanglement and proximity in space, and thus could constitute a breakthrough for future research at the forefront of physics.
The 3,000-year-old concept of monism may actually help modern physicists in their struggle to find a theory of quantum gravity and make sense out of black holes, the Higgs boson, and the early Universe. Chances are high that we witness the beginning of a new era where science is informed by monism and the Universe is perceived as a unified whole.
This Essay was made possible through the support of a grant to Aeon+Psyche from the John Templeton Foundation. The opinions expressed in this publication are those of the author and do not necessarily reflect the views of the Foundation. Funders to Aeon+Psyche are not involved in editorial decision-making."
How black hole thought experiments help explain the Universe | Aeon Essays,,https://aeon.co/essays/how-black-hole-thought-experiments-help-explain-the-universe,"Albert Einstein’s theory of gravitation, known as general relativity, is intimidating, even for highly trained theoretical physicists. In his theory, matter and energy cause space-time to curve. In most situations, this warping is so small as to be unobservable, even with powerful and sophisticated instruments. In fact, for many years after Einstein put forth his theory in 1916, there were only three situations in which small corrections to Newton’s classic laws of gravity (the force we feel here on Earth) could be observed: the bending of light by the Sun during a solar eclipse; a small anomaly in the motion of Mercury; and a small shift in the wavelength of light due to gravitation. Since that time, the situation has dramatically changed. General relativity has provided us with a framework for thinking about the Universe as a whole, and plays a role in much of what astronomers understand about stars. It even plays a role in the GPS system that helps us navigate the roads.
Einstein’s equations ultimately revealed a set of previously unknown, ultradense cosmological objects: black holes. The mathematics of Einstein’s equations showed that light starting inside the black hole could get only so far. That distance, known as the Schwarzschild radius, can be thought of as the surface of the black hole; this surface is known as the horizon, beyond which light cannot escape. Near and within the horizon, space and time are modified so violently that it even becomes tricky to figure out what is space and what is time.
No one could see inside this kind of object, but speculations on their nature date to the work of J Robert Oppenheimer (famed for his leadership of the atomic bomb project during the Second World War) and John Wheeler, a Princeton theorist who provided, among other things, the name ‘black hole’.
Over the past half-century, astronomers have found black holes in great numbers around the Universe. Some are the result of stellar collapse, and have masses typically a few times larger than that of our Sun. Much more massive ones exist at the centres of most galaxies, including our own. Smaller black holes are typically ‘seen’ as they swallow matter from companion stars; the large black hole at the centre of our galaxy was discovered through its effects on the motion of stars orbiting about it. We may never be able to literally peer inside a black hole, but knowledge of the cosmos and emerging theories of physics allow us to think through their nature; the modus operandi for this kind of exploration, the thought experiment, has been a cornerstone of physics since Einstein dramatically altered our understanding of space and time.
E instein’s theory that the Universe is curved and time is relative has been subject to direct experimental and observational study for more than a century – but thought experiments played a major role, as well. One of the most famous thought experiments of all time juxtaposed Einstein’s general relativity, which looked at systems as large as the cosmos, with quantum mechanics, also referred to as quantum theory, which resulted from experimental studies of objects on the scale of atoms or smaller.
Prior to the emergence of quantum mechanics, physicists thought of atoms as something like billiard balls. In the pre-quantum or classical view, their motion was governed by Isaac Newton’s laws, which allow a person, given knowledge of the basic forces of nature, to predict the motion of the particles in the future. But quantum mechanics called this viewpoint into question. Instead, it suggested an alternative picture of reality, coded in the Schrödinger equation – which provided the probability, though not the certainty, that an electron would be located at a given spot at a particular point in time. It was the physicist Max Born who made the radical proposal that quantum mechanics predicted probabilities of various outcomes, rather than a single certain result. Critical to his assertion was a set of thought experiments. Born asked what Schrödinger’s equation would predict for the outcome of the collision between two atoms, or an atom and an electron. Newton’s billiard ball outlook holds only when the probability of one particular outcome is far larger than that of any other.
Thought experiments suggested the widely separated elements would still be entangled
The notion deeply troubled Einstein, provoking his complaint in a letter to Born in December 1926: ‘Quantum mechanics is certainly imposing… The theory says a lot, but does not really bring us any closer to the secret of the “old one”. I, at any rate, am convinced that He does not throw dice.’
In 1927, Werner Heisenberg summarised the distinctions between the physics of Newton and that of the Schrödinger equation in his uncertainty principle, which sets limits on what one can measure about a system. The location of a particle, would always be a question of probability, never a sure thing. He arrived at this principle by considering various thought experiments, where he asked how particular measurements might actually be performed. Einstein tried to demolish the quantum theory through sharp critique, continually challenging Niels Bohr , a Danish founder of quantum mechanics and a leader in the effort to interpret the theory with thought experiments similar to those of Born and Heisenberg. At first glance, these seemed to show that quantum theory and its probability interpretation did not make sense. The questions Einstein asked were often tough, but Bohr, sometimes after a prolonged period of thought, invariably found a way to resolve each paradox. One such experiment, known as the EPR paradox (for Einstein and his two assistants, Boris Podolsky and Nathan Rosen), involved the connections between two widely separated parts of a single system. Thought experiments suggested the widely separated elements would still be entangled, with one part of the system invariably providing information about the other. This was eventually turned into a real experiment, proving quantum mechanics correct.
S o what does all this have to do with black holes? A real-world experiment sets the stage.
According to the rules of classical physics, an object with electric charge, like an electron or proton, emits light as it speeds up or slows down. Einstein understood that, in a similar manner, his general relativity would lead to waves of the gravitational field – gravity waves – when mass or other forms of energy sped up or slowed down. These waves, in turn, would push and pull on matter as they passed by. Because the gravitational force is so much weaker than electricity and magnetism, these effects would be minuscule, even when huge amounts of mass are involved.
The first experimental programme with any real hope to detect these tiny gravitational waves began in the 1990s, and was known as LIGO, for Laser Interferometer Gravitational-Wave Observatory.
The programme was based on an outcome of general relativity understood early on by Einstein: when two planets collide, the mass involved would be insufficient to perceptibly impact the shape of space-time. But when two superdense objects like black holes collide, they would distort space-time enough that the effect could be detected. According to Einstein’s theory, these waves, travelling through space from their source, would stretch the space around them, ever so slightly. Objects nearby would appear slightly longer and then slightly shorter, and then slightly longer again. This stretching and shrinking would alert us that the objects had been there at all.
Now, when I say slightly, I mean slightly . The LIGO gravitational-wave detectors are long metal tubes each 4 kilometres long. Waves from colliding black holes stretch and shrink these huge bars by about 10 -18 cm, an amount 10 5 times – 100,000 times – smaller than an atomic nucleus. Put another way, as a fraction of its length, each bar changes by about a trillionth of a trillionth of its length.
Throw in tables, chairs, planets, other stars, and the black hole’s mass increases and its horizon area increases
Only over the past decade has the detector picked up gravitational waves from collisions of neutron stars and black holes. With this discovery, a whole new way to study the Universe has emerged .
Yet these experiments go only so far. Indeed, in a universe governed by quantum mechanics, there are aspects of black holes that are far from clear. Because, in Einstein’s theory, a black hole can’t emit light or transmit information in other ways, they are almost featureless. If you know their mass, their electric charge, and how fast they spin, you know everything you can possibly know about them. They may have arisen from the collapse of a complicated star, surrounded by planets with advanced civilisations, but when they formed, all of that information simply vanished. This is different from a fire or an explosion, where you might hope, with a huge amount of work, to reconstruct all the original information by looking through the ashes and the outgoing light and heat. In the collapse of a black hole, such reconstruction seems impossible.
This new visualisation of a black hole illustrates how its gravity distorts our view, warping its surroundings as if seen in a carnival mirror. The visualisation simulates the appearance of a black hole where infalling matter has collected into a thin, hot structure called an accretion disk. The black hole’s extreme gravity skews light emitted by different regions of the disk, producing the misshapen appearance. Created by NASA Goddard Space Flight Center/Jeremy Schnittman
One physicist who tried to glean more through thought experiment was the late theorist Jacob Bekenstein of the Hebrew University of Jerusalem. He noted an analogy between black holes and the second law of thermodynamics. The second law says that entropy – which is a measure of disorder – always increases. For black holes, there is also a quantity that always increases: the area of the black hole surface, its horizon. Whenever you add something to a black hole – say throwing in tables, chairs, planets, other stars – the mass increases and the area of the horizon increases. Bekenstein proposed a precise relationship between the black hole area and entropy, and suggested that black holes were actually thermodynamic systems with a temperature.
In physics, we think of temperature as a measure of the energy within some set of particles – atoms, molecules, photons. Yet, from the outside, we have no information about the black hole apart from some gross properties such as its mass, and we certainly can’t identify things like particles.
It was Stephen Hawking who, in the early stages of his career, finally discovered the sense in which black holes have a temperature. Hawking had an interest in extreme situations in general relativity, such as the earliest instants after the Big Bang and the interior of black holes. Now thinking about the behaviour of particles such as electrons and photons near the horizon of a black hole – thought experiments again – he realised that black holes are not really black; they radiate particles now known as the ‘Hawking radiation’. This is an intrinsically quantum phenomenon. The uncertainty principle permits brief violations of energy conservation in ordinary space-time. As a result, for an extremely short time, a particle and its antiparticle (in the case of an electron, for example, the antiparticle has the same mass but the opposite electric charge, known as the positron) can appear, even in a complete vacuum, and then annihilate each other and disappear again. For us, there is no observable consequence because energy is conserved.
But Hawking realised that some of these flickering particles could borrow some of the enormous energy of the black hole and become real. If produced near the horizon, one of these virtual particles could fall back into the black hole while the other escapes. Hawking found that the particles were emitted just as they would be from an object with the temperature predicted by Bekenstein. (The radiation from an object with a given temperature is called ‘blackbody radiation’ and has characteristic features; the most dramatic example is the Universe itself, whose temperature is 2.7 degrees Kelvin).
In short, the black hole appears to be a much more complicated object in a quantum world than in a classical one. In the quantum world, there’s a lot going on inside. The black hole in the quantum universe is not static. As it emits particles, it gradually evaporates, eventually disappearing altogether.
For a black hole formed in the collapse of a star a bit more massive than our Sun, the time for the entire object to evaporate is very long – about 10 67 years, far, far longer than the present age of the Universe. But we can contemplate smaller black holes, which might be disappearing today. At the end of their lifetimes, there would be a large burst of energy. Astrophysicists are currently searching for this possibility. But we’d have to be quite lucky to find such a thing and, so far, there is no evidence for black holes of this size.
H awking’s theoretical discovery of the Hawking radiation, possible through thought experiment, was a major accomplishment. It brought general relativity and quantum theory together in a remarkable way. But performing still another thought experiment, Hawking was puzzled by features of this radiation – or more precisely, its lack of features. Critical to Born’s probability interpretation of quantum mechanics was that something always happens. If you add up the probabilities for anything that may happen, you will find that the total probability is one. This can be formulated as a statement about information: if one knows everything one can know about a system at one time, one can know everything about it at later times. But this did not seem to be the case for radiation from black holes.
These ideas may be unfamiliar – indeed they are unclear to many physicists, so it is worth elaborating a bit. The fact that the probability of all outcomes is one is illustrated by a familiar pastime. If you enter your state or national lottery, you focus on your chances of winning. If you buy one ticket and there are 10 million lottery tickets sold, your chances of winning the jackpot are 1 in 10 million. That’s a really minute chance. But I either win or lose the lottery: the chance of winning or losing is 100 per cent.
What does it mean for information to disappear? Of course, we all forget things, lose records of various types, or deliberately shred or burn papers. But we believe that with enough patience and resources, we could reconstruct this information. The amount of information in a system (or the Universe) doesn’t change, though much of it may be hard to access. For a complicated system, like a collapsing star, there is a lot of information – an unimaginably large amount. In classical physics, there would be the positions and velocities of all the nuclei and electrons. In quantum mechanics, there are complicated relations between all of them; one can’t give the probability that one particle is at a point without specifying also the probability of finding all the other particles at particular places as well.
There is a situation where black holes could exist and quantum mechanics could make sense: string theory
So a collapsing star contains a huge amount of information. Thanks to Hawking, we know that, if the star is heavy enough, it forms a black hole and then slowly evaporates, emitting radiation. The vast amount of information that was contained in the initial star has been reduced to just the temperature of a warm body. Hawking, in his 1976 paper , argued that the information was simply lost. Quantum mechanics, he asserted, breaks down near black holes.
Many leading theorists have struggled to resolve the puzzles raised by this thought experiment. Some have argued that, indeed, one has to redo quantum mechanics or general relativity to resolve Hawking’s paradox. Others have been more sceptical of Hawking. Perhaps, for example, the evaporation of a black hole is like a lump of ash from the burning of a log in a fireplace. Surely the laws of quantum mechanics don’t break down when an object burns? In that case, the resolution of the puzzle is that the outgoing radiation is not exactly that of a black body because subtle connections between the outgoing photons remain intact. But it was soon realised that the answer to Hawking’s question about the black hole problem could not be so simple; the structure of space and time makes it hard to understand how such correlations might arise. There were other proposals, none very satisfying. Perhaps Hawking was right: just as Newtonian physics was usurped by quantum mechanics and general relativity on large or tiny scales, something had to give here as well.
It turns out that there is a situation where black holes could exist and quantum mechanics could make sense: string theory. String theory, also emerging from thought experiments, replaces the particles of quantum mechanics with one-dimensional strings. That concept has provided at least a partial resolution of the puzzle. Two theorists at Harvard University – Cumrun Vafa and Andrew Strominger – building on the work of the late Joseph Polchinski, of the University of California at Santa Barbara, were able to understand the temperature of certain idealised black holes in quantum mechanical terms. In other words, the information, at least for these idealised systems, somehow survives, evading Hawking’s paradox.
But while this result settled the question in an abstract way, it left many physicists dissatisfied. Because the calculation is done in a situation that doesn’t much resemble an astrophysical black hole, it is hard to figure out just what went wrong with Hawking’s argument.
There remains something important about the way general relativity works that we don’t yet fully understand. It may be that the rest of the story will be rather mundane, but it seems likely that fully resolving these questions will yield dramatic new insights into the quantum nature of space-time, and might answer some big questions we have about the Universe as we observe it. One of the biggest puzzles in our current understanding of nature is that most of the energy of the Universe – about 70 per cent – exists in a strange form with negative pressure , known as the dark energy . But it is very hard to understand why there is so little of it.
It is conceivable that a thought experiment resolving Hawking’s puzzle might provide some clues. The most radical possibility is that space-time is not the basic arena for the phenomena of nature. A being living in a crystal, for instance, would experience something like space-time, but would have a very different character. Condensed matter physicists would say that space-time is emergent. The basic underlying entity might be something else entirely. Perhaps one day our science and technology will be so advanced that actual experiments will reveal what it is – but, until then, thought experiments involving black holes, among other phenomena, will have to light the way.
Adapted excerpt from the book This Way to the Universe by Michael Dine, published by Dutton, an imprint of Penguin Publishing Group, a division of Penguin Random House LLC. Copyright © 2022 by Michael Dine"
How imaginary numbers describe the fundamental shape of nature | Aeon Essays,,https://aeon.co/essays/how-imaginary-numbers-describe-the-fundamental-shape-of-nature,"Many science students may imagine a ball rolling down a hill or a car skidding because of friction as prototypical examples of the systems physicists care about. But much of modern physics consists of searching for objects and phenomena that are virtually invisible: the tiny electrons of quantum physics and the particles hidden within strange metals of materials science along with their highly energetic counterparts that only exist briefly within giant particle colliders.
In their quest to grasp these hidden building blocks of reality scientists have looked to mathematical theories and formalism. Ideally, an unexpected experimental observation leads a physicist to a new mathematical theory, and then mathematical work on said theory leads them to new experiments and new observations. Some part of this process inevitably happens in the physicist’s mind, where symbols and numbers help make invisible theoretical ideas visible in the tangible, measurable physical world.
Sometimes, however, as in the case of imaginary numbers – that is, numbers with negative square values – mathematics manages to stay ahead of experiments for a long time. Though imaginary numbers have been integral to quantum theory since its very beginnings in the 1920s, scientists have only recently been able to find their physical signatures in experiments and empirically prove their necessity.
In December of 2021 and January of 2022, two teams of physicists, one an international collaboration including researchers from the Institute for Quantum Optics and Quantum Information in Vienna and the Southern University of Science and Technology in China, and the other led by scientists at the University of Science and Technology of China (USTC), showed that a version of quantum mechanics devoid of imaginary numbers leads to a faulty description of nature. A month earlier, researchers at the University of California, Santa Barbara reconstructed a quantum wave function, another quantity that cannot be fully described by real numbers, from experimental data. In either case, physicists cajoled the very real world they study to reveal properties once so invisible as to be dubbed imaginary.
For most people the idea of a number has an association with counting. The number five may remind someone of fingers on their hand, which children often use as a counting aid, while 12 may make you think of buying eggs. For decades, scientists have held that some animals use numbers as well, exactly because many species, such as chimpanzees or dolphins , perform well in experiments that require them to count.
Counting has its limits: it only allows us to formulate so-called natural numbers. But, since ancient times, mathematicians have known that other types of numbers also exist. Rational numbers, for instance, are equivalent to fractions, familiar to us from cutting cakes at birthday parties or divvying up the cheque after dinner at a fancy restaurant. Irrational numbers are equivalent to decimal numbers with no periodically repeating digits. They are often obtained by taking the square root of some natural numbers. While writing down infinitely many digits of a decimal number or taking a square root of a natural number, such as five, seems less real than cutting a pizza pie into eighths or 12ths, some irrational numbers, such as pi, can still be matched to a concrete visual. Pi is equal to the ratio of a circle’s circumference and the diameter of the same circle. In other words, if you counted how many steps it takes you to walk in a circle and come back to where you started, then divided that by the number of steps you’d have to take to make it from one point on the circle to the opposite point in a straight line passing through the centre, you’d come up with the value of pi. This example may seem contrived, but measuring lengths or volumes of common objects also typically produces irrational numbers; nature rarely serves us up with perfect integers or exact fractions. Consequently, rational and irrational numbers are collectively referred to as ‘real numbers’.
Negative numbers can also seem tricky: for instance, there is no such thing as ‘negative three eggs’. At the same time, if we think of them as capturing the opposite or inverse of some quantity, the physical world once again offers up examples. Negative and positive electric charges correspond to unambiguous, measurable behaviour. In the centigrade scale, we can see the difference between negative and positive temperature since the former corresponds to ice rather than liquid water. Across the board then, with positive and negative real numbers, we are able to claim that numbers are symbols that simply help us keep track of well-defined, visible physical properties of nature. For hundreds of years, it was essentially impossible to make the same claim about imaginary numbers.
In their simplest mathematical formulation, imaginary numbers are square roots of negative numbers. This definition immediately leads to questioning their physical relevance: if it takes us an extra step to work out what negative numbers mean in the real world, how could we possibly visualise something that stays negative when multiplied by itself? Consider, for example, the number +4. It can be obtained by squaring either 2 or its negative counterpart -2. How could -4 ever be a square when 2 and -2 were both already determined to produce 4 when squared? Imaginary numbers offer a resolution by introducing the so-called imaginary unit i , which is the square root of -1. Now, -4 is the square of 2 i or -2 i , emulating the properties of +4. In this way, imaginary numbers are like a mirror image of real numbers: attaching i to any real number allows it to produce a square exactly the opposite of the one it was generating before.
W estern mathematicians started grappling with imaginary numbers in earnest in the 1520s when Scipione del Ferro, a professor at the University of Bologna in Italy, set out to solve the so-called cubic equation. One version of the challenge, later referred to as the irreducible case, required taking the square root of a negative number. Going further, in his book Ars Magna (1545), meant to summarise all of algebraic knowledge of the time, the Italian astronomer Girolamo Cardano declared this variety of the cubic equation to be impossible to solve.
Almost 30 years later, another Italian scholar, Rafael Bombelli, introduced the imaginary unit i more formally. He referred to it as più di meno , or ‘more of the less’, a paradoxical phrase in itself. Calling these numbers imaginary came later, in the 1600s, when the philosopher René Descartes argued that, in geometry, any structure corresponding to imaginary numbers must be impossible to visualise or draw. By the 1800s, thinkers such as Carl Friedrich Gauss and Leonhard Euler included imaginary numbers in their studies. They discussed complex numbers made up of a real number added to an imaginary number, such as 3+4 i , and found that complex-valued mathematical functions have different properties than those that only produce real numbers.
Yet, they still had misgivings about the philosophical implications of such functions existing at all. The French mathematician Augustin-Louis Cauchy wrote that he was ‘abandoning’ the imaginary unit ‘without regret because we do not know what this alleged symbolism signifies nor what meaning to give to it.’
In physics, however, the oddness of imaginary numbers was disregarded in favour of their usefulness. For instance, imaginary numbers can be used to describe opposition to changes in current within an electrical circuit. They are also used to model some oscillations, such as those found in grandfather clocks, where pendulums swing back and forth despite friction. Imaginary numbers are necessary in many equations pertaining to waves, be they vibrations of a plucked guitar string or undulations of water along a coast. And these numbers hide within mathematical functions of sine and cosine, familiar to many high-school trigonometry students.
At the same time, in all these cases imaginary numbers are used as more of a bookkeeping device than a stand-in for some fundamental part of physical reality. Measurement devices such as clocks or scales have never been known to display imaginary values. Physicists typically separate equations that contain imaginary numbers from those that do not. Then, they draw some set of conclusions from each, treating the infamous i as no more than an index or an extra label that helps organise this deductive process. Unless the physicist in question is confronted with the tiny and cold world of quantum mechanics.
Quantum theory predicts the physical behaviour of objects that are either very small, such as electrons that make up electric currents in every wire in your home, or millions of times colder than the insides of your fridge. And it is chock-full of complex and imaginary numbers.
Imaginary numbers went from a problem seeking a solution to a solution that had just been matched with its problem
Emerging in the 1920s, only about a decade after Albert Einstein ’s paradigm-shifting work on general relativity and the nature of spacetime, quantum mechanics complicated almost everything that physicists thought they knew about using mathematics to describe physical reality. One big upset was the proposition that quantum states, the fundamental way in which objects that behave according to the laws of quantum mechanics are described, are by default complex. In other words, the most generic, most basic description of anything quantum includes imaginary numbers.
In stark contrast to theories concerning electricity and oscillations, in quantum mechanics a physicist cannot look at an equation that involves imaginary numbers, extract a useful punchline, then forget all about them. When you set out to try and capture a quantum state in the language of mathematics, these seemingly impossible square roots of negative numbers are an integral part of your vocabulary. Eliminating imaginary numbers would highly limit how accurate of a statement you could make.
The discovery and development of quantum mechanics upgraded imaginary numbers from a problem seeking a solution to a solution that had just been matched with its problem. As the physicist and Nobel laureate Roger Penrose noted in the documentary series Why Are We Here? (2017): ‘[Imaginary numbers] were there all the time. They’ve been there since the beginning of time. These numbers are embedded in the way the world works at the smallest and, if you like, most basic level.’
The complex object at the heart of all of quantum mechanics is the so-called wave function. It reflects a striking fundamental truth uncovered by quantum researchers – that everything, no matter how solid or corpuscular it seems, sometimes behaves like a wave. And it works the other way as well: electrons, the stuff of waves, can behave like particles.
‘Louis de Broglie speculated that maybe these seemingly disparate features, undulatory and corpuscular, form a union not only in light but in everything,’ writes Smitha Vishveshwara, a physicist at the University of Illinois Urbana-Champaign in her forthcoming book, ‘Two Revolutions: Einstein’s Relativity and Quantum Physics’. ‘Maybe the stuff we’re made of, which we know to be composed of particles, can have wavy traits,’ she adds, paraphrasing the question that led the founders of quantum theory to make the complex-valued wave function the fundamental building block of their model of nature.
To determine the exact details of a quantum-mechanical wave function that describes some physical object, for example an electron moving within a metal, researchers turn to the Schrödinger equation. Named after the Austrian physicist Erwin Schrödinger, another architect of quantum theory’s foundations, this equation accounts not only for the kind of tiny particle one is trying to describe, but also its environment. Is the electron seeking a less energetic and more stable state like a ball rolling down a steep hill? Has it received an energy ‘kick’ and is consequently executing a fast and complex motion like a football thrown in a spiral by a very strong athlete? The mathematical form of the Schrödinger equation allows for this information to be taken into account. In this way, the Schrödinger equation is directly informed by the particle’s immediate physical reality. Nevertheless, its solution is always the wave function that inextricably contains imaginary numbers. Even Schrödinger was disturbed by this. In 1926, he wrote to his colleague Hendrik Lorentz, saying that: ‘What is unpleasant here, and indeed directly to be objected to, is the use of complex numbers.’
T oday, almost a century after Schrödinger first voiced his concern, three independent teams of physicists have cornered imaginary numbers in their labs.
In the first experiment , researchers from the University of California, Santa Barbara (UCSB) and Princeton University went after the quantum wave function itself. Their work, appearing in the journal Natur e, demonstrated a first-of-its-kind reconstruction of the quantum-mechanical wave function from a laboratory measurement. The researchers experimentally studied how the semiconductor material gallium arsenide behaves after being exposed to a very fast pulse of laser light. More specifically, gallium arsenide re-emits some of the light that a laser shines onto it, and the UCSB team was able to show that, remarkably, properties of that light depend not only on the details of the wave functions of particles inside the material, but in particular on the imaginary parts of those wave functions.
Semiconductors such as gallium arsenide take up the middle ground between conducting materials, where electrons form rivers of moving charges that we call currents, and insulators, which hold on to their electrons so tightly that the formation of a current is impossible. In a semiconductor, most electrons do stay put, but here and there a few can start moving around, constituting tiny currents. An odd feature of this type of conduction is that every electron that manages to move gains a partner automatically – a particle-like entity called a ‘hole’, which carries positive electric charge. If the electron were a droplet of water in a pond, the existence and motion of the hole would be like the vacancy left after the droplet is removed, gaining a life of its own. Both electrons and their partner holes follow the rules of quantum mechanics, so the best way that physicists have of describing them is to write down a wave function for each.
An important part of every such wave function is its phase, which contains an imaginary number. Often, it reflects interactions that a quantum particle may have experienced while travelling along some path in space. Two wave functions can overlap and combine just like two waves on the surface of water, and the resulting ripple pattern, which in the quantum case informs scientists of where particles corresponding to those wave functions are most likely to be, depends on the wave functions’ phases. In the UCSB and Princeton experiment, the phases of the wave functions of gallium arsenide’s holes and electrons also dictated what kind of light the material could re-emit.
To uncover that connection, researchers first gave electrons in the material an energy boost by shining a fast pulse of near-infrared laser light. This energy boost made the electrons move through the material and created their companion holes. The physicists used another laser to briefly separate the two kinds of particles. After a short time of lonely motion through the semiconductor, the electron and hole pairs were allowed to reunite. Because both particles acquired energy while they were moving alone, their reunion resulted in a flash of light. Researchers determined the imaginary wave-function phase for the holes involved in this process by measuring that light – which was a concrete entity in the natural world.
Other physicists, meanwhile, now wonder whether theories can be reconfigured to avoid the apparent conflict between the real and the imaginary. In this view, instead of looking for imaginary numbers in the lab, physicists just need to find a different labelling system, one that requires real numbers only. This type of theory is known as ‘real quantum mechanics’.
Some conclusions can never be reached without imaginary numbers
Historically, real quantum mechanics has had not only proponents but also some successes in the realm of mathematical proofs and investigations. Theorists have been able to show that certain properties of quantum-mechanical systems can indeed be captured without resorting to imaginarity. Within the last year, however, a new crop of proofs and experiments proved that this line of reasoning can only go so far. Laboratory experiments involving quantum computers and quantised light now strongly indicate that imaginary and complex numbers are an indispensable part of the quantum, and therefore our own, world.
The theoretical work , spearheaded by physicists at the Austrian Academy of Sciences in Vienna, and the experiments that put it to the test in laboratories in both Austria and China, approach the issue through a kind of game.
In the theoretical study, the ‘players’ are three imaginary physicists, Alice, Bob and Charlie, who use quantum states as their board-game pieces and a series of sophisticated quantum operations as their in-game moves. At the end of the game, the three can compare notes on what properties their quantum state acquired during play. The Vienna physicists showed that some conclusions can never be reached without imaginary numbers. It was as if they had found that real quantum theory could not help a sports analyst predict that a basketball player successfully shooting the basket from the three-point arc would score their team the full three points.
Such game-like tests of competing theories of nature are something of a tradition in quantum mechanics. They date back to the Northern Irish physicist John Bell who, in the 1960s, used a similar approach to prove that quantum mechanics itself is truly necessary for an accurate description of nature. In this case, physicists pitted quantum mechanics against classical physics, which dates all the way back to Isaac Newton, and found that the former always excelled in predicting the outcomes of their experiments.
This approach, dubbed the Bell test, included only two ‘players’, Alice and Bob, who could not make sense of their post-game results unless they viewed them through the lens of quantum theory. Classical physics, researchers concluded, simply was not the best description of the world. Miguel Navascués, a physicist at the Austrian Academy of Sciences and co-author of both experimental and theoretical studies of the new Bell game noted that his team’s effort provided a way to make exactly the same evaluation of real and complex-value quantum theories. ‘If you can conduct this experiment,’ he said , ‘then you will have refuted real-number quantum physics.’
In the experiment carried out at USTC, the Bell game took place inside a quantum computer, where superconducting units called ‘qubits’ were controlled by microwave pulses. In the experiment that Navascués was involved with, the arena was an optical setup where researchers worked with quantum light – in other words, a stream of photons that could be altered by beam-splitters and other lab equipment.
In either case, the outcome of the game was impossible to predict accurately by any version of quantum physics that renounced complex numbers. Not only did physicists infer that imaginary numbers can indeed show up in experiments, but that, even more strikingly, they had to be considered in order for experiments in the quantum realm to be understood correctly at all.
T he studies mentioned here carry important implications for the most heady and profound ideas about quantum mechanics and the nature of physical reality. They are also important milestones for the development of new quantum technologies. Manipulating wave functions and wave-function phases is an important tool in quantum information and quantum computing. Accordingly, the UCSB experiment may help advance device design in those fields. ‘If you’re thinking about building any sort of device that takes advantage of quantum mechanics, you’re going to need to know its [wave function’s] parameters really well,’ Joe Costello, a physics PhD student at UCSB and the lead author on the study, emphasised when discussing the work.
Similarly, when scientists write algorithms that deal with quantum information, they must consider whether there are any advantages to using complex-valued quantum states. Recent works led by USTC and Vienna strongly suggest the answer is ‘yes’. Quantum computers will ultimately vastly surpass their conventional counterparts, making the development of best algorithmic practices a critical task. Almost a hundred years after Schrödinger bemoaned imaginary numbers, physicists are finding they may be useful in very practical ways.
Quantum physics has revealed that we’ve misunderstood imaginary numbers all along
In his book The Road to Reality (2004), Penrose writes that: ‘In the development of mathematical ideas, one important initial driving force has always been to find mathematical structures that accurately mirror the behaviour of the physical world.’ In this way, he summarises the trajectory of theoretical physics overall. Notably, he adds that ‘in many instances, this drive for mathematical consistency and elegance takes us to mathematical structures and concepts which turn out to mirror the physical world in a much deeper and more broad-ranging way than those that we started with.’ Imaginary numbers have transcended their original place as mere placeholders, transforming our grasp of reality and illuminating this grand idea.
Quantum theory has historically challenged many seemingly ‘common sense’ assumptions about nature. It has, for example, changed the way physicists think about an experimenter’s ability to measure something with certainty, or the claim that objects can be affected only by other objects in their immediate surroundings. When quantum theory was first formulated, it scandalised many luminaries of science at the time, including Einstein who contributed to its foundations himself. Working with quantum ideas and poking quantum systems has always, by default, come with the possibility of uncovering something unexpected at best, and bizarre at worst. Now quantum physics has revealed that we’ve misunderstood imaginary numbers all along. They may have, for a time, seemed to be just a mental device inhabiting the minds of physicists and mathematicians, but since the real world that we inhabit is indeed quantum, it’s no surprise that imaginary numbers can be found, quite clearly, within it."
How Wittgenstein might ‘solve’ both philosophy and quantum physics | Aeon Essays,,https://aeon.co/essays/how-wittgenstein-might-solve-both-philosophy-and-quantum-physics,"I first learnt about Plato’s allegory of the cave when I was in senior high school. A mathematics and English nerd – a strange combination – I played cello and wrote short stories in my spare time. I knew a bit about philosophy and was taking a survey class in the humanities, but Plato’s theory of ideal forms arrived as a revelation: this notion that we could experience a shadow-play of a reality that was nonetheless eternal and immutable. Somewhere out there was a perfect circle; all the other circles we could see were pale copies of this single Circle, dust and ashes compared with its ethereal unity.
Chasing after this ideal as a young man, I studied mathematics. I could prove the number of primes to be infinite, and the square root of two to be irrational (a real number that cannot be made by dividing two whole numbers). These statements, I was told, were true at the beginning of time and would be true at its end, long after the last mathematician vanished from the cosmos. Yet, as I churned out proofs for my doctoral coursework, the human element of mathematics began to discomfit me. My proofs seemed more like arguments than irrefutable calculations. Each rested on self-evident axioms that, while apparently true, seemed to be based on little more than consensus among mathematicians.
These problems with mathematics turned out to be well known. The mathematician and philosopher Bertrand Russell spent much of his career trying to shore up this house built on sand. His attempt was published, with his collaborator Alfred North Whitehead, in the loftily titled Principia Mathematica (1910-13) – a dense three-volume tome, in which Russell introduces the extended proof of 1 + 1 = 2 with the witticism that ‘The above proposition is occasionally useful.’ Published at the authors’ considerable expense, their work set off a chain reaction that, by the 1930s, showed mathematics to be teetering on a precipice of inconsistency and incompleteness.
Eventually, I turned to physics, hoping to reground my Platonist aspirations in the eternal laws that governed the physical reality of the cosmos. But quantum theory exposed that, too, as a fantasy: even though we could define rules and equations for physical laws, we could not explain what they meant. Recent experiments in quantum information theory have shown that our most basic assumptions about reality, such as when something can be considered to have been observed and to have definite physical properties, are in the eye of the beholder.
Attempts to address these paradoxes date back to the dawn of quantum mechanics, when Albert Einstein and Niels Bohr debated how to interpret the baffling phenomena they’d uncovered. Yet it was only when I dived into the parallel milieu of Cambridge Philosophy, at the time of Ludwig Wittgenstein and Russell’s ascendancy, that I began to feel like my qualms about mathematics and physics might be addressed. Contemporaries of Einstein and Bohr, Wittgenstein and Russell didn’t engage with the quantum revolution directly. Yet it’s in the work of these philosophers that I began to see answers to some of our most fundamental questions about reality – answers that stem from recognising that we are not only asking the wrong questions; we are asking nonsensical ones.
T he great debates about quantum physics kicked off in the 1920s. Bohr and his protégé, Werner Heisenberg, were trying to figure out how to talk about the weird behaviour of quantum particles: how they appeared to ‘know’ when they were being observed, for example , and to act as a particle when observed and a wave when not observed.
How to describe this phenomenon flummoxed theorists. Heisenberg (and later, Erwin Schrödinger) came up with equations that described particles in terms of a wavefunction , where simple numbers became entities of infinite dimensions that lived in exotic mathematical spaces. The act of observation now had a complicated description that took into account what the experimenter was doing.
But all this mathematics didn’t get at what actually happened when the properties of a particle were being measured. At that moment, all the complex infinite-dimensional mathematics suddenly compressed into individual numbers, as if the particles had been there all along. Observation after observation of photons scattering on screens revealed that no simple explanation was possible. This description was deeply unsatisfying to Einstein, because the wavefunction appeared to prevent particles from having definite attributes before they were observed. Einstein wanted the wavefunction gone, replaced with some more sensible interpretation where things retained definite properties and locations. Despite decades of haggling over the incompleteness of quantum mechanics, however, the wavefunction couldn’t be dispensed with.
The view that emerged from this haggling came to be known as ‘the Copenhagen interpretation’ – coined by Heisenberg in 1955, and predicated on the presence of a fundamental split between the observer and the system being observed. Meanwhile, the polymath John von Neumann came up with an idealised mathematical description of what happened when you measured a particle’s wavefunction: it collapsed upon interacting with the observer. Where the rest of the wave went, or whether it was ever real in the first place, was anyone’s guess.
The Universe is a coin that’s already been flipped, heads or tails predetermined: all we’re doing is uncovering it
By the late 20th century, dozens of other interpretations had appeared under exotic names: the many-worlds theory, superdeterminism , consistent histories, the modal interpretation, superselection, Bohmian mechanics, Lindblad equations. I even invented my own: dynamic histories. While a few, like mine, proposed new theories that could come into conflict with quantum mechanics, most of them don’t. They are metaphysical, not physical.
The big question lurking behind all this is: what does the wavefunction mean? Does it represent something real or not? Most interpretations are ‘realist’ in the sense that they assume the wavefunction is a real entity and then go on to explain what it represents – but a few say it doesn’t exist at all, such as Quantum Bayesianism or QBism, as it is known. QBism owes its existence to the work of Wittgenstein’s friend and contemporary Frank Ramsey , who developed an anti-realist interpretation of probability. QBism holds that the wavefunction is purely an encoding of human uncertainty, representing a spectrum of probabilities that is updated when we make an observation. So the quantum wavefunction is not about objective reality at all, but about our future observations. QBism therefore refutes the Platonic idealism of the wavefunction and declares it to be a mere mathematical quantification of our beliefs.
Plenty of physicists have grown tired of this debate and its seemingly endless and unsatisfying arguments between realists and anti-realists. They want us to ‘Shut up and calculate!’ in the words of the physicist David Mermin: to stop trying to interpret quantum mechanics at all and get back to doing it. Philosophers, on the other hand, tend to dismiss this latter group as being philosophically ignorant. There’s a suspicion that, deep down, such physicists simply possess a metaphysics that they don’t want to admit, because they don’t want to come down on the side of an interpretation that has no scientific backing.
Yet those who follow Mermin’s injuction have a friend in one of the great philosophical minds of the 20th century – one who provides not only support for their position, but philosophical reasoning for why it is the only correct one.
W ittgenstein was a reluctant philosopher. Born in 1889 to a wealthy and powerful family in Vienna, Austria, philosophy seemed to be more of a compulsion for him than a love – a tendency to get stuck on certain questions, unable to move on without resolving them. Perhaps that’s why Wittgenstein felt the need to ‘solve’ philosophy once and for all, attacking its roots and, by doing so, tearing down all philosophical debates, including the broader quarrel between realists and anti-realists in all domains.
Wittgenstein was at once fantastically arrogant before his fellows and deeply humble before the questions he confronted. His task was no less than to discover what lay at the roots of logic. Starting out in the nascent field of aeronautical engineering in 1908, he quickly gravitated towards the philosophy of mathematics.
His German mentor Gottlob Frege sent him to the University of Cambridge to work with Russell. Of Wittgenstein, Russell wrote that: ‘An unknown German appeared … obstinate and perverse, but I think not stupid.’ Within a year, Wittgenstein had proved himself to Russell, who said: ‘I shall certainly encourage him. Perhaps he will do great things … I love him and feel he will solve the problems I am too old to solve.’
Russell’s motivations, however, were at odds with Wittgenstein’s. The grandson of an earl, Russell was raised in a noble household by his strict and devout grandmother. Finding no comfort in her religion, Russell sought it in mathematics, only to learn that the roots of the ancient discipline were rotten. He was horrified to discover that the geometer Euclid’s axioms, such as ‘two parallel lines do not intersect’, were just assumptions. Likewise, the number system is based on self-evident truths. If any were wrong, the whole thing might come tumbling down. Russell therefore dedicated his life to resolving all uncertainty in mathematics.
Russell – a product of the Victorian age – continued to look for certainty where there was none
Russell appropriated Wittgenstein’s philosophy to shore up basic logic, but Wittgenstein had other ideas. He wanted to understand what made facts true or false – not because he desired comfort from certainty, but because, well, it bothered him. Unlike Russell, Wittgenstein was devoted to the truth, no matter how ugly.
Wittgenstein’s life was no less unusual than his thoughts. He worked with Russell intensely from 1911-13, retreating to an isolated hut in rural Norway for months at a time in order to work out his ideas. In 1913, he returned to Austria, only to be swept up in the chaos of the First World War.
It was a time of massive upheaval at all levels of European society. Empires were in decline, and the old monarchical order was ebbing away. Women’s suffrage was in full swing, with the vote in Britain and the United States arriving after the war. Science and mathematics likewise were throwing off the shackles of 19th-century classicism. Einstein’s theory of relativity, both special and general, banished Isaac Newton’s concept of universal time and space, while Heisenberg’s uncertainty principle destroyed the certainty of measurement some years later.
Russell, meanwhile – a product of the Victorian age – continued to look for certainty where there was none. It fell to young Wittgenstein, picking up the zeitgeist, to seek to resolve the realism debate once and for all, even if that meant destroying it.
T he war years were not easy on Wittgenstein. Poor health exempted him from conscription, but he volunteered for service and eventually to go to the Front. His reasons were complex, but from his letters it seems he was seeking something that he felt he could not find in intellectual pursuits. Writing from the Eastern Front, he expressed the hope that ‘the nearness of death’ would bring about a spiritual transformation in him. Wracked by loneliness and spiritual longing, he contemplated suicide, only to be saved by faith. While before the war he’d rivalled Russell in his distaste for religion, a chance discovery of Leo Tolstoy’s The Gospel in Brief (1902) in a bookshop caused him to become a devout Christian. His faith would influence his later work, and vice versa. Captured by the Italians in 1918, he spent months in a prisoner-of-war camp.
It was during the war that he formed much of his ideas for his first great work, the Tractatus Logico-Philosophicus (1922) – a book that applied modern logic to metaphysics via language to relate facts about reality to reality itself. He called it the theory of meaning.
In the Tractatus , Wittgenstein developed a philosophy that was deeply embedded in the world – not in idealised realms of thought like the rational idealist Russell, but in how we talk about the world. Rather than coming up with a theory about how words and facts represent reality, which is crucial to both realists and anti-realists, he determined that representation is irrelevant. No one needs to say what facts and objects represent. They are simply there, embedded in our picture of reality. To say what they represent is actually nonsense, absurd. As in the visual realm, a second picture is not necessary to explain what a first picture means; if it were otherwise, we’d fall prey to infinite recursion.
Wittgenstein was serious in that he believed we could not talk about things that are not in the world
What Wittgenstein understood is that you can’t use words to explain representation, because words are representations themselves. It would be like trying to travel outside the Universe to show somebody what the Universe is – a feat that’s both impossible and unnecessary. A sentence shows what it means by its own sense. Thus, if I say ‘Jenny has an apple,’ I do not have to explain how the words ‘Jenny’ and ‘apple’ represent physical objects in the world; nor do I have to explain what ‘has’ means. We mutually understand that, if Jenny is right there and she has an orange in her hand, the proposition is false. It shows its sense. There is nothing more to say about it, as long as we both understand the rules of the language.
Thus, Wittgenstein, even in his early work, suggests that the realist versus anti-realist debate is meaningless because both sides are trying to say things that are only showable . From this early Wittgensteinian perspective, a mathematical equation – in fact, any equation, including the ones governing quantum mechanics – is like a photograph of reality. Like photographs, we do not need anyone to interpret its meaning as realist or anti-realist. We do not need a Copenhagen or a many-worlds to indicate the sense of the equation to us, because it is already as apparent as it is ever going to be. To ask what the wavefunction represents is like asking what Michelangelo’s statue of David or Van Gogh ’s painting The Starry Night represents: any explanation beyond the mere facts is insufficient and subjective.
You might find this explanation unsatisfactory. Yet Wittgenstein was serious in that he believed we could not talk about things that are not in the world. While we might talk about quantum mechanics in terms of particles, measurements and calculations, any philosophical attributes that ascribe significance to what we can observe (such as ‘real’ or ‘unreal’) are nonsense. We must be silent on ascribing additional meaning to the wavefunction.
Wittgenstein’s exploration about what we can and cannot talk about in philosophy, however, would evolve over the next several decades, and lead to a rejection of even those philosophical concepts such as the picture theory upon which he built the Tractatus .
H aving written the Tractatus , Wittgenstein believed that he had ‘solved’ philosophy. In his strange, haughty humility, he left the discipline in the 1920s, and worked various jobs as a gardener, teacher and architect.
This interregnum came to an end, however, when Wittgenstein was exposed to logical empiricism. This was a movement arising from a group of philosophers known as the Vienna Circle. They emphasised empirical knowledge and the theory of ‘logical positivism’, meaning that we can only ascribe meaning to what can be measured or observed. A strict logical positivist is unconcerned with explanations or interpretations; rather, they believe that understanding the world is built upon measurement and its prediction.
The Tractatus was a foundational pillar of the Vienna Circle, and this galvanised Wittgenstein to continue his work. He decided to return to Cambridge in 1929, but moved away from the philosophy of mathematics and logic, and towards ordinary language and psychology.
He rejects the theory of meaning entirely while making one of his most powerful contributions to it
Wittgenstein eventually collected his ideas in a book called Philosophical Investigations – probably one of the strangest books of philosophy ever published (and, perhaps for this reason, only released after Wittgenstein’s death from cancer in 1951). Rather than being organised as a sequence of topics or propositions, the Investigations is a stream-of-consciousness series of points, arguments and statements. That was in fact in keeping with its own philosophy, which is that philosophy itself can discover nothing. It is simply a form of therapy that can quickly become a disease of the intellect. Its only job is to remind us of that which we already know.
From this later Wittgensteinian position, all the varying quantum interpretations would be the result of diseased minds, and ultimately self-destructive. That’s because all philosophy is actually a debate over mere grammar. If we take seriously these metaphysical debates, he argues, we are not only wrong, but ill.
In his Philosophical Investigations , Wittgenstein rejects the theory of meaning entirely while making one of his most powerful contributions to it. All language, he says, gains its definitions from how it is used in specific cases. All language is a game like chess or poker – we learn the rules by playing, not theorising or defining. So the very notion of a universal definition is an artifice, a bit of subterfuge. One cannot talk about what words really mean; one can only use them. This applies as much to mathematics as it does to ordinary words.
W ittgenstein wants to show us that we need to stop trying to interpret language. Take the example of a road sign pointing to a village. We see the road sign and instantly understand its meaning. While there is an element of symbolic decoding involved, there is no deeper interpretive step, he says. In other words, we do not need to figure out how the sign represents reality, either in the ideal world of Plato or some subjective concept of reality in our heads. The sign could contain almost any kind of symbols, colour coding or numbers, as long as the action that people take upon seeing it is the correct one. The sign ‘shows’ us where the village is, because that is how signs of that kind are used. That is its true meaning.
The late Wittgenstein entirely rejects his own picture theory of reality. Pictures are nice and satisfying, but usage is what actually matters. The wavefunction, on this reading, isn’t like a picture of reality at all. All that matters is that physicists now have the ability to do calculations, which lead to predictions that can be verified by measurements. The point is not the measurements themselves, however – as a logical positivist might claim – but how the physicists behave . Do they calculate in a way that leads to more and better physics? Language and mathematics are a means of controlling and modifying collective human action so that work gets done.
This is language as culture rather than language as picture. And culture includes ritual. Like all ritualistic communities, physics contains its rules, interpretations, specialised vocabulary, a community of adherents who are admitted to the arcane arts, levels of indoctrination, and gatekeepers. While some societies relate ritual to the appeasement of gods and spirits, in science they serve to therapeutically appease our philosophical needs. Competition between interpretations is not unlike competition between clan gods, trying to achieve cultural dominance.
Evolutionary cultural anthropology backs up this view, having demonstrated that language is deeply connected to ritual and religion. Likewise, the vocabulary, grammar and procedures of science are themselves ritualistic, with each subdiscipline having its own mores and norms. These are necessary because it is impossible for scientists to evaluate new research purely based on factual merits; it often takes years to validate a new theory or experimental result. The role of ritual also makes perfect sense from an evolutionary perspective . Humans have spent hundreds of thousands of years navigating a hostile planet by encoding information crucial for survival into ritual, which can then be transmitted across generations. When we invented the scientific method only a few hundred years ago, we had to graft it onto that part of our nature in order to pass it down the generations, hijacking an ancient and effective cultural mechanism for a new purpose.
The activity of science, rather than its interpretations, defines what words and symbols mean
Hence, quantum interpretation is not really an investigation into reality, and it tells us nothing new about the world. Rather, it is a grammatical investigation or, in anthropological terms, a cultural one. It is a competition between differing philosophical therapies, satisfying different emotional-cultural needs.
Crucially, it is in the activity of science, whether via experiment or calculation, that all its useful information it generates exists. Wittgenstein explains, for example, how the act of determining the length of an object is not a case for learning theories and definitions, but an activity:
This description indicates that to learn what quantum physics means is to learn to calculate it – and vice versa.
Wittgenstein suggests that even mathematics is potentially a shared language and activity. He asks:
He suggests that ‘odd’ would be a better word for it, but we would have no common frame of reference to call it wrong. He goes on to suggest that mathematics is very much an activity, like a game, and we all know the same rules that form a system. Hence, we all come to the same conclusions and never argue about what is proved. Yet, some alien species could come up with different rules for their mathematical game that are no less valid because they are following their rules.
I f Wittgenstein were alive today, he might have couched his arguments in the vocabulary of cultural anthropology. For this shared grammar and these language games, in his view, form part of much larger ritualistic mechanisms that connect human activity with human knowledge, as deeply as DNA connects to human biology. It is also a perfect example of how evolution works by using pre-existing mechanisms to generate new behaviours.
The conclusion from all of this is that interpretation and representation in language and mathematics are little different than the supernatural explanations of ancient religions. Trying to resolve the debate between Bohr and Einstein is like trying to answer the Zen kōan about whether the tree falling in the forest makes a sound if no one can hear it. One cannot say definitely yes or no, because all human language must connect to human activity. And all human language and activity are ritual, signifying meaning by their interconnectedness. To ask what the wavefunction means without specifying an activity – and experiment – to extract that meaning is, therefore, as sensible as asking about the sound of the falling tree. It is nonsense.
I have come to think of the world not as filled with sharply defined truths but as a place of myriad possibilities
As a scientist and mathematician, Wittgenstein has challenged my own tendency to seek out interpretations of phenomena that have no scientific value – and to see such explanations as nothing more than narratives. He taught that all that philosophy can do is remind us of what is evidently true. It’s evidently true that the wavefunction has a multiverse interpretation, but one must assume the multiverse first, since it cannot be measured. So the interpretation is a tautology, not a discovery.
I have humbled myself to the fact that we can’t justify clinging to one interpretation of reality over another. In place of my early enthusiastic Platonism, I have come to think of the world not as one filled with sharply defined truths, but rather as a place containing myriad possibilities – each of which, like the possibilities within the wavefunction itself, can be simultaneously true. Likewise, mathematics and its surrounding language don’t represent reality so much as serve as a trusty tool for helping people to navigate the world. They are of human origin and for human purposes.
To shut up and calculate, then, recognises that there are limits to our pathways for understanding. Our only option as scientists is to look, predict and test. This might not be as glamorous an offering as the interpretations we can construct in our minds, but it is the royal road to real knowledge."
When will economists embrace the quantum revolution? | Aeon Essays,,https://aeon.co/essays/when-will-economists-embrace-the-quantum-revolution,"In her book Mother of Invention: How Good Ideas Get Ignored in an Economy Built for Men (2021), the writer Katrine Marçal argues that many useful innovations have failed to catch on because they are deemed ‘too feminine’ by marketers. A classic example is the wheeled suitcase. The wheel was invented in ancient Mesopotamia, however the possibility of attaching it to a case went against the whole idea of men showing off their strength by lugging heavy objects around, which is why wheeled suitcases weren’t a thing until 1972. As Marçal wrote in The Guardian : ‘Gender answers the riddle of why it took 5,000 years for us to put wheels on suitcases.’
Quantum is the scientific equivalent of suitcase wheels. The reason this useful innovation hasn’t caught on, or been rolled out, more generally in areas such as economics isn’t because it’s impractical or too hard – it’s because it’s too feminine. Or rather, too Female, in a sense to be defined below.
Now, that assertion will seem ridiculous to many readers for a number of reasons – beginning with the idea that quantum has somehow been ignored or repressed. Quantum physics is widely recognised as being a huge success, and is lauded for its ability to predict and explain the bizarre behaviour of tiny subatomic particles. For example, quantum physics says that subatomic entities can be in more than one place at the same time (superposition) and show both particle-like behaviours and wave-like behaviours including interference (they can cancel each other out). Something like the position of a particle is inherently indeterminate, and only takes on a definite value when measured through a poorly understood process of wave function collapse. Particles can also become mysteriously entangled, so that a measurement on one tells us something about an entangled partner, even if it is at the far end of the Universe. The ability to make sense of all this is rightly regarded as one of the triumphs of science.
Everyone also knows that quantum mechanics is both hard and highly counterintuitive, which is why only university graduates in physics and mathematics are typically exposed to it. As one university website once reassured its audience: ‘It’s OK to be a bit baffled by these concepts, since we don’t experience them in our day-to-day lives. It’s only when you look at the tiniest quantum particles – atoms, electrons, photons and the like – that you see intriguing things like superposition and entanglement.’
In this view, if quantum ideas haven’t reached a broader audience, that is a good thing, because they would be misunderstood and therefore ripe for abuse. As the physicist Sean Carroll stated in his portentously titled book The Big Picture: On the Origins of Life, Meaning, and the Universe Itself (2016): ‘No theory in the history of science has been more misused and abused by cranks and charlatans – and misunderstood by people struggling in good faith with difficult ideas – than quantum mechanics.’ The philosopher Slavoj Žižek similarly warned of ‘New Age obscurantist appropriations of today’s “hard” sciences which, in order to legitimise their position, invoke the authority of science itself.’ Stand back, social scientists, and leave the heavy lifting to the experts.
Quantum economics in particular sounds like ‘physics envy’ taken to its logical conclusion. Indeed, the assertion that quantum ideas – developed for tiny particles – could have anything to do with human systems such as the economy will seem patently absurd to most physicists. It is well known in physics that quantum effects wash out at larger scales, where classical behaviour dominates.
Finally, quantum mechanics isn’t commonly perceived as being feminine. For one thing, it is the ultimate example of a ‘hard’ reductionist science – it even has ‘mechanics’ in the name. Its ‘founding fathers’ were mostly young men in their 20s. In the postwar era it gained much of its funding and prestige from its association with nuclear weapons, which are pretty butch (and are one place where quantum effects don’t wash out). And anyway, science cares about objective results – not things such as gender. Indeed, the whole notion of gender is highly contested and the idea that entire scientific disciplines can be assigned gender labels is just unreconstructed, unsophisticated, reductionist nonsense that will offend and repel scientists, feminists and anyone with a brain.
So how on earth can it make sense in the electronic pages of this magazine to say that quantum hasn’t caught on because it is too girly?
T o start with, while quantum ideas certainly caught on in physics, they have had very little influence so far on the way that most people think about the world – apart from musings on things such as quantum healing, and something of a moment back in the 1970s with books such as The Tao of Physics (1975) by Fritjof Capra. Mentioning quantum ideas in polite conversation will see you marked as a phoney or worse. In his definition of what he calls the ‘Intellectual Yet Idiot’, Nassim Nicholas Taleb includes anyone who ‘has mentioned quantum mechanics at least twice in the past five years in conversations that had nothing to do with physics’. (Guilty as charged!)
Contrast that with the success and general social acceptability of mechanistic thinking, which is part of a Western scientific tradition whose roots extend to ancient Greece, and which has affected the way we think about everything from human psychology to the financial markets.
As the political scientist Alexander Wendt has noted , for example, the social sciences are based on a number of fundamental assumptions:
In other words, they are based on the cogs and levers of pre-quantum physics. No possibility of superposition or entanglement there.
In economics, prices are assumed to be mechanistically determined by the ‘invisible hand’ of global capitalism, where the actions of informed, rational, independent utility-optimising agents – aka rational economic man – conspire to drive prices to their optimal level, subject only to occasional ‘frictions’ or ‘market failures’, which might slow or impede the process. Markets are seen as being subject to random external perturbations that make them unpredictable, but this is a far cry from the indeterminacy of quantum systems.
One reason for this lack of uptake, as mentioned above, might be that quantum ideas really are hard for normal people, or at least those without a degree in quantum mechanics, to understand. This is certainly the standard message. Quotes that are commonly, if perhaps apocryphally, attributed to esteemed physicists include the observations that quantum mechanics is ‘fundamentally incomprehensible’ (Niels Bohr); ‘If you think you understand quantum mechanics, you don’t understand quantum mechanics’ (Richard Feynman); and ‘You don’t understand quantum mechanics, you just get used to it’ (John von Neumann).
However, it is more accurate to say that subatomic particles are hard to understand because they’re weird and almost no one has direct experience of them. And it is easy to imagine those men (and they do always seem to be men) saying the same thing about their spouses, or even their pets. ‘No one truly understands George, my tabby cat. He is a mystery even unto himself.’
Physicists tend to confuse their models with reality – after all, these are the same people who would prefer to believe that most of the Universe has somehow been rendered invisible as ‘dark matter’ than entertain the rather reasonable idea that the problem is with the ‘law’ of gravity. But in mathematical terms, quantum theory mostly boils down to being just a different form of probability, which is the next simplest after the usual one, and which naturally incorporates effects such as superposition and entanglement. The field of quantum cognition, for example, isn’t about comparing humans to invisible particles; it is about using quantum probability to model the way that decisions are shaped by things such as uncertainty and context, as when the way a question is framed or posed affects the answer.
Physicists are protective of quantum ideas, but often dislike aspects of them at the same time
In fact, the quantum physicist Niels Bohr borrowed the idea of superposition from the late 19th-century philosopher and psychologist William James, who had remarked on the human ability to hold conflicting ideas in our heads at the same time. And the concept of entanglement is hardly foreign to human experience. As Žižek also observed :
Researchers in the field of ‘quantum natural language processing’ would agree . So somehow we went from quantum physicists adopting words and concepts from social life, to social scientists omitting the same things from their study of social life. As the comedian John Cleese quipped : ‘people like psychologists and biologists have still got physics envy, but it’s envy of Newtonian physics and they haven’t really noticed what’s been happening the last 115 years .’
Viewed this way, the concern from physicists that quantum ideas will be ‘misused and abused’ in the social sciences, to use Carroll’s phrase, seems a little forced. For example, there was little outcry from physicists about what the quantitative analyst Paul Wilmott and I called the ‘industrial-scale abuse of mathematical models’ by the financial sector that led to the crisis of 2007-8 . So perhaps the problem is not with the misuse of physics-inspired models, but with worries about quantum ideas in particular.
Another reason for these concerns seems to be related to a kind of queasiness around quantum ideas in the first place. There is a strange dichotomy at play, where physicists are protective of quantum ideas, but often dislike aspects of them at the same time, and deal with this dislike by adopting a highly formal and abstract way of presenting the subject. Albert Einstein commented that the theory reminded him of ‘the system of delusions of an exceedingly intelligent paranoiac, concocted of incoherent elements of thought’, and spent years trying to show it was wrong or incomplete.
More recently, the late physicist Steven Weinberg said in an interview that quantum mechanics ‘has a number of features we find repulsive … What I don’t like about quantum mechanics is that it’s a formalism for calculating probabilities that human beings get when they make certain interventions in nature that we call experiments. And a theory should not refer to human beings in its postulates.’ (Perhaps it works better as a model of human beings.)
One problem is that, while physicists tend to claim ownership over the interpretation of quantum mathematics, they themselves have never reached a settled interpretation of what it all means. The notion of wave function collapse, for example, leads to all kinds of quandaries, which is why physicists continue to debate it, or come up with alarming alternatives such as the Many-Worlds hypothesis where, instead of the wave function collapsing, the Universe splits off into alternative paths.
The test for the use of quantum methods in the social sciences is not, then, whether people are just like particles. It is whether, if these methods hadn’t existed, social scientists would have had to invent them. Of course, this suggests another potential explanation for why quantum ideas are not applied outside of physics – which is that they just don’t work. But there is increasing evidence that they do. And what seems extraordinary, is the fact that for so long they hadn’t even been tried.
I first wrote about this for Aeon four years ago in an essay that made a case for a theory of quantum economics. The idea is that money is best understood as a quantum social technology, with quantum properties of its own. In financial transactions, for example, value can be modelled as a probabilistic wave function which ‘collapses’ down to an exact number when money is exchanged. When you put your house up for sale, you might have a fuzzy idea of its worth, but the actual price is only determined when a deal is made. An idea that seems bizarre in physics makes perfect sense in economics.
Financial contracts such as mortgages and other loans entangle the debtor and the creditor in a fashion that can be modelled using quantum mathematics. The debtor is treated as being in a superposed state, balanced somewhere between a propensity to honour the debt and a propensity to default. Methods from quantum cognition can handle those phenomena, such as mental interference between incompatible concepts, that first inspired quantum physicists.
And the argument that quantum effects don’t scale up has no relevance to economics. The idea isn’t that money inherits its quantum properties from subatomic properties, but that its properties can be modelled using quantum mathematics (the aim isn’t to use more maths, just different maths where needed). For example, the creation of money can be expressed using a quantum circuit in a way that captures effects such as uncertainty, power relationships, and so on. The effects of this substance scale up all the time (it’s called the financial system), and, like dark matter, exert a huge pull over the economy that goes undetected by classical approaches.
Of course, the article immediately attracted fierce criticism, and not just from internet trolls. One respected science writer described the piece on Twitter as ‘a load of hogwash’. Other physicists piled on to mock the article or accuse me that I had no idea how things like the mathematics of entanglement work (for the record, I am a mathematician, and it’s not that hard). One commenter summarised their feelings like this: ‘I feel bad for all the professional economists who might come across this nonsensical essay … Bad writer, bad.’
As someone who has long written about and critiqued our use of mathematical models in areas ranging from weather forecasting to particle physics to economics, I am used to receiving robust feedback on my work – but something about this felt different, like I had crossed a line. So what is it that makes quantum special? What is it that makes physicists so excited about maintaining control over it? And what line had I crossed which made the article so ‘bad’? The answer, oddly, might have something to do with gender – not with mine, or anyone else’s, but rather with a classical conception of gender.
Part of my above-mentioned critique of science is that the way we approach the subject is affected by a degree of bias, which can be traced back to the birth of Western philosophy and science in ancient Greece. Greek philosophy was dualistic and also what we would describe as blatantly sexist. The Pythagoreans, for example, saw the Universe as governed by opposing principles, which were divided into Good and Evil, and which included Male versus Female. Women were allowed into the group, but the female archetype was still associated with darkness and evil. Plato described women as originating from morally defective souls in Timaeus , and he and Aristotle excluded them from their schools.
The split between genders was tied up, in Greek philosophy, with the split between the real world and abstract ideas. The former was associated in Greek culture with the Female principle, the latter with the Male principle. According to the science writer Margaret Wertheim , writing in The New York Times : ‘Mathematics was associated with the gods, and with transcendence from the material world; women, by their nature, were supposedly rooted in this latter, baser realm.’ There were no female philosophers to argue against this, because they weren’t admitted to the club.
A performative emphasis on hard objectivity is the scientific equivalent of lugging a heavy suitcase up a flight of stairs
Since then, science has been dominated by men. In his book The Masculine Birth of Time , the 17th-century inventor of the scientific method, Francis Bacon, described the role of science as being to ‘conquer and subdue [Nature]’ and ‘storm and occupy her castles and strongholds’. When the Royal Society was founded in 1660, its secretary Henry Oldenburg, a theologian and natural philosopher, defined its aim as being to construct a ‘Masculine Philosophy’. Women began to be admitted to universities in significant numbers only in the early 20th century, with physics departments among the last to open their doors. As the philosopher Sandra Harding wrote in 1986: ‘Women have been more systematically excluded from doing serious science than from performing any other social activity except, perhaps, frontline warfare.’ With the result, as the physicist and feminist scholar Evelyn Fox Keller put it in 1985, that modern science was developed ‘not by humankind but by men.’ As already mentioned, quantum physics was constructed mostly by a small group of young men.
All of this has affected the way we do science. The philosopher Mary Midgley compiled a list of opposites in 1985, reminiscent of the Pythagoreans’ idea of opposing principles, which included:
Midgley commented that the list served for scientists as a ‘mental map … marked only with the general direction “keep to the left”’.
A similarly performative emphasis on hard objectivity – the scientific equivalent of lugging a heavy suitcase up a flight of stairs, while sweating profusely and wearing a rictus grin – is seen even in the social sciences, which take their cues from physics. In 1913, the psychologist John B Watson wrote : ‘Psychology, as the behaviourist views it, is a purely objective, experimental branch of natural science … it can dispense with consciousness in a psychological sense.’ A century later, the political scientist Alexander Wendt noted that ‘in most of contemporary social science there seems to be a “taboo” on subjectivity’, which is odd given that social relations are surely based largely on subjective factors.
Economics seems to be something of an extreme case, and remains, as the sociologist Elaine Coburn observed in 2016, ‘remarkably “pre-feminist”’. According to the economics professor Veronika Dolar: ‘there’s a strong case to be made that economics is the worst academic field in which to be a woman.’ One recent study used data science to analyse the gender gap, and concluded that the discipline was best described as ‘a crushing and unrewarding environment for female economists’. Not much of an advance over the ancient Greeks.
Mainstream economists, as the political economists Shimshon Bichler and Jonathan Nitzan noted in 2021, see their field as ‘the “hardest” social science of all’, which again has shaped the way it is practised. The feminist economist Julie A Nelson wrote in 1996 that: ‘Analytical methods associated with detachment, mathematical reasoning, formality, and abstraction have cultural associations that are positive and masculine, in contrast with methods associated with connectedness, verbal reasoning, informality, and concrete detail, which are culturally considered feminine.’ And yet most mainstream economists would reject the idea that their discipline has been shaped by such factors.
Consider, for example, the paper from 2020 in which the Nobel Memorial prize-winning economist George Akerlof puzzled over the question of why economics ‘gives rewards that favour the “hard” and disfavour the “soft”’. There is an entire section titled ‘Reasons for Bias toward Hard’, which manages to avoid the obvious one, namely association with a certain kind of masculinity. Indeed, his piece does not even mention words such as ‘women’, ‘female’ or ‘gender’. Obviously, he had never read Midgley, who had already explained how the map worked 35 years earlier.
N ow, I should again point out (and I feel my audience shrinking as I type – bad writer, bad) that this argument, raised in previous books, about the ongoing influence of ancient archetypes on modern science, doesn’t elicit a unanimously positive response; one physicist even worried that it was intended as a joke on the reader, which I can assure you is not the case (though humour is a help). Perhaps scientists see themselves as truth seekers who are free of such cultural influences. However the issue does seem especially relevant to the quantum approach – because quantum mixes hard and soft by design.
A defining feature of quantum mechanics, after all, is that it looks hard, but the picture that it paints of reality is soft and fuzzy. In many respects it isn’t a hard science, but a soft science. A wave equation, for example, looks hard when it is written out as a mathematical formula – but it is an equation of a wave, which is soft.
Instead of atoms being hard and independent – as the feminist theologian Catherine Keller notes, there is a strong correspondence between the ‘separate, impenetrable’ Newtonian atom and the male sense of self – they are indeterminate and entangled. Instead of predictive certainty, we have the uncertainty principle. If quantum mechanics had been invented, and its evolution and interpretation shaped, mostly by women instead of those young men – if its ‘founding fathers’ had been ‘founding mothers’ – we would be calling it the most feminist theory ever.
Quantum is therefore a soft science dressed up to look hard. When male physicists first stumbled upon these ‘soft’ quantum properties of matter, it is unsurprising that, rather than embrace their classically defined feminine side, they reacted by adopting a hardcore mathematical approach summed up later by the physicist David Mermin as the direction to ‘Shut up and calculate!’ Which, to non-physicists, reads like: ‘Keep away – this is much too hard!’
In contrast, the social science version counted women and feminists among its first inventors. Danah Zohar, who trained as a physicist, described how her book The Quantum Self (1990) was inspired in part by her experience of pregnancy and early motherhood: ‘There is something deeply feminine about seeing the self as part of a quantum process.’ Or as the feminist theorist (and trained physicist) Karen Barad put it in her quantum-queer-feminist (if that’s a thing) book Meeting the Universe Halfway: Quantum Physics and the Entanglement of Matter and Meaning (2007): ‘Existence is not an individual affair. Individuals do not pre-exist their interactions; rather, individuals emerge through and as part of their entangled intra-relating.’
The concept of ‘rational economic man’ will be replaced with something a little more uncertain and entangled
One of the most obvious features of modern science is that it carries with it the imprint of ancient divisions and biases. And one of the most obvious features of quantum ideas is that they undermine everything that might be considered ‘Hard’ and ‘Male’ about reality according to this (rather dated) scheme. Instead of being clearly defined and firmly independent, both mind and matter are better described as indeterminate and entangled. Which goes a long way to explain the rather remarkable fact that these quantum tools and ideas, which are designed to analyse such properties, have been effectively kept in their box for more than a century.
Of course, the universe is not ‘Male’ or ‘Female’ and nor does it align itself with ancient Greek archetypes. However, it would be naive to think that the same can be said of the human pursuit of science. In particular, as Barad wrote: ‘It would be ironic to find that the physical sciences, those sciences that have traditionally been most exclusive of women and people of colour, are unmarked by the politics of race, ethnicity, class, gender, sexuality, and other critical social variables.’ Or to think that the same variables have not affected economics.
Over the past few years, interest in applying quantum methods to other fields has grown considerably. Wendt and his colleagues received a grant from the Carnegie Corporation to host a series of ‘quantum bootcamps’ for social scientists. These are held at the Mershon Center for International Security Studies at the Ohio State University, and taught by an eclectic group, whose specialities include philosophy, psychology, physics, political science and applied mathematics (I present a section on quantum economics). And one group that certainly sees potential in quantum ideas is the specialist area of quantitative finance – as evidenced in by the Financial Times headline in 2020: ‘Wall Street Banks Ramp Up Research Into Quantum Finance.’ The excitement is, so far, mostly driven by the potential of using quantum computers, but interest is growing in ‘quantum-native’ applications, based on ideas from quantum economics, which can run on custom quantum circuits. An article in The Economist in 2021 noted that ‘finance bears a striking resemblance to the quantum world’ and concludes: ‘One way or another, finance will catch up.’
The real long-term impact of quantum ideas in economics won’t be to help traders make money, but to change the way that we think about the economy by replacing the concept of ‘rational economic man’, which serves as the atom of the classical model, with something a little more uncertain and entangled. As the former central banker Andrew Sheng told the Bretton Woods Committee, in a report commemorating the 75th anniversary of that postwar economic agreement:
In other words, the time has come to strap quantum wheels onto our models of the economy, and the world. This isn’t hard. It’s the opposite of hard."
‘Shut up and calculate’ does a disservice to quantum mechanics | Aeon Essays,,https://aeon.co/essays/shut-up-and-calculate-does-a-disservice-to-quantum-mechanics,"Physics is important. We rely on it to provide us with valid conceptions of the nature of the physical world and how it works, conceptions that underpin almost every aspect of our technologically advanced society. At root, physics as a discipline relies on foundational theories of space and time, and of matter and light. For the most part, physicists are content to make use of foundational theories that have remained broadly unchanged for centuries. These are good enough for most practical purposes. But as they explore the physics of the very fast, or of the very small, or as they ponder the large-scale structure of the Universe, they reach for younger theories that were established only a century ago. These are quantum mechanics and Albert Einstein’s theories of relativity.
Mechanics is that part of physics concerned with stuff that moves, and quantum mechanics is the theory of the motion of matter and light at the smallest scales: the realm of molecules, atoms, subatomic particles (such as electrons), and photons, the quanta (or ‘atoms’) of light. If you want to figure out how an electron will behave as it moves in time through space, then you need to reach for quantum mechanics.
But there’s a problem.
Quantum mechanics was discovered and developed largely by European physicists in the mid- to late 1920s. As they struggled to comprehend what nature was trying to tell them, these pioneers understood only too well what they were getting themselves into. Although there had been much discussion about the philosophical interpretation of some concepts that appear in the older theory that preceded it – now called classical mechanics – the nature and structure of the new quantum mechanics begged all kinds of difficult questions about the very purpose of a scientific theory, if not the purpose of science itself. The debate became polarised around the philosophies of its two principal protagonists: Einstein and the Danish physicist Niels Bohr. The small community of quantum physicists in continental Europe formed into two distinct camps, and the Austrian-born British philosopher Karl Popper later called this divergence a schism. At the heart of the debate was the interpretation of the theory’s central concept – a mathematical object called the wave function.
The wave function was introduced in the theory as a way of accounting for the surprising experimental behaviour exhibited by quantum entities such as electrons. Under certain circumstances, this behaviour can be described in terms of electrons as familiar self-contained particles, localised as they move through space. But in different (and mutually exclusive) circumstances, the behaviour can be understood only in terms of electrons moving and spreading out through space as unfamiliar, non-localised waves. The wave function accommodates this odd duality. It has obvious wave-like properties, but also obvious particle-like properties, such as mass. It underpins a formula that assigns probabilities for any given electron existing in any one place at a particular point in time. What we might have previously judged to be physically impossible, quantum mechanics judges to be merely improbable, lending a fungible quality to reality, and challenging the truth of a universe defined by the physics that came before.
And here’s the rub. We never observe the wave function. If we push an electron through a narrow aperture, we imagine that it will diffract, spreading out in all directions in the space beyond as a wave (think of what happens to a rolling ocean wave as it squeezes through a gap in a harbour wall). If we now allow this electron to impinge on a screen covered with a photographic emulsion, we will find that the electron is detected, leaving a single bright spot at a specific point on the screen. Repeating this with more and more electrons will give us a diffraction pattern – a pattern possible only with waves – made up of a myriad of individual spots, each of which is possible only with particles. Where will the next spot appear? We have no way of knowing in advance. All we can do is use the wave function to calculate the probability that the next electron will be detected here, or there, or way over there.
What are we supposed to make of this? If we interpret the wave function realistically, as a tangible physical thing, we then have to figure out how it ‘collapses’ to produce a spot at only one location out of all the other probable locations on the screen. Such a collapse implies what Einstein in 1927 called ‘an entirely peculiar mechanism of action at a distance’ – an anathema of ghostly physical effects transmitted instantaneously across space with no apparent direct cause, now generally referred to as the ‘measurement problem’. For Einstein, the lack of any kind of physical explanation for how this is supposed to happen meant that something is missing; that quantum mechanics is in some way incomplete.
Bohr disagreed. He argued that in quantum mechanics we have hit a fundamental limit. What we observe is quantum behaviour as projected into our classical world of direct experience. As we cannot transcend this experience, we have to accept that the wave function has no physical significance beyond its relevance to the calculation of probabilities. We must be content with a ‘purely symbolic’ mathematical formalism that works. The wave function doesn’t collapse (and there’s no peculiar action at a distance) because it doesn’t actually exist, and so there is no measurement problem. In other words, all we can know is the electron-as-it-appears in different experimental arrangements. We can never know what the electron really is.
This is an empiricist, ‘antirealist’, or (to some) an ‘instrumentalist’ interpretation, which judges a theory to be largely meaningless except as an instrument to connect together our empirical experiences. Such an antirealist theory doesn’t necessarily deny the existence of an objective reality (we can happily continue to assume that the Moon is still there even if nobody looks at it or thinks about it), nor does it necessarily deny the reality of unobserved electrons, however we imagine them. But it does deny a direct and exact correspondence between the wave function and the things that the wave function purportedly describes. The formalism appears simply to encode our experiences of quantum phenomena in ways that allow us to calculate the probability that this or that will happen next. Quantum mechanics is complete, and we just need to get over it.
T his, in essence, is the Copenhagen interpretation of quantum mechanics, named for the location of Bohr’s Institute for Theoretical Physics in Denmark. It is most closely associated with Bohr, whose writings on the subject are famously obscure to the point of impenetrability, though we will see below that this interpretation comes in different flavours and some care is required. As the US-born British physicist David Bohm explained in a 1987 interview : ‘The main point was whether you could get a unique description of reality. And Einstein took the ordinary view of a scientist that you could, and Bohr said you couldn’t … [Einstein] didn’t accept that Bohr’s approach could be taken as final, and Bohr insisted that it was.’ In a letter to Erwin Schrödinger in May 1928, Einstein called it a ‘tranquilising philosophy’.
The popular reading of subsequent history suggests that Bohr emerged the victor in the debate, browbeating the presumed-senile Einstein into submission, and the Copenhagen interpretation became a dogmatic orthodoxy. The Northern Irish physicist and quantum dissident John Stewart Bell was one of only a few physicists of the time prepared to push back against this orthodoxy, writing in 1981: ‘Making a virtue of necessity, and influenced by positivistic and instrumentalist philosophies, many came to hold not only that it is difficult to find a coherent picture but that it is wrong to look for one – if not actually immoral then certainly unprofessional.’
This reading was the basis for a column in Physics Today magazine in April 1989 by N David Mermin, a professor at Cornell University. He was concerned with attitudes towards quantum mechanics and how these had evolved from generation to generation of physics students in the US. Though few of his generation were likely to brood at length about what it all meant, Mermin expressed some personal discomfort with the Copenhagen interpretation. He wrote: ‘If I were forced to sum up in one sentence what the Copenhagen interpretation says to me, it would be “Shut up and calculate!”.’ Mermin’s meme would go on to become part of modern quantum folklore.
‘Shut up and calculate’ is the perfect foil, tantamount to declaring enough is enough
More years passed. Some commentators began to hint that ‘Shut up and calculate’ had actually been coined not by Mermin but by the charismatic US physicist Richard Feynman. In a follow-up column for Physics Today published 15 years later, Mermin was able to convince himself that it was indeed he who first used the phrase in the context of quantum foundations. He was also in no doubt about who was to blame, as he drew on
The phrase has since become deeply embedded in the literature on quantum foundations, repeated in academic papers and in popular articles and books. It has become a handy put-down, an easy slight, a catchy synonym, summarising in just four words everything that is wrong with a dogmatic, orthodox interpretation that insists there is nothing more to be understood from a supremely successful theory of physics that – to many – leaves just too many unanswered questions. For those seeking to push a preferred realist alternative, such as Sean Carroll in his bestselling popular book Something Deeply Hidden (2019), ‘Shut up and calculate’ is the perfect foil, tantamount to declaring enough is enough, demanding that we look again.
But this doesn’t quite add up.
If, as Mermin suggests, those Harvard teachers berating him in the late 1950s were indeed ‘agents of Copenhagen’, this would imply that they had studied the literature (especially Bohr) and had fully signed up to the Copenhagen orthodoxy. But, despite what a superficial reading of history might imply, the ‘Copenhagen interpretation’ didn’t actually exist as such until the mid-1950s (try typing ‘Copenhagen interpretation’ into Google’s Ngram Viewer). And this version of Copenhagen is largely an invention of the German physicist Werner Heisenberg, seeking rehabilitation with the international physics community after the war. Heisenberg’s interpretation differed from Bohr’s in many key respects, particularly in the former’s willingness to admit a substantial subjective element.
Make no mistake, the physicists of the 1950s understood that there was an orthodox interpretation. But what was known only vaguely from the early 1930s as the Kopenhagener Geist (the Copenhagen ‘spirit’ – Heisenberg again) was far from widely shared by US physicists. Harvard’s Percy Williams Bridgman had developed his own firmly empiricist philosophy of science, called operationalism, in 1927. Bridgman’s student Edwin Kemble, the first American to write a doctoral dissertation on quantum mechanics, had no need of the Copenhagen spirit. Neither did the Americans Edward Condon and Philip Morse, who wrote the first English-language textbook on quantum mechanics, published in 1929 (they referred questions on interpretation to Bridgman’s book The Logic of Modern Physics ).
It’s possible that the only entry point for the Copenhagen spirit into mainstream physics in the US during this period came from J Robert Oppenheimer’s lectures on quantum mechanics at Berkeley in the 1930s. But although Oppenheimer would later evangelise Bohr’s philosophy, at the time he delivered these lectures his understanding of Bohr was filtered through Wolfgang Pauli, with whom Oppenheimer had worked in Zurich in the late 1920s, and who had published his own text on quantum mechanics in the Handbuch der Physik in 1933.
Oppenheimer’s lectures informed Leonard Schiff’s student textbook Quantum Mechanics , first published in 1949, which would be used to teach quantum mechanics throughout North America, Europe and Asia, through three editions spanning 20 years. Schiff’s treatment of interpretation and problems related to measurement was rudimentary at best, and did nothing to satisfy the curiosity of the young Bell, in his final year of undergraduate study at Queen’s University in Belfast. In fact, according to Andrew Whitaker’s biography of Bell, it led him to conclude that Bohr was ‘annoyingly vague, and, indeed, [Bell] felt that, for Bohr, lack of precision seemed to be a virtue’.
But such observations relate only to the minority of physicists in the US who remotely cared about aspects of the philosophy of science and the interpretation of quantum mechanics; it appears that the majority just didn’t care at all.
U nlike in Europe, theoretical physics in universities of the prewar US was not the lofty preserve of a few exalted specialists, able to exert influence through the unquestionable authority of an academic hierarchy, until death. Physics departments in the US were more inclusive, collaborative and inherently democratic, with theorists working directly alongside their experimentalist colleagues. Their hierarchies and reward structures favoured theorists who engaged in experiments, and who could perform the theoretical calculations that were becoming increasingly difficult for the experimentalists to perform for themselves. On meeting Oppenheimer for the first time, the experimentalist Arthur Compton was impressed, judging him to be a model US theorist: ‘one of the very best interpreters of the mathematical theories to those of us who were working more directly with the experiments’.
This practical mindset extended to the students, most of whom had, according to the Dutch-born physicist Samuel Goudsmit, ‘at one time or another taken the family car apart and had put it together again’. Such ‘hands-on’, ‘can-do’ instincts fit comfortably within a culture that, from the 19th into the 20th century in the US, reputedly paid less attention to philosophy than any other country in the civilised world, and which had continued to foster, in the words of the historian Richard Hofstadter, a deep-seated anti-intellectualism, ‘a resentment and suspicion of the life of the mind and of those who are considered to represent it; and a disposition constantly to minimise the value of that life’.
The dominance of a more philosophically inclined European physics was soon to be ended by an avalanche of discoveries in nuclear physics in a time of impending war, the forced emigration of leading European physicists, and an allied atom bomb programme that at the time cost $2 billion and would of urgent necessity prize application above all else. It would be the bolder, brasher, more empirical ‘hands-on’ style of US theoretical physics that would come to dominate the postwar world.
The success of the Manhattan Project led to a postwar boom in student enrolments at university physics departments in the US. With most students electing to study physics as a means to a more financially rewarding end, the student body became noticeably less curious, more narrowminded and conformist. As the science historian David Kaiser puts it , physics in the US became ‘suburbanised’. Those students inclined to seek research careers in academia or the national laboratories relied on federal money and the major sources of funds, especially the US Atomic Energy Commission and the US Department of Defense, were ‘mission-oriented’. Even the National Science Foundation sought to avoid granting funding requests judged to lie outside mainstream physics. None encouraged the investigation of foundational questions. Research advisers, many already inclined towards empiricism or plain indifference, sought to steer their students towards projects more likely to attract funding, and so more likely to provide a firm basis on which to build careers as ‘professional calculating physicists’.
The dominance of US postwar science meant that such attitudes were inevitably exported back to Europe, and they continue to this day. In April 2018, I was invited to talk about Quantum Reality (2020), a new popular book I was then working on about the interpretation of quantum mechanics, at a dinner hosted by the Royal Society in London. After dinner, I was approached by a number of esteemed fellows who took the trouble to explain to me that ‘nobody cares about this’.
It was this ‘dogma of indifference’ that Mermin had experienced as a student in the late 1950s, and which he had retrospectively identified as a preference for the Copenhagen interpretation. Foundational questions were judged to belong in a philosophy class, and there was no place for philosophy in physics. As he explained to me in December 2019, his professors were ‘just indifferent to philosophy. Full stop. Quantum mechanics worked. Why worry about what it meant?’
The exploration of seemingly pointless philosophical issues can have profound practical consequences
In a quick follow-up discussion with me in July 2021, Mermin confessed that he now regrets his choice of words. Already by 2004 he had ‘come to hold a milder and more nuanced opinion of the Copenhagen view’. He had accepted that ‘Shut up and calculate’ was ‘not very clever. It’s snide and mindlessly dismissive.’ But he also felt that he had nothing to be ashamed of ‘other than having characterised the Copenhagen interpretation in such foolish terms’.
So, at what point did it become fashionable to gather together all the ills of quantum mechanics – all those conundrums that arise only in realist interpretations – and bundle them into a demonised version of the antirealist Copenhagen interpretation? The motives are fairly obvious. It’s hard to criticise a vague and amorphous culture of indifference in anything other than the most general terms and, in any case, such indifference is an issue for the sociology of science, not its content. Those more inquisitive physicists and philosophers looking to develop a more realist alternative interpretation needed a better foil, a more meaningful straw man to knock down.
And here was Bohr’s notorious obscurity and a handy, dogmatic, orthodox interpretation, a dogma that was not inspired by Bohr, but that was nevertheless inescapably associated with him. ‘Everybody pays lip service to Bohr,’ Bohm explained in 1987, ‘but nobody knows what he says. People then get brainwashed into saying Bohr is right, but when the time comes to do their physics, they are doing something different.’ Overlook (or ignore) its fragmented nature and questionable paternity, and the ‘Copenhagen interpretation’ is a great platform on which to build your counterarguments, or deepen discontent in order to foment your revolution. Or sell a few more books.
One of my favourite examples of this trend is an article by the US theorist Bryce DeWitt published in 1970 in Physics Today : ‘According to the Copenhagen interpretation of quantum mechanics,’ he wrote, ‘whenever a [wave function] attains a [certain form pertaining to measurement] it immediately collapses.’ DeWitt was seeking to validate an alternative reality based on the idea of ‘many worlds’, and no doubt his contrived version of Copenhagen helped him to breed discontent with the prevailing orthodoxy.
The timing is about right. The work of Bohm in the late 1950s, and Bell in the ’60s, had, by the early ’70s, led to another extraordinary conclusion. A so-called ‘locally real’ interpretation of quantum mechanics in which entities like photons or electrons are assumed to have intrinsic properties all along – and not just at their point of observation or measurement – makes predictions that differ from ‘ordinary’ quantum mechanics. It was realised that these predictions could be tested experimentally. Such tests have been performed at regular intervals ever since, with ever-increasing sophistication and precision, confirming that, despite how reasonable they might seem, all locally real interpretations are quite wrong. These experiments have, nonetheless, spawned entirely new disciplines – of quantum information and quantum computing – demonstrating that exploration of seemingly pointless philosophical issues can have profound practical consequences.
The deliberate conflation typified by DeWitt’s article has led to a world of confusion. In a 2016 survey of physicists, conducted by Sujeevan Sivasundaram and Kristian Hvidtfelt Nielsen at Aarhus University in Denmark, it was found that just a minority of physicists truly understood the meaning of the Copenhagen interpretation or the foundational concepts of quantum mechanics – based on the idea of a probabilistic universe, in which a particle is neither here nor there until measured, described by the wave function itself. In fact, only a minority of respondents had a proper grasp of the measurement problem that launched the field.
Mermin should be forgiven for following a trend that, by 1989, was entrenched in the quantum cultural mindset. I did much the same in my first book on quantum mechanics, published in 1992. We have both since learned to be more circumspect; we have to acknowledge that a dogma of indifference to philosophical questions was at least as much to blame for the rejection of foundational enquiry as anything Bohr might have said. Of course, the first to give expression to a meme such as ‘Shut up and calculate’ can claim no ownership over it and cannot control how others will use it. Irrespective of the historical rights and wrongs, those who continue to use it as a term of abuse directed at the Copenhagen interpretation are perfectly at liberty to do so.
But there is a growing number of commentators who are both familiar with the history and prepared to call this out. The purpose of this essay is to help you do the same."
How physics at the roots of reality point to a grand unified theory | Aeon Essays,,https://aeon.co/essays/how-physics-at-the-roots-of-reality-point-to-a-grand-unified-theory,"When trying to explain what motivates me as a physicist, the film A Passage to India (1984) comes to mind. Based on the play by Santha Rama Rau, adapted from the novel by E M Forster, it describes the fallout from a rape case in the fictional city of Chandrapore, during the British Raj in India in the 1920s. What keeps the viewer’s attention is the subtlety of the relationships between the characters – particularly the fragile friendship between the man accused of the rape, Dr Aziz, and an Englishman, Mr Fielding. Data about identity alone, such as race, class, gender or educational status, can never reveal these dynamics nor capture why they fascinate us. When the case arrives in court, ostensibly similar people behave very differently in relation to the defendant. The dynamics of individual behaviour trump any immutable labels we might apply; yet these static labels also impose constraints on just how far any individual can go. We watch, we theorise, and we update our knowledge of the characters and the forces at work. By the end, we find that Fielding and Aziz are more alike than we’d thought, having created a new bond on the basis of a more complete understanding of one another.
The curiosity that drives many particle physicists isn’t so different from what keeps us watching A Passage to India . The obvious and immutable data about the identity of elementary particles include their spins, their electric charges and their masses. From muons to charms, we can learn such information pretty quickly. But it takes years, even lifetimes, to reveal both the nature and degree of their relationships. The neutrino, for example, was introduced in 1930 by Wolfgang Pauli, who needed to account for the fact that energy was conserved when a nucleus broke apart. But he would never have guessed how deep the relationship is between a left-handed spinning electron and the neutrino. It took more than 40 years of careful observations and ingenious theoretical work to reveal the deeper unified relationship they have together: via the fundamental force we now know as the ‘weak force’. That’s where the deepest and most satisfying learning in particle physics is to be found: through painstaking observations and the sifting of evidence comes a creative willingness to allow for multiple possibilities.
With the discovery of the Higgs boson in 2012, every elementary particle predicted by the Standard Model of physics has now been found. Yet the field is far from ‘done’. Among the continuing work , physicists are still looking for a grand unified theory that explains the forces that operate at the subatomic level – a common understanding that accounts for the disparate phenomena we observe among the particles we have in hand. Not everyone agrees that this is worthwhile or even possible; some think we finished learning new things about elementary particles in 2012, and we must accept the cacophony of unrelated details in our physics tables. But I believe that to understand nature at its foundations, it’s necessary to push further, to unearth more subtle and surprising relationships beneath the surface of what we see. Our observations to date support the idea that a unified theory of subatomic forces can be achieved. If true, it would revolutionise our understanding of nature far beyond any discovery of particle physics in the past half century – akin to theological transitions from polytheistic religion (many deities, many fundamental forces) to a monotheistic religion (one unified God, one unified force).
U nification revelations – ‘they are more like each other than we thought’ – have been remarkably productive throughout science. We now know that nature is often simpler and more cohesive than it seems. For most of human history, our theories for why planets move was disconnected from beliefs about why boulders tumble down mountains and apples fall off trees. But in 1687, Isaac Newton revealed that gravity offered a single, unifying explanation. All the explanations that had one ‘force’ for planets wandering in the sky, and another for apples being pulled to the ground, were brought together in one economical framework.
Other odd forces revealed themselves to us, but good explanations were slow to arrive. Between his duties attending to the medical needs of Queen Elizabeth I and her court, the physician and physicist William Gilbert wrote his magnum opus De Magnete at the start of the 17th century on the forces and attractions of electric charges that explained the workings of a compass. But the challenge of how to reconcile electrical charges with magnetic attraction and repulsion fascinated and confused natural philosophers for centuries thereafter. The crowning achievement came in 1861, when James Clerk Maxwell unveiled a set of equations that put electricity and magnetism on equal footing. The theory of electromagnetism showed that they were ‘more alike than you think’.
What led Einstein to general relativity were thoughts of unifying disparate objects
However, a conundrum remained in Newton’s theory of gravity and his laws of motion. The mass of a particle that’s used in equations to predict the particle’s acceleration when subject to any force (electromagnetic force, gravitational force, force due to a spring, etc) is mysteriously exactly the same mass that’s used in different equations to determine what gravitational force exists between the particle and some other body. The first kind of mass is called the ‘inertial mass’ and the second kind ‘gravitational mass’. Newton had to arbitrarily assume their exact equivalence to get the correct answers, even though there was no compelling reason why it had to be so.
However, Albert Einstein’s general theory of relativity solved this mystery by theorising that there’s a single unified origin for both types of masses. Einstein recognised that the feeling of total weightlessness when you’re in freefall, even in the presence of gravity, is because of the equivalence of your inertial and gravitational masses. He elevated this observation to the principle of equivalence . In an acclaimed review article on relativity in 1907, he concluded that any new gravitational theory that included his new concepts had to conform with the principle of equivalence. It was this idea that ultimately helped him complete the formulation of the general theory of relativity in 1915.
What’s so interesting about the principle of equivalence, from our point of view, is that it could just as easily be called the principle of mass unification . What led Einstein to general relativity were thoughts of unifying disparate objects (these masses are ‘more alike than you think’), which in the old theory had no reason to be connected to each other. Newton unified planetary orbits and apple falls; Maxwell unified electricity and magnetism; and Einstein unified inertial mass and gravitational mass.
W hat new frontier can we identify in nature that calls out for deeper understanding of the relationships between particles – a new principle in the tradition of unifying planetary orbits with falling apples, electricity with magnetism, and inertial mass with gravitation? A good answer is a tighter relationship between elementary particles through the unification of certain forces that determine their interactions, known as the gauge forces . These three forces are electromagnetism, the weak force and the strong force.
With these three forces come many gauge bosons – a fancy way of describing the particles that are exchanged in order to activate the forces. There are a total of 12 such gauge bosons, or force carriers, in the Standard Model. There is one electromagnetic gauge boson (the photon) associated with the electromagnetic gauge force, three weak gauge bosons ( W+ , W- , Z) associated with the weak gauge force, and eight strong-force gauge bosons (the gluons) associated with the strong gauge force.
The electromagnetic force is mediated by photons, which get exchanged between particles that feel electric attractions and repulsions. The weak gauge force is what causes many particles to decay into others. For example, a neutron will spontaneously fall apart into three new particles: a proton, an electron and an antineutrino. We didn’t understand exactly how this decay could happen. After all, neither the neutron nor the neutrino have an electric charge, so they can’t talk to each other via photon exchange of the electromagnetic force. So if not via photons and electromagnetism, what force could enable the decay? We subsequently learned that something happened between the existence of the neutron and the appearance of the other three particles that explained the decay: a very brief, quantum mechanical, split-second existence of a W boson particle that’s exchanged between the particles, acting as a vehicle for the weak force. Just like the photon allows charged particles to interact with each other, the W boson allowed the proton, neutron, electron and neutrino to interact with each other, enabling the neutron to decay. Radioactive nuclear ‘beta decays’ proceed by the same weak-force exchange of W bosons. Meanwhile, the strong force is what keeps quarks together inside the proton and neutron.
Our observations to date push us in the direction of entertaining the existence of an Ur-theory of nature
Now, what if all three of the gauge forces were to be unified into a grand unified force – a single Ur-force? What would the observational consequences of such a reality be? For one, the relative charges of each particle under all three gauge forces would have to follow a very particular pattern consistent with what a grand unified force would require. Secondly, the strength of each of the three forces would need to converge to a unified strength as we go to higher energies. Third, there would be new particles beyond those we have already seen. And finally, there would be decays and interactions among known particles that are forced on us, even at low energies, by the grand unified theory.
Our observations to date push us in the direction of entertaining the existence of an Ur-theory of nature. Consider the fact that a left-handed electron has an electric charge of - 1 under the electromagnetic force, a charge of 2 (a ‘spin charge’) under the weak gauge force, and a charge of 0 under the strong force. At the same time, the right-handed down quark has an electric charge of - 1/3 , a weak force charge of 0, and a strong force charge of 3 (a ‘3-dimensional unitary group charge’, though the mathematical details don’t need to be understood here). So, between these two particles, we have charges of 0, - 1/3 , - 1 , 2 and 3, etc for the different forces arranged in a particular manner. It’s a motley crew of jumbled-up numbers, which doesn’t seem to have much rhyme or reason to it. However, a school of mathematics known as group theory tells us that this is exactly the collection of charges that are needed to form a new grand unified particle: let’s call it P, which can be represented as P=(left-handed electron, left-handed neutrino, right-handed down quark).
Likewise, we can analyse more particles in the Standard Model, such as right-handed electrons, right-handed up quarks and left-handed up and down quarks. After many measurements, we find another set of willy-nilly values for the charges they display under all three gauge forces. But upon closer inspection using group theory mathematics, we find that those numbers also magically fit exactly into a single grand unified particle: W=(right-handed electron, left-handed down quark, right-handed and left-handed up quarks). It’s as though 10 very raggedy puzzle pieces scattered on the floor were pieced together to make a perfect circle.
It didn’t have to be this way. The charges of the elementary particles in our Universe could have been such that there was no way to unify any two or more of them into a single unified particle. It’s the combination of observational data and mathematics that offers us strong hints that the charges for elementary particles in the standard model aren’t arbitrary, but rather arise by virtue of being embedded into a grand unified theory framework.
T here’s a second set of observational data hinting that the unification of the gauge forces is nature’s choice. This comes from measuring the strengths of the forces. When we measure the strength of the electromagnetic interaction and compare it with, say, the strong interaction, we get a very different answer. For particles colliding with energies about 100 times that of the proton mass, the weak-force interaction strength is a weak =0.033 , which contrasts with a strong-force interaction strength of about a strong =0.118 . We see that αa weak is much less than αa strong , which is hardly what we’d expect if the forces are unified. Rather, we’d normally think that they should be the same if they were truly ‘unified’.
The resolution of this conundrum is that the strength of the forces depends on the energy scale at which they are evaluated. That is, the forces should unify at the energy scale where the grand unified theory is valid, and not at lower energies where the grand unified force might spontaneously split itself into a multitude of other forces (electromagnetic, weak and strong forces). So the question isn’t whether the strength of the forces is all the same at some randomly chosen energy scale, but rather if there is any energy scale where the couplings correspond.
They could be cruel coincidences of nature that have led us astray
Fortunately, a set of techniques known as the renormalisation group flow of coupling strengths – first pioneered by Kenneth Wilson in the early 1970s – enable us to test the energies required for the unification of forces. First, we input the values of the force strengths measured at any scale, and then we ‘run the couplings’ using the mathematical methods to see if the couplings converge at some higher scale. Incredibly, we find that the strongest force (the strong force) decreases its strength between particles when the particles smash into each other at very high energies, while the weakest force (the hypercharge force, derived from electromagnetism) increases its strength. All three force strengths (hypercharge, weak, and strong) therefore come very close together, as a grand unification of forces requires, at an energy that is about 15 or 16 orders of magnitude (10 15 or 10 16 ) higher than a proton’s mass. Imagine three soccer players at different points on the field kicking their soccer balls and all three colliding at one point over midfield. You’d be forgiven for thinking they did it on purpose in an attempt to make a viral YouTube video. Analogously, many physicists don’t think it was an accident that all three force couplings converge at high energies, and therefore we have a very tantalising picture of grand unification of the forces – nature did it ‘on purpose’.
Again, it didn’t have to be like this. One of the force strengths could have moved away from the pack as we moved up the energy scale. This would have immediately made the project of grand unification look impossible or highly suspect. Furthermore, the scale of putative unification adds to the positive view of this picture. Its value is neither too low to run up against the problem of proton decay (to be discussed below), nor is it too high (10 17 or higher) to collude with the inscrutable dynamics of strong gravity that spoils all calculations and interpretations. We see again that observational data (force strength measurements) and theoretical work (group theory and renormalisation group techniques) have led us toward grand unification.
Is there any way to obtain direct proof of unification? What I’ve described so far count as strong hints, but by no means are they proof. They could be cruel coincidences of nature that have led us astray. To obtain ‘proof for all practical purposes’ would require us to do experiments at the unification scale and observe the production of new particles and new interactions directly through collisions. For example, many grand unified theory ideas require the existence of an additional grand unified gauge boson that could be directly produced in collisions, seen, and measured. Unfortunately, it’s out of the question to build a high-energy collider that could reach the energies where we think the grand unified theory resides. It took us many decades to reach energies of only a few thousand times the proton mass – and it might never be the case that experiments could reach energies of 15 orders of magnitude higher, which is what it would take to convince the most ardent sceptics.
T he search for a grand theory isn’t over, though. One of the most sought-after hints is the data connected to the search for proton decay. Along with the neutron, the proton makes up the nuclei within our bodies. If the proton were to decay quickly, it would disrupt our cells and give us cancer and we could never have reliable life. Fortunately, the proton lives a very long time: as far as we know, it lives for at least 10 34 years. That’s about 24 orders of magnitude longer than the lifetime of the Universe. The prediction of grand unified theories for the lifetime of the proton generally falls in the range of 10 30 to 10 36 years.
Any theories that predict a proton lifetime of less than 10 34 years can be ruled out. The simplest grand unified theory is sometimes called the Georgi-Glashow minimal SU(5) theory, proposed in 1974. In its infancy, researchers thought that it predicted the proton lifetime to be less than 10 30 years. At the time, confidence was so high that proponents believed they would quickly observe proton decay in experiments. Instead, the result came back negative in 1983 from the IMB (Irvine-Michigan-Brookhaven) experiment. The lifetime of a proton had to be greater than 10 31 years, which appeared to rule out the minimal Georgi-Glashow theory.
However, the computations that seemed so rigorous back in the early 1980s look like approximations on top of simplifications today. Taking into account high-scale quantum corrections leads to a perfectly acceptable grand unification with proton decay lifetime prediction greater than 10 34 years. This means that proton decay remains a promising frontier in the search for a grand unified theory.
The story of nature is all the more woven with an infinite number of patterns, most as yet unseen
Two experiments are now being built that will increase our ability to find out if the proton decays at lifetimes even greater than the current limits. The DUNE experiment in the United States and the Hyper-Kamiokande experiment in Japan are both attempting to find evidence. These experiments involve filling vessels with 40 kilotons of liquid argon and 260 kilotons of ultrapure water, respectively, and surrounding them with detection equipment to see the tiny flashes of electricity or light that would be the telltale signs of a proton decaying in the midst of all that material. The two main modes of decay they’re looking for are the proton decaying to positron and pion particles (the standard decay that almost all grand unified theories predict) and the proton disintegrating into kaon and neutrino particles (which is particularly important for unification that incorporates an idea known as supersymmetry).
If there really is a grand unified theory that explains the Universe beyond the standard model, it’s likely we should see both or either of these decays. For example, proton decay relies on the fact that the up quark, down quark and positron can all join together in a unified way to convert two up quarks in the proton into an anti-down quark in the pion and a positron – all made possible by the exchange of a very heavy X gauge boson present in the grand unified gauge force. Also, Hyper-Kamiokande expects to be able to find evidence for the proton decaying this way if its lifetime is less than 10 35 years. This is an order of magnitude more sensitive than current experiments, but it’s still unclear if it gives us enough to see actual decay. We should know in about 15 to 20 years, when the new generation of experiments, such as DUNE and Hyper-Kamiokande, have been built and have taken enough data to make a good test of the ideas.
It’s vital for us to find and catalogue the particles that serve as nature’s raw material. But if we stop there, we’re like impatient school children who merely read the Wikipedia synopsis of A Passage to India and then get on with writing their term papers. There’s so much more to learn and to synthesise about this complex narrative than the basic facts reveal. The story of nature is all the more woven with an infinite number of patterns, most as yet unseen. The subtle relationships between particles – the interactions between themselves in many different environments – is what lends our understanding its richness. The revelations of unification in science in general, and especially in physics, have been incredibly fruitful in the deepening of our knowledge and in lighting the way to future discoveries.
Among the many possibilities for unification, nature seems to have dropped us irresistible hints that our particles and our gauge forces are indeed unified into a grand unified theory of some kind. These hints are based on observational data along with the advanced theoretical tools of relativistic quantum field theory and group theory mathematics. However, the limitations of our technology have also made it extremely hard for us to get more direct proof. Seeing a proton decay is one of our few hopes for more direct corroboration – and that’s why so much effort is going into watching protons with an eagle eye to see if one disintegrates. Data will determine whether unified theories will continue to pay off as they have for so many centuries. If history is our guide, we have every reason to believe they will."
"How foreign capital can hinder, or help, economic development | Aeon Essays",,https://aeon.co/essays/how-foreign-capital-can-hinder-or-help-economic-development,"In Nigeria’s oil fields, international giants have pumped crude for decades – yet the country struggles to develop its own refineries or high-tech industries. In Mexico, global car companies churn out vehicles for export, but local suppliers and engineers remain stuck making low-value parts. Stories like these reflect a troubling pattern across the developing world: foreign capital pouring in, while domestic capabilities stagnate or even erode.
Policymakers in poor and middle-income countries are often told that attracting foreign investors is key to prosperity: inflows are treated as a ‘vote of confidence’ and their decline as a sign of distress. According to this dominant narrative, capital naturally flows to places where it’s needed most, bringing not just money but advanced technology and managerial expertise in its wake. By this logic, if investors hesitate to come, the fault lies in the host country’s policies or politics. In our era of globalisation, foreign investment is heralded as a lifeline for developing economies – a reflection of economic strength and a driver of future growth.
Yet history tells a more complicated story. Some of the world’s most successful economies rose to prosperity by keeping foreign capital on a tight leash. South Korea and Taiwan, for instance, leveraged foreign money and know-how on their own terms, ensuring it aligned with national priorities. Even the United States and Japan – today’s wealthy giants – were once very cautious about foreign investment on their soil. The US was one of the largest recipients of foreign capital in the 19th and early 20th c enturies, but it imposed strict rules on foreign investors, especially in banking, shipping, natural resources and other strategic sectors. Japan went even further, tightly regulating foreign business to protect and nurture its domestic industries. These countries didn’t simply throw open the doors. This historical lesson suggests that, while foreign investment can be a tool for development, it works only when a country retains control over how that capital is used and ensures the benefits are captured at home.
B eginning in the 1980s, however, a different approach to foreign capital took hold across much of the Global South. Influenced by a resurgence of free-market ideology, Washington institutions – in particular the IMF and the World Bank, backed by the US Treasury – urged (and often pressured) developing countries to ‘liberalise’ their economies: privatise state industries, deregulate markets, cut tariffs, and welcome multinational corporations with open arms. S ir Leo n Brittan, a former European trade official, captured the era’s optimism in 1995 when he proclaimed that investment was finally ‘recognised for what it is: a source of extra capital, a contribution to a healthy external balance, a basis for increased productivity, additional employment, effective competition, rational production, technology transfer, and a source of managerial know-how’. In other words, foreign capital was seen as an unambiguous good – a panacea for development.
The US trade representative Mickey Kantor signing the final measure of the Uruguay Round of the General Agreement on Tariffs and Trade in Marrakech, 1994. Photo © WTO
Under this gospel of globalisation, international agreements were also locked in a kind of one-size-fits-all openness. In the 1990s, the Uruguay Round created the World Trade Organization (WTO), a club of governments that writes common rules for trade and runs a court-like system to settle disputes. Two of its agreements mattered a lot for foreign investment. First, the Agreement on Trade-Related Investment Measures (TRIMs) says countries cannot use certain investment conditions that interfere with trade in goods. The most well-known example was the practice of demanding that foreign firms buy a set share of inputs (the parts, raw materials and services it needs to produce) from local suppliers (‘local content’). Other common demands, like making a foreign company partner with a local firm (a joint venture) to promote technology transfer, are not directly banned by TRIMs. In practice, however, they’ve become harder to sustain because firms can challenge them as discriminatory or as back-door restrictions on imports. Second, the General Agreement on Trade in Services (WTOS) sets the rules for services such as banking, telecoms and transport. To participate fully in GATS, a developing country generally cannot cap foreign ownership or require joint ventures.
For many developing economies, the result was a narrower policy toolkit than earlier success stories had used. Local-content and trade-balancing rules in manufacturing were taken off the table by TRIMs, and, where governments scheduled GATS commitments, ownership caps and joint-venture requirements in services became hard to defend. By the beginning of this century, many poor and middle-income countries had, in effect, lost control of foreign investment in their economies. They had opened up on the promise that foreign capital was by definition good for growth, only to find themselves constrained from steering that investment toward national development goals.
A country can host modern factories and still not acquire the capacity to create its own Nokia or Toyota
In a path-breaking book called The Rise of ‘The Rest’ (2001), Alice Amsden studied how a group of late-developing countries – including South Korea, Taiwan, China, India, Brazil, Mexico and others – managed to industrialise after the Second World War. These countries, which she called ‘the Rest’, typically began by importing or imitating technology from more advanced nations, rather than inventing it themselves. They built up basic manufacturing industries by leveraging know-how borrowed from abroad, creating modern factories often without proprietary innovations of their own. This strategy worked up to a point, fuelling impressive growth, but it could carry a country only so far. According to Amsden, the history of these countries tells us that, to climb from middle-income status to a truly advanced economy, a nation would eventually have to develop its own technological capabilities instead of relying indefinitely on foreign technology. In other words, the ladder that got you out of poverty – assembly plants, foreign machinery, off-the-shelf technology – would not by itself get you into the top tier. Sustained development required a transition from reliance on imported know-how to home-grown innovation. This transition, Amsden pointed out, is very hard. Why?
Amsden highlighted the problem of knowledge-based assets – the skills, expertise and design know-how that allow firms to produce complex products efficiently. To protect their competitive advantage, multinational corporations guard these assets fiercely. Much of this valuable knowledge is also tacit – in other words, difficult to fully codify or transfer in a handbook or a brief training session. So even when foreign firms set up operations in a developing country, some of the most valuable knowledge – key design skills, engineering tricks and management practices – often remain locked inside the foreign firm’s enclave. The result is a kind of glass ceiling on development: a country can host modern factories and still not acquire the capacity to create its own Nokia or Toyota.
In Latin America, many industries – especially those requiring advanced technology or large upfront investments – ended up dominated by foreign multinationals. In the 1980s and ’90s, Brazil, Mexico and Argentina opened their markets without strong safeguards for domestic industry. The immediate effect was a surge of foreign-led production. Local firms were often crowded out of these high-tech or capital-intensive sectors and relegated to simpler, low-value activities.
The first all-wheel-drive Kia Sportage to be built in Germany rolls off the production line in Osnabrück in April, 1995. Photo by Ingo Wagner/picture alliance/Getty
In contrast, several 20th-century East Asian countries took a very different path. South Korea, Taiwan and later China welcomed foreign capital, but selectively, and they retained domestic control over strategic industries. The East Asian governments encouraged joint ventures and imposed performance requirements on multinationals – insisting they share technology or use local suppliers – while aggressively supporting home-grown companies. In South Korea, for example, the state nurtured its own automotive champions (like Hyundai and Kia) behind protective barriers, instead of simply opening the floodgates to foreign carmakers. In Brazil, by comparison, the automobile market was dominated for decades by overseas giants with few linkages to local suppliers. The outcomes have been very different. South Korea and Taiwan managed to develop robust mid- and high-technology industries and even create ‘domestic multinationals’ of their own. When crises struck – such as the developing-country debt crisis of the 1980s – these East Asian nations proved more resilient and recovered faster, having built up their own industrial base. By contrast, Latin American countries, lacking a strong domestic technological footing, struggled to compete and to escape their dependence on exporting basic commodities or low-tech goods.
The Hyundai Pony, South Korea’s first mass-produced car, debuted in 1975. Courtesy Hyundai
T he idea that who owns the capital matters for development is not new. In the mid-20th ce ntury, economists from the Global South had warned about the dangers of unchecked dependence on foreign capital. Raúl Prebisch and Celso Furtado, working in Latin America, observed that developing countries tend to export low-value raw materials and import high-value manufactured products. This unequal exchange, Prebisch and Furtado saw, keeps poor nations poor. Flooding a primary-product economy with foreign capital, they argued, could perpetuate this imbalance. Without pro-development policies, foreign investors would gravitate to the easiest profits, often in minerals, oil or agricultural staples, reinforcing a country’s role as a provider of commodities and a consumer of finished goods. In the 1970s, the Chilean economist Osvaldo Sunkel highlighted a dual economy in developing nations, with capital-intensive sectors dominated by multinationals, while local firms remain in low-productivity areas. Sunkel argues that this segmentation worsens inequality and undermines local technology. With local firms excluded from dynamic sectors, opportunities for innovation and skills development diminish. Sunkel also noted that foreign companies often repatriate profits instead of reinvesting locally. In the 1990s, foreign banks in Argentina sent significant profits abroad, worsening the country’s balance-of-payments crisis. This profit drain makes host countries vulnerable, as export earnings leave the country as remittances.
These economists from the Global South formed a clear message: foreign capital is a double-edged sword. Yes, it brings investment and can jump-start industries, but without clear conditions and a well-defined strategy, it can trap a nation in an early stage of development.
A surge of foreign competitors can knock out local industry rather than lift it up
Recent research suggests that this development trap has been borne out on the ground. One crucial factor is the education and skill level of the local workforce – the human capital. Studies find that if a country lacks sufficiently skilled workers, foreign investment does little to boost the economy and can even have negative effects. For example, in sub-Saharan Africa, multinational companies have frequently invested in manufacturing or resource extraction, but with meagre benefits for the host country because there were too few trained local engineers or technicians to absorb the new technologies. The foreign firms ended up operating as garrisons of modernity with expatriate expertise, disconnected from the domestic economy. Economists call this ‘absorptive capacity’: without a baseline of education and infrastructure, a host nation simply can’t absorb the knowledge that foreign investors bring, and the presence of a multinational might not help and could hurt – for instance, by outcompeting and shutting down the few domestic firms that do exist, without generating broader gains.
Another factor that influences the effect of foreign direct investment (FDI) on the host economy is the strength of local businesses and financial systems. If domestic companies are fragile or banks are weak, a surge of foreign competitors can knock out local industry rather than lift it up. In a 1999 study of Venezuela, the economists Brian Aitken and Ann Harrison demonstrated that, when foreign firms entered an industry, the productivity of those foreign-owned plants went up – but the productivity of domestically owned plants in the same industry went down. In essence, the multinationals took the most productive slice of the market and enjoyed efficiency gains, while local firms lost business, invested less or even closed, dragging down overall progress. This kind of crowding-out effect is more likely in countries with poorly developed financial markets, where local entrepreneurs struggle to get credit. A big foreign company can often secure loans and dominate supply chains, sucking up the oxygen that local firms need to grow.
Foreign investment can also reshape market structure in ways that limit long-term development. A detailed study by Laura Alfaro and Maggie Chen in 2018 found that, when multinationals enter new markets, they often force domestic firms to abandon certain product lines or market segments, leading to highly concentrated industries. In some cases, the entire sector ends up focused on a narrow range of activities tailored to the foreign giant’s needs. This specialisation might make a few remaining local players more efficient, but it reduces diversification and leaves the economy overly dependent on a handful of low-value activities. Meanwhile, other research by Manuel Agosin and Roberto Machado in 2005 found that, in countries in Africa, Asia and Latin America, FDI tended to replace domestic investment rather than supplement it, aggravating structural weaknesses.
T he short-term outcome in many underdeveloped countries has shown that when multinational corporations arrive, they (1) in crease competition and wipe out less productive local firms; (2) c oncentrate market power in their own hands; (3) r einforce a low-skill equilibrium (hiring workers for basic tasks does not incentivise higher education); and (4) p ush the economy to specialise even more in low-value-added goods. These effects might yield some short-term efficiencies and consumer benefits – cheaper goods, perhaps temporarily more jobs – but they carry a hidden long-term cost. The country’s industrial structure becomes narrower and hardens around its static comparative advantages (like textiles, basic agriculture or assembly work), instead of evolving toward new, higher-value economic activities.
To investigate the long-run impact of foreign capital on development, I recently conducted an empirical study spanning several decades. I focused on a simple measure: the share of a country’s productive assets that were foreign-owned around 1980 (the stock of FDI as a percentage of GDP), and then looked at how that country’s economy performed over the next 40 y ears. Even after accounting for other factors like initial income, institutions, education and geography, countries that had a higher foreign capital share in 1980 experienced significantly slower income growth by 2019. In fact, the analysis suggests that, for low-income countries, an increase of 1 p ercentage point in the FDI-to-GDP ratio in 1980 was associated with roughly a 1.3 p er cent lower GDP per capita by 2019. Let’s consider Chile versus China. In 1980, Chile’s economy had a very large foreign stake – its FDI stock was about 35 p er cent of GDP – whereas China was essentially closed to FDI, with a foreign share of nearly 0 pe r cent. This difference in foreign ownership correlates with a big divergence in outcomes. If all other things had been equal, from 1980 to 2019, Chile’s per-person income growth could have been around 4 0 p er cent lower than China’s, simply due to Chile’s heavy reliance on foreign capital and China’s reliance on domestic capital during that period. Of course, China’s spectacular rise has had many causes, but one is its strategy of setting its own terms for foreign investment.
Particularly in less-developed countries, multinationals failed to deliver the promised technological upgrading
Countries that have welcomed lots of foreign investment tend to exhibit signs of the ownership trap. They developed less complex and less specialised exports, often continuing to export mostly raw materials or simple manufactured goods. They had a higher share of low-tech exports and saw weaker growth in productivity. These economies failed to climb the ladder of industrial sophistication. The findings are consistent with other economic research: very poor countries benefit from some diversification of their economies at first, but middle-income countries progress further only if they start specialising in more advanced, high-value industries. The trouble is that heavy foreign ownership seems to anchor many countries to their initial specialities, preventing that next step. When a multinational sets up an assembly plant in a low-wage country, that country may end up specialising even more in assembly work – doing it on a larger scale for the foreign firm – rather than moving into designing its own products. High-FDI countries often found themselves producing what multinational corporations needed (cheap inputs, basic components, commodities), reinforcing a low-tech economic structure. Domestic innovation and diversification suffered because the commanding heights of the economy were held by outside firms focused on efficiency and profit, not on developing local talent or suppliers.
A conventional wisdom in economics – that ‘capital has no country’: that it shouldn’t matter who owns the factories and mines as long as they produce growth – does not always hold up. In theory, free flows of capital should allow each nation to prosper by focusing on what it does best. In practice, when foreign capital flooded relatively weak economies, it often entrenched those societies in low-value activities while siphoning off the gains. Policymakers presumed that foreign investors’ technology and management prowess would catalyse broader, and deeper, development, but often those benefits remained elusive. Particularly in less-developed countries, multinationals failed to deliver the promised technological upgrading. Yes, there were instances where efficiency in certain factories improved, or output increased after a foreign takeover. But we should not confuse those productivity jumps with genuine knowledge transfer. In many cases, the gains came from foreign firms reorganising production around what the host country was already relatively good at – say, cheap labour for garment sewing – or exploiting economies of scale by integrating the host into their global supply chain. These moves can boost output and profits, but they don’t necessarily teach the country how to make more sophisticated products or invent new technologies. A nation’s economy ends up doing more of the same, just under foreign direction.
Over time, the dynamic consequences of this arrangement can be pernicious. By confining local firms to less complex tasks, the presence of dominant foreign players reduces the incentive and opportunity for local innovation. The demand for skilled labour stays low, so the education system has less pressure to produce engineers or scientists. A cycle sets in: weak local innovation leads to few skilled jobs, which in turn curtails the capacity for further innovation. In the worst case, an economy can plateau, stuck in a middle-income trap where wages rise to a certain point but the move into higher-value industries never occurs. Instead of becoming the next South Korea, a country might remain an assembly hub for someone else’s brand – relatively industrialised but not developed.
Computer manufacturing at the Tatung Company electronics factory in Taiwan in 1992. Photo by Gérard Sioen/Alamy
I s the answer then for countries to shut out foreign investment entirely? Not necessarily – the success stories suggest a more nuanced approach. Foreign capital can aid development, but only when countries harness it strategically. The East Asian Tigers exemplified this balance. South Korea and Taiwan borrowed heavily and welcomed joint ventures, but required foreign firms to partner with local companies and transfer technology, directing capital into strategic sectors as part of broader industrial policy. At the same time, they built up domestic champions in those sectors so that foreign operations would not dominate indefinitely.
This approach echoed earlier industrial powers such as the US and Japan: leverage foreign capital, but avoid dependency. South Korea in the 1960s and ’70s licensed technology and hosted foreign factories, but always with the goal of enabling local firms to absorb knowledge and compete globally. By the 1980s, it had nurtured world-class companies in autos, steel and electronics, loosening FDI restrictions only after domestic firms were firmly established.
China later followed a similar template. In the 1980s and ’90s, it attracted investors with cheap labour and huge markets, but demanded joint ventures and technology transfer. Volkswagen, for instance, could enter only through a partnership with a state-owned firm. Through such arrangements, Chinese automakers gradually gained industry knowledge. Similarly, in electronics, Huawei, Lenovo and other firms emerged after years of absorbing foreign technology and investment under guided conditions.
The anti-globalisation turn in wealthy nations often comes wrapped in nativist rhetoric and can veer into xenophobia
The contrast between strategic integration of foreign capital and the laissez-faire approach is stark. Countries that actively shaped foreign investment to serve their development goals managed to climb into the ranks of advanced economies. Those who simply opened the door wide, hoping for the best, often found themselves trapped in the lower rungs of the global economy.
Ironically, even some rich countries are today voicing doubts about unfettered capital flows – though for very different reasons. The US under Donald Trump has, in the name of economic nationalism, launched sweeping global tariff hikes. Europeans are also debating about screening foreign (often Chinese) investments in strategic sectors, to protect national interests. This anti-globalisation turn in wealthy nations often comes wrapped in nativist rhetoric and can veer into xenophobia. But it underscores a real issue: the free-market formula of the past few decades has left many communities – in both rich and poor countries – feeling betrayed by global capital. Factories closed, jobs moved overseas, and those promised ‘spillovers’ and shared benefits often failed to materialise. For developing countries, the stakes in this debate are higher, because it’s not just about jobs – it’s about sovereignty and future growth. If even the US is anxious about who owns its industries, how much more vital is that question for a country that has yet to establish a broad industrial base?
Rather than retreat into narrow economic nationalism or beggar-thy-neighbour trade wars, we can avail ourselves of an opportunity to forge a more inclusive form of globalisation. Progressive political forces should champion a strategic approach that recognises the right of nations, especially poorer ones, to protect and nurture domestic industries even as they participate in global trade and investment. This requires updating international rules to grant developing countries greater independence in managing foreign capital. Governments could be permitted – or even encouraged – to set conditions on foreign investment that ensure technology transfer, local hiring and local sourcing. These policies, maligned by neoclassical orthodoxy as protectionist sins, should instead be viewed as legitimate strategies for building national capabilities, addressing sovereignty concerns, and advancing shared global goals such as climate resilience. A nation that grows on a more equal footing not only empowers its own people but also becomes a more stable and constructive partner in the global system.
W hat might this look like in practice? First, it means reviving ideas like requiring joint ventures or a minimum use of local resources for production in strategic sectors. It means scrutinising foreign takeovers of key industries and possibly blocking them unless they clearly contribute to national development (for example, by bringing in technology that can’t be obtained otherwise). It also means targeting foreign investment into areas where it complements rather than substitutes for local effort. If a nation wants to build an electric vehicle industry, for instance, it might welcome a foreign battery manufacturer, but under terms that also develop local supply chains and R&D capabilities.
Crucially, managing foreign capital should not be seen as a rejection of globalisation, but as a way to make globalisation work for development rather than against it. We live in an interconnected world facing collective challenges – the climate crisis and the regulation of artificial intelligence, for example – that no nation can solve alone. A more equitable global economy, where developing nations have the industrial base and technological know-how to contribute solutions, is in everyone’s interest. Allowing those nations the space to build that capacity is part of a fair and forward-looking international order. Rich countries, for their part, might reconsider the indiscriminate promotion of deregulation and instead support policies that lead to shared prosperity – even if it means their multinationals face a few more rules when investing abroad.
The debate over foreign investment comes down to who controls the resources, the technology and the decision-making levers in a nation. It comes down to power, autonomy, long-term development and what kind of world we want to build. For too long, many developing nations have ceded control under the promise that growth would happen regardless of ownership. The evidence says otherwise. Economic development depends on building national capabilities, and that means having a measure of sovereignty over one’s economic destiny. Countries that remain merely hosts for other nations’ corporations risk seeing their growth stall out, their hopes of innovation frustrated, and their political autonomy curtailed by the whims of global capital.
Developing nations should rethink the terms of engagement with foreign capital to avoid the ownership trap where reliance on foreign investment today forecloses opportunities for a richer and more developed nation tomorrow. By judiciously managing foreign involvement and fortifying domestic strengths, developing countries can still harness the powerful benefits of globalisation, but in a way that preserves their popular sovereignty and long-term prospects for a more democratic and stable society. The struggle for economic development has always been, in part, a struggle for the freedom to choose one’s own path. Better forms of globalisation expand this freedom, ensuring that integration strengthens domestic capabilities rather than undermining them. By reclaiming that freedom – by reclaiming control – nations of the developing world can chart a more equitable and dynamic path forward, ensuring that globalisation becomes a story of shared progress rather than a tale of perpetual dependency."
"The planet, and human social life, depend on peasant farmers | Aeon Essays",,https://aeon.co/essays/the-planet-and-human-social-life-depend-on-peasant-farmers,"In 2007, the United Nations released a State of the World Population report noting that human life on Earth was quietly passing a tremendous benchmark. In 2008, the proportion of people residing in the countryside was falling – for the first time in history – below 50 per cent. Today, just 42 per cent of humanity lives in the countryside.
For many city dwellers, the urbanisation of our species is natural and inexorable. Extrapolating from past trends, they imagine a future in which the great majority has abandoned the land, leaving it bucolic, automated and empty. In the process, they predict – with some relief! – the imminent extinction of an ancient character: the peasant.
That word is avoided in polite conversation; in many languages, it is used as a term of abuse or contempt. Because peasants themselves are seen as an embarrassing vestige, the antithesis of ‘progress’. Whether Right or Left, Western thinkers have taught that, in order to become modern, societies have to get rid of their peasants. While Adam Smith looked forward to peasants giving way to landowners (for then ‘the land … would be much better improved’), Karl Marx foresaw their replacement by modern socialist management. It has been taken for granted that agriculture will eventually be monopolised by large capital and machinery, and cities will absorb the majority of the human population.
The Gleaners by Jean-François Millet (1857). Courtesy and © Musée d’Orsay. RMN-Grand Palais/Patrice Schmidt
Even in industrialising Europe, the process was not exactly like that. Yes, the traditional countryside was largely destroyed between the 18th and 20th centuries – but the resulting exodus was far greater than could be absorbed by urban factories. Sixty million Europeans had to escape, instead, to the New World. But, in any case, Europe plays a unique role in capitalist history, and it is wrong to extrapolate from it. Other regions have followed other paths.
In large parts of Africa, Latin America and Asia, urbanisation is slowing. Most of those who will enter factories have already done so. Those who value the securities of village life, meanwhile, have little appetite for urban slums, isolation and hypercompetition. Therefore, while humanity was urbanising at a rate of 1.06 per cent per year between 1950 and 1970, that rate has now dropped to 0.74 per cent, and it will fall to just over 0.6 per cent by 2030. Since the world population has tripled since 1950, absolute rural numbers remain greater than ever before. By my calculations, as many as 2 billion people live in the countryside of Africa, Latin America and Asia, where small family farms dominate. After 300 years of ‘modernisation’, in short, peasants still constitute as much as one-quarter of our species, vastly outnumbering assembly-line workers, miners, office drones or taxi drivers.
A M Menkethana, 59, returning from her rice field in Weweldigiliya, Sri Lanka. Photo by GMB Akash/Panos Pictures
Peasants are defined by family farms, usually 10 acres or less, whose output is optimised both for subsistence and for cash income. The work is carried out primarily by (unpaid) family labour. Far more than farmers in rich countries – many of whom are effectively state employees – peasants are entirely exposed to the fluctuations of climate and markets; their fortunes can vary greatly from one year to the next.
Peasants are fully integrated with the 21st-century economy, which could not operate without their production of sugar, cotton, cocoa and other essential commodities. Although they control less than one-quarter of the world’s agricultural land, peasant farming is highly efficient, and estimates suggest 70 per cent of the world’s population depends on them for some or all of their food. In many crucial sectors, peasant production is also more favoured by industry, and more suited to social conditions. Peasant agriculture is better than industrial alternatives, also, at managing soil health, water resources and biodiversity, so it is widely seen as a shield against climate change. Without peasants, in short, the global economy could not function, and our natural systems would collapse.
Life still depends on the peasantry. We are all affected, therefore, by the fact that the peasantry is today in an acute crisis. A crisis that rarely receives adequate attention in public discussion.
O n rare occasions, the countryside does make it to the front page. In September 2020, large-scale farmer protests erupted in India following the passage of new legislation that gave corporations a greater role in agricultural markets. Farmers across multiple states – especially Punjab and Haryana, where many relied on the state to procure their wheat and rice – staged demonstrations and blocked highways to New Delhi. Delhi is a global media hub; naturally, there was widespread coverage.
Protesting farmers burning an effigy of the farm laws at a protest site outside Delhi. Photo by Harsha Vadlamani/Panos Pictures
Such coverage is rare; farmer unrest, however, is endemic. In November and December 2020 – while the roads to Delhi were blocked by tractors – soldiers in Peru were firing at farmers protesting a law that absolved agribusiness from obligations to workers. In Uzbekistan, also in 2020, farmers protested the cluster system, whereby land was forcibly given over to corporate clusters, which were usually run by individuals close to the political elite. Over the past five years, serious farmer protests have occurred in Argentina, Brazil, Colombia, Ecuador, Ghana, Kenya, Indonesia, Nepal, Iran, Pakistan, the Philippines, Uganda – and the list goes on.
Much of what is usually reported as ‘terrorism’ or ‘militancy’ has its roots in the collapse of the countryside
Behind the protests lies an even greater swell of invisible discontent. In my travels through the villages of Latin America, Africa and Asia, I have everywhere encountered the fury of farmers faced with assaults on their land, and with policies designed to allow agribusiness and industrial processors to capture more and more of their income. ‘There is no point the government offering poverty relief to farmers,’ a farming organiser told me in India, ‘when your policies are keeping them in slavery. You first have to unchain the farmers’ arms and legs.’
Sometimes, the peasant crisis hits the media for other reasons. The Arab Spring uprisings of the early 2010s drew strength from agrarian protests in the Middle East and North Africa – even if the farmers were quickly marginalised after the fact. Much of what is usually reported as ‘terrorism’ or ‘militancy’ also has its roots in the collapse of the countryside. Boko Haram and other militant groups operating along the southern edge of the Sahara draw their forces from farmers and herders displaced by desertification, climate change and the closure of traditional nomadic routes. ‘Jihadist groups,’ writes one expert, ‘have realised that certain groups have been left to manage the devastating impacts of climate change on their traditional livelihoods on their own,’ which has ‘created fertile grounds for recruitment.’
Mass migration is another symptom of this crisis. Most rural refugees head for the nearest metropolis, but tens of millions are driven across international borders. The caravans of migrants leaving Guatemala, El Salvador and Honduras for Mexico and the United States are largely made up of such refugees. Other routes lead from Burkina Faso, Mali, Niger and Chad through North Africa to Europe, and from East Africa to western Asia.
Valli Kupuswamy, 55, weeding her paddy field. Photo by Sanjit Das/Panos Pictures
Then there is suicide. According to farmer advocates I have interviewed, more than 400,000 Indian farmers have taken their own lives. The greatest concentration has been in the cotton-growing regions of Maharashtra; cotton is a critical global resource, whose price is of acute political concern. A sophisticated mesh of laws and markets induces cotton farmers to continue selling below the cost of production – and so to enter a debt spiral from which they often find no mortal exit.
These are among the symptoms of the crisis of the global peasantry in the neoliberal era. We should be in no doubt: this is a political crisis. Everywhere, states are breaking their contract with peasants, and turning instead to anti-agrarian alliances with global corporations, local bigwigs, organised crime and gangsterism. Unchecked, this crisis will deliver terrifying consequences; it may even threaten our survival as a species. It is, in my view, the most important story of the 21st century.
F or most of history, peasants supplied the basic economic resource: without them, there was no state. A special bond therefore existed between peasants and kings. Successful rulers – for instance, in China, Persia, India, Egypt, Arabia, Ethiopia, West Africa, the Andes – nurtured the agrarian economy by instigating irrigation works, protecting peasant landholdings, guaranteeing crop prices, feeding populations when harvests failed, and controlling merchants, middlemen and land speculators. Many such systems were destroyed by European colonialism; restoring them was a major objective of Asian and African postcolonial governments. Similar issues gripped 20th-century Latin America, where agrarian democratic movements were continually pitted against landowning oligarchies and anti-Communist alliances.
Valle de los Ríos Apurímac coca farm in Ene y Mantaro (Peru). Supplied by the author
In the decades leading up to 1980, many developing countries saw uncompromising and dramatic agrarian reform. Governments redistributed land, ensured farmers had firm titles, and protected them from the need to sell in times of hardship. Seeds became a critical national resource; states set up seed banks and research centres to preserve the seed heritage, develop high-yield varieties, and guarantee supply. States also formalised agricultural markets, set minimum prices, and often themselves became procurers of last resort. At their best – in South Korea, for instance, or Mexico – such strategies enhanced both standards of living and agricultural output.
Many of those reforms were overturned during the neoliberal restructuring of the 1980s and ’90s. Engineered by such agencies as the World Bank, International Monetary Fund and World Trade Organization, this process forced states to reorganise themselves around the competition for global capital. They therefore established new alliances, not only with those agencies, but crucially with global agribusiness. As a result, significant control of rural affairs has been handed to international banks and corporations. Today’s peasant crisis springs from there.
For many peasants, land represents economic security and heritage, ancestors and the generations to come
This is not to say that global markets are replacing peasant farming with more ‘modern’ techniques. It is true that, in some crop sectors, plantations have replaced small farms. Wheat production, for instance, can be carried out on a large scale with little human labour, using mechanisation and typically high inputs of fossil fuels and fertilisers; such techniques have been generalised from Mexico and Ukraine to Kazakhstan and India. Oil palms, too: in parts of southeast Asia, small farmers have been forcibly dispossessed, and then brought back as wage labourers on corporate oil-palm plantations. And chickens: poultry megazones in southern China – where poultry farming was previously a peasant preserve – now concentrate close to a billion chickens in factory-like conditions.
Not all agriculture, however, can be industrialised. Rice, which is the staple for half the planet, is ill-suited to big plantations; it requires intensive human intervention and is best grown on small family farms. Despite decades of corporate influence, therefore, rice is still produced by about half a billion peasants. The same goes for other essential crops. Cotton production has been mechanised in the US and Europe, but the quality suffers, which is why peasant production continues to dominate; small family farms in India and China contribute by far the largest proportion of global supplies.
A cotton farmer harvesting in Maharashtra, India. Supplied by the author
Not only do peasants have essential skills, they are also, from the corporate perspective, desirable partners – precisely because they are small, politically weak and easy to coerce. In some sectors, corporations have even made the profitable discovery that peasants – whose first commitment is to the land – will continue to farm at a loss. The neoliberal reorganisation of the countryside has not, therefore, eradicated the peasantry. Instead, peasants have been legally reconstituted in such a way as to maximise efficiency and profit. In the process, states that had previously stood with their populations against multinational corporations – which they often viewed as a neocolonial influence – have switched sides, aligning with large capital against their agrarian masses.
T he first focus of neoliberal reform in the countryside has been to turn the global peasantry into agribusiness consumers. Seeds were at the heart of this: under the banner of World Trade Organization property protections, international foundations and funding agencies persuaded developing countries to illegalise traditional seed saving and exchange, and to dismantle state seed banks. Farmers were therefore dependent on corporate products – which often lasted only one season, and so could not be saved. Small farmers in many countries have protested against the resulting loss of ‘seed sovereignty’ and biodiversity. Thousands of Ghanaian farmers, for instance, protested the 2013 Plant Breeders’ Bill which advanced agribusiness interests by criminalising farmers who saved seeds for planting the next year; the bill was withdrawn under pressure, but reintroduced in 2020 under a different name.
A farmer working at the rice harvest in Goseong-gun, South Korea. Photo by Björn Steinz/Panos Pictures
Proponents of corporate seeds often point to the Green Revolution, a 1970s triumph of American laboratories and foundations. This was built on genetically engineered high-yield seed varieties allied to intensive irrigation and fertilisers. Its legacy is dubious indeed: in the state of Punjab, the home of India’s Green Revolution, agricultural land is saturated in chemicals, aquifers are disastrously depleted, and farmers are locked into an ever-rising cost cycle. In the era of the climate crisis, however, peasants themselves are desperate to find more resilient, higher-yielding seeds. With alternative seed sources removed, corporations have enjoyed a bonanza. Bayer (Germany) and Corteva (US) control 80 per cent of patents for genetically modified seeds. Allied to seeds are corporate fertilisers and pesticides; along with ChemChina and Sinochem (China) and BASF (Germany), for instance, those same companies control about 60 per cent of the global pesticide market.
By now, the global peasantry spends hundreds of billions of dollars each year on industrial seeds and chemicals. While farm production is unquestionably higher as a result, this expenditure is dangerously out of phase with peasant income. Traditionally, peasants have tried as much as possible to do without cash, which usually arrived in bulk at harvest season. They spent little on seeds and fertilisers, and they fed themselves, as much as possible, from their own resources. Today, peasants need to put up significant amounts of cash at the time of sowing, and throughout the growing season, in order to get to harvest.
Much of the world’s peasantry is now victimised both by free markets and by state-controlled socialist hangovers
Climate change also forces many peasants to resow several times, raising the cost of cultivation, sometimes by a lot. Since they are also spending far greater amounts on such regular expenses as children’s education, most of this cash needs to be borrowed. Most governments have agricultural credit schemes, but some peasants lack the collateral and paperwork to cover their needs in this way. Others quickly exhaust their potential and must seek loans elsewhere. Hence the enormous importance, in Asia and Africa especially, of rural moneylenders. Often charging 10 per cent interest or more per month, moneylenders can leave immense human destruction in their wake.
Second, neoliberal policies have transformed agricultural markets. Over the past few decades, farmers have been increasingly shut out from the revenues accruing from their production. The means by which this exclusion has been achieved, however, are varied and complex.
Obviously, large corporations have the power to dictate market prices, to the detriment of millions of tiny producers. In that sense, open markets seem to militate against farmers. But the whole story is more nuanced. Developing-world farmers of cocoa, sugar cane or cotton rarely get market prices for their product. Standing between them and those prices, often, are the very same state institutions that were set up in the 20th century to protect farmer income. These marketing boards, and their fixed prices, have since drifted to an almost opposite function.
A family of rice farmers in Bihar, India. Supplied by the author
In 1947, for instance, Ghana set up market monopolies to ensure fair prices were paid to cocoa farmers. Now, those institutions interpret the ‘national interest’ in the opposite way. They act to keep prices low, and so to generate a subsidy, not only for the state, but also for exporters, processors and chocolate consumers. In 2023-24, international cocoa prices soared as high as $12,000 per tonne, but farmer income was capped at the government price, which oscillated between $1,800 and $3,000 per tonne. The interactions between international confectionary giants and West African government agencies are complex, but the results are not. In the 1970s, cocoa farmers earned up to 50 per cent of the value of finished chocolate; this fell to 16 per cent in the 1980s, and is probably now around 6 per cent. While the value of the chocolate industry has surpassed $100 billion , some cocoa farmers in those countries earn less than $300 per year. Ghana and Côte d’Ivoire, whose cocoa industries once provided jobs to migrants from all over West Africa, are now significant sources of migration to Europe.
Much of the world’s peasantry is now victimised both by free markets and by state-controlled socialist hangovers. Peasant politics is more complex, therefore, than is usually imagined. Many peasants follow the Left-wing movement La Via Campesina, which seeks to restore traditional peasant systems, and so to oppose genetically modified seeds and the corporate takeover of agriculture. But there are equally committed supporters of an almost opposite position. In places where old socialist protections have turned into instruments of price suppression, many farmers dream of free markets. As one agrarian campaigner told me in India: ‘We simply want to sell our crops at market prices. By protecting us, the government has made us destitute. We say: “Remove your protections, and let us deal with the consequences.”’
The third target of neoliberal reform of the countryside is peasant land. Contrary to conventional – urban – wisdom, most peasants wish to keep their land. I recently asked an Indian cotton farmer why he continued his back-breaking labour, when he hardly covered the cost of cultivation, and he had to work other jobs to fund his loss-making farm. Why did he not simply sell his land and focus on those more lucrative activities? ‘The land is our mother,’ he replied. ‘Do you sell your mother?’ His sentiment is shared by many peasants, for whom land represents not only economic security but also heritage, ancestors and the generations to come.
In many countries, however, selling agricultural land is not just undesirable, it is also difficult. The pro-peasant policies adopted by Egypt, India, Mexico and so many other countries in the 1950s and ’60s – which illegalised large landholdings, and prevented farmland from being acquired for other uses – now ensure that rural land markets remain weak, and prices low. Selling land may not even provide farmers with enough capital to start a new life elsewhere. Very often, therefore, they carry on farming. Even if their plot has shrunk, after generations of inheritance, below the threshold of viability; even if it has become degraded by a perennial lack of investment; even if revenues turn negative – they continue to cultivate it, rather than allow it to return to wilderness. The farm is not an income source, therefore: it provides merely stability, a family base, a sense of home. Around it, peasants create hypermodern economies: they do shifts in factories to subsidise cultivation, they send family members to do construction work abroad, they run local transport and services. Much peasant production is funded today from such other sources, and run like a loss-making public service. The brightly painted new façades in Cambodian villages are paid for, not with non-existent rice profits, but with remittances from family members working in South Korean factories.
When agricultural land is diverted to other uses, the process is often violent. In Brazil, Cambodia, Ghana, India, the Philippines and many other countries, farmers have been forcibly expropriated so their lands may be repurposed for plantations, mines and tourism projects. Often, these evictions are enforced by state agencies; in Ethiopia, Honduras and elsewhere, police forces have imprisoned or even shot farmers for protesting. But large areas of the global countryside are also becoming criminalised, and farmers find themselves also in competition with violent non-state forces. Peasant farmers are the primary victims, for instance, of illegal gold miners in Peru and Colombia, of timber and mining hustlers in Myanmar, of Russian-linked paramilitary groups occupying mineral deposits in Mali and the Central African Republic. The mines which so often emerge from such turbulence pollute the remaining farmland with cyanide and other chemicals, further destroying the peasant economy.
T wo billion people cannot be relocated to cities. Yes, China’s rural population has fallen from 80 per cent in 1980 to 35 per cent – but China is unique. Even in neighbouring India, the rural population remains at 65 per cent, or 900 million people. The entire world’s manufacturing, construction and mining currently employ only 800 million people: clearly, the global peasantry cannot be absorbed into industry. We need to realise that, in the absence of exceptional levels of industrialisation, nothing can sustain large populations so well as the land. We need to stop seeing urbanisation as the main index of developmental progress, and realise that it is, in many cases, the sign of a major disaster: the destruction of rural life by big agriculture and industry, and the loss of irreplaceable human and ecological systems.
The everyday enemy of the peasantry is the same as that of all of us: climate change. It brings rising temperatures, droughts, more violent storms and many seasonal irregularities. Rains do not come at the time seeds need to be planted; unseasonal rainstorms ruin crops and encourage the spread of pests. What peasants really need is a vast programme of climate-change adaptation, which will primarily involve switching to other varieties and crops. Such adaptation, however, requires capital. Farms need to be remade; new botanical stock is required; there needs to be provision, as in any experiment, for failure. Given the scenario described, it is not surprising that most farmers cannot raise the necessary capital.
Traditionally, peasants returned nutrients to the soil in the form of vegetable residue, human and animal excrement , decomposed skins and fibres. Since most agricultural products are now consumed in cities, much of that material accumulates as urban sewage and garbage – and the only way nutrients may be restored to the land is in the form of chemical fertilisers. Over time, this causes soil fertility to decline. In my own field research, I have seen farmers weep over the damage they have done to farmland with chemical inputs. Unlike corporations, peasants cannot just shrug at the exhaustion of a particular expanse, and move on. Their plot came to them from their ancestors, who bequeathed also a sacred responsibility.
The only real solution is to hand responsibility to those who have a life-or-death stake in regenerative farming
The fact that the world’s arable land stock is so severely impaired today should be cause for enormous alarm. If the peasant economies of Latin America, Africa and Asia are destroyed, our food system will collapse. Given the vast numbers involved, meanwhile, even small ecological deteriorations may force millions of new refugees off the land. Depending on how things progress, the UN International Organization for Migration forecasts there will be between 25 million and 1 billion climate refugees by 2050. Those people will not likely find offices or factories in which to work. History suggests that some will be forced to extract a living by force, joining militant groups funded by smuggling, kidnapping and extortion. The global political balance is already fragile.
A sugarcane farmer in Maharashtra, India. Supplied by the author
Predictably, agribusiness presents itself as the solution. The websites of big food companies depict happy plantation workers in corporate uniforms. They boast of their commitment to ‘sustainable’ or ‘regenerative’ agriculture. Such companies as McDonald’s, Bayer, Mars and PepsiCo are part of an agribusiness taskforce within the Sustainable Markets Initiative, which declares its aim of engineering a more sustainable and resilient global food system. ‘We can only achieve this,’ explained the CEO of Bayer in 2022, ‘if we as an industry collectively step up our efforts to adopt regenerative farming practices.’
Even if such statements are sincerely meant, the past 40 years should make us suspicious. The partnership between large corporations and the global peasantry has enabled the former to capture revenues from the latter, and so to remove much liquidity from the global countryside itself. The people who live there, whose most precious assets are located there, and whose livelihoods are bound up there, have seen their capacity for responsible management and investment catastrophically diminished. The only real solution is to hand responsibility to those people who have a life-or-death stake in regenerative farming.
The peasant way of life is a critical buffer against climate change. Peasant villages recycle biochemical waste back to the land; many peasants also supply their nutritional needs from their own farms. Peasants, who directly manage about 10 per cent of the land on Earth – an area five times larger than all towns and cities – supply a countervailing principle to corporate extractionism and short-termism. They also preserve critical local knowledge of land and weather systems, and the interactions of plants and animals. The peasantry is one of humanity’s most crucial economic, social and ecological resources, and we need to invest in it if we are to flourish. Affluent and innovative, this class will insulate us from more extreme degradation of natural systems. Impoverished and terrorised, it will be forced, in the end, to leave the land en masse, with manifold catastrophic consequences.
In 1979, writing from a remote village in eastern France, John Berger observed that the objective of the peasant was
We all would do well – survival may depend on – generalising Berger’s apt characterisation of the peasant relationship to life and land. The crisis of the global peasantry sits at the centre of all other crises, and we have to solve it. We must put peasants back at the centre of our worldview. Their struggle to hold on to their vital place and role is our struggle. A species struggle."
How Chinese religious traditions shape corporate generosity | Aeon Essays,,https://aeon.co/essays/how-chinese-religious-traditions-shape-corporate-generosity,"Stepping out of a sleek office tower in Shanghai, I hear a bell ringing from Jing’an Temple. In the shadow of steel and glass stands a centuries-old Buddhist shrine. Its curled eaves and drifting incense offer a quiet counterpoint to the digital tickers flashing above trading floors. As a finance professor, I interpret corporate decisions like dividend payments through the lenses of incentives, governance and regulation. But, in that moment, I wondered: could the moral rhythms of the temple – with its teachings of harmony, reciprocity and impermanence – be quietly shaping the boardroom across the street? What if the chants of monks and the calculations of CEOs weren’t as far apart as we assume?
That question led me to think more broadly: how do values persist across time, even as belief systems shift? China’s spiritual and philosophical traditions – especially Buddhism, Taoism and Confucianism – have long shaped attitudes toward wealth, obligation and fairness. These influences may seem distant from today’s stock markets, yet their traces remain visible in how people understand legitimacy and virtue, even in the realm of commerce.
In imperial China, merchants occupied a low place in the social hierarchy, ranked below farmers, artisans and scholars. Yet many built reputations not only through commerce but through acts of generosity, supporting temples, schools and public works. Profit and virtue were not seen as opposites, but as forces that could be balanced through wisdom and restraint. This ethos was significantly challenged during the early decades of the People’s Republic, when traditional religions were classified as ‘feudal superstition’ and many temples were closed during the Cultural Revolution. These policies reflected the priorities of that historical period, focused on class struggle and rapid industrialisation. Yet even amid sweeping transformation, many moral traditions persisted. From the 1980s onward, as China pursued reform and opening-up, spiritual traditions gradually re-emerged, complementing the country’s economic development and cultural revitalisation efforts under the evolving framework of socialism with Chinese characteristics. That ancient ideal still lingers beneath the surface of modern Chinese capitalism. Today, companies listed on stock exchanges operate in a global market, but their decisions are often shaped by those older moral logics. As I was to discover, even something as technical as a corporate dividend may still bear the imprint of beliefs rooted in Buddhism, Taoism and centuries of ethical tradition.
T oday, the worlds of tradition and commerce often coexist in unexpected proximity. I grew up observing how deeply cultural values shape people’s behaviour – often unconsciously. Still, it might seem unlikely that spiritual traditions could influence something as technical as corporate dividend policy.
Dividend payments are the portion of profit that a company pays out to its shareholders – essentially, a reward to investors for their trust. In modern capitalism, dividends are not mandatory; they are a voluntary decision made by a firm’s leadership to return value to its owners. Unlike wages, which compensate labour, or charitable donations, which serve public good, dividends reflect a deliberate act of profit-sharing within capitalist enterprise. Choosing to pay them signals financial strength, accountability and a long-term commitment to shareholder value. Beyond just a payout, they represent a gesture of corporate accountability. In the realm of corporate governance, regular dividends are often interpreted as a sign of respect for shareholders and a mechanism to discipline management by reducing discretionary cash reserves.
Dividends are typically explained using financial theories or legal frameworks. Conventional finance emphasises formal factors such as taxation, investor protection laws and corporate governance rules. Yet in China, a country often described as having the Buddha on the bookshelf and the Tao in the boardroom, I began to suspect that there was more to the story.
Nearly half of all profitable firms still went three or more consecutive years without paying a cent
Take the term ‘iron rooster’. In Chinese, tie gong ji is a metaphor for stinginess – a rooster so iron-clad you cannot pluck a single feather from it. For years, Chinese investors used this colourful insult to describe companies that never pay dividends. As one financial newspaper lamented, ‘some companies would rather be iron roosters sitting on piles of cash than share a feather with investors.’ The problem was widespread: low or non-existent dividends have long undermined investor confidence in China’s stock markets. Throughout the 2000s and early 2010s, many listed firms would report profits yet refuse to share the bounty – a far cry from what shareholders might expect in more mature markets. Regulators grew so frustrated that they launched campaigns to crack down on these dividend-shy companies. In 2017, the head of the China Securities Regulatory Commission even threatened ‘hard measures’ against the perennial non-payers, invoking the iron rooster idiom in a fiery speech. And in 2023, authorities rolled out rules tying executive performance evaluations to dividend payouts and barring large shareholders from selling stock if their companies hadn’t paid dividends in years. The intent was clear: coax the roosters to finally shed some feathers.
These efforts have started to bear fruit. By 2023, total dividends from Shanghai’s major companies have risen substantially, with roughly 75 per cent of listed Chinese companies paying some dividend between 2009 and 2023. However, the typical payout remained meagre – only about 28 per cent of annual earnings on average. Nearly half of all profitable firms still went three or more consecutive years without paying a cent. Yet, amid this overall caution, a subset of companies stood out for their generosity. What made those companies different?
I began to suspect that the answer lay beyond balance sheets and policy directives – perhaps in the realm of values and culture. China’s modern economy rises from an ancient cultural soil rich with religious and philosophical traditions. Could a firm’s willingness to share profits be shaped by its moral environment? I was particularly drawn to Buddhism and Taoism, which have long guided Chinese notions of virtue. Max Weber had argued that the Protestant ethic of northern Europe helped lay the moral foundation for modern Western capitalism. Could the great spiritual traditions of China likewise shape the way business is done in the East? Might the values imparted by monks and sages influence something as technical as a corporation’s dividend policy? This was the question at the heart of my enquiry.
T o assess corporate ethics in practice, my colleagues Shixian Ling and Mengdi Jia and I used dividend policy as a proxy. Paying dividends is a voluntary act of profit-sharing – a conscious choice to reward shareholders rather than hoard profits. While it is not the only sign of responsibility, it offers a clear, measurable signal of fairness. This focus allowed us to explore how traditional values might shape behaviour. If ethics mattered, firms in more religious areas would likely share profits more readily and excel in broader measures of responsible management – a pattern we ultimately observed.
With this proxy in place, we embarked on a research project that straddled finance and cultural anthropology. We knew from prior studies that religion can meaningfully shape economic behaviour. Social scientists have found that religious belief often correlates with traits like thrift, trustworthiness and risk-aversion. In the corporate arena, firms headquartered in more religious areas (in the West, typically places with a strong church-going culture) have been shown to make more ethical choices and take fewer reckless risks. Religion, in effect, can act as an informal rulebook – a set of norms that executives internalise.
China offers a unique laboratory for this idea. Unlike the United States or Europe, where Christianity has long underpinned business morals, China’s spiritual tapestry is woven from Buddhism, Taoism and folk beliefs. These Eastern traditions emphasise concepts like karma, harmony and reciprocity – ideals that might naturally influence how a business treats its stakeholders. For example, a Buddhist ethos stresses compassion and karmic consequences: doing good leads to good outcomes. Taoism values balance and wu-wei (non-forceful action), implying that one should not aggressively grasp for gain but let things flow in a just way. Both traditions discourage unbridled greed and the hoarding of wealth. If such values seep into local business culture, we might expect companies in religious communities to behave a bit differently – perhaps being more inclined to share wealth rather than stash it.
It was as if the invisible hand of culture was nudging firms toward a more shareholder-friendly habit
In our study , we examined more than 33,000 firm-year observations of publicly listed non-financial firms across China from 2009 to 2023. To measure local religious influence, we geocoded Buddhist and Taoist temples, monasteries and shrines from official registries and counted how many influential institutions were located within a 100 km radius of each firm’s headquarters. This provided a spatially grounded proxy for the spiritual environment surrounding each company. To test the robustness of our findings, we experimented with multiple alternative measures of religious influence. These included expanding the radius (to 200 km and 300 km), calculating religious institution counts at the provincial and city levels, and adjusting for area size by computing religious institution density per 10,000 sq km. We also cross-validated using more recent 2022 records of registered religious institutions.
The results were striking. Firms in more religious regions were significantly more likely to pay dividends – and to pay more when they did. Even after controlling for company size, profitability and state ownership, the pattern held: proximity to temples strongly predicted generosity to shareholders. We tested every angle – alternative measures, lagged models, instrumental variables – to rule out confounding factors like regional wealth. Yet the link persisted. It was as if the invisible hand of culture was nudging firms toward a more shareholder-friendly habit in those heartlands of Buddhism and Taoism.
This effect of religion was strongest among private companies – the non-state-owned enterprises that are not subject to direct government control. State-owned enterprises often follow dividend policies tied to broader strategic or policy goals, leaving less space for local cultural norms to play a role. But private company leaders have more discretion, and the moral milieu seemed to play a bigger governance role. Religion arguably served as an alternative governance mechanism – an informal check on management’s impulse to hold back payouts. Where formal oversight is looser, informal norms step in more forcefully.
D igging deeper, we faced another question: were all religious influences created equal? Buddhism and Taoism, though often blending in folk practice, are distinct traditions with different teachings. Our analysis in fact revealed a notable contrast: Buddhism’s impact on corporate payouts was significantly stronger than Taoism’s. To explain why, it helps to appreciate the differing ethos of these two philosophies.
Buddhism in China has long emphasised merit and charity. A core Buddhist tenet is dāna , the practice of giving. Monasteries have survived on laypeople’s donations for millennia, and believers are taught that generosity yields spiritual merit (and, by the law of karma, future rewards). The Buddha is traditionally quoted as saying that, if people truly understood the power of giving, ‘Even if it were their last bite, their last mouthful, they would not eat without having shared.’ The very act of giving is seen as purifying and auspicious. A business leader influenced by Buddhist values might thus feel a duty to share the fruits of success – to give back to investors and the community. Indeed, Buddhism frames ethical leadership as a form of stewardship; wealth is transient, and to hoard it selfishly is spiritually foolish. As Confucius (whose philosophy intermingled with Chinese Buddhism) put it, ‘Wealth and rank attained through immoral means are nothing but drifting clouds’ – in other words, ill-gotten gains are ephemeral. Little wonder, then, that a company CEO mindful of such teachings might prioritise fair dealing and honourable distribution of profit over short-term enrichment.
Taoism, on the other hand, takes a more subtle route toward virtue. The Taoist worldview prizes naturalness, balance and simplicity. The ideal Taoist sage leads by non-assertion ( wu-wei ), doing only what is necessary and in harmony with the Tao (the way of nature). In the realm of wealth, Taoist texts often warn against excess and competition. ‘The sage does not hoard,’ says the classic Tao Te Ching . ‘Having bestowed all he has on others, he has yet more; having given all he has to others, he is richer still.’ This paradoxical line suggests that, by not clinging to wealth, one actually gains – a concept not far from the Buddhist idea of karmic returns. Taoism thus encourages a kind of detached generosity and contentment with ‘enough’. A Taoist-influenced CEO may avoid aggressive accumulation and favour stability, but without the proactive moral imperative to give. Whereas Buddhism celebrates the act of giving, Taoism emphasises not forcing or overreaching. This difference helps explain why Buddhist influence showed a stronger link to dividend generosity. In short, both faiths discourage greed, but Buddhism provides a clearer moral incentive to give .
Ethical norms, in effect, acted as an informal layer of oversight
Our research also delved into how exactly religious culture translates into corporate financial policy. The data hinted that it works through shaping corporate norms – essentially, by making companies more conscientious citizens. We found that firms in more religious regions scored higher on measures of corporate social responsibility. These companies were more attentive not just to shareholders, but to stakeholders broadly – employees, communities, the environment. It seems that the same ethics that encourage generosity to shareholders also foster responsibility to society at large. When a business leader internalises values of honesty, compassion and service (virtues extolled by both Buddhism and Taoism), that leader is likely to run the company in a way that respects all its relationships. In practical terms, this might mean a greater willingness to adopt policies that benefit a broader set of stakeholders, including shareholders. Paying a decent dividend can be seen as treating shareholders fairly – a slice of the moral duty to stakeholders.
Another key channel we uncovered is investor protection. In theory, strong legal safeguards – like anti-expropriation laws and strict disclosure rules – help ensure profits go to shareholders rather than being siphoned off by insiders. But in China, enforcement is uneven and often weak, especially for minority investors. Intriguingly, we found that religious regions tended to exhibit stronger investor protection, even beyond what laws required. In morally vigilant communities, blatant exploitation may be socially unacceptable, keeping controlling owners in check. Ethical norms, in effect, acted as an informal layer of oversight. Our statistical tests confirmed this: when we controlled for regional investor protection, it helped explain the link between religiosity and dividend payouts. In other words, cultural norms reduced the need for formal enforcement – encouraging fairer treatment of shareholders and more consistent dividends. It’s a vivid example of how ‘soft’ values can strengthen the ‘hard’ infrastructure of corporate governance.
We also observed that firms in religious regions tend to smooth dividends – maintaining stable payouts over time rather than reacting sharply to annual profits. This consistency reflects a long-term orientation rooted in Confucian and Buddhist traditions, which value reliability in relationships. Smoothing can be seen as an implicit promise to shareholders: a commitment to share the firm’s fruits year after year. Such reputational consistency builds trust and signals integrity – much like a person who keeps their word.
W hat does it all mean for the bigger picture of capitalism and ethics? My journey into ‘divine dividends’ revealed something profound: markets are not morality-free zones. Economics often treats corporate behaviour as driven solely by profit and competition, yet our findings show that deep-rooted ethical traditions can shape even technical financial policies. This echoes Weber’s insight from The Protestant Ethic and the Spirit of Capitalism (1905), where he argued that Protestant values such as diligence, thrift and a sense of vocational duty helped ignite the rise of capitalism in northern Europe. Those values became embedded in business practices, from the way people approached investing to how they reinvested earnings for growth.
In China, capitalism took root under very different cultural auspices. Confucian, Buddhist and Taoist principles were for centuries seen as antithetical to the West’s ravenous capitalism – Weber himself thought Confucian China was too tradition-bound to develop modern capitalism. Yet history surprised us. East Asian economies soared in the late 20th century, and not by shedding their traditional values, but often by repurposing them. Japan, Korea, China – each infused their own cultural ethos into corporate life (from samurai bushido -influenced management loyalty in Japan, to Confucian respect for hierarchy in Korean chaebols , large family-run businesses). Our findings add to this narrative by highlighting Buddhism and Taoism as part of the mix that shapes Chinese corporate behaviour.
Indeed, the idea of ethical capitalism has deep roots in Chinese thought. Confucius taught that a ruler – and, by extension, a business leader – must act with ren (benevolence) and yi (righteousness) to preserve harmony. Many historical Chinese merchants took this to heart, donating generously to their communities, building schools and temples, and earning a reputation as righteous businessmen . This was not just altruism; it was also savvy in a society where social trust was the currency that greased commerce. A merchant known for honouring deals and sharing profits gained respect and patronage. Likewise, a modern company that pays reliable dividends signals respect for shareholders and positions itself as part of a broader community of trust, not just a vehicle for insiders’ gain.
If policy is the hardware of governance, culture is the software. Both need to align for a system to run smoothly
At the heart of this lies a philosophical question: what is the purpose of a corporation? Is it solely to maximise shareholder value, as some free-market purists argue? Or does it bear responsibility to a broader set of values and stakeholders? In recent years, global debates have raged between shareholder primacy and stakeholder capitalism. What our study suggests is that, when imbued with certain cultural values, firms can simultaneously fulfil their duty to shareholders and uphold ethical principles – these goals need not be at odds. A company in a Buddhist/Taoist milieu that pays good dividends is arguably both serving shareholders and demonstrating virtue. In that sense, virtue can be profitable . Over time, such virtuous firms might even attract more investment, enjoy better employee loyalty, and face less regulatory ire – all because they built a reservoir of goodwill.
Of course, none of this implies that religion is a panacea for corporate governance issues. Not every temple-dotted town will produce saintly CEOs, and devoutness can be feigned or misused (eg, corrupt officials who flamboyantly burn incense at temples, hoping for absolution without reforming their conduct). Formal laws and regulations remain crucial – after all, many companies started paying dividends only in recent years, after the government wielded its sticks and carrots. But the interplay of culture and policy is what’s truly interesting. If policy is the hardware of governance, culture is the software. Both need to be aligned for a system to run smoothly.
In China’s case, the revival of traditional culture and religion – once suppressed during the Maoist years – has quietly reintroduced an ethical vocabulary into business. I find it poetic that karma and Tao have re-emerged in boardroom discussions, even implicitly – all the more remarkable given that China’s government remains officially atheist. Executives aren’t quoting scripture, but centuries-old values still shape decision-making. A local tycoon might fund a temple restoration and raise dividends in the same breath – acts rooted in a shared sense of doing what’s right.
That day in Shanghai, standing between the temple and the tower, I realised the sacred and the secular aren’t as far apart as we assume. Corporations, often cast as cold engines of profit, are still run by people – guided by beliefs, values and stories. China’s stock market may be modern, but the sensibilities of its participants are ancient. When companies pay dividends, they aren’t just distributing cash; they’re enacting a quiet ritual of reciprocity and virtue embedded in their culture.
This fusion of ancient wisdom and modern markets offers a hopeful message. Even in capitalism’s cut-and-thrust, there is space for principle – whether drawn from religion, philosophy or community norms. Perhaps next time we assess a company, we should consider its cultural soul – the invisible temple in its backyard. The bells of tradition may not ring as loudly as earnings calls, but if we listen closely, they are there – reminding those in power of their greater purpose. In an era when we are rethinking the soul of capitalism, the story of divine dividends from China offers a profound insight: when business meets belief, both profits and principles can prosper hand in hand."
Population decline will rob us of vital social force: youth | Aeon Essays,,https://aeon.co/essays/population-decline-will-rob-us-of-vital-social-force-youth,"A population crisis is unfolding – in my family. My paternal grandparents were born in India and had 10 children. I did my part to keep our lineage going (my wife was also involved) yet over the course of three generations, my family’s ‘fertility rate’ plummeted from 10 to less than 1.
During this same period, India’s population exploded from 350 million to 1.4 billion, in 2023 surpassing China as the most populous country in the world. But my family’s story mirrors India’s demographic transition. In the mid-20th century, when my parents were born, India’s fertility rate was 6. It’s now 2.
India is a striking case, but fertility rates are declining everywhere. Between 1950 and 2021, the global fertility rate fell from 4.8 to 2.2. American women have an average of 1.6 children. Japan’s fertility rate is 1.2, South Korea’s a startling 0.75.
The global population is still growing, for now. But demographers project that humanity’s numbers will peak near the end of this century before beginning a steep decline . I won’t be around by then, but my children will live to see the peak and peer over the precipice.
Is this a crisis?
S ome say it’s a blessing. Fewer people means fewer carbon emissions. Not only that, our collective pie could be divided into larger pieces. In theory, we’ll enjoy less competition for resources, more affordable housing, and higher wages due to labour scarcity. From this perspective, population decline isn’t a problem but a solution.
Alas, these hopes are misplaced.
The path forward on climate change is clear: rapid decarbonisation. We must transition as fast as possible from fossil fuels to renewable energy sources. If we fail, the future will be disastrous, and population size becomes irrelevant. If we succeed, additional humans won’t impact carbon emissions.
Moreover, the timelines don’t match. Climate change demands solutions within the next few decades; population decline won’t materialise until the next century. Having fewer humans arriving then won’t retroactively cool the planet.
Imagine, however, that population decline accelerates. That would worsen rather than salvage our climate prospects. Young people are more likely to support bold environmental policies, become climate activists, and invent green technologies. A shrinking, ageing population means fewer contributors to these efforts.
Hospitals will burst at the seams while playgrounds empty
This points to the fundamental reason that population decline will be a curse rather than a blessing: the loss of young people.
Schools will be converted to elder-care facilities. Hospitals will burst at the seams while playgrounds empty. This demographic skew will reshape every aspect of our world, from economics and innovation to culture and social progress.
Consider social security, often in the US misconceived as a savings account for retirement. In fact, tax contributions from workers are redistributed to retirees – a system of intergenerational cooperation that depends on a balanced age distribution. As the population ages, this system will crumble.
The problems run deeper. Fewer working-age adults entails a shrinking tax base, even as the need for public services swells. Education, healthcare, infrastructure, public safety and welfare will be underfunded. The pie will shrink. And as lifespans grow, the demand for healthcare will increase while the number of healthcare workers decreases. Vulnerable groups – not just the elderly but also those who experience poverty or disability – will suffer most.
To thrive, societies need young people. New generations drive economic growth, pioneer technologies, challenge outdated moral views, create art, and advance social change. They’re more likely to take risks, embrace new ideas, and imagine different futures. When we talk about population decline, what we’re really talking about is the gradual dissipation of this vital social force.
M any assume the solution is simple: open the borders and demographic problems will solve themselves. Yet while immigration can prevent domestic population decline, it can’t reverse imbalances in population structure. Research suggests that higher rates of immigration can prevent a nation’s population from shrinking but not from ageing. The mathematics is simple: immigrants are younger than the average citizen but older than newborns.
More fundamentally, immigration won’t help because population decline is a global problem. When wealthy nations import young workers from poorer nations, they’re effectively exporting their low fertility rate, exacerbating the global problem rather than solving it. Take my parents and their siblings, who emigrated to the West in their early 20s. (At that age, I still had not emigrated from my parents’ basement.) Adopting new homes, they also adopted new family models – none had more than two children. And their children had even fewer.
Progressive tax reform could boost public revenues but this is a tall order
Even so, economic and health repercussions may be tackled in other ways. Retirement ages could creep upward. Advances in medical technology might reduce age-related illness and disability, enabling longer healthy working lives. Automation of healthcare services might compensate for the dearth of healthcare workers.
More ambitious solutions are possible. Progressive tax reform could boost public revenues, offsetting the shrinking tax base and bolstering social programmes. This is a tall order, given that previous attempts have been unsuccessful. But since older people are more likely to vote, perhaps we’ll summon the political will.
These solutions, however, address only the surface. A shrinking, ageing population poses even deeper challenges, ones far more resistant to policy solutions.
P opulation growth drives economic growth. More people means more workers, more consumers, and more innovation. When populations expand, economies of scale become possible – efficiency increases and wealth multiplies. Younger generations are more likely to start new businesses, adopt novel technologies, and increase productivity. Historically, even relatively small populations have been economically vigorous so long as they had a high proportion of youth.
As fewer young people are born to replenish society, we’ll face a shrinking workforce, weakening consumer demand, and the unravelling of economies of scale. We’ll also lose the very demographic that drives economic development. Young people don’t just fill jobs; they reimagine how work should be done. They don’t just participate in the economy; they reshape it. Ageing societies will experience not just economic stagnation but degrowth.
Some people welcome this prospect, seeing degrowth as an antidote to climate change and capitalist exploitation . But this perspective ignores crucial realities. Economic growth can be decoupled from carbon emissions. More importantly, degrowth would devastate developing nations. Economic growth dramatically reduced global poverty. Reverse that growth, and poverty will resurge.
Most breakthrough discoveries come from younger researchers and entrepreneurs
Some think AI or another emerging technology can stimulate economic productivity. But the effects of population decline extend far beyond markets and material wealth.
When societies are large and interconnected, they’re able to generate new ideas, recombine old ideas in new ways, and forge new divisions of cognitive labour. A smaller population will thus shrink what the evolutionary theorist Joseph Henrich in The Secret of Our Success (2015) calls our ‘collective brain’. We’ll forgo not just particular innovations but entire fields of enquiry, impacting everything from basic research to practical applications in engineering and medicine. New technologies could potentially sustain economic productivity, but that will be harder if a shrinking population is technologically less innovative.
Young people aren’t just members of society’s collective brain; they’re its most innovative neurons. Most breakthrough discoveries come from younger researchers and entrepreneurs. Social progress, similarly, depends on young people rejecting prevailing bigotry and replacing older generations. For example, support for same-sex marriage is higher among Gen X and Millennials than in older generations. If the proportion of young people declines, so will our moral and political values.
You think we live in a decaying gerontocracy now? Just wait.
The impact on creative activity will be no less profound. Young people have always been the main source of art, fashion, music, literature and film. As their numbers diminish, the future will become a cultural wasteland. Imagine the 1960s without rock-and-roll, the 1970s without Hollywood auteurs, the 1980s without street art, or the 1990s without hip-hop.
More generally, young people are optimistic, willing to take risks, and open to new experiences. These traits drive cultural progress. An ageing society will become risk-averse, focused on preserving wealth rather than creating it, and resistant to necessary change.
I find myself in strange company. Right-wing pronatalists want to outlaw abortion, punish childlessness, and force women to marry young and forsake careers outside the home. In other words, they want to make fertility rates great again by restoring the patriarchal order of the early 20th century. Some of these pronatalists are only really worried about white population decline . (They’re not worried about my family’s fertility rate.) Many observers conclude that population decline isn’t a crisis – it’s a moral panic.
But just because a cure is toxic doesn’t mean the disease isn’t real.
If populations decline, humanity will unravel. Societies will become less productive and poorer, vulnerable people will lose crucial social support, innovation will slow, and values will deteriorate. Given my family’s trajectory, I might not have any descendants by the time the global population crashes, but the world – what’s left of it – will suffer enormously if we do nothing.
We have a shared responsibility to create conditions where people want to have children
To be clear, I don’t much care whether my own family lineage endures. My two children are perfect. That’s what I tell my wife too, but she argues we should have another. Lately, she’s been appealing to my conscience: if population decline is truly dire, shouldn’t I do my part for humanity?
The answer isn’t individual sacrifice. No one has a duty to procreate (conveniently for me). Rather, we have a shared responsibility to create conditions where people want to have children and can have all the children they want. This means grappling with the gendered division of reproductive labour and supporting caregivers. That’s how pronatalism can be a progressive movement.
The first step is to understand the source of the problem.
T he global fertility rate held steady around 5 until the 1960s. Today, it has fallen to 2.2. In all developed countries, fertility rates are well below 2.1, the ‘replacement rate’ needed to maintain a stable population.
Why this dramatic decline?
As societies grow wealthier, women gain access to education and careers. They secure reproductive freedom and marry later, if at all, encouraged by reproductive technology to postpone family formation. Teen pregnancy becomes rare. People concentrate in cities where housing costs soar and living spaces shrink. Extended families scatter, taking with them crucial support. Meanwhile, parenting itself has become more intensive and expensive but less valued by society. Childrearing also competes with growing access to luxuries such as travel, hobbies and creative pursuits. (Smartphones are more entertaining than kids.)
A great tragedy of our species is that procreation is an oppressive institution
Much of this fits my family’s story. My grandparents were born in India and died there, their progeny relocating to urban centres across the English-speaking world. None of my grandparents attended university, while nearly all their grandchildren did. One grandmother married at 16 and had her first child a year later. Only one of my cousins had children before 30; some prioritise careers or leisure, choosing not to marry or reproduce.
A great tragedy of our species is that procreation is an oppressive institution. Rising freedom and prosperity have provided women with more attractive options. This explains why fertility rates crashed in East Asia, where marriage and family remain particularly restrictive while economic opportunities for women have blossomed.
It’s hard to make predictions, especially about the future. But the tendency for fertility rates to fall as countries become wealthier and freer is as close as you can get to a law of nature in demography. So, as poor countries continue to become wealthier, their fertility rates are destined to fall below replacement too.
T o avoid demographic collapse, Right-wing extremists would drag society backwards. They want to ban contraception, turn women into breeders, , and promote heterosexual marriage while ignoring its oppressive structure. We must reject these coercive proposals.
Fortunately, coercion isn’t necessary to avert population decline. In many countries, a gap separates intended fertility and actual fertility. Fertility rates will recover if people are empowered to have as many children as they want.
Yet most factors driving fertility decline are overwhelmingly positive – affluence, economic opportunities, reproductive freedom. We should try to design wealthy, free and equal societies, even knowing that these very achievements tend to lower fertility rates.
Perhaps I’d want a third child if I’d found my partner and settled into a permanent job in my mid-20s
Our central challenge is to increase fertility while preserving and expanding valuable social gains. We need innovative approaches that separate high fertility from its historical causes – poverty, oppression and gender inequality. The solution isn’t to reverse progress but to reimagine family formation in a progressive society.
Sometimes I’m asked (for example, by my wife) why I don’t want a third child. ‘What kind of pronatalist are you?’ My family is the most meaningful part of my life, my children the only real consolation for my own mortality. But other things are meaningful too. I want time to write, travel and connect with my wife and with friends. Perhaps I’d want a third child, or even a fourth, if I’d found my partner and settled into a permanent job in my mid-20s instead of my mid-30s.
My story is unique, of course, but I see it reflected in the lives of others. Raising children has become enormously expensive – not just in money, but also in time, career opportunities and personal freedom. Effective solutions must address these costs.
S weden offers what seems like a blueprint for progressive pronatalism: childcare subsidies, parental leave , and family support programmes. Yet the results are sobering. Sweden’s fertility rate sits at 1.5 – well below replacement. While these programmes may have prevented even steeper declines, they haven’t reversed the trend. Other countries – from Singapore and Taiwan to Poland and Hungary – have implemented similar policies without success.
France offers a more promising model. With a fertility rate of 1.62, it hasn’t maintained replacement either but does surpass its European neighbours. The French approach is like Sweden’s except even more generous, pronatal expenses accounting for 3.5-4 per cent of its GDP. Tax benefits increase with each additional child – essentially paying parents more as their families grow.
The US briefly experimented with a lite version of France’s approach through the COVID-19 pandemic-era expanded child tax credit. Its lapse represents a missed opportunity. The programme should not only be restored but dramatically expanded. Other countries should follow suit.
Social norms about family size and parenting prove remarkably resistant to change
More broadly, the world needs to deploy different support programmes, measure their impacts, and scale up what works best. The best interventions may increase fertility rates only marginally, but that would be a form of harm reduction. Moreover, replacement becomes possible if marginally effective solutions are deployed in concert. The costs may be high but the benefits are worth it.
Another, complementary approach is to make housing more affordable so that young people can leave the nest and afford spaces large enough to contain the families they’d like to create. Economists warn that subsidies make housing less affordable by driving up demand. A better policy is to increase supply by eliminating restrictive zoning laws and other regulations that limit new multi-family housing. (YIMBYs and pronatalists unite!)
These policies may not be enough by themselves. Social norms about family size and parenting prove remarkably resistant to change, persisting even after the conditions that shaped them have shifted. This means we need to couple robust policy interventions with efforts to reshape cultural attitudes about parenthood.
O ne way to reduce parenting costs is for men to do their fair share, as the Harvard economist and Nobel laureate Claudia Goldin argues . Despite a century of rising gender equality, American women in heterosexual relationships still perform roughly twice the domestic labour of their male partners, even when both work outside the home. Add children to the equation, and this imbalance becomes crushing. (Childless readers: imagine multiplying your household workload by a factor of three or four.)
Meanwhile, modern parenting has evolved into an increasingly demanding enterprise: coordinating enrichment activities, monitoring academic progress, managing mental health, ensuring physical safety, and maintaining constant vigilance. No wonder many stop at one child. While some aspects of intensive parenting reflect novel challenges or deeper relationships with children, others are unnecessary or unhealthy. Dialling back expectations would boost fertility rates along with parent and child wellbeing, as Tom Hodgkinson argues in The Idle Parent (2010).
Parenting becomes more difficult when extended families disperse
Career compatibility presents another major hurdle. But workplaces can adapt. Imagine offices designed with family life in mind: flexible schedules that accommodate parent-teacher conferences, on-site childcare, and cultures that celebrate rather than penalise family commitments. For example, expansion of remote work seems to have fostered a pandemic baby bump in the US.
Historically, our species has relied on cooperative parenting, as Sarah Blaffer Hrdy writes in Mothers and Others (2011). Fertility rates have declined in part because parenting becomes more difficult when extended families disperse. Many young people want the freedom to pursue careers far from home and escape family control. But new parenting cooperatives are possible, with friends living together or neighbours sharing duties. Such arrangements are already being attempted – for example, in pandemic ‘pods’ – and, if successful, they might catch on.
Changing ingrained cultural patterns won’t be easy. But cultural shifts often follow material incentives. For instance, smart policy could help reshape workplace practices, with governments compensating employers for costs. While we can’t predict exactly how such changes might unfold, we can try to encourage more sustainable and equitable parenting. We should at least think harder about it.
H ow urgent is our demographic problem? History offers a sobering perspective: no developed nation has ever seen its fertility rate fall below replacement and then recover. The mathematics of population decline is unforgiving, as delays now compound into massive differences later. Every year we postpone action, the challenge becomes more daunting.
Yet we shouldn’t surrender to panic. The most severe consequences of population decline won’t materialise until the 22nd century. While we must think ahead, we shouldn’t be confident about shaping the distant future, which reduces the expected value of our efforts. Unknown factors might intervene and restore fertility rates to replacement levels. Or more pressing problems may intrude and demand the bulk of our attention.
Population decline is a serious challenge, but it’s not the civilisational emergency that extremists claim. This reality suggests a measured, two-track approach.
Every major advance in human history – technological, cultural, moral – has been driven by youth
First, we should undertake policy and cultural interventions that make sense independently of their impact on fertility. Expanded parental leave, subsidised childcare and pronatal tax policies would support caregivers even if they didn’t boost fertility rates. Likewise, increasing the supply of housing would benefit those suffering from poverty or homelessness. Beyond that, we have every reason to encourage men to contribute more to childcare, make parenting less intensive, and redesign workplaces to be more family-friendly. These interventions offer clear benefits with minimal downside risk.
Second, we should invest in researching and testing more ambitious and radical approaches, from cultural innovations surrounding cooperative parenting to technological revolutions in assisted reproduction. Perhaps the most effective solutions, ultimately, will relieve the costs of childrearing through artificial wombs or AI nannies. Let scientists and policy experts explore bold approaches while ensuring rigorous evaluation of their effects. Ask politicians, humanists and the general public to critically discuss them.
Still, pronatal policies raise hard questions. Unless the gendered division of labour changes radically, increasing fertility rates may expand burdens on women and entrench traditional gender norms. Given the gap between intended and actual fertility, the benefits of having children may outweigh these moral costs. But we can’t pretend to fully understand the tradeoffs. We need to study not just whether pronatalism can be effective but also under what conditions it is ethically defensible. Philosophers have work to do.
At bottom, progressive pronatalism isn’t just about increasing our numbers – it’s about preserving society’s engine of progress. Every major advance in human history – technological, cultural, moral – has been driven by youth.
Maintaining this vital force will be possible only if population decline doesn’t remain solely a preoccupation of extremists. Too much depends on finding humane solutions. We need approaches that contribute to human flourishing while preserving the advances in freedom and equality that precipitated declining fertility. The future of humanity – and of my unlikely grandchildren – depends on striking this delicate balance."
How did Kerala go from poor to prosperous among India’s states? | Aeon Essays,,https://aeon.co/essays/how-did-kerala-go-from-poor-to-prosperous-among-indias-states,"India is a union of 28 states (provinces). The population in some of these states is bigger than that of the largest European countries. For example, Uttar Pradesh is home to more than 240 million people, almost three times the population of Germany. Although a part of a federal union, every state has a unique history, shaped by its environment and natural resources, princely or British colonial heritage, language and culture. Since the end of British rule in the region in 1947, their economic trajectories have diverged, too.
With roughly 35 million people, Kerala, which sits along India’s southwestern tip on the Indian Ocean, is among the smaller Indian states, though it is densely populated. In the 1970s, Kerala’s average income was about two-thirds of the Indian average, making it among the poorest states in India. This difference persisted through the 1980s. In the coming decades, a miracle occurred. Kerala, one of the poorest regions in India, became one of the richest. In 2022, Kerala’s per-capita income was 50-60 per cent higher than the national average. What happened?
Even when it was poor, Kerala was different. Though income-poor, Kerala enjoyed the highest average literacy levels, health conditions and life expectancy – components of human development – in all of India. Among economists in the 1970s and ’80s and among locals, ‘Kerala is different’ became a catchphrase. But why, and different from whom? One big difference Kerala presented was with North India, which had an abysmal record of education and healthcare. While the population grew at more than 2 per cent per year in the rest of India, Kerala’s population growth rates remained significantly lower in the 1970s. High literacy and healthcare levels contributed to this transition.
Kerala’s unusual mix of high levels of human development and low incomes drew wide attention, including from leading scholars. Among the most influential writers, K N Raj played a big part in projecting Kerala as a model for other states. Anthropologists like Polly Hill and Robin Jeffrey drew attention to some of the unique features of the society that led to these achievements. In a series of influential works , the Nobel-laureate Amartya Sen and his co-author the economist Jean Drèze praised Kerala’s development model for prioritising health and education, even with limited resources, and claimed that this pathway led to significant improvements in quality of life. Kerala vindicated the intuition that Sen and others held that health and education improved wellbeing and shaped economic change by enhancing choices and capabilities.
Why do Kerala’s differences matter? What lessons did the economists draw from the state’s unique record? Around 1975, India’s economic growth had faltered, and a debate started over whether the country should give up its socialist economic policy in favour of a more market-oriented one, in which the government would take a backseat. Kerala suggested three lessons for those engaged in the debate: (a) income growth rate was a weak measure of standards of living; (b) what mattered was quality of life, including education, good health and longer lives; and (c) the government was necessary to ensure investment in schools and hospitals. The three lessons would coalesce into the Kerala Model, an alternative recipe for development to the neoliberal model then being pushed by Right-wing lobbies.
But Kerala was about to grow even more different, confounding orthodoxies in political science and economics. In the 2000s, average income in the state forged ahead of the Indian average. Compared with Indian averages, the post-1990 growth record was less impressive regarding human development, as India caught up with Kerala (see graph below). The forging-ahead in income was offbeat and is still poorly understood. This question remains unanswered because, so far, the attention of economists has been elsewhere – welfare policies – whereas the income turnaround suggests an emerging pattern of private investment that strides in basic health and literacy alone cannot explain.
Before we tackle that question, it will be useful to discuss the huge presence of the state in development studies. Where does it come from? Why does the state fascinate so many social scientists?
F rom a historical perspective, Kerala has at least four distinct qualities that most states in India do not share. First, it has a centuries-long history of trade and migration, particularly with West Asia and Europe. Second, Kerala is rich in natural resources, which have been commercially exploited. Third, Kerala boasts a highly literate, skilled and mobile workforce. Finally, the state has a strong Left political movement. Any story we tell about its advances in health and education or its recent income growth must refer to some of these longstanding variables.
Why was Kerala different? In the minds of many economists, the state’s heritage of Leftist trade unions (more on this later) and successive rule by Leftist political parties helped provide the foundation for strong human development. Socialism was not just a popular ideology but had a real chance to deliver in this state. Others stressed geography, princely heritage and social reform movements. For example, the British anthropologist Polly Hill noted that Kerala differed due to its coastal position, semi-equatorial climate, maritime tradition, mixed-faith society and princely rule. The combined share of the population following Islam and Christianity in Kerala is about 45 per cent; for India as a whole, it is 16.5 per cent. The state is home to one of the oldest branches of Christianity. Further, the strategic location along the Arabian Sea facilitated interactions with traders worldwide, including Arabs, Europeans and others. The local rulers were generally tolerant of diverse religious practices.
Many economists in Kerala who noted the difference did not think there was much reason to celebrate. Some said that the record on healthcare and education hid a profound inequality from view. Others said the low and stagnant income pushed the state’s fiscals into bankruptcy, making the model unsustainable without active markets driving investment and income growth. By the 1990s, the model’s limitations became apparent as the state struggled with low economic growth and financial strains.
If the situation did not lead to a severe crisis, this was due to inward remittances. The state had a long history of labour migration, with significant numbers of people moving to the rest of India and the Persian Gulf states for work. This migration led to substantial remittances, which sustained private consumption, income and investment. By 2010, the excitement over the Kerala Model was dead, and incomes started forging ahead.
The Left changed their focus from land and educational reforms to private investment and decentralisation
The economists (above) who joined the developmental debate took Kerala’s income poverty for granted. They neither saw the income growth coming nor were prepared to explain it. Some Left-leaning economists attributed the resurgence in per-capita income to education and healthcare. But this is not persuasive. A surge in economic growth everywhere and at all times implies rising investment in productive capital, and basic education and healthcare would not deliver that.
The Indian economy in the 2000s saw robust investment and economic growth. But Kerala was not a major destination for mobile private capital. The forging ahead owed to more specific factors, some more peculiar and powerful than those driving India’s transformation.
Here, we must return to Kerala’s historical engagement with the world economy, its natural resources, its literate workforce and its distinctive political landscape. In different ways, all these reinforced private investment. Deep connections with the global economy were pivotal to the recent history of labour migration. While migration created a flow of remittances into consumption, another significant flow went into investment, especially in service sector enterprises in healthcare, education, hospitality and tourism. The state’s temperate semi-equatorial climate, mountainous topography and abundant water resources supported plantations and natural-resource extraction and processing industries for centuries. Some declined in the mid-20th century, but investment in these activities revived later.
The communist movement in Kerala began in the 1930s with the formation of the Congress Socialist Party, driven by peasant and labour movements and anticolonial struggles. The movement joined electoral politics after the formation of the state in 1956, and since then, Left-ruled governments have formed from time to time, almost always with coalition partners. The Leftist political movement in Kerala helped shape the state’s economic policies. In recent years, the Left also changed their focus from land and educational reforms to private investment and decentralisation. Capable local self-government institutions strengthened democratic governance.
In short ways, four forces of change – Kerala’s reintegration with the global economy, remittances from the Persian Gulf, strong welfare policies from a legacy of Leftist government, and private investment from individuals and businesses who shared the remittance flows – have combined to form the structure of Kerala’s miracle of human wellbeing with economic growth.
A round 1900, Kerala was a region composed of three political units: the princely states of Travancore and Cochin, and the British Indian district of Malabar. There were a few other smaller princely states as well. There was a broad similarity in the geography across the three units. India’s climatic-ecological map will show that all of Kerala is a semi-equatorial zone with exceptionally heavy monsoon rains, whereas most of India is arid or semi-arid tropical. The region has plentiful water and almost no history of famines, unlike the rest of India.
Geologically, too, Kerala was distinct. The mighty Western Ghats mountain range runs along its eastern borders throughout. Although the southwestern coast offered little scope for agriculture because good land occurred in a narrow strip between the sea and the mountains, the uplands produced goods like black pepper, cardamom, cloves, cinnamon and ginger, which had a ready demand in the world market. Plentiful coconut trees offered scope for coir rope manufacture. The climate was suitable for rubber and tea plantations. The sailing ship construction industry on the western coast obtained timber from the Malabar forests. In the present day, plywood is a major industry.
In the interwar period, poorer and deprived people circulated more
Around 1900, the authorities in all three regions helped foreign capital, which produced or traded in plantation crops like coffee, tea and pepper, and forest-based industries including timber, rayon, coir and rubber. Some of these products were traded globally. These businesses relied heavily on local partners and suppliers, which led to the accumulation of wealth in the hands of groups like the Syrian Christians.
Some of this wealth was invested in small-scale plantations and urban businesses, which encouraged the local migration of agricultural labourers. In the interwar period, poorer and deprived people circulated more. They sought work outside traditional channels like agricultural labour where they had been at the beck and call of upper castes or caste Hindus. At the same time, protestant missions, social reformers and Leftist political movements became active in ameliorating their conditions. These forces led to a significant focus on mass education. The princely states stepped into mass education late but with greater resources on average than a British Indian district. Their investment reinforced the great strides in health and education that made Kerala different.
N ine years after India gained independence, Malabar merged with Cochin and Travancore to form the Kerala state. At that time, the livelihoods in the region, like the rest of the country, were based on agriculture. However, a much larger proportion (half or more) of the domestic product was urban and non-agricultural, compared with India as a whole. Nearly 40 per cent of the workforce was employed in industry, trade, commerce and finance, compared with 20-35 per cent in the larger states in India.
One reason for this was the scarcity of farmlands. The state’s mountainous geography made good land extremely scarce. The exceptionally high population density in the areas of intensive paddy cultivation ensured a level of available land per head (0. 6 acres) that was a fraction of the Indian average (3 .1 acres ) around 1970, and low by any benchmark. Paddy yield was high in these areas. Still, with the low size of landholding, most farmers were families of small resources.
Urban businesses processing abundant natural resources were another story. Some of these businesses were small, non-mechanised factories processing commercial products like coir in Alappuzha (Alleppey) and cashew in Kollam (Quilon). Some areas, such as Aluva (Alwaye), had larger, mechanised factories producing textiles, fertilisers, aluminium, glass and rayon. The region also had tea estates in the hills, and rubber and spice plantations east of Kottayam. Kerala today is a leading region in Indian financial entrepreneurship. Businesses from the region established banks, deposit companies and companies supplying gold-backed loans, which have a presence throughout India. Several of these companies emerged in the interwar period to finance trading and the production of exportable crops.
Thrissur (Trichur) and Kottayam were service-based cities with a concentration of banks, colleges and wealthy churches. Most local businesses were small-scale, semi-rural and household enterprises. Foreign multinationals owned tea estates and export trading firms at the apex of the spectrum of firms. Nearly everything else – from banks to small plantations, trading firms, agencies, transport and most small-scale industries – were Indian-owned family businesses.
Before statehood began in 1956, a powerful communist movement had emerged
From this base, the two decades after 1956 saw a retreat of private investment from industry and agriculture. Partly because of adverse political pressure, the foreign firms left the businesses, and plantations changed ownership. A militant trade union movement rose in the coir- and cashew-processing industries, and most firms, being relatively small, could not withstand the pressure to raise wages. Some shifted operations across the border with Tamil Nadu, where the state did not protect trade unions and labour costs were cheaper. With the central government’s heavy repression of private financial firms and the retreat of private banks, the synergy between industry, banking and commerce was broken. Private capital retreated from industrial production and trading. Following the socialist trend present in India in the 1960s, Kerala state invested in government-owned industries, which were inefficiently managed and ran heavy losses, usually resulting in negative economic contributions.
Private investment in agriculture declined, too. The Left political movement, which was concentrated in agriculture, was again partly responsible. Before statehood began in 1956, a powerful communist movement had emerged. The movement’s leaders understood that inequality in this part of India was not based on class alone. The agricultural countryside was characterised by inequality between the landholders and landless workers, which was only partly based on landownership but also drew strength from oppression and deprivation of lower castes by upper castes.
A narrow strip of highly fertile rice-growing plains in the central part of the state was the original home of Leftist politics. From the 1940s, it was a political battleground. The Leftist political parties organised the poorest tenants and workers into unions. Class-based movements to get higher wages, better employment terms or more land merged with movements to achieve equal social status. The agricultural labourers came from the depressed castes so they were interested in both class and caste politics .
When in power for a second time (from 1967), the communists ruling in coalition delivered on a promise made long ago: radical land reform. The policy involved taking over private land above a ceiling, redistributing it to landless workers, and bringing them under trade unions. The policy was successful in the extent of land redistributed (compared with most states that followed a similar policy) and in sharply raising wages. However, it did have a damaging effect on investment.
Many employers migrated to the Persian Gulf, leaving their land unattended
From the 1970s, private investment withdrew from agriculture. The cultivation of tree crops held steady, if on a low key. But cultivation of seasonal field crops, especially paddy for which the lowlands and the river basins were especially suitable, fell throughout the 1980s. By 1990, traditional agriculture was reduced to an insignificant employer and earner, and for most people still engaged in it, the land provided no more than a subsidiary income. A relative retreat from traditional agriculture is not unique to Kerala, it happened all over India. But in Kerala, the fall was spectacular.
In this densely populated area, the average landholding was small. Most landholders were middle-class people and not particularly rich. The policy squeezed their resources. Investment and acreage cropped fell. Those who remained tied to land did so because they had nowhere to go or worked the land mainly with family labour. The first Green Revolution unfolded in the rest of India, including Tamil Nadu, and had little impact on the state. Many employers migrated to the Persian Gulf in the late-1970s or ’80s, leaving their homesteads and the land unattended. What made all this anomalous was the high unemployment rate in the countryside, possibly the highest in the country. How were high wages and the retreat of a significant livelihood possible in this condition?
The answer is Gulf remittance. Hundreds of thousands of people migrated to the Persian Gulf states like Saudi Arabia, Kuwait, the United Arab Emirates, Bahrain and Qatar to work in construction, retail and services, sectors that saw a massive investment boom following the two oil shocks of 1973 and 1979. As they did, the money from the Gulf flowed into construction, retail trade, transport, cinema halls, restaurants and shops in Kerala. An emerging service sector labour market absorbed the effort of those who had been made redundant in agriculture or did not want to work there anymore.
What drove emigration to the Gulf? And why did Kerala lead the emigration of Indians to the Gulf? One answer is that the region had for centuries deeper ties with West Asia than any other part of India. Also, high unemployment pushed skilled individuals to seek work outside the state. Kerala, for at least three decades (1975-2005), supplied a significant share of the workers who moved to these labour markets. The demand for skilled workers increased as the Gulf economies diversified from oil-based jobs to finance and business services. While offering jobs in the millions, the migration also had a series of broad effects back home on occupational diversification, skill accumulation, changing gender roles, consumption, economic and social mobility, and demographic transitions.
I n the 1990s, the Indian economy liberalised, reducing protectionist tariffs and restrictions on foreign and domestic private investment. In the following decades, increased private investment led to generally elevated economic growth rates. At the same time, the political culture shifted away from emphasis on socialist ideas, becoming more market-friendly than before. Kerala was not untouched by these tendencies, but its specificities – natural resource abundance, Leftist legacy, migration history – joined the pan-Indian trend distinctly. There were three prominent elements in the story.
First, a demographic transition completed by 1990, when population growth decreased substantially. The fall in population growth rate was not unique to the state but aligned with broader Indian trends. However, the levels differed. Of all states in India, Kerala was ageing much faster than the rest and from earlier times.
Second, politics changed. Again, the legacy of Left rule was an important factor behind the shift. A communist alliance won the first state assembly elections in 1957, lost in 1960, returned to power and ruled the state in 1967-70 (with breaks), 1970-77, 1978-79, 1980-82, 1987-91, 1996-2001, 2006-11, and since 2016. The composition of the Left coalition changed multiple times, never consisting only of ideologically Left parties. It included, for example, the Muslim League and some Christian factions allied with the communists. However, until 1964, the main constituent of the coalition was the Communist Party of India (CPI), called CPI (Marxist), or CPI (M), after 1964. In no other state in India, except West Bengal (and later Tripura), did the CPI/CPI (M) command a popular support base large enough to win elections.
The Left turned friendly towards private capital and shed the rhetoric of class struggle
The Left Democratic Front, which had ruled Kerala in different years, returned to power in 2016 and has been in power since then. In the 2000s, the Leftists quietly reinvented themselves. They needed to because the older agenda was almost dead. In elections in the 1960s and ’70s, agricultural labourers in this land-poor state formed the main support base for communist victories based on the promise of land reforms. Caste-equality social reform movements coalesced around the Leftist movement. After the Leftists delivered land reforms, there was not much of an agenda.
From 2000, the Left turned friendly towards private capital and shed the rhetoric of class struggle. In practical terms, the state retreated from regulating private capital and strengthening trade unions, and focused on infrastructure investment to strengthen small businesses. The reinvention was a success and delivered election victories. As the private sector took charge of investment in education and healthcare, the state could afford to focus on decentralised governance, corruption-free administration, improved public services and urban infrastructure. The class-based politics of the 1960s and ’70s died. With private investment rising, the state had more capacity to fund welfare schemes and public administration. Tourism promotion is an excellent example of a new form of synergy: the state builds roads, private capital builds hotels, and lakes and mountains supply the landscape.
Third, investment in Kerala revived. Over the past three decades, the private sector has increasingly driven education and healthcare. Since 1990, many new types of small-scale businesses have flourished in the state. There is no single story of where the money came from and what these enterprises add to employment potential. We know much of it happened on the back of natural-resource processing. In all fields, value was added by accessing niche export markets, using new technologies, and forming many micro, medium and small enterprises. The state has one of the highest concentrations of startups. Natural resource extraction does not mean any more plantations packaging harvested spices but the extraction of nutraceuticals. Jewellery manufacture involves invention and experimentation with designs. Rubber products diversified from automotive tyres to surgical accessories.
Although foreign investment inflow, which supported business development in the princely areas, was revived via the Gulf route, most of the business development is concentrated in non-corporate family firms. Few raise significant equity capital or are publicly held. Most service sector enterprises in tourism, trade, transport, banking and real estate are relatively small. Family business remains a strong organisational model. Little research exists on the externalities that these businesses generate. The one large exception to this rule is investment in IT clusters near the big cities.
L et us start with a restatement of the main points of the story. Not long ago, Kerala was celebrated for its exceptional human development indices in education and healthcare, with many scholars attributing this to an enlightened political ideology and communist influence. These advances also resulted from factors like the princely states’ higher fiscal capacity, favourable environmental conditions, and a globally connected capitalism. During the 1970s and ’80s, government interventions weakened market activity and growth, making human development look even more striking than otherwise. Since previous commitments to social infrastructure were maintained, the state was heading toward a fiscal crisis.
In the 2000s, an economic revival came through mass migration and remittances, initially supporting consumption and construction. At the same time, a wealthier and technically skilled diaspora invested in the state, in services and manufacturing. New sectors like tourism, hotels, spice extracts, ayurvedic products, rubber products and information technology drove this revival. Remittances also flowed into new forms of consumption. The urban landscape transformed, with towns developing shopping malls, restaurants and modern businesses. While earlier regimes discouraged private investment, now there is a symbiosis between the private sector and the state, as market activity supports public welfare commitments.
The New Left, unlike the Old Left, is open to private capital and acknowledges the importance of the market, including the global market. Without compromising welfare expenditure, the state has expanded the hitherto neglected infrastructure projects, crowding in private investments. This is the second turnaround in the development trajectories of the state. The first turnaround happened during the early 1980s fuelled by remittance money. The second turnaround happened in the 2010s, when social growth, always Kerala’s strength, joined unprecedented levels of capital expenditure. If both the Left and non-Left political parties could take credit for the first turnaround, the credit for the second one should rest with the New Left.
Recent climate change and overdevelopment have increased disaster risks
Looking forward, the pathway of recent economic change has both strengths and challenges. The strengths include the generally high quality of life in small towns, improved youth aspirations often marked by an increased flow to foreign universities, better worker safety, the ability to attract skilled and unskilled migrants, unique natural-resource advantages and a degree of sociability in relations between castes and religions. The challenges are poor higher education quality, environmental threats from new forms of tourism infrastructure and climate change, a rapidly ageing population, and the possibility of a fiscal crisis.
Some of these challenges are enormous, and are already straining the budget and state capacity. Land reforms brought some equality, but the absence of follow-up actions prevented productivity improvements. Kerala produces less than 15 per cent of its food requirements, and relies heavily on central supplies and neighbouring states. To respond to this problem, the government has strengthened its public distribution system. That, along with the care of the elderly and scaling up of public services, particularly education and health, will place enormous burdens on the state’s public finances in the near future.
Historically, the state’s unique climate with abundant rainfall provided natural advantages, supporting high life expectancy and diverse agricultural opportunities. However, recent climate change and overdevelopment have increased disaster risks. The environmental transformation has been primarily driven by private construction, especially Gulf-funded developments in dwellings, hotels and service sectors. Land has become the single most speculative asset of the real-estate lobbyists. Extensive economic activities in ecologically sensitive regions, possibly accompanying tourism development with its tagline of ‘God’s own country’, allegedly led to landslides, soil erosion and environmental vulnerabilities. In recent years, an accent on ‘responsible tourism’ has tried to reduce the potential risks.
There is more. Human-wildlife conflicts and soil erosion have increased, and declining rainfall poses significant challenges. The devastating floods in 2018 and the near-disaster in 2019 highlighted the consequences of excessive construction and poor environmental management. The state now has one of India’s highest levels of consumption inequality. The quality of higher and technical education remains poor, contributing to educated unemployment.
The state’s future success will depend on balancing economic growth with environmental sustainability, improving the quality of education, improving the employability of graduates, and social equity. It is a complicated task precisely because so much of the recent growth owes to exploiting the environment. There is a real prospect of worsening inequality along caste, class, gender and age lines if the current pattern of growth slows. On the other hand, recent advancements in the digital and knowledge economy, combined with sustainable infrastructure, open fresh spaces for egalitarian development. Still, the future is hard to predict because the regional economy is deeply dependent on integration with the world economy and the ever-changing ideological alliances."
What makes a person poor and what should we do about it? | Aeon Essays,,https://aeon.co/essays/what-makes-a-person-poor-and-what-should-we-do-about-it,"Not seeing it requires effort. In the world today, despite ongoing economic growth and unprecedented levels of wealth accumulation, those who are poor are all around us. Poverty exists in every community, often lives right next door to (or in the houses of) those with immense affluence. The very present-ness of poverty gives it a taken-for-granted nature: those forced to live in poverty, apparently, are always with us.
But does the fact that the poor have been with us for a long time justify the continued existence of poverty? Are better remedies not possible?
We propose a set of measures that, by getting to its causes, will help get rid of poverty.
P overty is ordinarily defined as lacking resources for a socially acceptable standard of living. However, the socially acceptable level of living in one country or at one time can be higher or lower than in another, affecting the assessment of poverty. Governments often manipulate poverty lines to present their performance in more favourable terms. The World Bank and others have established universal poverty lines, with different thresholds corresponding to degrees of poverty: daily per-capita rates of $2.15 (extreme poverty), $3.65, $6.85 or, as in the case of the US, $15 a day. While some progress against extreme poverty has been made, the number of poor people increases rapidly at higher and more realistic thresholds. In 2019, one-quarter of Earth’s people, almost 2 billion people, lived on less than $3.65 per person per day, and almost half, 47 per cent, lived on less than $6.85 a day, with many lacking basic necessities like healthy food, potable water and clean cooking stoves.
Meanwhile, wealth has mushroomed. The richest eight people own as much wealth as the bottom 4 billion of the entire planet’s population. That’s an almost impossible fact to wrap one’s head around. It’s like the entire population of the United States – 335 million Americans – were somehow crammed into the authors’ hometown of Durham, North Carolina, while just eight people hogged all the rest of the country. Inequality of this stunning magnitude is known to corrode and destroy critical pillars of modern societies – democracy, opportunity, justice, social peace itself.
The economic pie keeps growing, yet vast poverty remains amid enormous wealth. To deflect from this dark reality, people in power keep advocating economic growth, claiming that a growing pie expands even the smallest pieces.
What kind of growth would be needed to eradicate poverty, and at what cost to people and planet?
On the surface, their logic seems intuitive. If the problem is insufficient food, income or healthcare, increasing the supply should solve the problem. But poverty resists this simple equation. Increasingly, the benefits of economic growth go almost entirely to the already rich, often at the expense of those who have the least. The relationship between economic growth and poverty is complex and often contradictory – sometimes alleviating poverty, but just as often deepening exclusion, inequality and environmental degradation.
This paradox raises difficult and pressing questions: can further economic growth eliminate poverty, or does it perpetuate it? How much, and what kind of, growth would be needed to eradicate poverty, and at what cost to people and planet? Understanding these questions requires understanding poverty not just as a lack of resources, but as a pervasive social reality – a condition of exclusion from work, health, dignity, autonomy and a healthy environment.
Our experience positions us to delve into these complexities. One of us (Anirudh Krishna) is a social scientist who has spent decades collaborating with teams of local investigators in Kenya, Uganda, Malawi, India, Bangladesh, Jamaica, Peru and the US to understand the lived experiences of poor communities. The other (Dirk Philipsen) is a political economist engaged in a global movement to reframe economic thinking around the wellbeing of both people and the planet.
We have both learned how two interlocking processes – those that produce uncertainty, and those that erect barriers and enforce exclusion – work together to trap people in cycles of poverty. However, before rethinking what poverty really means, we must first address the pervasive myths that distort its reality.
T he first myth about poverty is that it has always been with us and always will be. For roughly the first 200,000 years, or about 95 per cent, of human history, there was less technological sophistication and material wealth. There was also, usually, far less inequality . Importantly, while humans experienced hardships both prior to and after the emergence of so-called ‘civilisation’, the evidence that is available relating to early Indigenous cultures on every continent indicates they had not experienced, nor would have been able to recognise, ‘poverty’.
Poverty emerges when social stratification increases, leading to exclusion and restricted access to Earth’s resources. This exclusion worsens when money becomes the primary prerequisite for obtaining essential goods and services. Denying growing numbers of people direct access to food, shelter, trust and other vital resources makes them dependent on money to meet their basic needs and desires. Property enables and enriches those who own it as much as it excludes and impoverishes those without.
Humans were born to an immense abundance naturally existing on our planet
Accounts from ancient history through to the 21st century tell of people made poor by the greed of others. In England, the Enclosure Acts of the 17th to 20th centuries privatised common lands, leaving peasants without means to sustain themselves. Across the Atlantic, enslaved Africans were violently uprooted, their labour extracted in brutal conditions to fuel the wealth of colonial empires. Indigenous peoples across the Americas, Australia and Africa, meanwhile, were subjected to genocidal wars, forcibly displaced, and stripped of their ancestral lands by colonising powers. Behind every tale of dispossession lies a choice – by governments, corporations and elites – to prioritise power and profit over people and the planet. Woody Guthrie’s words ring true: ‘Some will rob you with a six-gun, and some with a fountain pen.’ In today’s digital age, he might add: And some with a keystroke. Predatory lending, financial manipulation and the unchecked power of multinational corporations continue to deepen global inequality. Land grabs in the Global South, where foreign investors displace small farmers, echo the enclosures of centuries past.
Whether the initial dispossession took place via war, theft, genocide, decree or the rule of law, it is not, however, where humans started. On the contrary, humans were born to an immense abundance naturally existing on our planet. This fountain of natural wealth first had to be parcelled out, fenced in, codified, privatised and licensed, before those not on the receiving end of the bounty could be made into ‘the poor’. As the historian R H Tawney concluded in 1913: ‘what thoughtful people call the problem of poverty, thoughtful poor people call … a problem of riches.’
Is poverty always going to be with us? That will depend upon what we do about the conditions that lead to people becoming poor. Poverty is not a natural state but the result of deliberate systems and policies.
T he second myth about poverty is that it is about lack of money. Only when the accumulation of wealth became the operating logic of modern societies did our collective journey become all about money and capital. We are schooled to envy those racing along in their expensive Tesla SUVs, and variously to pity, ignore or ‘aid’ those who struggle along in hoopties, rickshaws or simply on foot. Regardless of ideological values, most people assume that ‘lifting up’ everyone to the standards of the (mostly white, male, capitalist, money-rich) Tesla driver is the primary goal. A few alternative voices on the sidelines warn about running out of land to pave over, air and water to pollute, and humans to exploit. However, most remain busy trying to keep up with the flow of traffic, eager to pass those in our way.
Notably, some stretches of highway (the Scandinavian parts, for instance) provide more protections – speed limits, guard rails, better emergency services. Other parts still resemble the caricatures of the 19th-century ‘Wild West’ of North America, with barefoot travellers pushed to the side or simply ignored and run over. Yet, whether the highway runs relatively safely and slowly through Costa Rica, chaotically and dangerously through Nigeria, or is channelled with authoritarian efficiency through China, its logic and direction follow the same rules.
Even a casual observer from another world could not help but notice some odd features of this worldwide race. People seem forever frantically busy – yet neither the purpose nor the destination of the journey are discussed. The pervasive assumption simply seems to be that more is better. But this is a highway to nowhere.
Poverty stems from denied access to resources that society and the planet have in abundance
Above all, the nature of poverty cannot be reduced to a dollar value. If it were that easy, eradicating poverty would be a trivial matter of transferring funds. But in truth poverty is a web of deprivations that extend beyond a person’s bank account. That’s why more nuanced metrics, like the United Nations Human Development Index, attempt to capture the multifaceted nature of poverty by incorporating factors like education and healthcare alongside income. Even these measures, however, fall short of conveying the ways in which poverty disempowers individuals and communities, depriving them of opportunity, stripping them of agency, and stifling their potential.
Poverty is a lived experience. As the antipoverty organisation Five Talents poignantly describes, poverty is ‘an unmet need and an unfulfilled longing’. It is the lack of food, shelter and security. It is being sick but unable to see a doctor, or losing a child to a preventable disease. It is wearing clothes that don’t fit, drinking water that isn’t safe. Poverty is the vulnerability that makes people susceptible to lies, schemes and exploitation. It is an empty refrigerator, a home without electricity, a single toilet shared by a hundred neighbours. It is stress; it is shame; it is pain.
At its core, in short, poverty stems from systemic barriers: the denial of access to resources and opportunities that human society and the planet provide in abundance. Inequality is perpetuated by policymakers’ failure – or unwillingness – to adopt measures that reduce risk and lower precarity.
Poverty thrives not simply because people lack money but because the risks to their wellbeing remain unchecked, and the pathways to flourishing remain blocked. The solution does not lie in merely giving money to the poor but in addressing the root causes of insecurity.
A third myth about poverty is that it is self-inflicted. Such familiar refrains around wealth and poverty are pervasive. ‘People deserve to be rich’ and ‘The poor just need to work harder’ are little more than lazy thinking, unsupported by evidence and unmoored from logic. The reality is that a complex array of factors, from systemic inequalities to economic structures, shape wealth and poverty and, mostly, decide who is rich and who is poor. Platitudes about individualism are woefully inadequate to understand how any of this works.
Take the birth lottery, for instance. Imagine an eight-year-old girl and let’s play through different scenarios of her birth and life. If born in Finland, she would be likely to have parents enrolling her in a good school, with access to universal healthcare. She would live, by global standards, in immense safety and comfort, with the prospect of many professional opportunities, international travel, and life expectancy to her mid-80s. If born in Afghanistan, however, she would likely be denied a quality education, rarely have access to a competent and well-equipped doctor, go through her day navigating hidden landmines and violent police, with the prospect of utter dependence, desperately few chances of escape, and a life expectancy around 60. What is true across nations (Finland and Afghanistan) is also true within them. The accidents of birth shape people’s life opportunities in essential ways.
In the US, social amenities vary greatly by locality. More than any other single factor, one’s postal code determines one’s opportunities – the quality of schools, the safety of neighbourhoods, the availability of jobs. Growing up in a rural area – with lower-quality schools and clinics, erratic electricity, slower roads, patchy internet connection – is a significant liability in many countries, compared with living in the capital city. Everywhere, the intergenerational transmission of privilege is more flagrant in situations of greater inequality.
If poverty is inflicted, it is done by society, not any individual
Neither the eight-year-old in Finland or the one in Afghanistan, nor the little girl in wealthy Presidio Heights in San Francisco or the one in impoverished Riverdale in Detroit deserve these circumstances. They have picked their parents no more than they picked their life chances.
Overall, people rarely become poor or remain poor for faults of their own. On the contrary, poverty follows a distinct pathway. Tracing the life stories of those forced to live in poverty lays bare how, even as they work very hard and expect no handouts, powerful forces are stacked against them and precariousness almost always defeats individual qualities.
To the extent that poverty is inflicted, it is done by society, not any individual. Systems and policies sustain the conditions that keep people trapped in poverty – systems that amplify risks while offering few opportunities for upward mobility. These structures generate vulnerabilities and perpetuate inequality, making it exceedingly difficult for individuals to escape poverty’s grip.
So, to sum up, if poverty is not natural, has not always been with us, cannot be reduced to a number or monetary value, and is not self-inflicted, what is it? What makes one poor or not poor? And what should we do about it?
T o answer such questions turns out to be surprisingly difficult. Poverty is a lived experience defined by the crippling social reality of not being able to meet one’s basic needs. It is not, however, a condition that allows for a neat separation between the poor, the near poor, and the not poor. Instead of a clear dividing line with black on one side and white on the other, poverty is better represented in bands of grey.
It requires a moving picture and not a snapshot to visualise accurately the conditions of poverty. While hundreds of millions of people spend their lives in dire need of essentials, a possibly equal number move in and out of poverty, mostly in response to circumstances outside their control. Calamity can come in many forms – a child’s illness requiring doctor’s fees and medicines and unpaid time off; the sudden closure of one’s workplace; eviction from a rented property; the death of a loved one; a drought, flood, theft or lawsuit. For the poor or near poor, any incident is enough to bring disaster.
Many others live in an inbetween state: never far from the precipice, a vast precariat. Most of those who get routinely counted as ‘middle class’ live in the permanent shadow of poverty – one job, one illness and often one nudge away from no longer being able to meet basic needs.
It is this constantly changing nature of poverty that lies at its essence: even as many formerly poor people escape poverty, others in the same neighbourhoods and communities fall into it. Nearly two-thirds of those who are currently poor were not born poor but fell into chronic poverty. For most people in the world, poverty lurks just around the corner, a constant threat.
An effective agenda of poverty elimination is not radical, it is commonsensical
Reducing poverty effectively requires mounting a series of public programmes of increasing complexity. Scholars have long understood that personal income or wealth is a flawed metric for measuring the quality of life. Nearly every aspect of individual wealth relies on a broader public good for its value. To return to the highway analogy: my sleek Tesla is of little use on a road that is full of deep potholes, or too crowded, or unprotected from desperate thieves or corrupt bureaucrats. This truth extends far beyond transportation. Personal wellbeing is inextricably linked to social wellbeing – on access to safe public spaces, good healthcare, potable water and untainted foods, a functioning economy, an educated citizenry, a supportive community and, notably, essentials like peace or a resilient environment. Without public goods, private goods are rendered largely useless. A significant portion of poverty reduction hinges on improved public provisioning and public institutions.
An effective agenda of poverty elimination is not radical, it is commonsensical. It comes with short-term, intermediate-term and longer-term components.
In the short term, the principal focus needs to be on poverty prevention. To bring people experiencing poverty out from under the abyss and to secure for them a less shaky foundation on which to build the future, societies have a responsibility to make people’s lives less risky and more predictable.
Three kinds of risks bear thinking about. The first and biggest source of volatility in poor people’s lives arises from the manner of their incorporation into the global economy. To put it simply, the informal sector is the devil’s workshop. What can be the employers’ wildest dream-come-true – an almost endless supply of cheap, pliant and instantly hired-and-fired labour – is often the workers’ worst nightmare, an endless source of risk, indignities and insecurities.
According to the International Labor Organization, more than two-thirds of the workforce in Sub-Saharan Africa and South and Southeast Asia, and in parts of Central and South America, is composed of informal workers – those who have no contract, no regularity of employment, few (if any) legal protections, no healthcare or old-age provision. All of them are liable to be dismissed at a moment’s notice, and carry on in this way from day to day, ad infinitum. The proportion of informal workers is nearly 90 per cent in India and approximately 95 per cent in Chad and Mozambique.
The constant fear of losing their jobs gives rise to the spectre of the wolf at the door and the consequent inability to plan and invest meaningfully for the longer term. It’s not just that there is very little surplus; any that exists must be squirreled away for the inevitable rainy days.
Fixing the informal sector is imperative but that is hardly simple since powerful people benefit from access to cheap, plentiful labourers. Yet it is not a demand that requires the overthrow of capitalism. Rather, it is a question of choosing a better capitalism – written contracts, pensions and healthcare, protections against arbitrary dismissal, and access to conflict-resolution mechanisms.
T wo other aspects of informality significantly increase risks and make people’s lives unbearably uncertain: informal housing, commonly referred to as slums and shanties, and the lack of formal identity papers. Without formal titles or legal recognition of ownership, people cannot secure business loans, neighbourhoods are excluded from municipal services, and homes remain vulnerable to demolition, adding another source of anxiety and volatility. In addition, many who come into cities from rural areas lack urban identity papers and remain unrecognised by officials and politicians, adding a deeper dimension (and another consequence) of informality. Their children cannot access public health and education. World Bank and UN-Habitat data indicate that as many as 1 billion people could be living in this kind of ‘triple informality’.
Successfully addressing these elements of poverty requires several steps, including granting legal recognition to homes and land ownership, and enabling access to loans, utilities and municipal services. It also involves providing identity documents that ensure access to public health systems, education and legal protections. By transforming informal jobs and homes into reliable sources of stability rather than anxiety, people living near the poverty line are less likely to fall deeper into poverty. With reduced uncertainty and fear of losing what little they have, they are also more likely to make long-term investments in education, skills and their families’ futures, paving the way toward economic and social mobility.
In the US, where healthcare remains expensive for many, the average child poverty rate was 26.2 per cent
Second, health risks constitute another reason why people, no matter if they work in informal or formal positions, are vulnerable to being impoverished. Our team interviewed thousands of people around the world who routinely succumb to what researchers recognise as the medical poverty trap. This problem is particularly severe in countries where the bulk of the population is not covered either by public healthcare or by private/public health insurance. The share of medical expenditure that is paid ‘out-of-pocket’ (OOP) is less than 5 per cent in Botswana and around 9 per cent in Mozambique (as in France), and only around 10 per cent in Thailand (and Cuba and the Netherlands), but rises alarmingly to almost 50 per cent in India and the Philippines, and to 65 per cent in oil-rich Azerbaijan, and an astonishing 76 per cent in oil-rich Nigeria. To take care of their loved one’s medical expenses, families need to come up with a 10 per cent co-pay in Thailand and a 75 per cent co-pay in Nigeria – and that can make the difference between staying afloat and falling into poverty. Families pile up debts and sell assets.
OOP expenses reflect policy choices rather than national wealth. Countries like Costa Rica, Denmark and Tunisia have prioritised affordable healthcare to protect citizens, while others, such as the US, Australia, Nigeria and Pakistan, have not. In the US, where healthcare remains fragmented and expensive for many, the average child poverty rate between 2019-21 was 26.2 per cent, more than 6 percentage points higher than in 2012-14, and comparable to Greece or Chile. Prioritising equitable healthcare access is essential to reducing poverty risks.
Third, the vulnerabilities of both childhood and old age can overturn the best-laid plans of poorer families and communities. Precarious living tends to be most pernicious at the earliest and latest stages of an individual’s life. Pension plans and early childhood interventions are especially powerful ways to counter the risks that push people into persistent poverty.
Prioritising the reduction of social risk and vulnerability will be assisted by instituting a different kind of competition among nations. Comparing the rate at which people fall into poverty – and not the total number in poverty – is a better way of evaluating the effectiveness of countries’ protective antipoverty policies. Nation-states should promote low, preferably zero, rates of people falling into poverty, as they do high GDPs. That would make for a laudable collective achievement – one that is eminently reachable over a relatively short period of 8-10 years.
T he next transformative step is to promote and enable upward mobility, taking a medium-term perspective of 10-20 years. Talent – whether we call it ability or creativity – is randomly distributed at birth. However, too many individuals draw the ‘wrong’ tickets in the birth lottery, get surrounded by obstacles to self-advancement, and are unable to benefit from the talents they possess. The lucky few born to richer and more educated parents are better able to hone their inner potential. Most of the rest languish for want of opportunity. Consider the mathematics prodigy who ends up as a janitor because going to college is inaccessible. Or the gifted composer whose talent remains undiscovered because music is not any part of her school curriculum. Yet examples also show what can happen when opportunities are deliberately cultivated. Take the girl from a Kampala slum who became a chess champion because an extraordinary schoolteacher decided to set up a chess club in the community, defying conventional expectations.
Reversing the injustices of the past requires a shift toward ensuring universal access to meaningful opportunities of diverse kinds corresponding to individuals’ different talents and proclivities. Traditional poverty policies have long focused on merely lifting people above the poverty line, neglecting the vast, untapped potential of millions. It’s time for a new approach – one that builds ‘ladders of opportunity’ to unlock individuals’ talents, giving them viable avenues of upward mobility.
Examples from around the world demonstrate this approach. Jamaica, for instance, has nurtured world-class runners, while Estonia has given rise to a flow of world-class tech enterprises; Venezuela produced a series of excellent classical musicians, and smaller countries like Sweden have excelled in everything from high-tech information technology to furniture design.
New ‘ladders of opportunity’ can be constructed everywhere, unlocking human potential in the process
These success stories share common characteristics, such as open access with transparent standards, allowing anyone to participate and know what’s expected of them. Role models and mentors have a crucial part to play in guiding individuals along the way, while intermediate rewards and alternative careers for those who fail to reach the top offer a safety net and encourage participation. Decentralised administration, involving volunteers and civil society actors, ensures that these initiatives are community-driven and responsive to local needs.
TED Talk Talent Is Everywhere (2025) by Anirudh Krishna
By replicating these design principles, new ‘ladders of opportunity’ can be constructed everywhere, unlocking human potential in the process. The truth is, when we connect human potential with commensurate opportunity, we elevate our collective wellbeing.
The rise of automation poses a threat, echoing the enclosures that robbed people of livelihoods in past generations. Experts warn of a dystopian future where robots and AI displace workers, leading to widespread joblessness and technological polarisation. In this scenario, those who control the machines accumulate immense power and wealth, while those who get displaced are forced into poverty. To prevent this bleak outcome, we must establish protections against economic insecurity and recognise the fundamental right of all individuals to share in the planet’s resources and benefits.
The eradication of poverty is a pivotal challenge of our time. As the international development scholar Duncan Green succinctly put it: ‘Fail, and future generations will not forgive us. Succeed, and they will wonder how the world could have tolerated such needless injustice and suffering for so long.’ This fight is not just about alleviating suffering, but also about creating a more just and equitable world for all.
A serious and sustained focus on human wellbeing within existing planetary boundaries requires getting smarter. Instead of relentlessly pursuing more , humanity must learn to say enough , and build systems of collective prosperity that don’t rely on exponential growth. While there is enough for everyone to thrive, the planet will crumble under the weight of endless extraction and rampant consumerism .
Importantly, reducing poverty, minimising inequality and creating more opportunities for everyone benefits not just those who are currently disadvantaged but society as a whole. Societies with less inequality are healthier, safer and more stable, fostering greater trust, innovation and collective progress.
People around the globe are already working on practical plans to radically reduce and eradicate poverty. We do not have a particular preference among these alternative imaginings, and we are agnostic about what to call such a newly fashioned reality – common-good society, post-growth economy, capitalism 3.0, or some other. Whatever the terminology, societies that foster wellbeing for all without violating planetary boundaries and resilience will be different from all previously existing, growth-centred and extractive systems, whether capitalism, communism or fascism. Why? Because they will be designed to be in the service of all, not only the few, and they will include planning for future generations to thrive. Instead of profit, growth or power, such a system prioritises the flourishing of individuals and larger communities, ecosystems and cultures, ensuring that decisions are made with long-term sustainability and the welfare of all life in mind.
We must see healthcare, education, housing and food as human rights, not commodities to be bought and sold
The moral foundation is simple: every individual has a fundamental right to access the basic necessities of life. Call it ‘dignity for all’. We can achieve this by taking human essentials out of the ever-encroaching marketplace. Needs like housing, nutrition, community, education, healthcare and a protected environment should be provided by virtue of being human, not dependent on an individual financial transaction. From land trusts like Vienna’s to publicly owned utilities like those in the US, from universal childcare like Sweden’s to walkable cities like Copenhagen, and from free public transportation to robust public services, we can build societies where everyone has access to what they need to flourish, regardless of their financial situation. While markets can efficiently produce and distribute non-essential consumer goods, they are fundamentally unsuited to ensuring equitable access to resources vital for a dignified life. In market systems, goods and services are allocated not based on need, but merely on the ability to pay, which inherently excludes the poor from accessing basic necessities.
To create a more just and equitable society, we need to de-financialise the provision of basic needs. This means disconnecting essential goods and services from the market economy and recognising that people’s wellbeing is essential to the common good . In other words, we need to start seeing healthcare, education, housing and food as human rights, rather than commodities to be bought and sold.
Few think it is a good idea to deny one’s child opportunities to thrive. The components of a plan for eliminating poverty are clear – in the short term, reducing risks and curbing uncertainty; in the medium term, further developing an infrastructure of opportunity; and over the longer term, achieving a richer and fuller vision of a good life. How much longer, clinging to the highway to nowhere, must we abide the myth that poverty is inevitable for so many of Earth’s children?"
What is behind the explosion in talk about decolonisation? | Aeon Essays,,https://aeon.co/essays/what-is-behind-the-explosion-in-talk-about-decolonisation,"Decolonisation talk is everywhere. Scholars write books about decolonising elite universities. The government of India, a country that has been independent for 77 years, built a new parliament building in order to ‘remove all traces of the colonial era’. There are infographics on how to decolonise introductory psychology courses and guides on how businesses may decolonise their work places. Some Christians from regions that used to be colonies look to decolonise mission work through Biblical readings of Christ’s suffering. Why have expressions of decolonisation become so popular? And is there coherence to these many disparate uses of the term?
All these varied and even contradictory forms of decolonisation talk seek to draw upon the moral authority, impact and popular legitimacy of the 20th century’s great anticolonial liberation movements. And it is the gap between these movements’ promise of liberation and the actuality of continued power inequalities even after independence that has given the analytical and political space for such a wide, eclectic and contrasting array of individuals, groups and projects to wield the concept of ‘decolonisation’ to generate support for their endeavours. In the process, decolonisation talk has become more and more attenuated from the historical events of decolonisation.
The events of decolonisation involved colonised peoples, predominantly in Asia and Africa, rising up in the mid-20th century and overthrowing colonial systems of rule. National liberation movements that became postcolonial governments transformed the world order through the historical events of decolonisation. In 1945, for example, there were just 64 independent states, while today there are between 193 and 205, depending on who counts them. Before the Second World War, there were only three sovereign states with a Black head of state – Ethiopia, Haiti and Liberia.
Colonialism itself was uneven, complex and variegated. In practice, empires ruled by governing different communities differently, intensifying and maintaining often elaborate hierarchies of communities based on region, race, religion or ethnicity. For instance, colonial life looked very different in the French settler colonial cities of Algiers and Oran than it did in the Berber regions of Eastern Algeria. British colonial India was a patchwork of direct rule, princely states that were semi-autonomous regarding domestic policy, and excluded areas with a rather light imperial footprint, among other political configurations.
Other examples of colonial difference include the US insular island cases (1901), which determined that particular territories seized during the Spanish-American War would have unequal legal relationships with the continental United States: Puerto Rico, Guam and the Philippines were made into unincorporated territories where the US constitution did not fully apply, while Hawaii (and Alaska) were placed on the path to US statehood. Elsewhere, in East Africa, the German Empire recruited African soldiers who became a class of colonial intermediaries. The Japanese Empire included direct occupations in Manchuria and Korea with collaborations with anticolonial nationalists in Burma and Indonesia. Yet amid all the complexity, popular understandings of colonialism today have a clear iconography of Western conquest – of maps, pith helmets and boots bestriding non-Western continents.
I n contrast to the seeming clarity of colonialism (however much it elides that process’s own complications), current discussions of decolonisation can seem amorphous and slippery. This is not surprising since so many different people and groups are using the concept, at times at cross purposes. There are many forms of decolonisation talk drawn from the realms of culture, education, economics, politics, ideology, psychology, business, religion and more. They include postimperial (related to places that used to be empires or to institutions that used to facilitate empire) and postcolonial (related to places that used to be colonies, or to dependent power relationships created by colonialism) institutions and nations.
People across the political spectrum invoke decolonisation as an ideal and claim to represent its spirit. Some of these decolonisation discourses come from regions in what we now call the Global South, countries that were former colonies or have had continuously dependent economic relationships with postimperial countries and institutions. Calls for decolonisation coming from those regions seek more autonomy and freedom for these postcolonial countries, though they do not necessarily share the same political orientation. There are demands for economic decolonisation, as the New International Economic Order made in the 1970s. There are also claims for the need for cultural decolonisation, as the Indian government does today, which the anthropologist Alpa Shah considers a hijacking of the original concept.
Decolonisation talk also emanates from those affiliated with metropolitan or postimperial institutions, such as universities or corporations . Calls to decolonise curriculums, disciplines and university programmes seek to shift which ideas and communities should be the primary focus for these institutions’ prestige and resources. They highlight some of the seemingly surprising, even counterintuitive applications of decolonisation talk – that it can come from former centres and even agents of empire, such as universities that trained imperial civil servants. Even museums built upon collections amassed through colonial conquest promote their decolonising work.
During the summer of 1960, the United Nations seemed to recognise a new member state every week
We also hear people today use the term ‘decolonisation’ as a pejorative, in order to critique movements that seek to revise the contemporary status quo of international politics, such as those for Palestinian statehood. This is not an exhaustive set of cases. Readers will identify others. But we can see from the use of the term in economics, culture, education and political ideology that it’s a malleable concept and one a lot of people want to use and claim, in both celebration and critique. As the formal political independence moment of the mid-20th century has receded in time, decolonisation has grown less clearly tied to specific events.
Historical decolonisation, the 20th-century process where empires were broken up into independent states through a combination of warfare, protest and political negotiation – was the most significant global event since the Second World War. In the decades following 1945, more than 50 countries, primarily in Asia and on the African continent, gained their independence, mostly from European empires. The creation of newly independent states reached its apex in 1960, when 17 European colonies on the African continent gained independent statehood, such as the Republic of Congo, Nigeria, Madagascar and Somalia. During the summer of 1960, the United Nations seemed to recognise a new member state every week and national independence from colonial control became an international norm.
Countries had different paths to independence. India and Pakistan achieved self-rule (1947) through long, drawn-out, predominantly non-violent mass civil society protest movements. The Dutch were forced to accept Indonesia’s independence (1949) when they lost US financial recovery aid. Algeria (1962) won a war of national liberation. Zambia (1964) had a nationalist leader, Kenneth Kaunda, who held authority with fellow nationals as a legitimate anticolonial figure and also was recognised by US and UK mining companies as a reliable negotiating partner. Botswana became independent (1966) with its original capital located in a different country, as the UK government sought to quickly divest from an increasingly unpopular and financially unsustainable empire.
‘Decolonisation’ – the word for this transformation from a world of empires to that of postimperial and postcolonial states – was first a scholar’s or bureaucrat’s term, a description that gained popularity after the fact for the events it described. Outside of Frantz Fanon (himself a scholar) or some of the imperial civil servants that sought to constrain nationalist independence movements, few physically involved in the historic events of mid-20th-century decolonisation used the term itself at the time. Independence movements generally spoke of national liberation, which signalled the active purpose of their struggle. ‘Decolonisation’ has passivity embedded within. It does not point to who were the colonial subjects or who was the colonial ruler, who fought for independence or who fought to prevent it.
Since the word does not identify or signal who might be the specific actors responsible for ending (or perpetuating) colonial rule, it was a fundamentally small-c conservative label for revolutionary forms of politics – of regime change. The passive voice, the lack of a clear, active subject, has also helped to create analytical space for multiple and mutating meanings, which have made the term popular with governments and relatively powerful institutions today who can benefit from such strategic ambiguity.
A mid the moral legacy of national liberation movements’ pursuit of independence, the passivity embedded in the actual term, and the reappearance of decolonisation talk, it is worth remembering that, for many communities, the promise of such liberation remains largely unfulfilled. Not all peoples who sought national independence at the end of colonial rule received statehood. Kurds and Palestinians, Nagas and Tibetans, Catalans and West Papuans, among many others, have claimed independence without receiving the international recognition of statehood for their nationalist movement.
In addition, not all governments have fully enfranchised peoples claiming that they have been colonised and deserve national sovereignty: Uyghurs in China and Roma in Europe can attest to this. And for many postcolonial states, political sovereignty did not lead to economic empowerment in global systems of trade and resource extraction. Today, refugees and migrants regularly risk their lives in search of viable livelihoods, demonstrating the limits of the political decolonisation of the 20th century in fulfilling the goals of nationalist revolutionaries. Since the promise of decolonisation remains so clearly incomplete, the concept remains open to continuous interpretation.
Soon after the wave of political independence of former colonies in the 20th century, critics of neocolonial power relationships promoted the continuing need for economic decolonisation. As Margarita Fajardo has shown , Latin American international civil servants who founded and operated CEPAL – the UN’s Economic Commission for Latin America and the Caribbean – sought to broaden the project of political independence to include economic and social rights. Their endeavour, which began in the late-1940s, before national self-determination became an international norm, remains ongoing. Economic dependency theorists, cepalinos , advocates of the New International Economic Order and their heirs all called for decolonisation following political independence. They wanted to found a democratic global order of economically sovereign states with greater economic and social prosperity than that allowed by the postcolonial world order.
The Argentine economist and UN bureaucrat Raúl Prebisch emerged as a central figure in the movement for economic decolonisation. Prebisch’s leadership of CEPAL (1950-63) and then as founding secretary general of the UN Conference on Trade and Development (UNCTAD) supported the creation in 1964 of the group of 77 ‘developing countries’ (G-77) to ‘promote their collective economic interests and enhance their joint negotiating capacity on all major international economic issues.’ The G-77 was (and is, now made up of 134 countries) a group of Latin American, Asian, African and Middle Eastern countries that sought to reinforce each other’s political and economic sovereignty. They harmonised their views on global economic issues and worked for higher prices in global economic markets for the raw commodities their nations produced. The demand that greater economic equality – not just political independence – was part of decolonisation transformed what had been a set of movements to overthrow colonial rule into a mode of analysis employed by postcolonial states for challenging enduring hierarchies of economic power.
India assumed global standing as the exemplary case of peaceful, successful, anticolonial national liberation
Due to the power of a US-led global financial order, few of these economic decolonisation projects achieved their goals. However, their impact included the 1973 Oil Embargo where the Arab members of the Organization of Petroleum Exporting Countries (OPEC) used their control of a tight energy market to punish countries that had allied with Israel during the 1973 War, including the US. The embargo caused Western countries to become less reliant on Middle Eastern oil. In addition, the international economic boycott and divestment campaign against apartheid South Africa helped isolate the regime and push it towards negotiations by 1990. In this way, the economic projects of CEPAL, UNCTAD, the G-77 and those in their surrounding orbits shifted decolonisation from a historical process to a critical analysis of enduring economic inequality.
The Hindu nationalist government of Narendra Modi in India, which governs the largest postcolonial state in the world, has championed a very different decolonisation discourse, that of cultural decolonisation. India became independent in 1947, to a large extent through the peaceful popular mobilisation of the anticolonial leader M K Gandhi . After independence, India assumed global standing as the exemplary case of peaceful, successful, anticolonial national liberation in general perception. The government of Jawaharlal Nehru (1947-64) offered strong rhetorical support to decolonisation efforts across the world.
Nehru was a British-trained lawyer who traded his Western tailoring for traditional Indian clothes. In broad, simplistic strokes, Nehru came to symbolise a modern idiom of Indian politics – of the constitution, administration and secularism, meaning that the Indian state had no established religion. In contrast, Gandhi embodied a saintly idiom , focused on voluntary sacrifice, nonviolence even at the potential cost of life, attempting to reform politics by remaining at a distance from the functions of government. Modi himself has embraced a traditional idiom, connected to popular mobilisation along lines of religion and caste.
Modi is not only India’s head of government since 2014. He is the central figure of a large, organised, popular movement, the Bharatiya Janata Party (BJP) constructed around a form of politicised Hinduism ( Hindutva ) as well as the Hindi language (which is not the birth-language of most Indians) in national politics. The Modi regime has attempted to re-cast Nehru’s ‘secular modernity’ as a form of self-hating, colonial hangover. Modi has argued that independent India continues to require decolonisation from English-language education, as well as from the legacies of Muslim Mughal rule (1526-1857) which predated the British Raj. This call for cultural decolonisation combines the BJP’s ideological commitment to Hinduism and the Hindi language (the bridge language of the regions from which the party draws upon for its traditional strength) with Modi’s political charisma and skill.
The Modi government uses cultural decolonisation talk to combat Nehruvian secularism. They consider British imperial cultural remnants such as the English language within contemporary India and the presence of Islam as necessary targets for decolonisation. Since Nehru was a famous anticolonial nationalist leader during the events of historical decolonisation, it is ironic that the BJP has claimed that his legacy now requires cultural decolonisation.
The Modi regime’s use of decolonisation talk highlights the continuities between imperial cultural institutions and practices – of law, racism, language – in postcolonial states. Colonial governments prioritised European over vernacular languages, provided administrative and military jobs to particular communities defined by race or religion (or both), and enshrined these distinctions and illiberal forms of divide-and-rule into legal codes. Yet, with the exception of the anti-English-language policies, the Indian government does little to divest itself of the tools from the British Empire that they still find useful. For example, the Armed Forces (Special Powers) Act 1958 originated in a piece of British colonial law that shields the military from legal accountability in particular regions where it is deployed. The Indian government is not planning to decolonise itself of such a potent tool of government, despite its imperial origins. This logic of power is unsurprising. However, it remains seemingly incongruous for a dominant government, rather than dissidents or protestors, to wield decolonisation talk, a shift from the concept’s original meaning. While both economic decolonisation and cultural decolonisation are reappraisals of the results of historic decolonisation, the latter represents a change in who mobilises the discourse: from those who resist powerful established governments, to one of those governments itself. Therefore, it is a reconstitution of the history of decolonisation.
S tudents, scholars and activists within elite metropolitan and postimperial institutions also engage in decolonisation talk. An exemplary case is the Rhodes Must Fall movement, which originated as a student protest against a large bronze statue of Cecil Rhodes at the University of Cape Town in South Africa (a postimperial institution set up under empire) and spread globally, with particular resonance at the University of Oxford in the UK (a metropolitan institution located at the former empire’s centre).
Rhodes served as the prime minister of the Cape Colony in British South Africa, colonised Rhodesia (present-day Zimbabwe and Zambia), and donated the land on which the University of Cape Town was constructed, as well as the endowment for the prestigious Rhodes Scholarship at Oxford. In 2015, students at Cape Town demanded the removal of his statue on campus and called for the ‘decolonisation of education’. They launched a series of protests and an occupation of the university, which eventually removed the statue. Student protesters saw the university as ‘a microcosm of society’. What did it mean for contemporary South Africa that their university held ‘a landmark that bears this person’s name’? What can ‘decolonisation’ mean for those who attend a university built upon the legacy of one of the ultimate imperialists?
A new form of decolonisation talk, responding to the Gaza War, utilises ‘decolonisation’ as a pejorative
The Rhodes Must Fall protests spread nationally in South Africa and globally. They focused on emblematic images of colonialism and apartheid, which was the legal policy of racial segregation, discrimination and disenfranchisement in South Africa from 1948 to the early 1990s. The movement’s protests extended into calls for the decolonisation of education as a broad aspiration. They sought to shift whose histories curriculums celebrate or endorse – those who built empire, such as Rhodes, or those from historically disenfranchised communities? Valuable and important political ideals are by no means the unique property of Europeans and their descendants. The movement also focused its attention on which people should have access to the material resources of elite educational institutions such as scholarships, places at universities and employment opportunities. In this way, the call to decolonise education attempted to link the moral legitimacy of anticolonialism (and, in South Africa, the anti-apartheid movement) to conversations of decolonising curriculums, syllabuses and institutions.
The people – economists and civil servants, heads of government, and student protest movement members – who call for economic, cultural and educational decolonisation all claim the legacy of anticolonial national liberation movements and link their professional aspirations to that legacy. They draw upon the political legitimacy of these historic struggles against empire, even as they call for further transformations. In contrast, there is a relatively new form of decolonisation talk, responding to the Gaza War, that utilises ‘decolonisation’ as a pejorative. In a piece in The Atlantic in October 2023, the historian Simon Sebag Montefiore called decolonisation an ideology ‘taught in our universities as a theory of history’ that supports Palestinian self-determination at the expense of Israeli sovereignty. He argued that decolonisation ideology ‘dehumanises [the] entire nation’ of Israel. This condemnation and description of decolonisation ideology reacts to and reflects upon educational decolonisation, rather than the historic events of decolonisation.
All these discourses of economic, cultural, educational and ideological decolonisation are not compatible with each other. Economic decolonisation, for which Prebisch’s work and career were emblematic, critiqued the power of postimperial states to economically control ‘developing’ countries. In contrast, Modi’s call for cultural decolonisation has sought to strengthen the influence of his governmental authority within a postcolonial state (that was a symbol of historic decolonisation’s national liberation) by revising that same history. The Rhodes Must Fall protest movement and its global responses have focused on elite institutions that were at the heart of empire as they attempt to reconfigure who are the primary beneficiaries of those institutions’ resources, prestige and perceived legitimacy. In riposte, Sebag Montefiore’s characterisation of ‘decolonisation ideology’, ascribes a level of political influence and raw power to the ‘decolonisation of education’ that proponents of the movement have not attained.
A s the actual events of historical decolonisation grow more distant, forms of decolonisation talk increase. Decolonisation was once primarily a scholar’s term that effectively depolarised violent national liberation. Now it ascribes radicalism to projects in the realms of economics, culture, education and ideology – spheres whose purpose is not violent regime change.
Historical decolonisation was an international liberation project that reached the height of its political optimism in the 1960s and ’70s. It also provided an attractive source of inspiration and even analogy for movements that sought to rectify racism and other forms of injustice in the US and elsewhere. These connections dissolved as many postcolonial states were unable to provide peace and prosperity to their residents and citizens, the Black freedom movement grew less united, and deindustrialisation in so-called ‘developed’ countries (and the perception that it was caused by cheap imports and labour from predominantly postcolonial countries) eroded global solidarities.
The declining appeal of historical decolonisation led to its transformation into decolonisation talk. At the same time, it is the original promise (and the perceived viability of postcolonial states to deliver upon that promise) of its national liberatory potential that has made it a recurring source material to legitimise movements, even – or especially – for those whose aims are far removed from historic decolonisation’s regime change. While historic decolonisation is a continued reservoir of legitimacy for decolonisation talk, its inability to deliver liberation to many has created the space for so many discourses to flourish, even as they become increasingly distant from the history of decolonisation."
The surprising truth about wealth and inequality in the West | Aeon Essays,,https://aeon.co/essays/the-surprising-truth-about-wealth-and-inequality-in-the-west,"Recent decades have seen private wealth multiply around the Western world, making us richer than ever before. A hasty glance at the soaring number of billionaires – some doubling as international celebrities – prompts the question: are we also living in a time of unparalleled wealth inequality? Influential scholars have argued that indeed we are. Their narrative of a new gilded age paints wealth as an instrument of power and inequality. The 19th-century era with low taxes and minimal market regulation allowed for unchecked capital accumulation and then, in the 20th century, the two world wars and progressive taxation policies diminished the fortunes of the wealthy and reduced wealth gaps. Since 1980, the orthodoxy continues, a wave of market-friendly policies reversed this equalising historical trend, boosting capital values and sending wealth inequality back towards historic highs.
The trouble with the powerful new orthodoxy that tries to explain the history of wealth is that it doesn’t fully square with reality. New research studies, and more careful inspection of the previous historical data, paint a picture where the main catalysts for wealth equalisation are neither the devastations of war nor progressive tax regimes. War and progressive taxation have had influence, but they cannot count as the main forces that led to wealth inequality falling dramatically over the past century. The real influences are instead the expansion from below of asset ownership among everyday citizens, constituted by the rise of homeownership and pension savings. This popular ownership movement was made possible by institutional changes, most important democracy, and followed suit by educational reforms and labour laws, and the technological advancements lifting everyone’s income. As a result, workers became more productive and better paid, which allowed them to get mortgages to purchase their own homes; homeownership rates soared in the West from the middle of the century. As standards of living improved, life spans increased so that people started saving for retirement, accumulating another important popular asset.
Today, the populations of Europe and the United States are substantially richer in terms of real purchasing-power wealth than ever before. We define wealth as the value of all assets, such as homes, bank deposits, stocks and pension funds, less all debts, mainly mortgages. When counting wealth among all adults, data show that its value has increased more than threefold since 1980, and nearly 10 times over the past century. Since much of this wealth growth has occurred in the types of assets that ordinary people hold – homes and pension savings – wealth has also become more equally distributed over time. Wealth inequality has decreased dramatically over the past century and, despite the recent years’ emergence of super-rich entrepreneurs, wealth concentration has remained at its historically low levels in Europe and has increased mainly in the US.
Among scholars in economics and economic history, a new narrative is just beginning to emerge, one that accentuates this massive rise of middle-class ownership and its implications for society’s total capital stock and its distribution. Capitalism, it seems, did not result in boundless inequality, even after the liberalisations of the 1980s and corporate growth in the globalised era. The key to progress, measured as a combination of wealth growth and falling or sustained inequality, has been political and institutional change that enabled citizens to become educated, better paid, and to amass wealth through housing and pension savings.
I n his book Capital in the Twenty-First Century (2014), Thomas Piketty examined the long-run evolution of capital and wealth inequality since industrialisation in a few Western economies. The book quickly received wide acclaim among both academics and policymakers, and it even became a worldwide bestseller.
Piketty’s narrative outlined wealth accumulation and concentration as following a U-shaped pattern over the past century. At the time of the outbreak of the First World War, wealth levels and inequality peaked as a result of an unregulated capitalism, low taxation or democratic influence. During the 20th century, wartime capital destruction and postwar progressive taxes slashed wealth among the rich and equalised ownership. Since 1980, however, goes Piketty’s narrative, neoliberal policies have boosted capital values and wealth inequality towards historic levels.
Immediately after publication, Capital generated fierce debate among economists, focused primarily on the book’s theoretical underpinnings. For example, Piketty had sketched a couple of ‘fundamental laws’ of capitalism, defining the economic importance of aggregate wealth. The first law stated that the share of capital income in total income (the other share coming from labour) is a function of how much capital there is in the economy and its rate of return to capital owners. The second law stated that the amount of capital in the economy, measured as its share in total output, is determined by the balance between saving to accumulate capital and income growth. While these laws were actually fairly uncontroversial relationships, almost definitions, they laid out a mechanistic view of inequality trends that attracted considerable attention and scrutiny among Piketty’s fellow theoretical economists.
My work arrives at a striking new conclusion for the history of wealth and inequality in the West
However, what the academic debate cared less about was the empirical side of the analysis. Almost nothing was said about the historical data and the empirical conclusions underlying the claims about U-shaped patterns and main driving forces. The void in critical scrutiny exposed a widespread disinterest among mainstream economists in history and the fine-grained aspects of source materials, measurement and institutional contexts.
In recent years, a new strand of historical wealth inequality research has emerged from universities around the world. It offers a more nuanced empirical picture, including new data and revised evidence, pointing to different results and interpretations. In Piketty’s book, most of the analysis centred on the historical experiences of France, and then there was additional evidence presented for the United Kingdom and Germany (together making up Europe) and the US. Newer work reexamines and extends the historical wealth accumulation and inequality trends. Some of these contributions also revise the earlier data series, such as those analysing Germany and the UK. Other studies expand the empirical base by incorporating previously unexplored countries, such as Spain and Sweden. A number of ongoing research projects into the history of wealth distribution examine more new countries, including Switzerland, the Netherlands and Canada. Their findings will soon be added to this historical wealth database.
My work with new data, published in my book Richer and More Equal (2024), arrives at a new conclusion for the history of wealth and inequality in the West. The new results are striking. Data show that we are both richer and more equal today than we were in the past. An accumulation of housing wealth and pension savings among workers in the middle classes emerges as the main factor producing greater equality: today, three-fourths of all private assets are either homes or long-term pension and insurance savings.
U nderlying the change in personal wealth formation over the 20th century are a number of political and economic developments. The democratisation of the Western world began with the extension of universal suffrage during the 1910s. This movement initiated a process of reforming the educational system, to extend basic schooling to the population and facilitating access to higher education. New labour laws improved working life by restricting the working hours per day, allowing unions to be active. Better training and nicer workplaces raised worker productivity and earnings, creating opportunity for working- and middle-class households to purchase their own homes. The improved living standards also led to longer lives. Between the 1940s and today, life expectancy at birth increased by almost 20 years in Western countries, most of which were spent in retirement. Pension systems started evolving during the postwar era, both as public-sector unfunded systems based on promises about a future income, and as private-sector funded systems where individual pension funds were accumulated as part of people’s long-term saving.
At the core of the new findings are three empirical observations.
The first is that the populations in Western countries are richer today than ever before in history. By rich, again, I mean having a high level of average wealth in the adult population. Why this measure of riches captures relevant aspects of welfare is because higher wealth permits a lot of good things in life. It allows for higher consumption, more savings and larger investment for future prosperity. It also promises better insurance against unforeseen events. Figure 1 below illustrates the growth in the average real per-capita wealth in a selection of Western countries over the past 130 years. It is dramatic. During the first half of the past century, the average wealth in the Western population hovered at a stable level. Since the end of the Second World War, asset values started to increase, doubling the level in only a couple of decades. From 1950 to 2020, average wealth in the West increased sevenfold.
Over the past 130 years, a monumental shift in wealth composition has taken place
A fact to notice specifically is how wealth has grown each single postwar decade up to the present day. For several reasons, this consistency of growth is a marvel. It affirms the robustness of the result: we are wealthier today than in history, and this fact does not depend on the choice of start or end date but holds regardless of the time period considered. The steady increase in wealth is not confined to investment-driven growth in Europe’s early postwar decades. Neither does it hinge on the market liberalisations of the 1980s and ’90s. However, it is notable how the lifting of regulations and the historically high taxes since the 1980s are indeed associated with the highest pace of value-creation that the Western world has ever experienced.
Figure 1: rising real average wealth in the Western world. Note: wealth is expressed in real terms, meaning that it is adjusted for the rise in consumer prices and thus expresses change in purchasing power. The line is an unweighted mean of the average wealth in the adult population in six countries (France, Germany, Spain, Sweden, the UK and the US) expressed in constant 2022 US dollars. Source: Waldenström (2024, Chapter 2)
A second fact coming out of the historical evidence is that wealth in the aggregate has changed in its appearance. The composition of assets people hold tells us about the economic structure of society and what functions wealth plays in the population. For example, whether most assets are tied to the agricultural economy or to industrial activities signifies the degree of economic modernisation in the historical analysis. The importance of ordinary people’s assets in the aggregate signifies the degree to which workers take part in the value-creation processes of the market economy. Figure 2 below displays the division across asset classes in the aggregate portfolio since the end of the 19th century. It is evident that, over the past 130 years, a monumental shift in wealth composition has taken place. A century ago, wealth comprised primarily agricultural land and industrial capital. Today, the majority of personal wealth is tied up in housing and pension funds.
Figure 2: the aggregate composition of assets: from elite wealth to people’s wealth . Note: unweighted average of six countries (France, Germany, Spain, Sweden, the UK and the US). Source: Waldenström (2024, Chapter 3)
The transformation of wealth composition has strong distributional implications. Individual ownership data, often called microdata, show how ownership structures across wealth distribution bear a pattern of who owns what. Historically, the rich held agricultural estates and shares in industrial corporations. This is especially true over the long term of history, but it remains so now too. In contrast, the working population acquires wealth in their homes and long-term savings in pension funds. Homeownership rates today range from 50 to 80 per cent. Labor-force participation rates are even higher. In substance, this tells us that housing and work-related pension funds are assets that dominate the ownership of ordinary people in the lower and middle classes, which in turn links the relative aggregate importance of housing and pension funds for wealth inequality.
L ooking closer at the relationship between the share of a country’s citizens who own their homes and the level of wealth inequality, the distributional pattern becomes evident. Figure 3 below plots countries according to their homeownership rates and wealth inequality, as measured by the common Gini coefficient that ranges from 0 (no inequality) to 1 (one individual owns everything), using recent wealth and homeownership surveys. Countries with higher levels of homeownership have lower wealth inequality. The straight line in the figure has a negative slope, which suggests that raising the homeownership rate by 10 points leads to an expected reduction in wealth inequality by 0.04 Gini points. As an example, France has a lower homeownership than Italy ( 60 per cent compared with 70 per cent), and a higher wealth inequality (0.67 versus Italy’s 0.61).
Figure 3: homeownership and wealth inequality in Europe and the US. Source: Waldenström (2024, Chapter 6)
The historical shift in the nature of wealth, from being elite-centric to more democratic, can thus be expected to have profound implications for the distribution of wealth. Figure 4 below presents the most recent data from European countries and the US. They reveal in graphical form how wealth inequality has decreased substantially over the past century. The wealthiest percentile once held around 60 per cent of all wealth. The share ranged from 50 per cent of wealth in the US and Germany to 70 per cent in the UK.
Most wealth today is in homes and pensions, assets predominantly of low- and middle-wealth households
Since the first half of the 20th century, the tide has turned. A great wealth equalisation took place throughout the Western world. From the 1920s to the 1970s, wealth concentration fell steadily. In the 1970s, wealth equalisation stopped, but then Europe and the US follow separate paths. In Europe, top wealth shares stabilise at historically low levels, perhaps with a slight increasing tendency. As of 2010, the richest 1 per cent in society holds a share of total wealth at around 20 per cent in Europe. That is roughly one-third of its share of national wealth from a century earlier. Countries like the UK, the Netherlands, Italy and Finland have top percentile shares of around 16-18 per cent. A bit higher are countries like Spain, Denmark, Norway and Sweden with top shares at around 21-24 per cent. Germany has an even higher share, around 27 per cent, and Switzerland’s richest percentile group owns about 30 per cent of all wealth.
This stability of post-1970 top wealth shares may seem contradictory when contrasted with the large increases in aggregate wealth values over recent decades. However, it is consistent with most of the asset ownership patterns documented above, with most of wealth today being in housing and pensions, assets predominantly held by low- and middle-wealth households.
The US wealth concentration experience is somewhat different. Wealth inequality in the beginning of the 20th century was somewhat lower in the US than in most European countries, perhaps reflecting being a younger nation with less established elite structures. The equalisation trend also happened in the US, but it was less pronounced than in Europe. Today, US wealth concentration is currently much higher than in Europe. This situation, as the figure below shows, is the result of several years of steady increase. In historical perspective, however, even the current US level of wealth inequality is lower than it was before the Second World War, and it pales in comparison with the extreme levels of wealth concentration that the people of Europe experienced 100 years ago.
Figure 4: the great wealth equalisation over the 20th century. Source: Waldenström (2024, Chapter 5)
H ow can we account for these historical trends showing a steady growth in average household wealth and, at the same time, wealth inequality falling to historically low levels, where it has remained in Europe but has risen lately in the US? One approach is to break down the top wealth shares into the accumulation of wealth in the top and bottom groups of the distribution. In other words, we decompose the change in top wealth shares by documenting the changes in absolute wealth holdings in the numerator and denominator of the top wealth-share ratio. Figure 5 below shows these numbers, and they are striking.
During no historical time period during the past century did the wealth amounts of the rich fall on average. The falling wealth concentration from 1910 to 1980 was instead the result of wealth accumulating faster in the middle classes than in the top. Since 1950, wealth holdings have actually grown in the entire population. Between 1950 and 1980, it grew faster among the lower groups in the wealth distribution, explaining the continued equalisation. After 1980, wealth has instead grown faster in the top percentile than in the lower classes, which accounts for the halt of the long equalisation trend and a slight upward trend in the top wealth share, driven by the US development, whereas the European countries remained at its historically low levels.
Figure 5: Western wealth growth: the middle class vs the rich. The graph shows a six-country average (France, Germany, Spain, Sweden, the UK, the US) of the average annual growth rate of real (inflation-adjusted) net wealth per adult individual in the top 1 per cent and the lower 90 per cent of the wealth distribution during three time periods. Source: Waldenström (2024, Chapter 6)
Looking at the specific factors that could account for these trends in wealth growth and wealth inequality, there are some that match the evidence better than others. According to the orthodox narrative, the main explanation was the shocks to capital during the world wars and postwar capital taxes, all of which are believed to have created equality through lowering the top of the wealth distribution. In this telling, the physical capital destruction in wars reduced the fortunes of the rich, and the immediate postwar hikes in capital taxes and market regulations, such as price controls and capital market restrictions, prevented the entrepreneurs from rebuilding their wealth.
Wealth and inheritance taxes reached almost confiscatory levels in the early 1970s
However, the thesis has some issues. One is that the evidence shows little difference between belligerent and non-belligerent countries. During both wars, the wealth share of the top 1 per cent fell equally in belligerent countries like France and the UK as in non-belligerent Sweden. Including the immediate postwar years, which were heavily influenced by wartime turbulence, does not change this pattern. Germany’s data from the wars is less clear, but it appears that the country experienced larger losses than others, reducing top wealth shares. Spain, which stayed out of both world wars but fought a civil war in the 1930s, saw the wealth share of the richest 1 per cent remain virtually unchanged between 1936 and 1939, according to preliminary estimates. Looking at the US, top wealth shares fell during both wars.
Analysing instead the changes in absolute wealth held by the rich and by the rest reinforces the conclusion that wars were not a devastating moment for capital owners. In fact, the fortunes of the elite did not shrink significantly, except in France during the First World War and seemingly in Germany during both wars. In other cases, the capital values of the rich remained almost constant, and the wealth equalisation observed can be attributed to growing ownership among groups below the top tier.
Progressive tax policies after the Second World War offer another potential explanation for the wealth-equalisation trend. Capital taxation increased rapidly between the 1950s and the 1980s in most Western countries. Wealth and inheritance taxes reached almost confiscatory levels in the early 1970s, and this coincided with stagnating business activities, few startups, slowed economic growth, and an exodus of prominent entrepreneurs from high- to low-tax countries. Few studies have been able to analyse systematically the extent to which these taxes prevented the rise of new large fortunes, but studies of later periods suggest that there are good grounds to believe they did.
A general problem for the factors above – which focus on shocks to the capital of the rich and thus lowering the top of wealth distribution as the primus motor behind the great wealth equalisation of the 20th century – is that the evidence presented in Figure 5 above shows that it was instead the lifting of the bottom of the distribution that accounted for the equalisation. Let us therefore shift focus and examine the two main channels through which this happened: the accumulation of homeownership and saving for retirement.
At the turn of the 20th century, owning a decent home and saving for retirement were luxuries enjoyed by only a select few – maybe a couple of tens of millions in Western countries. Today, the once-elusive dreams of home ownership and pensions have become a reality for several hundreds of millions of people. Homeownership rates went from 20-40 per cent in the first half of the former century to 50-80 per cent in the modern era. Retirement savings also increased in the postwar period, reflecting the longer life spans that came with the general improvement of living standards. Funded pensions and other insurance savings comprised 5-10 per cent of household portfolios around 1950, but this share increased to 20-40 per cent in the 2000s.
The most crucial equalisation resulted from expanded wealth ownership among ordinary citizens
History demonstrates that the significant wealth equalisation over the past century was primarily driven by a massive increase in homeownership and retirement savings. But what initiated this accumulation of assets by households? The most comprehensive evidence highlights the role of political changes and economic developments that explicitly included new groups in the productive market economy. Firstly, the 1910s and ’20s witnessed a broad wave of political democratisation, extending universal suffrage to the Western world. Following this regime shift, a series of reforms transformed the economic reality for the masses. Educational attainment was expanded, and higher education became accessible to broader segments of society. New labour laws improved workers’ rights, making workplaces safer and reducing working hours. These changes enhanced workers’ productivity and real incomes. Simultaneously, the financial system evolved by offering better services to this new constituency of potential customers, including cheaper loans, savings plans, mutual funds and other financial services.
Thus, the primary drivers behind the great wealth equalisation of the 20th century were not wars or the redistributive effects of capital taxation. While these factors had some impact, the most crucial equalisation resulted from expanded wealth ownership among ordinary citizens, particularly through homeownership and pension savings, and the institutional shifts that enabled the accumulation of these assets.
A general lesson from history is that wealth accumulation is a positive, welfare-enhancing force in free-market economies. It is closely linked to the growth of successful businesses, which leads to new jobs, higher incomes and more tax revenue for the public sector. Various historical, social and economic factors have contributed to the rise of wealth accumulation in the middle class, with homeownership and pension savings being the primary ones.
As a closing remark, it should be recognised that the story of wealth equalisation is not one of unmitigated success. There are still significant disparities in wealth within and among nations, generating instability and injustice. Over the past years, wealth concentration has increased in some countries, most notably in the US. The extent to which this is due to productive entrepreneurship generating products, jobs, incomes and taxes, or to forces that exclude groups from acquiring personal wealth causing tensions and erosive developments in society, is a question that needs to be studied more. However, at this point it is still vital to acknowledge the progress toward greater equality that has been made in our past and understand how it has happened. Only then can we be in a stronger position to lay the foundation for further advancements in our quest for a more just and prosperous world."
What should Econ 101 courses teach students today? | Aeon Essays,,https://aeon.co/essays/what-should-econ-101-courses-teach-students-today,"What happens to the job market when the government raises the minimum wage? For decades, higher education in the United States has taught economics students to answer this question by reasoning from first principles. When the price of something rises, people tend to buy less of it. Therefore, if the price of labour rises, businesses will choose to ‘buy’ less of it – meaning they’ll hire fewer people. Students learn that a higher minimum wage means fewer jobs.
But there’s another way to answer the question, and in the early 1990s the economists David Card and Alan Krueger tried it: they went out and looked. Card and Krueger collected data on fast-food jobs along the border between New Jersey and Pennsylvania, before and after New Jersey’s minimum wage increase. The fast-food restaurants on the New Jersey side of the border were similar to the ones on the Pennsylvania side in nearly every respect, except that they now had to pay higher wages. Would they hire fewer workers in response?
‘The prediction from conventional economic theory is unambiguous,’ Card and Krueger wrote. It was also wrong. Fast-food restaurants in New Jersey didn’t hire fewer workers – instead, Card and Krueger found that employment slightly increased. Their paper set off a hunt for other ‘natural experiments’ that could rigorously test economic theory and – alongside other research agendas like behavioural economics – transformed the field.
Over the past 30 years, PhD-level education in economics has become more empirical, more psychological, and more attuned to the many ways that markets can fail. Introductory economics courses, however, are not so easy to transform. Big, synoptic textbooks are hard to put together and, once they are adopted as the foundation of introductory courses, professors and institutions are slow to abandon them. So introductory economics textbooks have continued to teach that a higher minimum wage leads to fewer people working – usually as an example of how useful and relevant the simple model of competitive markets could be. As a result of this lag between what economists know and how introductory economics is taught, a gulf developed between the way students first encounter economics and how most leading economists practise it. Students learned about the virtues of markets, deduced from a few seemingly simple assumptions. Economists and their graduate students, meanwhile, catalogued more and more ways those assumptions could go wrong.
Today, 30 years after Card and Krueger’s paper, economics curriculums around the world continue to challenge the facile view that students used to learn, in which unfettered markets work wonders. These changes – like spending more time studying market failures or emphasising individuals’ capacity for altruism, not just selfishness – have a political valence since conservatives often hide behind the laissez-faire logic of introductory economics. But the evolution of Econ 101 is not as subversive as it may sound. Instead, it reflects the direction the wider discipline has taken toward empiricism and more varied models of economic behaviour. Econ 101 is not changing to reflect a particular ideology; it is finally catching up to the field it purports to represent.
I n 2019, Harvard University’s introduction to economics course, Ec10, changed hands. The respected conservative economist and textbook author Greg Mankiw handed it over to Jason Furman and David Laibson. Furman was chair of the Council of Economic Advisers under the US president Barack Obama. Laibson, also a textbook author, focuses his research on behavioural economics – which he prefers to describe as ‘psychology and economics’. As part of this transition, the course textbook shifted from Mankiw’s popular Principles of Economics (5th ed, 2015) to Economics (2nd ed, 2018) by Laibson, Daron Acemoglu of MIT, and John List of the University of Chicago.
Their goal in revising the course was threefold, says Furman. First, the course should be coherent and helpful for students, even if they never take another economics course. Second, it should speak to issues students care about – climate change, poverty and inequality, for example. Third, it should reflect the way economics is practised today, which means more empiricism, more psychology, and more attention to market failures and public policy.
Historically, introductory courses have reflected the way that the field of economics evolved, says David Martin, an economist at Harvard and section leader for the course. Theory came first: 18th-century philosophers like Adam Smith and David Ricardo sketched out principles of how markets operate; 20th-century economists like Paul Samuelson and Kenneth Arrow turned those ideas into mathematical models.
This is science as described by the theoretician. Since then, a subtle but evident shift has taken place
Two developments in the late 20th century changed the field’s direction. First, computers made data much easier to find and to analyse. Second, advances in statistical theory led to new methods of inferring cause and effect from data. Those methods ushered in what economists dubbed the ‘credibility revolution’, and in 2021 three of its architects, including Card, received a Nobel Prize.
The empirical turn in economics upended the discipline, but textbooks have lagged behind. Publishers typically require that authors not change more than 15 per cent for any new textbook edition to avoid upsetting instructors, which effectively capped the pace at which Econ 101 could evolve. The 1997 edition of Mankiw’s introductory textbook, for example, includes a section on observation and the scientific method. It also quotes Albert Einstein’s claim that ‘The whole of science is nothing more than a refinement of everyday thinking’ and describes Isaac Newton seeing an apple fall and being motivated to develop a theory of gravity. This is science as described by the theoretician. Since then, a subtle but evident shift has taken place. In the textbook that Harvard uses, first published in 2015, empiricism is elevated to one of the three core principles of economics, alongside ‘optimisation’ and ‘equilibrium’. Their book includes sections on ‘evidence-based economics’ in every chapter.
Undergraduates in Harvard’s Ec10 read the Card-Krueger minimum wage paper in the second week of class. It’s introduced in a session on empiricism in economics, and the students complete a simplified version of the analysis, calculating the difference in employment at fast-food restaurants in New Jersey and Philadelphia before and after New Jersey raised the minimum wage. The lesson is that ‘economic theories are only as good as the predictions they allow us to make about behaviour,’ says Martin. ‘The way we generally teach is facts first,’ he says of the course. Where theory once led, it now follows.
T he theory side of Econ 101 is changing, too. The workhorse of introductory economics courses is the model of a perfectly competitive market. Students were traditionally introduced to its principles by reasoning about a consumer good with which they were already familiar, like pizza or ice cream. If a slice of pizza is free, how many will you take? (Several.) What if each slice costs $4? (Fewer.) What if each slice costs $40? (None at all.) This armchair reasoning forms the basis of a demand curve, where the quantity of a good (pizza) is higher when its price is lower.
The exercise is then repeated for the supply side where things work in reverse: the higher the price, the more people will find it worthwhile to make and sell pizza. And the point where supply and demand meet is the market equilibrium. The model assumes that buyers and sellers are all rationally optimising according to their preferences; they act so as to maximise their ‘utility’.
Harvard students still learn this model, in the second week of class. But the third week of Ec10 kicks off a series of three lectures challenging its key premises – in particular, the idea that people are purely selfish, perfectly rational maximisers. Instead, over three lessons, students are introduced to psychology, game theory and ‘social economics’ – which includes questions of fairness, trust and altruism.
Students learn that, even when people are motivated and trying to optimise, they often aren’t perfectly rational
In one class, students play a game called the ‘Keynesian beauty contest’, where everyone picks a number between one and 100. The rule is that the student whose pick is closest to two-thirds of the class average wins $10. What number should they pick? If guesses are random between one and 100, the average will be around 50, and two-thirds of 50 is 33⅓. But is 33⅓ a good guess? If everyone does that math, they’ll all guess 33 – and two-thirds of 33 is 22. But what if everyone does that math? Then the best guess would be two-thirds of 22, and so on. If everyone is purely rational and believes everyone else to be rational too, then the best guess is zero. That, in game theory lingo, is the Nash equilibrium.
In reality, the most common guess is 33, followed by 22; the third most-common guess is zero. The point of the exercise is that the game-theoretic prediction fails to match up with the real-world behaviour. Students learn that, even when people are motivated and trying to optimise, they often aren’t perfectly rational (even Harvard students).
Students also play the dictator game, where one student is given money and has the option to keep it all for themselves or to share it with another student. Most people share at least some of their windfall, says Martin, showing that ‘even when given the opportunity with no repercussions to be super greedy, a lot of people will give some money to the other person.’
These exercises challenge the notion that human behaviour is mostly selfish and rational. Such challenges to ‘ Homo economicus ’ have long had a place in economics textbooks – in the very back. Courses mirrored the textbooks, with ‘back of the textbook’ lectures on topics like altruism coming at the end of the semester.
Ec10 integrates this material throughout the course and teaches it alongside more classic models. ‘From the first second we teach [the competitive model of supply and demand], we say we’re going to teach tons of ways it fails or goes wrong,’ says Furman. What was once supplementary is now a central part of Econ 101.
H arvard is not alone in its shifting approach. In fact, for a team of economists in the UK, it doesn’t go far enough. A decade ago, they set out to ‘bring the back of the textbook to the front’ and, most controversially, to relegate the classic model of a perfectly competitive market to the back of the book.
‘The spark was the financial crisis,’ says Wendy Carlin, an economist at University College London, of the unorthodox textbook she helped to create. Students wanted to know what had gone wrong in the global economy, and introductory courses struggled to provide an answer. Margaret Stevens was having the same problem at the University of Oxford: many of her students were undergraduates in philosophy, politics and economics – and finding that the latter couldn’t answer the questions they had about the post-crisis economy.
In truth, the examples in economics textbooks were ‘chosen to fit the model’ being taught, says Carlin. Whereas ‘when researchers work on a problem, we start with a question in the world – and often some descriptive data, some hunches,’ she says. ‘And then we step back and think: “Which economic tools and which concepts are going to help us make progress on this question?”’
Carlin and Stevens teamed up with Sam Bowles, an economist at the Santa Fe Institute, to launch a new, open-source economics textbook published by CORE Econ and called T he Economy 1.0 . The first edition launched online in 2017. Earlier this year, the project – now with dozens of contributors from around the world – published the second version of its microeconomics curriculum.
They wanted to write a textbook that would draw in students and keep them motivated
For Bowles, the project recalled his correspondence with Martin Luther King Jr in the late 1960s. They’d met through anti-war activities, and King sent Bowles a list of economic questions he wanted help in answering. ‘I opened the list when it came,’ says Bowles. ‘I didn’t have a clue about how to answer any of them. It wasn’t just that I didn’t know the answers – I didn’t know where to look.’
Carlin, Stevens and Bowles had all been teachers before they were economists: Carlin has a degree in education; Stevens taught high-school mathematics; and Bowles taught high school in Nigeria. They wanted to write a textbook that would draw in students and keep them motivated.
The result is a textbook unlike the ones most economics students encounter. CORE Econ begins by charting the ‘hockey stick’ trendline of both economic growth and greenhouse gas emissions. The first chapter spans technological innovation, Thomas Malthus’s theory of population growth, and colonialism. It is now used by almost 400 universities on six continents, according to CORE Econ, and in just over half of UK universities that offer an economics degree.
In CORE, the classic model of a competitive market does not make an entrance until Chapter 8. ‘What CORE is doing in micro[economics] is trying to bring to the intro classroom what grad students have been taught for a long time,’ says Bowles.
For example, in the CORE textbook, firms are introduced as having the power to set prices. That might sound obvious but it’s not how things work in the typical introductory model of a perfectly competitive market. In that model, there are lots of identical sellers and the market sets a price. Firms can choose to either sell at that market price or not sell at all. Imagine a street with several very similar pizza parlours: if one tries to charge a much higher price than the others, customers will notice and stop shopping there, and that parlour will have to lower its price.
At least that’s the old Econ 101 logic. CORE puts that at the back of its approach to signify that it’s the special case rather than the norm, says Bowles. Instead, the CORE pedagogy teaches a model where firms sell different goods, and each has at least some power to dictate prices and wages. This choice has implications for more than prices. By eschewing the perfect competition model, CORE introduces the idea that power is a central aspect of market interactions.
CORE includes many other topics that, once, may not have made it even into the back of a textbook, including forced labour and the gender wage gap. Pirate ships of the 18th century are used to explore the role that institutions play in deciding who gets paid how much. The most recent version contains a unit on colonialism and its role in the industrial revolution.
I t’s tempting to judge CORE and even Harvard’s Ec10 in ideological terms – as an overdue response or countermeasure to a laissez-faire approach. But the evolution of Econ 101 is about more than politics. (Despite its focus on traditionally more progressive topics, CORE has been criticised for being insufficiently ‘heterodox’, according to Stevens.) By elevating empiricism and by teaching multiple models of the economy, students in these new curriculums are learning how social sciences actually work.
‘A model is just an allegory,’ says the economist David Autor in his intermediate microeconomics course at MIT. For decades, Econ 101 taught one major allegory, in which markets worked well of their own accord, and buyers and sellers all emerged better off. Government, when it was mentioned at all, was frequently portrayed as an overzealous maintenance man – able to solve some problems but also meddling in markets that were fine on their own.
That is not how most contemporary economists think. Instead, they see the competitive market as one model among many. ‘The multiplicity of models is economics’ strength,’ writes the Harvard economist Dani Rodrik in Economics Rules (2015). ‘[W]e have a menu to choose from and need an empirical method for making that choice.’ As the Econ 101 curriculum catches up, economics students are finally getting a taste of the variety that the field has to offer.
As much of an improvement as the new curriculums are, they raise a puzzle. The traditional Econ 101 course was, for all its flaws, coherent and memorable. Students came away with a clear framework for thinking about the world. What does the new Econ 101 leave students with, other than an appreciation that the world is complicated, and that data is important?
Carlin’s answer is that ‘the workhorse [of Econ 101] is that actors make decisions.’ Modelling those decisions remains a central part of economics. What’s changed is the way decision-makers are represented: they can be selfish, but they can also be altruistic. They can be rational, but they can also be biased or blinkered. They are social and strategic, and they interact with one another not just with the faceless market. Models help approximate the most salient features of these interactions, and students learn several different ones to guide their understanding. They also learn that models must fit the facts, and that a crucial part of economics is leaving the armchair and observing what is going on in the world."
How to think about the prospects of truly green growth | Aeon Essays,,https://aeon.co/essays/how-to-think-about-the-prospects-of-truly-green-growth,"At the heart of current environmental debates is a crucial question: is economic growth possible without environmental destruction? Climate change, biodiversity degradation, overexploitation of natural resources and many forms of pollution are evident problems, and their recognition is backed by a strong consensus of the sciences. It is a sign of the times that most people no longer deny the maxim that endless material growth is impossible on a limited planet. There are limits to growth, material and ecological. Only people dreaming about asteroid mining fuelled by fusion and facilitated by terraforming deny this tenet. Despite being often super-rich, like Elon Musk, in this regard they are fringe. Recognition of key environmental problems is becoming increasingly widespread, even if this is not met by necessary action.
However, alongside this is the dominant and widely accepted economic belief that growth is necessary – economic growth. It is needed not only to feed and clothe the poor of the world, but it is necessary for the energy and momentum of societies everywhere. Actually, if one listens to the eulogies of economic growth by politicians in wealthy countries, like my home country Finland, the latter is usually the main argument. Growth is needed, or else unemployment soars, pensions are unpaid, and debts accumulate.
It is widely acknowledged that in recent decades and centuries, economic growth has caused environmental problems. The severity of the ecological crisis is debated, some of it even denied, but this is the big picture. However, in recent years there has been a consistent effort to resolve this tension between economic growth and ecological limits with the notion of ‘decoupling’. (This should not be confused with the ‘decoupling’ of global trade and ‘friendshoring’ that is debated around US-China relations.) The basic idea is that economic growth can continue and literally decouple, or part ways, with material growth and environmental degradation. Growth can be green.
There are surely ecological problems, but they are not insurmountable. Just like the ‘dark Satanic Mills’ were cleaned, just like ozone depletion was overcome, any and all environmental problems will be resolved with growth, not without it. The connection between economic growth and negative impacts can be broken.
B ut not everyone agrees, and decoupling has become a hot topic in public debates. In February 2023, The New York Times published Paul Krugman’s column ‘Wonking Out: Why Growth Can Be Green’. Krugman stated that ‘it’s possible to decouple growth from environmental harm’ and aimed his column explicitly at people who claim that economic growth and environmental protection cannot be combined. He saw this putatively false claim emanating both from environmentalists (or the Left) and from people who oppose environmental policies. In effect, the former were labelled as useful idiots for the latter.
To back himself up, Krugman drew statistics from the Our World in Data site. Similar sentiments have been published in articles on the site over the years. In his article ‘How Much Economic Growth Is Necessary to Reduce Global Poverty Substantially?’ (2021), Max Roser stated forcefully that it will be possible to decouple economic growth from environmental harm. He too aimed his critique at an opposing group of people. For him, doubters of decoupling do not take poverty seriously:
In the article ‘Shrink Emissions, Not the Economy’ (2018), also published on the Our World in Data website, four authors made a case for decoupling, focusing on climate emissions. They too noted how denying decoupling brings anti-environmentalists and some environmentalists into the same camp. However, now the opposition was named specifically: ‘degrowthers’. In a nutshell, degrowth is a loose intellectual and political movement that calls for stopping the kind of economic growth that is driving ecological destruction.
The idea is that environmental harm increases as countries become more affluent but tapers off after that
One vocal representative of the degrowth movement is Timothée Parrique. He wrote a detailed response to Krugman, accusing him of cherry-picking his evidence. Parrique ended his response with strong words:
A similar public exchange of words took place after Andrew McAfee’s combative essay ‘Why Degrowth Is the Worst Idea on the Planet’ (2020) was published in Wired . He claimed that the world’s richest countries have learned how to reduce their ‘footprint on Earth’. He also attacked degrowthers as being practically anti-science:
Referring to examples of cleaning local air pollution, he invoked another specialist concept: ‘the environmental Kuznets curve’. The idea is that environmental harm increases as countries become more affluent but tapers off after that. In a nutshell: only wealthy societies can really take care of the environment.
One of the big names in the degrowther circles, Jason Hickel, responded to McAfee on his blog. Like Parrique, he referred to the extensive research literature on the decoupling question, and pointed out how partial or false is the rosy picture about wealthy societies. He also claimed that, in criticising Hickel and degrowth in general, McAfee had distorted his views. His ending was, however, diplomatic:
S hould not this engagement with empirical work and honest debate just be a matter of looking at the facts, checking out the right statistics? But this is not a simple matter. If we want to be serious about the issue, we have to frame the question properly. Meaningful discussion and even disagreement must happen within a shared frame.
Certain key questions have to be answered before any facts can be brought to bear on this issue. First of all: what kind of decoupling are we talking about? Is it relative or absolute? (More on this below.) Secondly: what is decoupled? It does not suffice to answer ‘environmental harm’ or ‘footprint on Earth’. As I wrote in an earlier essay for Aeon , environmental problems are legion, and aggregating them is highly problematic. And thirdly: from what are we supposed to decouple? What should keep growing?
If these questions are not answered, any discussion and debate will end up in confusion. There are no meaningful generic answers to the questions ‘Is decoupling possible?’ or ‘Is decoupling happening?’
If the current situation is unsustainable, relative decoupling is not enough
If decoupling of environmental impacts (or use of natural resources) from economic growth takes place, it can be of two kinds. It can be relative or absolute . In relative decoupling, both graphs keep climbing up, but environmental impacts grow slower. The speeds of growth decouple relative to each other. But environmental impacts still keep getting worse.
Depending on what kinds of impacts you are talking about – local air pollution, deforestation, climate emissions, plastic pollution – the critical limits are set differently. Perhaps some increase of some environmental impact can be stomached somewhere , if it is necessary to raise people from poverty or grow more food, for example. But if the current situation is unsustainable, relative decoupling is not enough. You need to diminish the impacts. When connected to the growth imperative, this means you need absolute decoupling. The economy grows while the impacts decrease.
It is, however, not sufficient that the impacts decrease. This has to take place fast enough. Again, here, environmental problems differ in their ecological dynamics, but with many there are specific timetables of mitigation. With climate change, you have the aspirational ‘safety limits’ of 1.5°C or 2°C and the carbon budgets that dictate the target years of net zero. In general, when there are serious risks of ecosystems (and, with them, human systems) flipping into new dangerous regimes, so-called ‘tipping points’, or else of severe irreversible damage, meeting the timetable becomes crucial.
B ut what do we need to decouple absolutely? As environmental problems differ in their geographic scale, their causes and their ecological dynamics, not all decouplings are equal. Successful elimination of local air pollution in cities is often used as an example of decoupling. However, one can quite easily understand that such pollution can be combatted by legislation or technological fixes irrespective of economic growth or decline. If certain practices are banned or made mandatory, that is it. As Parrique notes, air quality can very well be better in poorer circumstances.
Perhaps declining climate emissions is a more representative example? First of all, climate emissions are directly connected to the overall activity in society, especially energy production. If there is absolute decoupling there, that says something more general than, say, removing sewage from streets. Secondly, it is possible, quite self-evidently, to decrease climate emissions while growing the economy. If you shut down a lot of coal plants and transition to low-emission energy sources, emissions go down. Of course the economy can grow: new power systems have to be constructed, old ones dismantled. Eventually, societies will run into the realm of ‘hard to abate’ emissions from industry and agriculture, but the opening salvos are no-brainers. And, as Krugman notes, they can be good business.
As the climate scientist Zeke Hausfather noted in 2021, absolute decoupling of climate emissions has taken place already in many countries, mostly affluent Western countries that have increasingly moved away from coal and have more climate legislation. Nowhere has emission decline been fast enough: no country is on track to meet the stringent emission-cut targets. Note that absolute decoupling in these cases is true even if consumption-based emissions, that is, emissions inherent in imported goods, are counted in. Some decades ago, domestic emission decline was still somewhat illusory: it was mainly due to ‘externalising’ emissions through trade, that is, buying stuff from less wealthy countries. Since then, emission decline has stemmed more and more from domestic changes in energy production. Externalisation of some emissions takes place still, but it is not large enough to offset the general decline in emissions. And, of course, energy systems can change in the export countries too. Just look at China. Too little, too late, most likely, but still good news.
Because the current situation is unsustainable, the sustainable way is down – towards material degrowth
However, it is crucial to keep in mind that behind the façade of ‘the environmental crisis’ there is a legion of diverse issues. It is surely possible to lower climate emissions while other problems get worse: overfishing, plastic pollution, freshwater scarcity, erosion of farmland, deforestation… and, especially in its myriad forms, biodiversity decline .
Actually, climate mitigation, if done wrong, can end up exacerbating other problems. Bioenergy with carbon capture and sequestration (BECCS) is inherent in most climate scenarios. In a nutshell, it means that energy crops are grown, burned for energy, and then most of the emissions are captured technologically and stored somewhere. Large-scale cultivation of energy crops threatens to cause further deforestation, soil degradation and biodiversity decline, and added pressure on already scarce water resources in many areas. Bioenergy in general, for example excessive reliance on forest wood as fuel, tends to erode biodiversity. And many countries are waking up to realise (like Finland) that, as carbon sinks and storages in forests diminish, the putative green credentials of wood go up in smoke.
This is why, increasingly in studies of decoupling, extraction of natural resources has become a favourite metric. It is a rough tool, of course. Overfishing and sustainable fishing are different. Clearcutting rainforests is a particularly bad form of forestry. Sand is not herrings is not uranium is not apples is not oranges. If done carelessly, using flows of natural resources as a metric risks all kinds of problems of aggregation. Quality is paramount: we cannot read from those flows what we can and should do.
But we can read what we should not do. We are in an unsustainable and potentially disastrous situation with many dimensions of the environmental crisis. And material flows, the sheer amount of stuff extracted, processed, transported, used and discarded simply is the key driver for a variety of central forms of environmental impact. Because the current situation is unsustainable, the sustainable way is down – towards material degrowth. Here the analogy with climate emissions breaks down: there is no evidence of economy-wide absolute resource decoupling.
Economic growth all over the world is still strongly coupled with growing material consumption. Modern life has not ‘dematerialised’ with the promise of paperless offices, with internet and with the growing service economy. What’s more, ‘externalisation’ of resource use is still a fact of life. It is not analogous with the case of climate emissions. This makes sense: importing fruit and coffee from water-scarcity areas still uses up water; stuff produced with cleaner energy still needs raw materials.
This is why answers to the question ‘Is decoupling happening?’ can diverge radically depending on what environmental metric you are using and in which part of the world you are. And there is still the other question: ‘Is it happening fast enough?’ If not, the question arises whether it could happen faster without the growth imperative.
T hirdly, from what are we supposed to decouple? Remember, the issue of decoupling is not about whether environmental pressures can be diminished. Of course they can be. The issue is whether this can happen in the context of continuous economic growth. Some even claim that it is possible only with economic growth, the engine of innovation. At the extreme, the most technologically obsessed see acceleration through the worst of it, ‘burning through’ the historical bottleneck, as the only answer. We can clean up afterwards. Staying still is stagnation. But coining a phrase from James Baldwin, the fire next time can burn down the house and the foundations, along with the fire brigade.
The whole idea of decoupling remains tied to economic growth, and in our world that still means GDP. Pretty much everyone agrees that it is a crappy metric, but still those who argue for the necessity of growth end up arguing for continued GDP growth. Decoupling gives this crude tool a new lease of life. Alongside economic growth, people are of course talking about poverty elimination, education, ending hunger and creating human wellbeing. But the question is, are these things fatefully tethered to increasing GDP and thus increasing extraction of natural resources?
A crucial ancillary question is, does the world need to see economic growth for everyone, everywhere, all the time? This is actually an issue pointed out repeatedly by degrowther figures like Parrique and Hickel. We live in a world of deep systemic inequality within regions and between regions, with overconsumption and underconsumption, with humongous carbon and material footprints for some, and specific problems related to poverty and insecurity for some. The latter people surely need economic growth, and material and energetic growth. But the future will be bleak, if bettering the lot of billions requires overconsumption by the affluent minority.
The real issue is whether diverse environmental pressures are relieved fast enough to safeguard our continuation
This perhaps is the great lure of the idea of decoupling: it promises that everything can change without anything really changing. We are already on our way to a better world, so any kind of systemic change is not needed. All boats are raised, wealth trickles down, and so on.
In the end, however, the whole toing and froing about decoupling may itself be a confusing dead end. It anchors us to the abstraction of GDP, to any idea of aggregated and undifferentiated growth and, conversely, it seems to invite an equally abstract environmental metric as a counterpoint. Decoupling A from B. It plays havoc with contextuality.
The crux of the matter is not whether decoupling is possible. The real issue is whether, both globally and in regions all around the world, diverse environmental pressures are relieved fast enough to safeguard the continuation of our societies. It would be quite nice if we also safeguarded the continuation and resilience of the diverse ecosystems we live among. In the end, these goals are connected, even if the connections are not always clearly defined, and thus are easily ignored. Living without a stable life support system is not possible. And, of course, the world is inhabited by other beings too.
It is a secondary question how the economic accounting of that development is made. GDP itself is a creature conjured by accounting, and not a stable one at that. Changes in accounting practices can and have caused shifts of GDP without corresponding changes in material reality. So perhaps clever collective accounting tricks can be devised to put a positive economic sign for reaching a safe ecological zone for our societies. That would be a radical change.
The other crux of the matter is not growth, it is wellbeing. This has been grasped beautifully under the banner ‘A Good Life for All Within Planetary Boundaries’. Starting with a landmark article in Nature in 2018, a group of researchers has tried to answer a devilishly difficult question. Is it possible to realise sufficiently good life for all people of the world, even until the global population stabilises, and remain within a safe ecological zone? Currently, no society in the world manages to do both. And, conversely to what the eulogists of decoupling repeatedly state, the wealthier societies tend to overstep overall ecological boundaries more, even if they succeed better in some dimensions.
Answering this question, and making it a reality, will require contextual understanding and tailored solutions. It requires changing the provision systems of societies, the ways we are housed, clothed, fed, healed, transported, educated and so on. People live in different circumstances, and societies too face different challenges. There is no one grand question of decoupling that needs answering. We need many better questions and a legion of answers for the host of our problems."
How did America become the nation of credit cards? | Aeon Essays,,https://aeon.co/essays/how-did-america-become-the-nation-of-credit-cards,"The American economy has always relied on household borrowing. Since before the founding, the colonies had been ever short of metallic currency. Our 18th-century forebears substituted credit for cash. They bought goods ‘on account’, borrowing to buy time until the harvest came in or some other windfall enabled them to repay what they owed.
The 19th-century shift from agriculture to industry brought many American workers predictable wages and fixed salaries. Industrial businesses – selling sewing machines, pianos, home appliances, and especially automobiles – developed novel credit arrangements to transform steady paychecks into steady repayments. Instalment credit enabled consumers to purchase expensive durable goods with a small down payment, followed by weekly or monthly payments thereafter.
In cities, department stores refined another form of borrowing: the charge account . These accounts granted affluent consumers a fixed line of credit, which they repaid monthly without paying interest. Like instalment credit, charge accounts existed to sell goods, rather than generating profits from lending as such. Charge accounts made credit convenient, encouraging consumers to buy more.
Convenience came in part through a new link between credit and identification media. Stores issued charge tokens and later charge plates – fobs and metal cards that carried consumer account information – granting affluent consumers the prestige of recognition in cities full of strangers.
Mass consumer credit greased the wheels of mass production. In the early 20th century, proponents praised a virtuous lending cycle. Credit generated consumer demand; which encouraged industrial investment; which led to economies of scale, lower costs, and more industrial work; finally encouraging further consumer demand. Critics worried that consumers, having committed future income to present consumption, would have no future buying power to turn the wheel the next cycle, or the next. ‘Larger and larger doses of the stimulant must be injected merely to prevent a relapse,’ two prominent critics warned in 1926.
The Great Depression ended the debate. The 1929 stock market crash stalled credit buying. Consumers worried. They waited. They postponed credit purchases – a month, two months, three. Individual delays, in the aggregate, froze the economy. Without credit purchases, factories had fewer orders. With fewer orders, factories idled and laid off workers. Unemployed workers cut spending further. They did not borrow to buy. They did not buy at all. The virtuous credit circle that turned in the 1920s shuttered and stopped in the 1930s.
Policymakers took an unexpected lesson from this experience: the United States’ industrial capacity had been built to run on a steady fuel of consumer borrowing. If private lenders would not provide that fuel, New Dealers reasoned, the federal government should.
Undated postcard of Brandeis department store; Omaha, Nebraska. Courtesy History Nebraska
The New Deal has many conflicting legacies but, from that point forward, federal policy unambiguously supported a political economy with household borrowing at the centre. Federal lending programmes legitimised credit buying by aligning it with national economic priorities of stable employment and steady growth.
T hose national priorities changed course when the nation shifted from recovery to warmaking during the Second World War. Policymakers wanted consumers to save, not spend, a policy the US Federal Reserve pursued through firm controls on consumer credit. Government controls encouraged credit innovation, first to circumvent the rules, then to comply with them.
Policymakers initially targeted instalment credit, which consumers used to buy durable goods like cars and home appliances. Retailers, still eager to generate sales, modified their unregulated charge account plans to enable consumers to pay over longer periods of time. Charge accounts gained the now-familiar 30-days interest-free period, with interest charged monthly on the remaining balance.
Brandeis ‘Boston Store’; Omaha, Nebraska, 1938. Courtesy the Library of Congress
Fed officials clamped down on regulatory avoidance, prompting businesses to invest in information technology that would help them stay within the rules. Retailers needed to maintain thorough records of their customers’ credit activity and to halt credit sales that violated Fed regulations. Large department stores adopted card-based accounting systems, called Charga-Plate, that streamlined the links between their sales floors and credit departments. These systems solidified the connection between metallic identification cards and retail credit: the credit card was born.
Federal policymakers restrained consumer spending during wartime on the explicit promise that the postwar years would bring unprecedented abundance. Department stores were apostles of this future, and they entered the postwar era with a new credit product to draw in customers. Credit cards were a key feature of department store expansion in the 1940s and ’50s, out of city centres and into the growing suburbs.
The card allowed executives to wine and dine clients at restaurants and clubs
Other businesses also made credit cards central to their postwar plans. Gasoline companies, like Standard Oil of New Jersey, had developed nationwide charge account networks linking service stations in the years before the war. Wartime rationing halted credit sales. But in the late 1940s, service stations heavily promoted gasoline credit cards. Railroads, too, rolled out unified, card-based credit plans.
Courtesy Nicholas Eckhart/Flickr
These travel cards set the stage for ‘universal’ travel-and-entertainment cards. Department store cards, offered by firms like Macy’s or Gimbels, were store specific. Gasoline and rail cards linked independent businesses within the travel industry under a unified credit plan. The watershed came in 1950, when Frank McNamara introduced the Diners Club card to executives in New York City. The name was self-explanatory. The card allowed executives to wine and dine clients at restaurants and clubs, first in New York and soon around the country. The plan quickly expanded to include the full suite of travel and entertainment services. The card was, in its own estimation, the ‘Indispensable New CONVENIENCE for the Executive – the Salesman – the man who gets around!’
Bankers came to universal cards by another road. In the 1950s, most US banks were small, local institutions. Financial regulations imposed during the New Deal bound banks in a web of rules that constrained their growth and profitability. Banks primarily served business customers, but many were eager to find new products and services to expand beyond the era’s tight regulatory limits.
The postwar growth of department store chains provided an opportunity. Department stores offered credit cards. Small retailers did not. In the early 1950s, a cohort of bankers in cities and towns across the country began experimenting with local card plans that linked small retailers into local credit networks. Although the plans were modest, bankers saw opportunity. Banks ‘should be the reservoirs for every type of credit in their communities,’ a Virginia banker observed in 1953, predicting that ‘banks may be handling the bulk, maybe all, charge account financing’ in the near future.
The scale of the bank credit-card market changed in the late 1950s when California’s Bank of America launched its BankAmericard plan. Unlike most US banks, the Bank of America was big. By 1958, it had more than 800 branches across the Golden State. Unlike most US banks, Bank of America also focused on consumer lending, financing the home mortgages and auto loans that made California suburban. To manage millions of consumer accounts, Bank of America invested in information processing technology. The bank was the first to adapt mainframe computers to banking in the 1950s, and executives believed that, with computers, a state-wide credit card could be possible – even profitable.
Bank of America’s executives recognised a fundamental challenge that confronted all universal credit-card plans: the bank needed to recruit enough merchant and consumer participants to make the card plan worthwhile to each group. Bankers had initially solved this problem by signing up merchants first, and then relying on merchants to solicit cardholders among their existing customers. Bank of America started from the other end. The bank had a large customer base. If it recruited cardholders first, executives reasoned, card-carrying consumers would draw merchants into the plan.
Beginning in Fresno in September 1958, Bank of America mailed unsolicited BankAmericards to its consumer accountholders, ultimately sending more than 2 million cards across California in the first few years. The cards came unexpectedly. They were also special: for the first time, the credit cards were not metal or cardboard, but embossed plastic, an innovation that added gee-wiz novelty to the bank’s card plan. The strategy proved expensive, reckless and effective. Bank of America gained a huge cardholder base, but at the cost of massive delinquencies and fraud losses.
B y the early 1960s, department store and travel cards were well rooted in American wallets, but it was not yet clear that bank cards would succeed. Chase Manhattan, the nation’s second-largest bank, abandoned its credit card experiment after less than four years of trying in 1962. But other banks in the US were also struggling. Strict regulations ensured that, although banks were safe – very few banks failed in the postwar years – they were not very profitable. By the late 1960s, bankers increasingly saw credit cards, which combined innovative information technology with access to affluent consumer markets, as the road to the future – as the key to innovating around the restrictive financial rules.
A 1960s postcard showing the Brandeis department store relocated out of downtown Omaha, Nebraska. Public domain
The route to this future, however, was not initially obvious. Early on, credit card executives obsessed over scale. Card systems had high fixed costs, so that the more transactions a card plan processed, the lower the cost of each transaction. Yet the quest for scale ran first into the market’s fundamental division by gender. Travel and entertainment cards, like Diners Club and American Express – the latter of which began issuing cards in 1958 – catered to male business executives. Department store and bank cards, by contrast, focused on female-led family shopping. The BankAmericard was the ‘family credit card!’
In the early 1960s, Bank of America set out to merge rather than segment. The bank enrolled hotels and airlines, expanding its market from suburban housewives to their husbands. In these ambitions, Bank of America faced the second impediment to scale: geography. Under existing banking laws, it could not build branches outside of California. Its travel card competitors offered cards from coast to coast. ‘We do not believe that the Bank of America is a competitor of the Diners Club,’ boasted Alfred Bloomingdale, the Diners Club chairman, in October 1966.
In their haste to get cards into consumer hands, most banks failed to perform thorough credit checks
Bloomingdale was correct, but the tide was turning against him. As he wrote, Bank of America executives were building a credit card network that would link the plans of local banks into a nationwide system. To overcome the regulatory barriers that confined banks within individual states, Bank of America executives recruited large banks across the country, using their local merchant and consumer relationships in their states to create a national bank card plan.
The California bank also lit a fuse. BankAmericard was exclusive: only banks chosen by Bank of America could join. Competing firms responded by forming their own card networks. At first, these were regional – the Midwest Bank Card System centred on Chicago, while the New England Bankcard Association centred on Boston. Quickly, these regional consortia coalesced into a competing national network – Master Charge (later renamed MasterCard).
It is difficult to overstate the frantic growth of bank card plans in the late 1960s. Fewer than 70 banks issued cards in 1965. More than 1,200 did by 1970. Consumers experienced this growth as a steady hail of plastic. Bank of America encouraged licensees to adopt unsolicited mailing. As the banker Dee Hock recalled in his memoir, the strategy ‘all boiled down to “mass issue hundreds of thousands of unsolicited cards, sign every merchant in sight, and all will come right in the end.”’ Competing banks adopted the strategy as well. Not to be outdone, department stores and gasoline companies also mass-mailed cards. Tens of millions of unsolicited cards flooded consumer mailboxes from the start of the boom in 1966 until US Congress banned unsolicited card mailing in May 1970.
Banks careened into the credit card market to recruit new customers, meaning that, while many used their existing accountholder lists as the starting point for mass solicitation campaigns, banks invariably bought catalogues of potential cardholders from credit bureaus, mass mailing firms and other sources. David M Kennedy, chairman of Continental Illinois Bank in Chicago in the 1960s, explained that his bank mailed cards to ‘customers and shareholders and a few others in whom there was reason to place confidence [emphasis added].’ His vagueness was suggestive. Later investigations revealed that, in their haste to get cards into consumer hands, most banks failed to perform anything like thorough credit checks. Beating competitors to the mailbox was the overriding concern.
To call these campaigns reckless fails to do them justice. Unsolicited mailing delivered plastic credit that was live and ready to use. There were no systems for activating credit cards. Recipients – intended or unintended, legitimate or fraudulent – needed only open the envelope, sign the card, and use it.
T he cascade of unsolicited credit drew the attention of consumer groups, labour unions and policymakers at all levels of government. Bankers pursued cards to innovate around New Deal regulatory restrictions and to build consumer lending markets from scratch. Many did so to expand beyond downtowns that were becoming Blacker and poorer, and to reach affluent, white suburbs. In doing so, bankers inadvertently sparked a political backlash that would drastically hem in their ambitions.
The first thrust came in the form of price controls, which reflected consumers’ desire for fair credit prices and protection from excessive debt. At the time, state laws regulated the interest rates lenders could charge on most consumer lending. Credit cards were expensive and initially fell under an exemption that allowed banks and other card issuers to charge whatever rates they saw fit. As cards rained down, organisations like the American Federation of Labor and Congress Industrial Organizations and the Consumer Federation of America led state-level campaigns to bring credit card lending under state interest-rate rules. By the early 1970s, most states restricted card rates to between 12 and 18 per cent.
These restrictions, in turn, discouraged banks and other lenders from putting consumers into long-term debt. Banks borrowed the money that they ultimately lent consumers. Price controls forced banks to hold the risk that if their borrowing costs went up – for example, because the Fed raised rates to fight inflation – they would not be able to pass higher costs on to borrowers. Price controls encouraged bankers to keep repayment periods short, making cards tools for convenient credit, not long-term debt.
Mass credit card mailing sparked a full-scale moral panic and a massive policy response
As consumer and labour groups organised to control the price of credit, they also sought protection from the multiplying risks that appeared with the deluge of cards. Consumers feared identity theft and potential liability if their cards were lost or stolen (especially cards they had never requested and never seen). The cascade of cards alerted consumers to the new power of national credit bureaus and to the infuriating problems of correcting billing errors generated through automated accounting systems. Some even feared for the moral integrity of the household, as banks mailed cards to wives, children and a few deceased relatives.
Mass mailing sparked, in short, a full-scale moral panic and a massive policy response. Congress enacted a wave of consumer legislation in the late 1960s and early ’70s, including the Truth-in-Lending Act, the Fair Credit Reporting Act, and the Fair Credit Billing Act, all meant to make consumer credit markets safer. By 1976, Fed analysts reflected that ‘credit cards have been the principal subject of by far the majority of the consumer credit legislation proposed and/or enacted since 1970.’
Bankers leapt into the credit card market in the late 1960s to escape the tight regulations that constrained their industry. The consumer backlash and the wave of legislation that followed fundamentally constrained credit card plans. Instead of a source of profits, cards became another source of low-margin lending. What had once seemed like the road to the future appeared as just another dead end.
T he political emphasis on credit cards as a consumer protection issue missed their importance as a bank regulatory problem. After the Great Depression, US Congress tried to keep banks small and geographic restrictions were a cornerstone of this policy. Bankers built card networks to overcome these restrictions, but in the 1960s they still thought of cards as local products. Banks signed up merchants and consumers in their geographic territories. Because cards did not generate large profits on their own, bankers expected cards to create relationships with merchants and consumers who would then use other banking services. Consumers could use their BankAmericards and Master Charge cards across the country, but they mostly used them close to home.
The card networks, though, also opened vectors to the gradual breakdown of geographic restrictions and with them the state-level protections consumers had so recently won. The process was gradual and largely unanticipated. In some parts of the country, bank card plans began to cross state lines. In the Midwest, for example, large urban banks – like the First National Bank of Omaha, in Nebraska – partnered with small community banks in neighbouring states – like the Coralville Bank and Trust, in Iowa. The Coralville Bank suggested potential cardholders and signed up local merchants, while First of Omaha issued BankAmericards to the Coralville Bank’s customers and managed their credit accounts.
These arrangements raised difficult legal questions about where card plans were regulated – in Iowa where consumers lived and used their cards, or in Nebraska where the bank was located? Over time, federal courts and eventually the US Supreme Court determined that cards would be regulated by the state where the bank was located. At first, the cases attracted little attention – nearly all states regulated credit card rates, and the difference between 12 and 15 per cent interest was significant but small. Soon, though, the stakes would change.
At the same time that legal questions about regulatory place and space worked through the Midwestern courts, large banks in cities like New York and Chicago began to look for new ways to innovate around geographic regulations and other restrictive policies. State consumer regulations enforced a limited view of cards as strictly a credit device. Bankers, like Walter Wriston, chairman of New York’s Citibank, came to see cards as something else: banks in miniature. Citibank was a giant, global bank. It operated more than 200 branches abroad, but it could not build branches outside New York State. Paired with other technologies, like point-of-sale terminals and ATMs, Citibank executives and their peers increasingly saw cards as a way to break the territorial restrictions on consumer banking.
At the peak in 1979, banks approved nearly 75,000 credit card applications a day
In August 1977, Citibank made its move. The year before, BankAmericard announced plans to rebrand itself as Visa, distancing the card from Bank of America and signalling the globe-spanning ambitions of the network. By this time, Citibank executives believed that consumers saw only the card network – Visa and Master Charge – rather than the local banks that issued cards and managed consumer accounts. Timed with Visa’s rebranding, Citibank launched a nationwide card-solicitation campaign, mailing reams of pre-approved applications to consumers across the country. Consumers, many assuming the offer was part of the Visa changeover, accepted Citi cards by the millions.
Citibank’s card blitz took its competitors by surprise. Bankers still believed that local ties were essential to making card plans profitable and ensuring that borrowers repaid their debts. But bankers also knew not to be complacent. Just like in the 1960s, Citi’s competitors responded in kind, sparking a renewed cycle of mass solicitation. At the peak in 1979, banks approved nearly 75,000 credit card applications a day, ultimately surpassing department stores as the leading source of credit card borrowing.
Citibank’s timing initially proved ill-chosen. In the late 1970s, policymakers in the Jimmy Carter administration and at the Federal Reserve struggled to deal with persistent, unyielding inflation. Prices ratcheted up 6.5 per cent in 1977, then 7.5 per cent in 1978, before jumping over 11 per cent in 1979. The causes of inflation were heavily debated, and some blamed consumer credit generally – and credit cards specifically. Consumers used credit to engage in ‘anticipatory buying’, seeking to buy ahead of price increases and thereby creating more demand and driving prices higher.
Beginning in October 1979, Paul Volcker, the chairman of the Federal Reserve, began a dramatic campaign to kill inflation. The Fed determined to limit the growth of the money supply, allowing interest rates to rise as high as necessary to constrain monetary growth. Because lenders like banks and department stores borrowed the money they lent consumers through cards, and because state interest rate restrictions limited the prices card lenders could charge consumers, surging interest rates punished card issuers. By March 1980, one Fed governor observed that ‘there is also a good deal of concern regarding losses on consumer credit cards, but that’s news to no one.’
What was a problem for the industry was a crisis for Citibank. As Volcker ratcheted up rates, Citi’s cost of funds exceeded what it could charge cardholders. New York’s consumer protection laws were strict. Despite Citi’s pleading, policymakers would not budge. The bank, which already had more than 2 million MasterCard customers in and around New York before Citibank’s Visa campaign, now had 4 million customers across the country. Citi had staked its future on its card business; Fed policy threatened to destroy it.
The usual explanation is that consumers are irrational, a polite way to say they’re always getting tricked
Desperate, Citibank’s lawyers searched for a loophole. They found one in an obscure law called the Bank Holding Company Act, which allowed them to relocate their card operation to another state – one without New York’s strict rules – but only if the state’s legislature invited them to do so. Citi executives promised hundreds of financial services jobs, an offer South Dakota accepted in March 1980. South Dakota had no applicable price restrictions, and Citibank exported its high rates to cardholders across the country.
State-level price controls had fundamentally restrained credit card plans, forcing card issuers to internalise interest-rate risk and discouraging them from keeping borrowers in long-term debt. South Dakota, though, traded regulatory benefits for jobs and tax revenues. Other states quickly followed. In 1981, Delaware copied South Dakota’s legislation verbatim, luring banks like Chase Manhattan. As Volcker’s Fed eased rates in the early 1980s, banks launched card promotions from their new homes, offering not only high interest rates but also new annual fees.
Some states tried to hold out. For a time, Massachusetts maintained its 18 per cent rate limit and did not allow annual cardholder fees. Beginning in 1983, out-of-state banks flooded the state with card offers. Expensive credit pushed out inexpensive credit. Massachusetts’s banks lost patience. They threatened to relocate if the state legislature did not raise rates, a concession they won in early 1985.
A t first glance, the Massachusetts story is counterintuitive. If consumers prefer lower prices, wouldn’t they just choose the lowest-cost cards? Yet as the Massachusetts bankers recognised, card issuers in high-interest states could afford heavy marketing costs, pushing out low-cost alternatives. Comparison shopping is not costless. For consumers, it was easier to take the offer in the mailbox. For banks, it was cheaper to fight for deregulation than to fight for market share. Bad plastic money crowded out good.
In the years since, bank credit card plans have been consistently more profitable than other forms of bank lending. In theory, competition should bring down these profits, but it doesn’t. The traditional explanation is that consumers are irrational, which is a polite way to say they are always getting tricked. Over time, banks and other card issuers found new ways to manipulate consumers: affinity cards to create emotional and social connections (I got my first card from my university’s alumni association); low minimum payments to drag out debt; teaser rates to encourage consumers to shift debts from one bank to another; high fees for the slightest contractual infraction.
The largest card-issuers are the worst offenders . Consolidation—like the recently proposed Capital One and Discover merger—further restrains competition and ensures that the practices of the most exploitative firms become industry norms.
Part of the story, too, is that the US political economy has continued to rely on household borrowing, even as the high-paying jobs that enabled Americans to repay their debts have disappeared for many. Indeed, the great benefit – if it can be called that – of unregulated credit is that banks are willing to offer cards to consumers deemed less creditworthy: often poor and minority borrowers who had been excluded from mainstream finance. Such borrowers, often with precarious incomes, are that much more likely to get stuck in a high-interest debt trap, always revolving, never repaying.
All the while, aggregate credit card debt continued on a relentless upward ratchet. Outstanding card debt doubled from 1980 to 1985, doubled again by 1990, again by 1995, and again by 2005, topping out in April 2008 at more than $1 trillion. The crisis of 2008 brought a brief reckoning. By 2011, debt growth resumed until the COVID-19 pandemic, broke when the economy broke, and has since spiked up, reaching new all-time highs month after month.
The obvious question is: what to do? A federal interest rate cap would be a good start, as would strict limits on cardholder fees. US Congress could (and should) eliminate reward plans that make cards engines of upward redistribution, from struggling borrowers to affluent point-collectors. These would be dramatic changes and would still not get close to the root of the problem. In the United States, mass consumption came into being paired with mass credit. Critics recognised then the dangers of this combination, the need for larger and larger doses of the credit stimulant to prevent an economic relapse. Absent some fundamental restructuring, there is little to do but keep the plastic juice flowing.
Plastic Capitalism: Banks, Credit Cards, and the End of Financial Control is the forthcoming book by Sean H Vanatta, publishing in July 2024 via Yale University Press."
Why governments and business like to offload risk to individuals | Aeon Essays,,https://aeon.co/essays/why-governments-and-business-like-to-offload-risk-to-individuals,"I am sitting in my daughter’s hospital room – she is prepping for a common procedure – when the surgeon pulls up a chair. I expect he will review the literal order of operations and offer the comforting words parents and children require even in the face of routine procedures. Instead, he asks us which of two surgical techniques we think would be best. I look at him incredulously and then manage to say: ‘I don’t know. I’m not that kind of doctor.’ After a brief discussion, my husband and I tell him what, to us, seems obvious: the doctor should choose the procedure that, in his professional opinion, carries the greatest chance of success and the least risk. He should act as if our daughter is his.
In truth, this encounter should not have surprised me. I have for several years been working on a book about risk as a form of social and political logic: a lens for apprehending the world and a set of tools for taming uncertainty within it. It is impossible to tell the contemporary story of risk without considering what scholars call responsibilisation or individuation – essentially, the practice of thrusting increasing amounts of responsibility onto individuals who become, as the scholar Tina Besley wrote , ‘morally responsible for navigating the social realm using rational choice and cost-benefit calculations’. In the United States, business groups and politicians often call offloading more responsibilities onto citizens ‘empowering’ them. It’s maybe telling that this jargon prevails in the private healthcare sector, just as moves to privatise social security in the US are cast as empowering employees to invest their retirement savings however they see fit.
In Individualism and Economic Order (1948), F A Hayek wrote: ‘if the individual is to be free to choose, it is inevitable that he should bear the risk attaching to that choice,’ further noting that ‘the preservation of individual freedom is incompatible with a full satisfaction of our views of distributive justice.’ Over the past several decades, Hayek’s position has gone mainstream but also become somewhat emaciated, such that individual freedom substantively means consumer choice – the presumed sanctity of which must be preserved across all sectors. But devolved responsibility favours those with more capacity to evaluate and make decisions about complex phenomena – those of us, for instance, with high levels of education and social access to doctors and investment managers to call for advice. And indeed, it’s striking that the embrace of responsibilisation as a form of individual ‘empowerment’ has accompanied the deepening of inequality in Western democracies.
This trend persists despite growing recognition from psychologists and economists that most of us are not rational decision-makers and that we are particularly terrible at assessing risk. Whether it is the likelihood of dying in a terrorist attack or succumbing to COVID-19, research shows that people tend to overestimate high-profile threats and underestimate everyday ones like driving a car. A cottage industry has sprung up to cater to this observable gap between perceived and actual risk. Books like Gerd Gigerenzer’s Risk Savv y (2014) and Dan Gardner’s Risk: The Science and Politics of Fear (2008) try to guide individuals toward more rational risk assessments; online tools invite users to calculate their chance of heart disease or becoming a crime victim ; and websites like oddsofdying.com compile data on dozens of deadly threats – from earthquakes and plane crashes to bee stings and malaria. Yet there is an appreciable disconnect between what the data say and how many of us feel. As Gardner’s book puts it: ‘We are the safest humans who ever lived – the statistics prove it. So why has anxiety become the stuff of daily life? Why do we live in a culture of fear?’
How can we account for this disconnect? Why, given our demonstrable lack of capacity, are we still charged with becoming better risk calculators? And what type of political subject is the actuarial self?
G iven that few objects elicit yawns like an actuarial table, it is worth underscoring what is at stake in conversations about risk: death, fear, and our ability to control what befalls us in the future. In his triumphal history Against the Gods: The Remarkable Story of Risk (1996), Peter Bernstein argued that risk management could not emerge without ‘the notion that the future is more than a whim of the gods.’ According to Bernstein, the idea of risk made the world modern by allowing humans to slough off centuries of theological baggage and chart their own paths. One can see how this notion contributed to the philosophical edifice of Western individualism, and why this conceptualisation of individual autonomy and responsibility has remained attractive down to our own time.
Bernstein traced the origins of risk to the 17th century, when these new ideas about human agency began to circulate alongside the mathematics of probability, which posited that past events could be systematically analysed to yield insights about the future. Such notions could arise only in post-Renaissance Europe, he argued, which explains why ‘the fatalistic Muslims were not yet ready to take the leap’ despite their mathematical sophistication. In Bernstein’s telling, the exceptionalism of European culture – not shifts in governance or the rise of capitalism – accounts for how the West broke the bonds of fatalism.
While positive associations still adhere to entrepreneurs and other ‘risk-takers’ today, they have become a minority discourse. This is one reason why Bernstein’s End-of-History-era euphoria sounds like a dispatch from another planet. For many, the language of risk instead denotes everything that could go wrong: climate risk, health risks, cyber risk, financial risk, terrorism risk, AI risk, and so forth. Outside certain pockets of Silicon Valley – such as the self-described techno-optimist Marc Andreessen – thoughts of the future are more likely to elicit anxiety or dread than excitement.
In 1603, deaths from consumption represented 20 per cent of the total burials
Human attempts to scientifically manage risk have expanded dramatically alongside this dimming of hopes for the future. While many risk-management practitioners stress that theirs is a young field – not more than 40-45 years old, according to Terje Aven, judging by when ‘the first scientific journals, papers and conferences covering fundamental ideas and principles on how to appropriately assess and manage risk’ emerged – certain patterns date back several centuries.
In 1603, in the midst of a plague outbreak that would kill a fifth of the population, the City of London began producing weekly bills of mortality. It was the duty of parish clerks to record and report the total number of burials, plague deaths and christenings for their congregation (in an early instance of sampling bias, the data thus excluded information about nonconformers). By the 1660s, these bills came to include a range of ailments that one might succumb to: ‘aged’, ‘childbed’, ‘dropsie’, ‘executed’, ‘griping in the guts’, ‘infants’, ‘rickets’, ‘sore brest’ [sic], ‘teeth’, and dozens more (see below).
London’s Dreadful Visitation, or, A Collection of All the Bills of Mortality for this Present Year (1665). Courtesy of the Public Domain Review
As the data shows, deaths from consumption represented just over 20 per cent of the total burials, while fever, ‘flox and small-pox’, old age and ‘convulsion’ together accounted for another third. The table also underscores the high rate of infant mortality, with 10 deaths, eight stillborn, and two abortions recorded. As public health records – indeed, as artefacts of the emerging idea that health was something that existed beyond individuals and required management at population level – the London mortality bills are invaluable. They also helped give rise to a new type in public life, the data-wielding expert who compared human fears about death with the actual human experience of death, according to numbers.
J ohn Graunt (1620-74) was a London haberdasher turned demography pioneer who published the work Natural and Political Observations … Made Upon the Bills of Mortality (1662). Alongside his friend and sometimes collaborator, William Petty, Graunt is considered one of the founding fathers of statistics. Sensing that the mortality bills might be good for something beyond warning the rich when to retreat to their country homes, Graunt used them to tabulate and compare casualties for the years 1629-60, as well as the total number of burials and christenings from 1604-61. Applying a series of crude mathematical assumptions, Graunt was able to calculate the total population of London – and crucially, the number of military-aged men – and estimated the likelihood of dying from varied causes. As he explained his task, ‘whereas many persons live in great fear, and apprehension of some of the more formidable, and notorious diseases following; I shall only set down how many died of each, being compared to the total 229,250, those persons may the better understand the hazard they are in.’ With his data, Graunt assured readers that the chance of succumbing to much-feared toads or snakes was quite rare, and likewise that any man at present ‘well in his Wits’ had less than a 1/1000 chance of dying ‘a Lunatick in Bedlam ’ over the next seven years.
Graunt’s attempts to better understand the objective risk of harm resemble those found in the Port Royal Logic (1662), published in France the same year that Graunt’s Observations appeared in England. The work of Antoine Arnauld and Pierre Nicole, philosopher-theologians associated with the Port Royal Abbey, the Port Royal Logic charted a path between scepticism and credulity in an effort ‘to make us more reasonable in our hopes and fears’. The authors considered the princess who’d heard of someone being crushed by a falling ceiling, and so refused to enter any home without having it first inspected. ‘The flaw in this reasoning is that in order to decide what we ought to do to obtain some good or avoid some harm, it is necessary to consider not only the good or harm in itself, but also the probability that it will or will not occur,’ they wrote. For instance, though many feared thunder because of ‘the danger of dying by lightning … it is easy to show it is unreasonable. For out of 2 million people, at most there is one who dies in this way … So, then, our fear of some harm ought to be proportional not only to the magnitude of the harm, but also to the probability of the event.’
Nearly 50 per cent of the ‘very liberal’ believed that COVID-19 presented a ‘great risk’
Despite the apparent similarities in tone, the Port Royal Logic and Graunt’s mortality tables advanced very different approaches to risk. The Port Royal authors had no empirical data to guide them about the frequency of lightning or anything else. Their project was rather a rationalist attempt to hone man’s common sense which, they noted, was none too common. They provided a set of tools and guidelines that aimed at increasing the practical reasoning capabilities of the reader so that they could make better judgments. In contrast, Graunt was among the first empiricists, who argued that real-world data could lead us to truth if only we would follow the numbers. The point was not to perfect human judgment, but to obviate it. In this way, his mortality tables provide an early example of the ‘quantitative authority’ (to borrow from the sociologist Wendy Espeland) that still adheres to numbers in our own time.
Natural and Political Observations Mentioned in a Following Index, and Made Upon the Bills of Mortality (1662) by John Graunt. Courtesy of the NIH Digital Collections
Compare, for instance, Graunt’s Observations with the COVID-era New York Times newsletter by David Leonhardt. In March 2022, as the Omicron wave of COVID-19 was cresting in much of the US, Leonhardt examined COVID fears among the self-identified ‘very liberal’. Drawing on recent polling data, he noted that nearly 50 per cent of that subgroup believed that COVID presented a ‘great risk’ both to their personal and their children’s health. He further noted that the ‘very liberal’ were disproportionally urban professionals who embraced a risk-adverse style of parenting: ‘Parents seek out the healthiest food, sturdiest car seats and safest playgrounds. They do not let their children play tackle football, and they worry about soccer concussions.’ While this mode of parenting had produced some notable gains – two cheers for bicycle helmets! – Leonhardt noted it also came with downsides: ‘It can lead people to obsess over small, salient risks while ignoring bigger ones.’ With regard to COVID, that meant ‘there is abundant evidence that the most liberal Americans are exaggerating the risks to the vaccinated and to children’ while ignoring more routine ones: ‘the flu kills more children in a typical year and car crashes kill about five times as many.’ Leonhardt thus joined the ranks of those who believe the main problem with the actuarial self is that most of us remain poor risk-calculators.
F ew academic fields have done more to promote the idea of a gap between perceived and ‘objective’ risk than behavioural economics, which is arguably the most important academic export of the past 30 years. Besides popular bestsellers like Dan Ariely’s Predictably Irrational (2008), Daniel Kahneman’s Thinking, Fast and Slow (2011) and, perhaps most famously, Richard Thaler and Cass Sunstein’s Nudge (2008), the field has made significant public policy inroads. In 2010, David Cameron’s government established the ‘Nudge Unit’ to apply the field’s insights to UK social and economic problems ranging from obesity to online scams. As of 2019, they reported organising 780 projects in dozens of countries since its inception. The US president Barack Obama followed suit in 2015 with the creation of the Social and Behavioral Sciences Team.
Pioneered by Kahneman and fellow Israeli cognitive psychologist Amos Tversky, behavioural economics considers the real-life departures of human decision-making from the rationalist model associated with Homo economicus , or economic man. Homo economicus expresses the idea that humanity is composed of autonomous individual actors who are rational and narrowly concerned with maximising their own benefit. This idea is often traced to Adam Smith’s The Wealth of Nations (1776), which argued that the pursuit of selfish interests actually serves the common good: ‘It is not from the benevolence of the butcher, the brewer, or the baker that we expect our dinner, but from their regard to their own interest.’ Economic man reached his apex in the mid-20th century at the hands of the game theorists John von Neumann and Oskar Morgenstern – who tried to sketch a model of how the rational actor should behave under conditions of uncertainty – and remains a mainstay of mainstream economics.
Behavioural economists take aim at Homo economicus by uncovering the biases, heuristics and logical flaws that cloud human decision-making – which is not, from their perspective, fully rational. For example, if your judgment about a $40 T-shirt is swayed by seeing one costing $60, you might be ‘anchoring’. If that doesn’t sound like something you would do, rest assured there are dozens of other biases and sins of logic to choose from. Behavioural economics does not, however, question the individual as the foundational economic and social unit. Just as, for their neoclassical economic ancestors, autonomous, self-interested individuals remain the building blocks of society, and maintaining consumer choice (‘freedom’) remains the greatest good. The innovation comes in arguing that, because humans are not fully rational, they require outside encouragement – ‘nudges’ – to help them make better choices. The intent is not to free people entirely to make their own decisions (remember, we’re bad at it), but rather for elite experts to guide them toward the choices they deem best. That might sound reassuring until you meet the experts.
Thaler and Sunstein are primary exponents of what’s come to be known as nudge theory. Their 2008 book urged companies, governments and public institutions to create ‘choice architectures’ that encourage ‘appropriate’ choices among individuals trying to navigate the maze of modern life: ‘A good system of choice architecture helps people to improve their ability to map choices onto outcomes and hence select options that will make them better off.’ For example, placing fresh fruit at eye level in a school cafeteria can nudge students to eat healthier foods, and automatically enrolling employees in 401(k) retirement plans may result in higher levels of personal savings. Crucially, nudges cannot be coercive, and thus cannot reduce the range of options available, even if some of them might be harmful. While Sunstein and Thaler admit that products like extended warranties take advantage of consumers and should be avoided, as self-described libertarian paternalists they stop short of recommending legislation that would outlaw them. For higher-stakes products including credit cards, mortgages and insurance policies, they recommend a light regulatory approach called RECAP (record, evaluate, and compare alternative prices). RECAP requires more robust disclosures and transparent pricing information so citizen-consumers can make informed choices.
Cap-and-trade solutions have actually enabled major polluters to increase their emissions
As these cases illustrate, nudgers reject strong regulatory alternatives, what Thaler and Sunstein call ‘command-and-control’ regulations: ‘we libertarian paternalists do not favour bans. Instead, we prefer an improvement in choice architecture that will help people make better choices…’ This means, in effect, opposing financial regulations like those that constituted the New Deal regulatory regime, which expressed a government mandate not merely to encourage better choices among consumers but to offer protections to citizens. So too, the nudgers worry that command-and-control environmental regulations are a slippery slope to totalitarianism. ‘Such limitations [eg, on vehicle emissions],’ write Thaler and Sunstein, ‘have sometimes been effective; the air is much cleaner than it was in 1970. Philosophically, however, such limitations look uncomfortably similar to Soviet-style five-year plans.’ This judgment is particularly striking when considered alongside the extremely high and consistent levels of public support – ranging between 70 and 80 per cent of Americans across the political spectrum – for mandated vehicle efficiency standards since they were introduced in 1975. Never mind their actual popularity: ambitious standards related to vehicle emissions, clean air and clean water are precisely the sort of ‘1970s environmentalism’ that, argues Sunstein in Risk and Reason (2002), states need to move beyond in favour of cost-benefit analyses and ‘free market environmentalism’.
In lieu of public mandates and restrictive legislation, Thaler and Sunstein endorse economic incentives and market-based solutions, such as cap-and-trade deals that encourage industrial polluters to reduce their emissions. Yet, as even they acknowledged, such ‘solutions’ come with loopholes: ‘If a polluter wants to increase its level of activity, and hence its level of pollution, it isn’t entirely blocked. It can purchase a permit via the free market.’ And indeed, several studies have now shown the critical flaws in cap-and-trade and other market-based solutions, which have actually enabled major polluters to increase their emissions and concentrate pollution in low-income neighbourhoods. In 2009, President Obama appointed Sunstein head of the White House’s Office of Information and Regulatory Affairs – essentially, the country’s top regulator.
Nudgers are in essence tweakers who eschew the sort of major policy shifts that might, for instance, ensure senior citizens are able to retire even without a 401(k). As Nudge ’s subtitle – ‘Improving Decisions about Health, Wealth, and Happiness’ – makes clear, the project’s utmost concern is to preserve the individual decision-maker as the primary social actor. Indeed, the champions of behavioural economics share many structural assumptions with their neoclassical forebears about the supremacy of efficiency, the relative benevolence of the private market, and the need to maximise individual choice as a bulwark against serfdom. This framework has particularly perverse consequences when it comes to navigating risk in the 21st century.
Take financial risk, for instance. As reported by the Financial Times in May 2023, the UK bank TSB found that 80 per cent of fraud activity took place through Facebook, WhatsApp and Instagram. Yet the UK government’s 2023 anti-fraud strategy had ‘scrapped plans to force tech companies to compensate victims of online financial scams’, opting instead for a voluntary ‘online fraud charter’ and better reporting tools. So, rather than require platforms to police their content, the onus now falls on individuals to become better fraud detectors.
Or consider the various ‘green’ behaviours individuals are directed to perform in an effort to stave off the climate crisis. My five-year old’s class is picking up litter ‘to save the Earth’, all while the leading carbon-intensive industries – fossil fuels, transportation, fashion – continue with business as usual. The limitations are particularly galling when it comes to the ways people in the US are encouraged to navigate health risks. While nudgers offer a variety of life hacks to eat better, and favour user-friendly ways to navigate the private insurance market, such fixes ignore the far larger, structural risks that stem from Americans’ lack of affordable and proximate healthcare. Yet we know that an estimated 45,000 working-age Americans die each year because of lack of medical insurance, that the uninsured are less likely to receive preventative care and cancer screenings, and that two-thirds of bankruptcy filings are attributable to medical debt. Only a comically impoverished theoretical framework could consider health risks in the US and deduce that Americans need to eat more salads.
These cases underscore the mismatch between the systemic nature of the risks humans face and the individualistic tools promoted to manage them. No wonder many feel that, despite their conscientious attempts to reduce various risks, nothing is working. Cultivating the actuarial self as a political subject shifts the conversation away from public, structural, effective solutions in favour of tips and hacks that can never address the root of problems.
Y ears ago, I attended a National Rifle Association personal safety course called ‘Refuse To Be A Victim’. While the appeal to gun ownership was never far below the surface, this is primarily a crash-course in personal risk assessment, powered by the data point that a violent crime occurs every 25 seconds in the US. As I learned during my three-hour training, refusing to be a victim requires cultivating a posture of constant vigilance – one that carefully surveys situations, exercises caution, and never, ever trusts strangers. The course conjured a world in which security was an individual responsibility and, perversely, victimhood a personal failure. Uncertainty, which is both a gift and a challenge, was weaponised to make racialised paranoia seem like the only responsible choice. Protecting myself and my children required thinking like the retired counterterrorism officer who taught my course – becoming, in short, my own personal risk assessor.
However extreme this case may seem, I’ve realised that Refuse To Be A Victim exists on a broad continuum of practices that envision, and even idealise, the individual qua actuary. Whether you find (like Thaler and Sunstein) that humans are poor decision-makers who need to be nudged toward virtue, or affirm (like Gigerenzer) that better public numeracy can improve our risk calculations, an antisocial logic clings to the actuarial self. Security becomes an individual privilege procured through the marketplace rather than a public right achieved at the social level. When it comes to personal safety, people of means are encouraged to manage risk by engaging in various kinds of social insulation (what I have called security hoarding), while those without are largely transformed into the ‘risks’ themselves. The uptick in vigilante acts and paranoid shootings in the US for ringing the wrong doorbell is a symptom of this antisocial logic taken to its natural, bloody end. The privatisation of security and violence are also forms of responsibilisation – ones where we can clearly see the costs associated with this form of ‘freedom’.
The good news is that there are wonderful alternatives to the hyper-individualised world of the actuarial self: apprehensions of security that think at the level of communities, neighbourhoods and towns or cities or nations. In a world facing rising temperatures, pandemics and a globalised financial system, many have come to recognise the highly individualised approach to risk as a relic of an irresponsible and iniquitous era. Building more capacious forms of security and networks of care will require looking beyond the actuarial self and the fundamentally conservative political agenda it serves."
The cruelty of crypto in its promise to revive the American dream | Aeon Essays,,https://aeon.co/essays/the-cruelty-of-crypto-in-its-promise-to-revive-the-american-dream,"The 2021 Crypto.com ad aired during the Super Bowl and was called ‘Fortune Favors the Brave’. ‘History is filled with almosts,’ says the actor Matt Damon, gliding through a museum set like an eager docent, ‘with those who “almost” adventured, who “almost” achieved, but ultimately for them it proved to be too much.’ He halts reflectively at an effigy of Ferdinand Magellan dissolving into the waves. ‘Then,’ he says, moving swiftly on to the other, more inspiring exhibits (a climber, a Wright brother, two replicants kissing in a night club, a troupe of racially diverse astronauts stepping proudly onto a gangway in a scene from nowhere in history), ‘there are others, the ones who embrace the moment and commit . And, in those moments of truth, these men and women, these mere mortals – just like you and me – as they peer over the edge, they calm their minds and steel their nerves with four simple words that have been whispered by the intrepid since the time of the Romans: “Fortune favors the brave.”’
The first recorded use of the phrase is indeed from Roman times, but Damon’s speech did not age as well. The ad was removed from Crypto.com channels in 2022, less than a year after its release. And had you bought $1,000 worth of bitcoin on the day the ad aired in October 2021, by the time of writing (two years later), you would have just $400.
The following year’s Crypto.com Super Bowl offering featured the basketball star LeBron James. An older LeBron has travelled back to 2003, to his messy high-school bedroom and his 18-year-old self. ‘All right, so… cordless headphones?’ says the young LeBron, dressed in his Fighting Irish jersey. ‘You can watch movies through your phone? And y’all got electric cars?’ The teenage LeBron pronounces: ‘The future is crunk. ’ He’s pacing the room, palming his basketball, brimming with what the agency pitch might have called ‘youthful optimism’. ‘Anything else you want to know?’ his older self asks, looking on like a sage. Young LeBron replies: ‘Is the hype too much?’ He then flops down beside himself on the edge of the single bed. The camera pans to a newspaper cutting taped to the wall, a sports page with the bold headline ‘Is LeBron James Going Straight To The League?’ He’s suddenly serious: ‘Am I ready?’ His older self replies: ‘I can’t tell you everything but, if you want to make history, you got to call your own shots.’ There’s a pause. The camera pivots above as the two LeBrons begin to chant: ‘We going to the league!’ It’s close to ‘We’re all gonna make it!’ – the battle cry of optimistic crypto investors everywhere before the FTX crash.
One millionaire could imply that crypto was the new pathway to generational wealth. Another could shill the ultimate American dream – that wealth is a meritocracy granted to those brave enough to risk it all. Poverty is a blight for refusing to dream hard enough: was Damon calling you a coward for not buying crypto? Was LeBron selling bitcoin as a pathway to wealth for Black Americans? Was this really the new route to the good life?
B itcoin emerged in the aftermath of the global financial crash in 2008. But many of the technologies that underpin it appeared in the 1990s and early 2000s, on niche, Reddit-like mailing lists with an anarchist flavour and names like Cypherpunk and the Extropians. An excavation of these lists, which linger as messy file dumps on GitHub, read like the rise and fall of a tiny Roman Empire (though the majority of traffic originated in the San Francisco Bay area). Back then, the internet was still something you connect to, slowly, with your landline. Matt Damon and Ben Affleck have just won an Oscar for Good Will Hunting (1997). LeBron is playing basketball at school in Ohio. Sam Bankman-Fried of FTX and Vitalik Buterin, the founder of Ethereum, are aged six and four, and eagerly hot-housed by parents in Palo Alto and Russia respectively.
The threads trace the Bill Clinton administration, the dotcom boom and bust, and the dawn of e-commerce. Everything is shot through with a hubris called ‘the Californian ideology’, a name given to the toxic blend of individualism, libertarianism and technological determinism that would come to be associated with the politics of men like Peter Thiel, Elon Musk and the Winklevoss twins. The ideology’s adherents believed in personal autonomy and the wisdom of the free market and, on the more radical end of the spectrum, in total anarchy, life extension and space colonisation . What did they share? Maybe it’s a sense that, with enough money and processing power, you could leave the world and all its problems behind, bank the future, live forever, overcome death itself.
Crypto did not level the playing field. It exposed the vulnerable to fraud and scams
The bitcoin whitepaper was published in 2008, at a moment when trust in the state and the mainstream financial system was at an all-time low. It promised a different kind of money, as so many ordinary people lost their trust in the global financial system along with their homes. Suddenly it had an appeal beyond these niche communities, with people who had never heard of Thiel and didn’t care what his politics were.
In the early 2010s, bitcoin was predominantly bought and owned by affluent white men in tech and investment circles. But, by 2021, advocates were shilling the token as a pathway to generational wealth for Black Americans. Historically, Black Americans have struggled to build intergenerational wealth. Centuries of economic practices, from slavery to redlining , made it almost impossible to hold property, to own homes, or to build and transfer wealth from generation to generation. To this day, Black Americans still have the lowest rate of home ownership of any racial group in the United States. These rates have only declined since the financial crash, falling as low in 2019 as they were in the early 1960s, when race-based financial discrimination was still legal.
In the absence of a clear pathway to the good life, crypto was framed as a way out of financial distress. Phrases like ‘financial inclusion’ and ‘economic empowerment’ were parried by unofficial Black ambassadors, among them JayZ and Mike Tyson. The phrase ‘bitcoin investor’ might still conjure someone from a Brett Easton Ellis novel: white, monied, functionally sociopathic, but the accounts told a different story; by 2021, Black Americans were more likely to hold cryptocurrency than white investors were. Young people, Black folk, the poor and the indebted were all scrambling into the latest scheme, in the hope of winning a secure future. Buying in at the height of the market, most were left holding the bag. Crypto did not level the playing field. It exposed the vulnerable to fraud and scams. It offset risk on to the poorest in society, all while paying lip service to a dream. No shots. No leagues. And ‘we’ didn’t all get to make it.
I n 2010, the cultural theorist Lauren Berlant wrote of ‘cruel optimism’, a desire that keeps us attached to something that ultimately harms us. Cruel optimism takes different forms, from desires for romantic love to upward mobility. At the centre of it all are dreams of something we call ‘the good life’, the fantasies we recruit to make sense of the world, the stories we tell about how we and the world around us should ‘add up to something’. We seek comfort in tales of hard work rewarded, of bullies beaten, of winning the system. Why, Berlant asks, do people stay attached to these fantasies despite all evidence to the contrary? And ‘what happens’, as in the financial crash and, later, the pandemic, ‘when those fantasies start to fray?’ When, to put it in terms that Berlant never would, shit hits the fan?
Berlant was a scholar of affect, of the many ways in which the present is sensed and felt. Today, these feelings are reflected in shows like Beef (2023) on Netflix, on subreddits like WallStreetBets, even, arguably, in the financial apathy of stay-at-home girlfriends on TikTok practising self-care, money witches manifesting financial rewards, and ‘finfluencers’ urging their followers to buy worthless Tupperware stock, all trying to figure out what future security might look like. This affect is felt in tradwives shilling financial dependence on a strong male provider, and in advertisements for crypto that aired during the Super Bowl, distilled artefacts of what the good life has become.
Published in the fallout from the financial crisis, Berlant’s book Cruel Optimism (2011) had a mainstream hold that’s unusual for an academic text because it spoke to an unnamed but felt sensibility. The condition of being disillusioned and precarious spoke to the working class, but also to a disenchanted middle class – to a university-educated precariat graduating into the recession. Post-2008, Berlant wrote, ‘precarity provides the dominant structure and experience of the present moment, cutting across class and localities’. ‘[D]escriptions of the affected populations veer wildly from … the historical working class; to the global managerial class; neo-bohemians who go to university, live off part-time or temporary jobs, and sometimes the dole while making art; and, well, everyone whose bodies and lives are saturated by capitalist forces and rhythms.’ Maybe, Berlant suggests, it’s more accurate to call this phenomenon a new ‘global class’ or ‘a way of life’ or ‘an affective atmosphere’ or ‘an existential truth’, namely, that ‘there are no guarantees that the life one intends can or will be built’. What happens when the dream of the good life begins to fray, when subjects come unstuck from their dogged optimism? And how has this frayed optimism, this shorn fantasy of the good life, also played out in the perma-crisis of the pandemic and the rise of crypto?
For many, crypto and meme stocks symbolised a spot in the lifeboats
Lee Sun Jin’s Beef aired on Netflix in April 2023 and starred Ali Wong and Steven Yeun. It opens with a road rage scene in suburban Los Angeles, with the petite businesswoman Amy Lau (Wong) in her Mercedes SUV tailed by a contractor, Danny Cho (Yeun), in his beaten-up pickup. The show is about these two main characters joined together in shared impotence – shared rage – but, for me, it’s Danny’s brother Paul who captures the beef they all have with society. Danny and Paul live together in a cramped apartment. Paul spends most of his time online, indoors – not playing videos games or watching porn but investing. Paul and Danny’s parents followed a traditional immigrant path, working hard to run their motel, until a cousin’s drug deals runs it into the ground and forces their return to Korea.
Danny, the eldest, is still trying to make good on the American dream through old-fashioned graft, growing a contracting business and working to build a solid reputation on Yelp. Paul has no interest in the grind. He shares his philosophy as the brothers have a barbecue by their apartment complex pool. A lithe blonde in a bikini appears on an upper balcony with a beach towel slung over one shoulder, scowls when she sees the smoking grill and huffs back to her apartment. Paul’s plan is ‘to become a crypto millionaire and travel the world with [his] bros’. All he needs is three 10x trades. He makes it sound so simple, the figures almost tripping over themselves as they leave his mouth: ‘1K to 10K, 10K to a 100, 100 to a million. Boom.’ He tears off a mouthful of steak with his teeth. ‘That’s not a plan. You’re just saying higher numbers,’ Danny splutters. Paul parries his brother’s impotent rage with a kind of relentless optimism. It’s not the end of the good life, but a different pathway to the mansion in southern California, to early retirement, world travel, to building your parents a retirement home, to making them proud.
Retail trading suddenly became popular during the pandemic. Mobile trading apps with zero fees, such as Robinhood, allowed users with little financial experience to dabble in the market. The media framed it as gambling in the absence of sports or horse racing, but that didn’t really capture the desperation, the individualisation of risk. Crypto and meme stocks symbolised, for many, a spot in the lifeboats. It was ‘Lambos or food stamps!’ Millennials and Gen Zs, in debt and with no chance of financing their futures through so-called ‘legitimate’ channels, were investing in high-risk, high-return stocks for the chance to win a down payment.
I n contrast to the Extropians, for whom digital gold seems to have been part of a desire to live forever, the hashtag for this philosophy was YOLO, as in ‘You only live once.’ The phrase was popularised by the Canadian rapper Drake in 2011, and by another rapper who posted, minutes before his death in a car crash: ‘Drunk af going 120 drifting corners #Fuckit YOLO’. On Reddit forums like WallStreetBets, YOLO had a different currency. It meant going all-in on one risky, but potentially life-changing, bet. YOLO is the opposite of the safe or steady investment. It’s not concerned with diversification or risk management because these safe bets cost all the money you don’t have. If there is no safe or legitimate pathway to security, then why not try to win some in an all-in gamble. It was all or nothing. YOLO was the only sane response to perpetual crisis, to the end of the future. The good life wasn’t coming any other way. And still we held tight with diamond hands to the broken shards of this dream. Money was the only path to the future. It still meant, as the journalist Annie Minoff put it in the podcast The Journal, ‘the ability to remake the world, so that, for once, it was actually built for you.’
The investors on Reddit channels like WallStreetBets share stocks, risky trades and loss porn, but they also talk a lot about houses. There are countless members ‘waiting around for the housing market to go up in the smoke so they can finally buy some real estate’. Or waiting for those dreamy 10x trades that would lead like a dream to a down payment. Boom. One member called coffeebeerwhiskey posts about an amazing trade that allowed him to finally put a down payment on a house. ‘Must have been one cheap ass house,’ someone writes. ‘Where is this house anyway, Mars?’ Someone else called waitthere-shut-up posts that he’s down $7,000 on GameStop stock (GME) but he’s holding. He’s going to the Moon. He wants a down payment so that he doesn’t have to beg his wife’s father for a handout: ‘I don’t want to be licking his nuts for the next 30 year[s]. GME will be freedom from being under my father-in-law’s thumb. That is worth more to me than anything!’ ‘This is for my children!’ someone posts below a screengrab of a crazy bet, only half ironically.
When I search ‘down payment’ and sit back to read the results, half of the posts are written by people trying to win enough to buy a house, and half are from people who have lost all of their savings in an all-in gamble. Somebody called yolobistro posts to the WallStreetBets/lossporn thread. After growing their stocks to nearly $130,000, they’re now down to just $6,000. ‘My life is over,’ they write. The helpful suggestions pour in, from working in Wendy’s to suicide. ‘It’s just money bro,’ someone chips in, a little incongruously for the thread. It’s not ‘just’ money, puts in Edwardpaperhands. ‘Money buys food, shelter, and heating. It is a crystallized, physical version of thousands of hours spent working my ass off and getting shat on by customers and shitty managers. We literally need money to survive and it is not something to be trivialized either by a comment likes [sic] yours or by gambling it in a casino masquerading as an “investment”.’ ‘It’s not real,’ writes someone calling themselves Oswald_Hydrabot. ‘None of this shit is, come back to the trash fire wjth [sic] the rest of us while we party through extinction.’
The market is a giant lottery in search of the prize of security, gambling for a spot in the lifeboats
Millennials and Gen Zs were raised to be entrepreneurs of the self, to believe that, if they simply worked and studied hard enough, success and security were waiting in their futures. Failure was a personal blight for refusing to invest their time wisely, for failing to grind hard enough. Post-2008, that dream was shot. You could work and work, but that did not mean that you would have job security and freedom from roommates by your mid-30s. Maybe this was what was meant by burnout culture. In the aftermath of the crash, middle-class people spoke of the death of the dream – the postwar ethos that, if you were willing to work hard enough and play by the rules, upper mobility and success were waiting in your future. If their parents had believed in climbing the ladder and just rewards for their hard work, this path was now closed to their children.
These generations are also a product of the speculative environment they were raised in. Most of the day-traders were teenagers or children in the financial crash, or just graduating college. Fledgling adults in the COVID-19 pandemic. Born between the mid-1980s and early 2000s, their identity is shaped by the vacuum of post-communist politics (I, personally, was sent, age five, to a fancy-dress party styled as the Berlin Wall) or shaped by the speculation and excess of the dotcom era, or racked by the uncertainty of the 2008 financial crash. They’ve encountered the death of the American dream (or in Ireland, where I’m from, the optimism of the Celtic Tiger) and felt the withdrawal of the state’s contract in everything from mounting student debt to inferior healthcare to the rising cost of living. The postwar security and investment in public goods like education and housing their grandparents and parents enjoyed has been replaced by volatility and risk. Retail trading forums like WallStreetBets and NFT Discords are spaces where people trade crazy investment advice, but it’s also where they articulate their loss of hope in those same dreams.
What replaced the fantasy of the good life? Dreams of prepping for life on Mars or in the metaverse? Of financial security through wild trades, or finding a good man to take care of you so you could leave the hustle behind? And who are these new dreams in service of? If the tale of hard work and upward mobility kept us yoked to our employers and our 9-to-5 jobs, the fantasy of the YOLO investment ‘Lambos or food stamps!’ keeps its subjects attached to the market. To risking it all. And these dreams feed the market, as in the crypto winter of 2021 where many vulnerable investors were left holding the bag, or the post-GameStop frenzy where, despite feelgood stories about David and Goliath, the significant profiteer was the market-maker behind the Robinhood trading app.
Financial markets are no longer a space where investors allocate capital to businesses to grow a profit. It’s all about gambling on vibes in the gulf left by financial and social and political systems in total freefall. Nihilistic vibes, desperate vibes, hopeless vibes. The market is a giant lottery in search of the prize of security, gambling for a spot in the lifeboats. Of course, financial markets have been divorced from the so-called ‘real’ economy since the 1970s. But, maybe, in the era of post-truth and political apathy, what is new is an acceleration of these sensations, a total sense that nothing matters anymore. Hard work doesn’t matter. Good sense doesn’t matter, and neither do good bets or doing all the right things.
There are no more dreams of the good life. And no more futures. Fuckit. YOLO."
"Going cashless is a bad idea, but it’s not a conspiracy | Aeon Essays",,https://aeon.co/essays/going-cashless-is-a-bad-idea-but-its-not-a-conspiracy,"Four centuries ago, a woman named Else Knutsdatter was executed in Vardø, a small coastal town in Norway. She was accused of having used witchcraft to raise an ocean storm that claimed the lives of 40 men. She wasn’t the only one to fall victim to 17th-century folk who – in the absence of other explanations – could be convinced that disasters were conjured by malevolent sorcerers. Ninety others were executed for conspiring to produce the same storm.
Today, we know that physics and atmospheric pressures produced those storms. So, in the realm of weather, we’ve moved to systemic thinking , where bad things don’t need to be explained with reference to bad actors. When it comes to descriptions of politics and economics, the progress is not so unequivocal. Do bad things like climate change, conflict and corporate greed happen because powerful politicians and CEOs construct it like that, or do they emerge in the vacuum of human agency, in the fact that nobody’s actually in control? This is a question that confronts me in the campaign to protect the physical cash system against the digital takeover by Big Finance and Big Tech.
Photo supplied by the author
For more than eight years, I’ve advocated for the protection and promotion of physical notes and coins. I wrote a book called Cloudmoney: Why the War on Cash Endangers Our Freedom (2023). In that book, I point out that the public has swallowed a false just-so story that says we are pining for a cashless society. All over the world, public and private sector leaders claim that ‘our’ desire for speed, convenience, scale and interconnection drives an inevitable digital transition. This is supposed to bring a ‘frictionless’ world of digital payment-fuelled commerce, done at the click of a button or scan of the iris. The message is: keep up or else face being left behind .
The fact that so many leaders recite this script triggers some folks into thinking ulterior motives are guiding them, and it is true that the finance and tech sectors, for example, gain massively from the digitisation hype. Over the past few decades, they’ve launched various top-down attacks against the cash system, something I chronicle in my book. Physical cash is issued by governments (via central banks), whereas the units in your bank account are basically ‘digital casino chips’ issued by the likes of Barclays, HSBC and Santander. ‘Cashless society’ is a privatisation , in which power over payments is transferred to the banking sector. Every tap of a contactless card or Apple Pay triggers banks into moving these digital casino chips around for you. It gives them enormous power, revenue and data. They can share that data with governments but, more often than not, they’re using it for their own purposes (such as passing it through AI models to decide whether you get access to things or not).
By rejecting the story that cashless society is driven primarily from the bottom up, I sometimes get accused of being a conspiracy theorist. It’s not hard to imagine the outlines of a ‘conspiracy’ when you look at who benefits most from payments privatisation. Not only are Visa, Mastercard and the banking sector big beneficiaries, the fixation on digitisation also extends the power of Amazon and other corporate behemoths that are moving beyond the internet into the physical world via smart devices and automated stores that plug into digital finance systems. It’s a small jump to imagine how governments can piggyback on this digital enclosure to spy on us, or manipulate us.
A ngst about this creeping enclosure finds widespread expression on social media. In London, and other places where the use of cash has plummeted, it’s turning up in the form of warning posters and pamphlets handed out by conscientious objectors against ‘cashless’ establishments. They warn against a looming digital takeover, but what they don’t realise is that the powerful corporations leading this takeover are themselves led by a larger puppetmaster, and this ‘puppetmaster of puppetmasters’ is no conspiring group of elites. It’s a system , and the dominant stories about digital progress are its ideology .
Systemic thinking requires stretching out the mind to picture powerful but invisible forces. So, let’s ease in through a simple thought experiment: imagine a million blindfolded people tied together, trying to find a direction to walk. They collectively form a system , but its interdependence is so complex that it’s almost impossible for people to coordinate. This means they default to some lowest common denominator, vaguely stumbling in a direction without knowing why. This resembles how our global economic system works. We’re all tied into complex webs of interdependency, and the system generates pressures that require it to expand and accelerate. Its logic demonstrates almost evolutionary properties, such that anyone who goes against its default tendencies hits a wall, while anyone who stumbles in the direction of its prevailing current doesn’t. This may sound abstract, but we can see it clearly at work in the world with physical cash.
For centuries, the capitalist system has been underpinned by nation-states that have fostered the growth of large firms. For a long time, cash helped that system to expand and accelerate. In the 1950s, corporates were more than happy to have adverts featuring people using cash to buy their products, but in the contemporary moment firms are turning against it. Cash is hard to automate. It cannot be plugged into globe-spanning digital infrastructures. It operates at human scale and speed within a system that increasingly demands inhuman scale and speed. It’s creating ‘friction’ at a systemic level , so even if you like cash at a local level, you’ll gradually find yourself coerced away from it.
Amazon lacks infrastructure to process cash, and street-level shops are drawn into this systemic recalibration
‘Coercion’ in this situation doesn’t mean a consortium of CEOs or politicians will force you to stop using cash. If you are tied into a system that contains processes beyond your control, then the system itself can just pull you along. Capitalism often operates on autopilot, with the players following a set formula to boost profits, and one part of that formula is to automate stuff. In 1759, Adam Smith introduced the metaphor of the ‘invisible hand’ to illustrate how all these movements, and these chains of interdependency, can be mapped. For example, Lloyds Bank, guided by shareholder demands for profits, shuts down physical branches to cut costs by pushing you on to automated apps. Having no branches makes it harder for small businesses to deposit cash, so they are nudged toward putting up signs saying ‘We’re cashless.’ That then sends a message to customers that there’s something newly unacceptable about cash. At the same time, people will notice that banks have shut down many ATMs, with the banks justifying this by saying their customers are ‘going digital’, but this creates a self-fulfilling prophesy because removing ATMs lowers public access to cash, making it harder to use. Lloyds and other banks then see the resulting up-tick in digital finance as implicit permission to close down further branches.
What we have here are a series of feedback loops, all serving the prevailing systemic logic of expansion and acceleration. Cashless society, then, is not just a privatisation process, but also an automation process. Automated giants like Amazon in fact lack any infrastructure to process physical cash, and street-level shops are being drawn into this systemic recalibration. Hipster cafés in London have signs saying ‘We’ve gone cashless’; what they are actually saying is ‘We’ve joined an automation alliance with Big Finance, Big Tech, Visa and Mastercard. To interact with us you must interact with them.’
The politics of the ‘invisible hand’ can be visualised with a pyramid:
Where does power lie in this pyramid? Anyone who wishes to divert attention away from the top will likely claim that it resides in numbers, at the bottom. Appealing to legitimacy-from-below is a major tactic used by politicians, who present their governments as reflecting the will of the people, with industry following suit. Rather than admitting to their own interests, banks and fintech companies present the decline of cash as a bottom-up phenomenon driven by popular support. In this view, HSBC’s decision to close ATMs must simply reflect the fact that ordinary people no longer care for cash. In this view, industry simply responds to our demands.
Big firms turn to freemarket doctrine in these situations, which maintains that businesses survive only if they mould themselves to our needs. So the presence of thriving corporations can indicate only that they’re serving us well. Left-wing thinkers reject this freemarket dogma, pointing out that some industries are powerful enough to effectively legislate the conditions of our lives. We all know that firms invest heavily in warping our perceptions via marketing, and often secure our consent only through tricks and misrepresentation. Left-wing calls for government regulation in turn compel freemarketeers to accuse them of stifling both popular will and business. Market conservatives paint a picture of consumers, workers and small entrepreneurs battling the clumsy state, while Lefties present workers, citizens and mom-and-pop shops fighting the corporate behemoths. Economic politics is all about painting these contrasting David-and-Goliath options.
When it comes to money, though, the battle lines get more confusing, because the monetary system is a public-private hybrid . Physical cash is government money, but it has properties – like anonymity – that appeal to some anti-government libertarians. Privacy-invading card-payment systems, by contrast, have historically been run by the private sector, so those pro-business libertarians who are concerned by surveillance are forced to accuse banks of being phoney ‘crony capitalists’ collaborating with controlling governments.
This collaboration can be seen in the case of the 2022 anti-vax ‘Freedom Convoy’ truckers, whose bank accounts were frozen by a Canadian government order. Libertarians rallied in support of the truckers, but there’s many variations of these alliances between states and payments firms. For example, the US government agency USAID has funded programmes like Catalyst: Inclusive Cashless Payment Partnership, pushing Visa as a tool of empowerment in India. In its 2017 annual report , Visa talks about doubling its market penetration into India after it ‘worked closely’ with Narendra Modi’s government in its ‘demonetisation’ efforts in 2016, during which time certain banknotes were outlawed. The Indian prime minister’s open attacks on the public cash system also drew fawning praise from Indian digital-payments firms.
I t’s easy to get stuck in a binary of explaining cashless society as either a bottom-up phenomenon demanded by us, or a top-down enclosure pushed by power players. The reality is a more complex mix. Because at scale it’s cheaper to push billions of people through a handful of centralised players, almost every industry in the world is dominated by oligopolies of large firms. Those firms will inevitably build political connections, while smaller firms get relegated to the periphery. Oligopolistic firms fluctuate between collaboration and competition, but the evolutionary logic of our economic system is always towards greater automation. Corporate executives benefit if they nudge everyone in this direction, and they have a niggling insecurity that, if they don’t, competitors will leave them behind. The problem is that many people don’t love digital acceleration, and it takes a considerable effort over time to erode their resistance. This is why big retailers like Tesco start by tentatively testing cashless stores in certain locations to set a precedent. It took years for the airline industry to make it feel ‘normal’ to refuse cash, but that norm is still not universal. Even last year, I found myself seated next to a man on a flight who was humiliated and flustered when the attendants refused his banknote.
The man wasn’t a frequent flyer and came from a working-class background, pointing toward an important fact: when a capitalist system is resetting to a state of higher speed and automation, it often does so first through social elites. In London, a hipster barber targeting yuppies may very well refuse cash, but a hair salon targeting working-class immigrants will almost certainly ‘still’ take it. Words like ‘still’ are loaded, because they imply that whoever is still doing the thing has yet to go through some evolutionary upgrade.
Digital payments giants like Visa invest heavily in presenting ‘going cashless’ as a grassroots triumph for the small entrepreneur who wants to cut costs. In reality, this alliance between Big Finance/Big Tech and small and medium-sized enterprises applies only to businesses with middle-class customers. A decade ago, many of those customers didn’t even perceive cash as particularly inconvenient. Even now, they would prefer choice (the fact that I sometimes use my card doesn’t mean I asked a shop to remove its cash till). It’s businesses that remove our payments choice, but they rely on the fact that most middle-class people simply adapt their expectations and edit their memories to forget those old days when cash felt totally normal. Once new cultural norms are established, it compels compliance. Eventually, you get discriminated against if you insist on being that guy who complains that the London bar won’t accept your coins.
The fact that people fall into line and begin displaying a preference for card payments is read by politicians as a signal to support the transition. They too are worried about being ‘left behind’. This pressure to go along with the transnational automation drive means that the average UK Labour Party politician doesn’t challenge cashless society. Rather, they call for a slight slowdown in the imagined ‘race’ towards it, to give cash-dependent communities a chance to ‘catch up’.
Cashless pubs allow hundreds of unmasked people in while refusing cash to protect their employees
So, capitalism has inherent trends, but it also has inherent contradictions. Here’s one of them. Our cashless card payments rely upon ‘digital casino chips’ issued to us by banks, but – as anyone who has been to a casino knows – such chips have power only because you believe they can be redeemed for cash. In the total absence of cash, there could be a collapse in the public’s belief in bank-issued digital money. Banks and corporates make private decisions that erode our cash infrastructure, but in doing so they are undermining the public basis of confidence in their private systems.
This was accelerated by the outbreak of COVID-19, which gave companies a convenient cover to fast-track their automation plans. It’s easier for a retailer to announce they don’t accept cash because of COVID-19 than to admit that they’re trying to shave a percent off their costs. For example, Visa entered a deal with the US National Football League to promote cashless Super Bowls. Signed in 2019 and piloted in 2020, it went public in 2021 during the pandemic, with attendant media coverage presenting it as a measure of public hygiene. Cashless pubs in London allow hundreds of unmasked people in their establishments while claiming to refuse cash to protect their employees from any coronavirus that may be stuck to the notes (a contention that is scientifically inaccurate ).
In 2020, such scaremongering, along with the fact that so many of us were forced into online shopping during the pandemic, caused a precipitous drop in transactional cash use. This raised the possibility of a financial stability problem, because cash psychologically (and legally) backs our cashless digital casino chips. This puts central banks in a bind. They know that the trajectory leads to a crisis-prone bank-dominated version of cashless society. So they think about how to maintain public access to government money without upsetting the transnational automation agenda. One way they are trying to resolve this is with a new form of ‘digital cash’ – central bank digital currency (CBDC).
To understand CBDC, imagine being able to download a payments app on the iPhone App Store from your nation’s central bank (like the US Federal Reserve or the Bank of England). Various countries have appointed teams to experiment with this hypothetical government payment system, but it creates a new problem. In a country like the UK, a state-issued digital pound would upset banking giants like Barclays, Lloyds and HSBC. They would rightly perceive it as competition to their own digital money empires. Given that central banks are supposed to maintain the stability of private banks, rather than directly compete with them, the Bank of England (and all other central banks) will have to make concessions: any future CBDC will be watered down to prevent disruption to the banking sector, and its operation will be outsourced to private partners… like the banks themselves.
I n 2015, I was one of the few people raising awareness of the dangers of cashless society from a Left-wing perspective. Then the pandemic hit, and a new generation of pro-cash activism emerged in the so-called populist Right. Libertarians seized upon early COVID-19 controls as evidence of a new era in totalitarianism. Social conservatives had already cast Big Tech firms as hives of ‘wokeness’. Conservative commentators began to weave these perspectives together. They presented themselves as rebellious champions protecting the everyman from an alliance of liberal corporate elites and authoritarian socialist governments.
In May 2020, my mother was sent a video by her friend on Facebook. It claimed that Bill Gates had orchestrated COVID-19 to microchip us via vaccines and to usher in a cashless society where our every economic move could be monitored. Her friend was very excited to announce that ‘Your son is in this! You must be so proud.’ Sure enough, there was a clip of me (used without my permission), in which I was describing how financial institutions engage in a war on cash. It was followed by a clip of an evangelical pastor warning that ‘the Bible clearly links the mark of the beast with the emergence of a cashless society’.
How is it that I end up in a video like this? Conspiracy theorists happily take my work out of context in order to push their version of events. Rather than analysing the logic of capitalism, many of them have decided that behind digital innovation-speak lie satanic overlords, paedophiles, Marxists, Jews or caricatured banksters smoking cigars.
Ironically, it’s central banks’ response to the corporate attack on cash that has really spurred the new wave of pro-cash activism. The possibility of a state-controlled digital pound or digital euro replacing the battered cash system has galvanised the imagination of libertarian activists. Libertarians have always faced a tension when complaining about the surveillance that accompanies cashless society. This is because digital payment systems are pushed by private sector fintech entrepreneurs, and libertarians are supposed to be pro-entrepreneurialism. CBDC has enabled them to escape this bind. It allows them to rework the story of cashless society as being driven by an oppressive digital state.
These systems limit choice, and can be used to push people’s business to big retailers, rather than small ones
This mutated version of the cashless society story is now spreading virally. My dad recently forwarded me a video, which he received on WhatsApp, about the looming spectre of CBDC. The anonymous producers stitched together clips from libertarian activists, self-help gurus and even the populist UK politician Nigel Farage, all of whom cast CBDC as a new form of digital totalitarianism. They argued that this centralised digital money will be sold to us under the banner of convenience, but that the true agenda is to enable governments to micromanage us by controlling our payments. The conclusion? Say no to CBDC. Say yes to physical cash.
They’re not wrong to point out the dangers of digital control, but their selective curation of the form and examples misrepresents why it is happening and how to oppose it. The cashless system is run by transnational corporations, and the actually existing examples of payments control often concern welfare recipients: for instance, the Australian ‘cashless welfare card’ was a Visa card system that blocked Indigenous Australians on benefits from buying non-approved goods in non-approved stores. These systems not only limit choice, but can be used to push people’s business to big retailers, rather than small ones.
Farage and his contemporaries don’t focus on the payments censorship of Indigenous welfare recipients. They fixate on conservative fears, like the hypothetical blocking of transactions for guns and meat. This is causing me problems, because moderate progressives – who previously would have expressed some concern about corporate power – have started associating a pro-cash stance with reactionaries, and to a broader suite of ideas that they espouse. In Germany, I’ve even been accused of being aligned with the neo-Nazi Reichsbürger movement, purely on the basis that they too are pro-cash. I’ve seen digital payments promoters use this disorientation to their advantage. They can suggest that critiques of their industry are the realm of crackpot antisemites. If conspiracy theorists are the ones leading the charge against digitisation, surely it must show the concern is built from the wild fantasies of paranoid flat-Earthers. Rather than fight cashless society, then, they suggest we should promote corporate financial inclusion : give a helping hand to all those people who have yet to be absorbed into Big Finance. Get them accounts. Help them become corporate consumers.
Moderate progressives are often taken in by this story and in backing away from the cashless society battle they cede territory to the far Right. It’s an example of a trend in our post-pandemic moment, where the meeting of two sides of the political horseshoe has led to the spread of Right-wing ideas among people who previously considered themselves Leftists. The new Right has appropriated the rebellious language of Left-wing hacker culture, which pushed digital privacy for decades (for a pop-culture version of this, watch the TV series Mr. Robot , in which anti-capitalist hackers target the corporate giant ‘Evil Corp’). Top-down power has been re-ascribed to a generic blob of ‘globalists’, acting via institutions like the World Economic Forum (WEF), but anti-WEF campaigning was a standard part of Left-wing culture in the 1990s and ’00s. To Left-wingers, the WEF represented venal corporate capitalists, which is why the ‘alter-globalisation’ movement championed the World Social Forum as an alternative. In the midst of lockdowns, however, it was anti-mask and anti-vax campaigners who took on the aesthetics of Occupy Wall Street, holding street protests with placards warning about cashless society and digital ID.
A surreal twilight zone has formed between the language of the old Left and that of the populist Right, and into it has stepped a character like Russell Brand. In 2013, he came out as an anti-corporate socialist and, back then, every Lefty activist I knew was clamouring to find his email address in the hope that he’d platform their cause. Fast-forward several years, and he renamed his podcast to Stay Free, peppering it with libertarian language and topics that appeal to the Right. He presents himself as being on an open-minded search for the truth that the mainstream media won’t tell us, and it increasingly involves him having discussions with conservative edge-lords. In November 2022, he released an obligatory video about CBDCs, entitled ‘Oh Sh*t, It’s REALLY Happening’.
Notably, no cashless establishments use CBDC, because it doesn’t exist yet. They all use the private sector digital payments system but, in choosing to focus on the fantasy version of cashless society, rather than the actual one, Brand signals that his allegiance lies with the Right wing.
I n the martial arts classic Kill Bill: Vol 2 (2004), the five-point palm exploding-heart technique is a precise sequence of five hits that cause an opponent’s heart to stop. In the conspiracy world, the five-point punch of the globalists involves them hitting us with digital IDs, 5G technology, vaccines, COVID-19 passports and now CBDCs. This is supposed to trigger a global cardiac arrest called the ‘Great Reset’. The Great Reset is actually the name of a real programme convened by the WEF, in which they talk about the need for a post-pandemic digital and green transition. Those goals emerge from different sources because, while capitalism generates a digitisation agenda to speed things up, it doesn’t generate a conservation impulse to slow things down. Green transition rhetoric doesn’t emerge from market processes: it’s the result of decades of relentless campaigning from civil society groups, who pushed past the lobbying of the fossil fuel industry to showcase the economic risks of climate change. Big business and politicians now pay lip service to that.
Nevertheless, they attempt to subordinate it to their automation fixation by proposing digital techno-fixes for climate change. This is a gift to our conspiracy theorists. They can now present CBDCs as being a future tool to force us to buy only low-carbon vegan sausages, under the control of Greta Thunberg and the Bank for International Settlements (a BIS v ideo about CBDC is a favourite among them).
An anti-cashless society propaganda leaflet. Supplied by the author
Cashless society authentically sucks. It’s a world where your kid cannot sell lemonade on the side of the road without paying Mastercard executives in New York. It’s an attack on privacy, autonomy, local independence and casual informal interactions in favour of surveillance, dependence and centralisation of power in large institutions. I frequently interact with people who have very real concerns about it, but who – like our 17th-century folk who lost loved ones to a storm – have been steered into reactionary ideas about it. Our struggle to see large-scale systemic processes gives oxygen to conspiracy theorists. I frequently get asked to go on Right-wing media channels, such as GB News, to be interviewed by anti-woke libertarians or Christian evangelists. Many of them imagine capitalism to be the realm of the small individual, and present elites as being malevolent actors who attack the system from above. It’s an easy story to tell. But the reality is that elites are a by-product of our system. The invisible hand likes tapping the contactless card, regardless of whether you as an individual do, and the role of the elites in the war on cash is to simply unblock resistance to that. More often than not, they’re examples of Hannah Arendt’s banality of evil . They’re just people ‘doing their job’, serving a system that wants to commodify any aspect of our lives that remains un-commodified and un-automated.
The dominant tendencies in capitalism pull upon all of us but it’s possible to demand space for other values. It’s been done before. There was a time when the automobile industry seemed ascendant, and bikes were pushed off the roads, but we built a cultural movement to demand bicycle lanes. That’s why we should see cash as being like the public bicycle of payments, and support efforts across the political spectrum to protect and promote it. Digital bank systems are the private Uber of payments: they may appear convenient, but total Uberisation unleashes demons that cash historically kept in check – surveillance, censorship, digital exclusion, and serious resilience and financial stability problems. The point isn’t to argue that everyone must always use the ‘bicycle’. It’s to ensure that we don’t get totally ‘Uberised’ in private and public life. We need to promote a healthy balance of power between different forms of money in the system, and that’s within our collective political abilities."
Finance fraud is not a deviation from the norm but a reflection of it | Aeon Essays,,https://aeon.co/essays/finance-fraud-is-not-a-deviation-from-the-norm-but-a-reflection-of-it,"When the German banking giant Wirecard collapsed in June 2020 amid a roaring fraud scandal, public opinion was shocked. The company, praised as the country’s innovative answer to the fintech industry of Silicon Valley, had been widely seen as a ‘German miracle’ following recovery from the 2008 financial crisis. Its bankruptcy involved a massive state prosecution that sent shockwaves through world markets. To the astonishment of German and international observers, Wirecard executives were found to be involved in all manner of deception: direct falsification of accounts, fake cash-flows, re-routing of payments through non-existing shell companies, ghost subsidiaries. While forging profits, they had obscured a mammoth debt of €3.5 billion.
This, of course, is not an unfamiliar tale. The explosive growth of finance as a percentage of the ‘real’ economy in recent decades has been matched by an equally dazzling scale of financial fraud, from the Enron scandal to Bernie Madoff’s pyramid scheme (the largest recorded fraud in world history) in the 2000s, to more recent scams in cryptocurrency markets such as FTX. Denizens of finance – both system insiders (Madoff was a former chairman of the Nasdaq exchange) and ‘maverick’ outsiders (Sam Bankman-Fried of FTX had been seen as a challenger of mainstream banking elites) – have displayed a unique capacity for alchemy: whipping up distorted realities in which false truth and true fact become indistinguishable. Their plotting is often aided by regulatory bodies, rating agencies and consultancies that firm up such distorted realities through either action or inaction.
A recent Netflix documentary follows the Financial Times reporter Dan McCrum in his quest to reveal Wirecard’s own big con. The ‘aha’ moment comes when McCrum and his FT colleagues show up at the Singapore address of one of the company’s supposed subsidiaries only to find an unassuming farmhouse. Behind Wirecard’s opaque structure lay simply nothing : no accounts, no offices, no cash. Much of the bank’s alleged business had been conjured out of thin air. Evocatively subtitled ‘a fight for the truth’, McCrum’s bestselling book Money Men (2022), on which the Netflix show is based, offers an animated account not only of Wirecard’s fraudsters, but also of their victims – those led to believe the company bosses and their outlandish myths of stratospheric growth, despite ominous clouds of deceit hovering over. What made a fake story so readily believable by so many? How is it possible that a DAX 30- listed bank (with the backing of Germany’s former chancellor, Angela Merkel, herself) transpired to be a giant Ponzi scheme?
T oday’s financial fraud is part of a bigger story unravelling outside of trading floors and corporate board rooms: a growing preoccupation over the nature of reality itself. On one level this preoccupation is fuelled by Big Tech, which has been pumping financial value through innovations ostensibly geared towards tackling ‘existential’ future threats via the production of simulated realities. Facebook’s launch of the Metaverse, an avatar world promising to revolutionise work and everyday life, has been critiqued as a fluke that aimed to distract from the company’s legal troubles, and the more recent release of programs such as ChatGPT and DALL-E by OpenAI intensified concerns about the use of artificial intelligence in Silicon Valley to address fabricated, rather than real, problems.
While some artists, teachers and writers grew uneasy about the increasing ‘realness’ of such AI outputs, others see new opportunities in implementing these chatbots into their work and daily tasks – even though OpenAI has admitted that its ‘large language model’ suffers from so-called hallucination problems: a propensity to cheat by weaving fictitious facts into its answers to user prompts.
Everything appears ‘almost true’ and nothing seems ‘entirely false’
This fuzzy line between authenticity and fakeness is also reflected in social media debates around, for instance Twitter’s use of ‘blue ticks’, originally introduced for identity verification and recently turned into a monetisation tool that made legitimate and feigned accounts harder to tell apart. In popular social media platforms, newcomers such as BeReal strive to capture more spontaneous ‘authentic images’, responding to a growing demand for less staged (yet, still, curated) content among younger users – a feature now incorporated by Instagram and Snapchat. The latest trend in these platforms is ‘dupes’: user forums around fake products mimicking luxury items, which explicitly challenge the distinction between bootleg and original goods (with both being often manufactured in the same supply chains to identical blueprints).
Meanwhile, concerns about ‘fake news’ breed new political conflicts. As a disparate alliance of conspiracy ‘truth-seekers’, New Age entrepreneurs and populist demagogues attack time-honoured certainties and scientific facts, nervous advocates of democratic capitalism strive to expose disinformation and repair our hollowed trust in liberal values. Out of these battles we are often told that a new ‘post-truth’ era emerges, in which material struggles give way to a relentless ‘epistemological crisis’ – as the former U S president Barack Obama put it in the wake of the 2020 election: the dismantling of the means by which we seek and recognise truth. As this state of doubt and confusion takes hold over everyday life, our capacity to tell fact from fiction weakens. Everything appears ‘almost true’ and nothing seems ‘entirely false’.
C ontemporary finance has become emblematic of this state of affairs. How true is the reality of mark-to-market accounting practices (the ‘marking’ of fictional future values as ‘present’) used recently by Wirecard and by Enron before that? How real is the wildly fluctuating value of non-fungible tokens (NFTs), the blockchain technologies used to certify authenticity of digital or physical assets traded in exchanges like FTX?
Last February, it transpired that more than 80 per cent of NFTs minted in OpenSea, the most popular marketplace for such tokens, were wash trades (simultaneous sells and buys of the same NFTs creating a false impression of market activity) or straight spam: fake and plagiarised works. Because blockchain asset markets are inherently opaque and built around a belief system that defies ‘real’ valuation – the consensus real set by central banks, ratings agencies and forex trade – they are especially fertile ground for fraud. They often become the stage where traditional forms of scam (such as phone impersonations) are combined with AI technologies (such as those underpinning ChatGPT) to deceive unsuspecting lay investors. In social media, fake celebrity endorsements abound, and ‘pump-and-dump’ schemes artificially inflate the price of crypto-assets before selling them to retail investors.
Despite this, financial wizards have been among the first to take off their gloves and defend their own versions of market reality and truth. Fraud schemes now routinely deploy the well-rehearsed populist rhetoric of ‘fake news’ to respond to allegations of corruption. In the months and weeks before its collapse, Wirecard’s defence line (adopted fully by the German Chancellery and the country’s financial authorities) was that the FT investigation was rigged by short-sellers spreading misinformation for profit. Turning the tables, the company’s bosses pointed the finger to finance itself, blaming their meddling with reality on the ruthless games of greedy speculators. Earlier this year, the Indian commodity trading giant Adani shrugged off similar allegations of market manipulation as fake news sown by market opportunists, who were distorting market reality with bad data – what is commonly referred to in trading as ‘noise’.
Soros suggested that a far wiser move would be to accept the distorted reality of finance
Unravelling the deceptiveness of these worlds appears less straightforward than calling out the lies of fabulists such as Donald Trump or the congressman George Santos. Fraud technologies themselves tend to be spectacularly ‘low fi’ – fake trading at Madoff involved practices as ‘sophisticated’ as manually cobbling together accounts on spreadsheets, keeping cash at office safes, or even hiding it in grocery bags. But finance’s big con hides in plain sight. As financialised culture proliferates and ordinary digital life becomes gamified, the impact of finance on our everyday reality becomes insidious. For those coming of age in today’s middle-class United States, speculation’s augmented reality is only a few scrolls or swipes away from the worlds of gaming, dating, wellness or even the realms of digital astrology and the occult. The deeper we immerse ourselves in the simulated worlds of finance, the more difficult it becomes to explain its alchemy.
One way to make sense of it is to ask how alchemy is imagined by financiers themselves. The leading liberal philanthropist George Soros first took centre-stage during the currency speculation wars of the 1990s, when he gambled against the Bank of England to make an alleged $1 billion profit. In so doing, he became a symbol of greed during a period of unhinged expansion of financial markets. Soros had given important hints of the mindset driving his sensational wagers in his book The Alchemy of Finance: Reading the Mind of the Market (1987).
One year before the publication of Paulo Coelho’s hit novel The Alchemist , the master speculator sought to draw the outlines of financial alchemy. Soros understood it as the capacity to control the fakeness of markets by becoming immersed in it. He challenged the proclaimed association of financial forecasting with ‘hard science’ and quashed mainstream economists’ assumptions regarding the ‘underlying truths’ of market prognostication. Insofar as no financial theory can ever be ‘verified’, he argued, all modelling of price movements can be based only on ritual and incantation – a belief later spurring his infamous ‘discretionary macro’ strategy, which came to be known as a sort of market sorcery. Instead of trying to decipher the unassailable truth of market prices and wrestle it apart from the ‘noise’ of human bias and irrational behaviour, Soros suggested that a far wiser move would be to accept the distorted reality of finance.
He has not been alone in this gambit. Bankman-Fried, the crypto swindler at the helm of the collapsed exchange FTX, allegedly played the popular League of Legends video game while negotiating capital investments. A passionate gamer, he treated the worlds of cryptocurrency markets and action role-playing games with the same knack for plotting. Jan Marsalek, the now-fugitive former boss of Wirecard, had a reputation for evading ‘finer details’ when negotiating trades, often shifting the conversation to diverting stories about Cold War secrets and spies. His zest for casual forgery was not all that different from the tales and rumours greasing the reality of contemporary venture capital. However, rather than simply warping economic ‘facts’, Bankman-Fried and Marsalek also strived to control the forces moulding those facts. Like Soros, they did not aspire to merely interpret ‘the mind of the market’. They sought to re- shape its material reality, too.
T he figure of the market alchemist long predates such contemporary villains of finance. The most adventurous confidence tricksters were always to be found in markets: in the fin-de-siècle US, stock touts and tipsters dominated news headlines, stirring fierce debates on the legitimacy and morality of speculation that defined the history of modern finance. Memorable works of fiction during that era at once mirrored and fuelled the public’s fascination with rogue financiers. From Anthony Trollope’s sensational account of Victorian England’s corrupt traders in The Way We Live Now (1875), to Frank Norris’s epic of greed and wheat speculation at the Chicago Board of Trade in The Pit (1903), and from Theodore Dreiser’s fictionalisation of the notorious tycoon Charles Yerkes in The Financier (1912), to Edwin Lefèvre’s celebrated Reminiscences of a Stock Operator (1923), financial fiction invoked an underworld of greed and deception in which ruthless con men reigned supreme. Real figures like Charles Ponzi – the most infamous of all market fraudsters – could have jumped right out of the pages of these novels.
Still, Ponzi and others like him were seen as deviant in ‘efficient markets’ whose hallmark principle of instrumental rationality epitomised the spirit of scientific modernity. The late 19th century was a time when mathematical forecasting took off in earnest in the major stock exchanges in the US. The trading floor became a testing ground for methods of scientific prognostication writ large, including in the fields of meteorology and climate-related prediction. The rise of statistical techniques such as time series analysis, the bell curve and Gaussian distribution revolutionised the study of market price movements, further unmooring it from the material reality of assets. At the dawn of the 20th century, a conviction settled in among traders that stock prices were in some fundamental sense right, their fluctuation conferring a godlike truth.
But while faith in the power of statistical prognostication was growing in the pits, it was not the only method of deciphering finance’s inner truths. The turn of the century saw a renaissance of mystical foreknowledge seeping right into the heart of markets, spawning influential financial practices including ‘technical analysis’ (gaining immense popularity under the aegis of Charles Dow, a co-founder of Dow Jones) and popular trading manuals expounding the virtues of ‘gnostic reason’. Contrary to accounts of markets as cardinal sites of a disenchanted, scientific modernity, fin-de-siècle finance was the stage of a lavish spectacle that swept economic and political life alike. Rather than augurs of a stifling rationality, stockbrokers became the shamans and magicians of a ‘pecuniary enchantment’ – in the words of the historian Eugene McCarraher . Their alchemy, however, did not merely aspire to departures from the material reality of economic doings. Guided by a growing conviction in both material-scientific and spiritual practices, they sought to transmute the base materials of finance (capital, labour) and create a gold-coated reality in the image of markets. In it, their hermetic quests were undergirded by an unwavering belief in market rationality.
Futures trading was legal and desirable because it enabled ‘the self-adjustment of society to the probable’
The great sweep of the traders’ gospel was strengthened through significant developments in US finance over the ensuing decades, most notably the establishment of a national market for financial securities through a widespread distribution of stocks and bonds championed by the government. The promise of more democratic markets encouraged larger swathes of society to reap the benefits of ‘market wisdom’ alongside the professional financiers. However, while the predecessors of today’s securities analysts adeptly reaped the rewards of financial alchemy, those at the bottom of finance’s pecking order were proving more vulnerable to its evils. Economic and political observers of the fin-de-siècle era became gripped by the spectre of ‘irrational crowds’: mobs of market dwellers purportedly marred by manias, panics and delusions, and thus prone to manipulation and deception.
This negative view of the world of lay finance was bookended by Charles Mackay’s 1841 account of ‘manias’ in Victorian-era markets and the historian Richard Hofstadter’s damning 1960s treatise of the Populist Movement as a ‘paranoid style’ of politics: collective action taken by exuberant publics who were led astray by misinformation, gossip and hearsay. If professional financiers were seen as competent helmsmen in turbulent speculative markets, amateur bettors were cast as deluded crowds threatening market stability. Their ‘noise’ was seen as a distortion of market reality, an inflection of the fundamental truths summoned by the signals of stock prices. The early period of market ‘democratisation’ had let the genie out of the alchemist’s bottle, and the fever of speculative finance was spreading to thousands of ‘bucket shops’ in the far corners of the US.
But the trading of commodities did not merely excite publics by fuelling their speculative longings. It also invited them into a ‘marketplace of ideas’ that bolstered the vision of liberal democracy that came to define 20th-century politics. The term itself is often traced back to Justice Oliver Wendell Holmes and his 1919 ruling in the Abrams v United States Supreme Court case, in which he asserted the superiority of the truth defined by market competition.
This avowal was not incidental – it reflected the broader convergence around the ideas of democracy and speculative finance that were congealing in US capitalism. A few years earlier, Holmes had been a key figure behind a lesser-known – yet just as influential – Supreme Court ruling: the 1905 Chicago Board of Trade v Christie Grain & Stock Co , which declared futures trading (the most speculative kind of financial activity) legal and desirable because it enables ‘the self-adjustment of society to the probable’ (thus distinguishing professional exchanges from the lay trading in bucket shops, which he regarded as pure gambling). Sanctioning the enchanting world of markets while asserting its vast power inequity conferred on financial alchemy an enduring force that is still with us today.
O ur time’s market alchemists, like their forebears in the postbellum stock exchanges, are typically seen through the binary of fraud: the flip side of institutional norms assumed to be constant, fair and tending towards equilibrium. Popular depictions of high-octane finance continue to focus on stories of smoke and mirrors woven around lies and greed – and they do so for a good reason. But by singling out the ‘excess’ of a few fraudsters, they ultimately distract us from the messier reality of finance, where alchemy is at the core, not an outlier. The ways in which Madoff and Bankman-Fried steered their multibillion scams through global markets were not as much a deviation from that reality as a window into it. Because markets are worlds where noise and signal are impossible to distinguish, the boundaries between real and fake are much more porous than what is assumed in mainstream accounts of fraud.
This, as I hope to have shown, has been the case throughout the history of a modern finance capitalism powered by alchemy. But it has become especially pronounced in our time, because contemporary forms of (computational, quantified) finance thrive in the uncertain space of big data and correlation, where noise reigns supreme. Fraud becomes both more insidious and harder to parse out in this context. Financial alchemy, in that sense, is more akin to distortion than to deception. Rather than a neat movement from facts to fibs, it represents an ambivalent coexistence of truths and falsehoods, which – as is often the case in today’s gamified markets – can even embrace fakeness. From J P Morgan’s avowed passion for astrology during the Gilded Age, to contemporary bankers’ enthusiastic endorsement of memetic NFTs, the history of finance brims with distortions that make no totalising claims of truthfulness. Financiers have long understood themselves as performers of alchemy, often being entirely transparent about their own gimmicks.
Today, paradoxically, it is this open admission – the ‘exposure of the trick’ – that makes financial alchemy even more effective. Its mass appeal emanates from being rooted into our volatile social and cultural worlds. In them, opacity and spectacle so often become accepted features of everyday reality. Far from dupes in the grip of ‘collective hallucinations’, modern financial subjects have been entwined with the forces of alchemy in much more dynamic, imaginative – and, often, wilful – ways. Their collective expression has produced a politics rich in myth, stretching today from outlandish conspiracy movements like QAnon, to TikTok communities of ‘vibes’, and gaming and crypto-trading subcultures.
It is for this reason that calls to break the spell of financialisation in everyday life offer insufficient answers to our so-called ‘post-truth’ moment. The ghosts of mob psychology and irrational exuberance have continued to haunt our perceptions of fraud and financial deception. But the present ‘reality crisis’ demands greater sensitivity towards the capacity of market distortion to create absorbing other-worlds. Distortion has been a critical force across fields as diverse as scientific and cultural production, from data science to music. Interpreting signals has been closely entangled with studying the generative possibilities of ‘noise’: looking for an ally in the glitches, dupes and ‘bad data’ that inhere within all forms of life and permeate our technologies of representing truth. At stake in the ‘fake worlds’ of financial alchemy is not merely resisting their will to deceive us but understanding their capacity to condition our struggles for other, more democratic realities."
Why everyone needs to learn (some) economics | Aeon Essays,,https://aeon.co/essays/why-everyone-needs-to-learn-some-economics,"In 1986, I left my native South Korea and came to Britain to study economics as a graduate student at the University of Cambridge.
Things were difficult. My spoken English was poor. Racism and cultural prejudices were rampant. And the weather was rubbish. But the most difficult thing was the food. Before coming to Britain, I had not realised how bad food can be. Meat was overcooked and under-seasoned. It was difficult to eat, unless accompanied by gravy, which could be very good but also very bad. English mustard, which I fell in love with, became a vital weapon in my struggle to eat dinners. Vegetables were boiled long beyond the point of death to become textureless, and there was only salt around to make them edible. Some British friends would argue valiantly that their food was under-seasoned (err… tasteless?) because the ingredients were so good that you oughtn’t ruin them with fussy things like sauces , which those devious French used because they needed to hide bad meat and old vegetables. Any shred of plausibility of that argument quickly vanished when I visited France at the end of my first year in Cambridge and first tasted real French food.
British food culture of the 1980s was – in a word – conservative; deeply so. The British ate nothing unfamiliar. Food considered foreign was viewed with near-religious scepticism and visceral aversion. Other than completely Anglicised – and generally dire-quality – Chinese, Indian and Italian, you could not get any other national cuisine, unless you travelled to Soho or another sophisticated district in London. British food conservatism was for me epitomised by the now defunct but then-rampant chain, Pizzaland. Realising that pizza could be traumatically ‘foreign’, the menu lured customers with an option to have their pizza served with a baked potato – the culinary equivalent of a security blanket for British people.
As with all discussions of foreignness, of course, this attitude gets pretty absurd when you scrutinise it. The UK’s beloved Christmas dinner consists of turkey (North America), potatoes (Peru or Chile), carrots (Afghanistan) and Brussels sprouts (from, yep, Belgium). But never mind that. Brits then simply didn’t ‘do foreign’.
What a contrast to the British food scene of today – diverse, sophisticated and even experimental. London especially offers everything – cheap yet excellent Turkish doner kebab, eaten at 1am from a van on the street; eye-wateringly expensive Japanese kaiseki dinner; vibrant Spanish tapas bars where you can mix and match things according to your mood and budget; whatever . Flavours span from vibrant, in-your-face Korean levels, to understated but heart-warming Polish. You get to choose between the complexity of Peruvian dishes – with Iberian, Asian and Inca roots – and the simple succulence of Argentinian steak. Most supermarkets and food stores sell ingredients for Italian, Mexican, French, Chinese, Caribbean, Jewish, Greek, Indian, Thai, North African, Japanese, Turkish, Polish and perhaps even Korean cuisines. If you want a more specialist condiment or ingredient, it can likely be found. This in a country where, in the late 1970s, according to an American friend who was then an exchange student, the only place you could score olive oil in Oxford was a pharmacy (for softening ear wax, if you’re wondering).
My theory is that the British people had a collective epiphany sometime in the mid- to late-1990s that their own food sucks, having experienced different – and mostly more exciting – cuisines during their foreign holidays and, more importantly, through the increasingly diverse immigrant communities. Once they did that, they were free to embrace all the cuisines in the world. There is no reason to insist on Indian over Thai, or favour Turkish over Mexican. Everything tasty is fine. The British freedom to consider equally all the choices available has led to it developing perhaps one of the most sophisticated food cultures anywhere.
W hile my food universe was expanding at lightning speed, the other universe of mine – economics – was, sadly, being sucked into a black hole.
Up to the 1970s, economics was populated by a diverse range of ‘schools’ containing different visions and research methods – classical, Marxist, neoclassical, Keynesian, developmentalist, Austrian, Schumpeterian, institutionalist, and behaviouralist, to name only the most significant. These schools of economics – or different approaches to economics – had (and still have) distinct visions in the sense that they had conflicting moral values and political positions, while understanding the way the economy works in divergent ways. I explain the competing methods of economists in my book Economics: The User’s Guide (2014), in a chapter called ‘Let a Hundred Flowers Bloom – How to “Do” Economics’.
Not only did the different methods coexist but they interacted with each other. Sometimes, the competing schools of economics clashed in a ‘death match’ – the Austrians vs the Marxists in the 1920s and ’30s, or the Keynesians vs the neoclassicals in the 1960s and ’70s. At other times, the interactions were more benign. Through debates and policy experiments tried by different governments around the world, each school was forced to hone its arguments. Different schools borrowed ideas from each other (often without proper acknowledgement). Some economists even tried the fusion of different theories – for example, some economists fused the Keynesian and the Marxist theories and created ‘post-Keynesian’ economics.
Economics until the 1970s was, then, rather like the British food scene today: many different cuisines, each with different strengths and weaknesses, competing for attention; all of them proud of their traditions but obliged to learn from each other; with lots of deliberate and unintentional fusion happening.
This intellectual ‘monocropping’ of economics has narrowed the intellectual gene pool of the subject
Since the 1980s, however, economics has become the British food scene before the 1990s. One tradition – neoclassical economics – is the only item on the menu. Like all other schools, it has its strengths; it also has serious limitations. This ascent of the neoclassical school is a complex story, which can’t be adequately considered here.
If told, the story would have many ingredients. Academic factors – like the merits and demerits of different schools, and the increasing dominance of mathematics as a research tool (which advanced knowledge of particular kind while suppressing others) – have mattered, of course. However, the ascent has also been critically shaped by power politics – both within the economics profession and in the outside world. In terms of professional power politics, the promotion of neoclassical economics by the so-called Nobel Prize in economic sciences (it is not a real Nobel prize but only a prize ‘in memory of Alfred Nobel’, given by Sveriges Riksbank, the Swedish central bank) has played a big role. In terms of power politics beyond the profession, the neoclassical school’s inherent reticence to question the distribution of income, wealth and power underlying any existing socioeconomic order has made it more palatable to the ruling elite. The globalisation of education during the post-Second World War era, in which the disproportionate ‘soft’ cultural power of the United States has been the biggest influence, has played a crucial role in spreading neoclassical economics, which had become dominant in the US first (in the 1960s).
But, whatever the causes, neoclassical economics is today so dominant in most countries (Japan and Brazil, and, to a lesser extent, Italy and Turkey are exceptions) that the term ‘economics’ has – for many – become synonymous with ‘neoclassical economics’. This intellectual ‘monocropping’ has narrowed the intellectual gene pool of the subject. Few neoclassical economists (that is, the vast majority of economists today) even acknowledge the existence, never mind the intellectual merits, of other schools. Those who do, assert the other varieties to be inferior. Some ideas, like those of the Marxist school, they will argue, are ‘not even economics’. It’s claimed that the few useful insights these other schools once possessed – say, for instance, the Schumpeterian school’s idea of innovation, or the idea of limited human rationality from the behaviouralist school – have already been incorporated into the ‘mainstream’ of economics, that is, neoclassical economics. They fail to see that these incorporations are mere ‘bolt-ons’, like the baked potato beside a Pizzaland pizza, rather than genuine fusions – like Peruvian cuisine, with Inca, Spanish, Chinese and Japanese influences, or the dishes by the Korean American chef David Chang (no relation), with American, Korean, Japanese, Chinese and Mexican influences.
I am not saying that neoclassical economics is particularly bad. Like all other schools of economics, it was built to explain particular things on the basis of certain ethical and political premises. So it is very good at some things but very bad at other things. The problem, rather, is the almost total dominance of one school, which has limited the scope of economics and created theoretical biases and blindspots.
In the same way in which the country’s refusal to accept diverse culinary traditions made Britain before the 1990s a place with a boring and unhealthy diet, the dominance of economics by one school has made economics limited in its coverage and narrow in its ethical foundation.
S ome readers may legitimately ask: why should I care if a bunch of academics become narrow-minded and engage in intellectual monocropping? However, you should all care, because, like it or not, economics has become the language of power. You cannot change the world without understanding it. In fact, I think that, in a capitalist economy, democracy cannot function effectively without all citizens understanding at least some economics. These days, with the dominance of market-oriented economics, even decisions about non-economic issues (such as health, education, literature or the arts) are dominated by economic logic. I have even met some British people who are trying to justify the monarchy in terms of the tourist revenue it allegedly generates. I am not a monarchist, but how insulting is it for the institution to be defended in that kind of way?
When so many collective decisions are formulated and justified with the help of the dominant economic theory, you don’t really know what you are voting for or against, if you don’t understand at least some economics.
Economics is not like studying, say, the Norse language or trying to identify Earth-like planets hundreds of light-years away. Economics has a direct and massive impact on our lives.
We all know that economic theories affect government policies regarding taxes, welfare spending, interest rates and labour market regulations, which in turn affect our daily material lives by influencing our jobs, working conditions, wages and the repayment burdens on our mortgages or student loans. Economic theories also shape the long-term collective prospects of an economy by influencing policies that determine its abilities to engage in high-productivity industries, to innovate, and to develop in an environmentally sustainable way. But beyond even that: economics doesn’t just influence economic variables, whether personal or collective. It changes who we are.
Believing humans to be driven by self-interest will create a society where cooperation is more difficult
Economics shapes us in two ways. First, it creates ideas: different economic theories assume different qualities to be at the essence of human nature, so the prevailing economic theory forms cultural norms about what people see as ‘natural’ and ‘human nature’. The dominance in the last few decades of neoclassical economics, which assumes that human beings are selfish, has normalised self-seeking behaviour. People who act in an altruistic way are derided as ‘suckers’ or are suspected of having some (selfish) ulterior motives. Were behaviouralist or institutionalist economic theories dominant, we would believe that human beings have complex motivations, of which self-seeking is only one of many; in these views, different designs of society can bring out varying motivations and even shape people’s motivations in diverse ways. In other words, economics affects what people see as normal, how people view each other, and what behaviour people exhibit to fit in.
Economics also influences who we are by affecting the way the economy develops and thus the way we live and work, which in turn shapes us. For example, different economic theories offer contrasting views on whether developing countries should promote industrialisation through public policy intervention. Different degrees of industrialisation, in turn, produce a variety of types of individuals. For example, compared with those who live in agrarian societies, people who live in more industrialised countries tend to be better at time-keeping, as their work – and consequently the rest of their lives – is organised according to the clock. Industrialisation also promotes trade union movements by amassing large numbers of workers in factories where they also need to cooperate much more closely with each other than in farms. These movements in turn create centre-Left political parties that push for more egalitarian policies, which may be weakened but do not disappear even when factories disappear, as has happened in most rich countries in the past few decades.
We can go further and assert that economics influences the kind of society we have. First, by shaping individuals differently, varying economic theories make societies of contrasting types. Thus, an economic theory that encourages industrialisation will lead to a society with more forces pushing for more egalitarian policies, as explained above. For another example, an economic theory that believes humans to be (almost) exclusively driven by self-interest will create a society where cooperation is more difficult. Second, different economic theories have different views on where the boundary of the ‘economic sphere’ should lie. So, if an economic theory recommends privatisation of what many consider to be essential services – healthcare, education, water, public transport, electricity and housing, for example – it is recommending that the market logic of ‘one-dollar-one-vote’ should be expanded against the democratic logic of ‘one-person-one-vote’. Finally, economic theories represent contrasting impacts on economic variables, such as inequality (of income or wealth) or economic rights (labour vs capital, consumer vs producer). Differences in these variables, in turn, influence how much conflict exists in society: greater income inequality or fewer labour rights generate not just more clashes between the powerful and those under them but also more conflicts among the less privileged, as they fight over the dwindling piece of pie available to them.
Understood like this, economics affects us in many more fundamental ways than when it is narrowly defined – income, jobs and pensions. That is why it is vital that every citizen needs to learn at least some economics. If we are to reform the economy for the benefit of the majority, make our democracy more effective, and make the world a better place to live for us and for the coming generations, we must ensure some basic economic literacy.
T he 2007-08 global financial crisis, and the stagnation and polarisation of the economy that followed it, has been a brutal reminder that we cannot leave our economy to professional economists and other ‘technocrats’. We should all get involved in its management – as active economic citizens .
Of course, there is ‘should’ and there is ‘can’. Many of us are physically too exhausted by our daily struggle for existence and mentally occupied with our own personal and financial affairs. The prospect of making the investments necessary to become an active economic citizen – learning economics and paying attention to what is going on in the economy – may seem daunting.
Being an active economic citizen gets easier over time, once you keep practising it
However, these investments are much easier to make than you might think. Economics is far more accessible than many economists would have you believe. In my book 23 Things They Don’t Tell You About Capitalism (2010), I invited the wrath of some of my professional colleagues by declaring that 95 per cent of economics is common sense – made to look difficult with the use of jargon, mathematics and statistics – while even the remaining 5 per cent can be understood in its essence (if not in full technical details), if explained well.
Once you have some basic understanding of how the economy works, monitoring what is going on becomes a lot less demanding in terms of your time and attention. Like many other things in life – learning to ride a bicycle, learning a new language, or learning to use your new tablet computer – being an active economic citizen gets easier over time, once you overcome the initial difficulties and keep practising it. And you don’t have to do it alone. Talking to your family and friends about everyday economic issues – whether it is jobs, inflation or banking crises – will enhance your knowledge and sharpen your arguments. These days, there are even activist groups that deliver – online and in person – economics education to ordinary citizens.
It is to make this endeavour by ordinary citizens to learn and think about economics more interesting and pleasant that I have brought the stories of food and economics together in my forthcoming book , Edible Economics: A Hungry Economist Explains the World (2023). Please enjoy it."
It is not democratic to go to war without the people’s consent | Aeon Essays,,https://aeon.co/essays/it-is-not-democratic-to-go-to-war-without-the-peoples-consent,"Listen to this essay
War is as old as humanity. Long before it became a matter of law or strategy, it was written into our myths and instincts: the urge to dominate, the struggle for survival, the fall from paradise into conflict. As the author Robert Ardrey once observed, we are not only animals; we are armed ones.
In modern times, the reality of war has become impossible to ignore. The photographs of Hiroshima’s ruins, the televised images from the Vietnam War, the viral pictures from the Bucha massacre in Ukraine and the latest livestreamed bombardments of Gaza all make clear what earlier generations could more easily conceal or forget. War is not only the threat of violence. It is the collapse of moral and political order: a moment when the rules of coexistence fall apart, laws twist into something else, and human life loses its value, or takes on a new one.
Yet even in such devastation, responsibility does not disappear. To inherit war is to inherit the responsibility to restrain it. In democracies, that burden is said to be shared – rooted in consent, exercised through representation. And still, while war may be as old as humanity, democracy is not, and that difference reshapes the questions citizens must ask.
What happens when the gravest of political acts – the decision to kill – is taken without the citizens’ assent? That is the paradox of what I call the silent mandate: leaders treat the people’s silence, which is structurally imposed, as if it were consent, turning absence into acquiescence. It is the point where democracy keeps its name but loses its meaning. If conflict is the most serious act a political community can undertake, shouldn’t it face a higher bar than ordinary political decisions in a democratic state? And even if a conflict seems to meet the conditions of a ‘just war’ – if such a thing exists – it may still be illegitimate if those in whose name it is fought were never asked.
In democracies, wars are rarely directly fought by civil society itself, but they are financed by it, and money carries its own moral weight. The decision is made not on the battlefield but within institutions that claim to speak for the people while avoiding their voice. Some doctrines allow for war in so-called ‘supreme emergencies’. But can such reasoning ever suffice in a democracy, where legitimacy should flow from the will of the people? Such silence is dangerous. It binds us, citizens, to the consequences of war while denying us any role in deciding whether it should be waged.
When violence is carried out in our name but without our voice, what kind of democracy remains?
T his kind of democratic deficit is not new. In modern democracy, the United States offers perhaps one of the most emblematic cases of a recurring paradox: a democracy that, at the height of its power, exposes the fragility of its own principle. Among the many moments that reveal it, none is more telling than the US president Harry S Truman’s decision in 1945 to use the atomic bomb. Although the US Constitution grants Congress the power to deliberate on matters of war, the conflict with Japan had been declared years earlier, and Truman was under no legal obligation to seek a new authorisation. But that does not change the substance of the matter. A decision of such magnitude, the use of a weapon capable of erasing entire cities, should have required, at least to my mind, a broader sense of responsibility, a gesture of accountability no less than toward the institutions that claimed to represent the people.
Telegram from the US Secretary of War Henry Stimson asking for Truman’s permission to release a public statement announcing the dropping of the atomic bomb on Hiroshima, 30 July 1945.
On the reverse is Truman’s handwritten reply authorising the release of the statement with the remark ‘release when ready’. Images courtesy the National Archives
Yet, no record exists of any dialogue with US Congress and certainly not of any public debate. One of the most irreversible choices in human history was made by a single man, probably with the support of a handful of advisers, and formalised in a brief handwritten note during the Potsdam Conference. In that note, addressed to the US Secretary of War, Truman authorised the release of a public statement with the remark ‘release when ready’. Those three words, written in the margin of a communiqué, have often been read by history as the final seal on Hiroshima. Everything happened in the silence of institutions and, even more profoundly, in the silence of citizens. The decision did not pass through the US Congress but through the president’s own conscience. Only after Hiroshima did Truman turn to Congress, not to justify his act but to propose the creation of an Atomic Energy Commission. Crucially, it was a gesture that borders on irony. However, that moment marked a point of no return: the atomic bomb entered history not with public justification, but with the quiet efficiency of routine.
At first glance, this might seem a modern failure of democracy, a symptom of institutions that have drifted away from their people. But the logic runs deeper. The idea that leaders can decide on war without consulting those who will bear its costs is far older than democracy itself. It stems from a moral inheritance that has shaped political authority for centuries, teaching rulers to frame violence as duty and obedience as virtue. What I now call a ‘democratic deficit’ is, in truth, the echo of that ancient order: a way of justifying force while keeping power in a privilege of the powerful.
The legitimacy of war depended on reason and principles that all people, in theory, could grasp
Before it became a matter of politics, conflict was made thinkable through stories that tried to make it seem right . The moral vocabulary of war began to take shape early. The first Christians, once fiercely pacifist, gradually learned to reconcile faith with force. After Constantine made their faith the creed of the Roman Empire, the Church struck a bargain with power. Armies and generals could not be wished out of existence, so war was reimagined as tragic but sometimes righteous, a bitter response to a broken world. It was Augustine of Hippo who gave this new reality its language, later distilled into the bellum iustum or just war theory. What mattered, accordingly, was the purpose .
A conflict could be acceptable if it restored peace, punished injustice or defended the weak. It had to be guided by necessity, not ambition, and tempered with mercy even for the defeated. From this moral balancing act, the roots of what we now call the laws of war began to grow. More than a thousand years later, Europe was burning with religious conflict. Armies of Catholics and Protestants ravaged each other’s lands in brutal cycles of revenge. Out of this chaos came Hugo Grotius, a 17th-century Dutch jurist who tried to strip war of its theological frame, arguing that the rules of a battle rested not on divine revelation but on reason itself. Grotius dared to suggest that, though it would be wicked to assume it, even if God did not exist, the norms of justice would still hold. With that single claim, he offered the foundations of a secular international order, where the legitimacy of war depended on reason and principles that all people, in theory, could grasp.
Here the problem lies in the fact that these ideas on the justification of a conflict, however influential, were not born in democracies. They came from kingdoms, empires and the shadow of divine right. In those worlds, the question of who had the authority to declare war was never posed in democratic terms. And this is where much of the moral tradition – despite its conceptual richness – falls short of the democratic ideal. Today, we have inherited sophisticated debates about what makes a war just: whether there is a just cause, such as self-defence, and whether it is pursued with the right intention. What remains unexamined is the power to decide, that is, the right to turn judgment into action. For centuries, the moral burden of ferocity was carried by rulers. But, in a democracy, it ought to be carried by the people.
A first, cautious attempt to update the old theory of just war came centuries later. Yet the same logic returned in new forms, and modern democracies inherited the language of just war without its democratic correction. In the 20th century, several thinkers returned to that old moral tradition, seeking to make sense of it in a democratic world, none more influential than Michael Walzer. Writing in the long shadow of the Second World War, Walzer introduced the idea of a ‘supreme emergency’, a situation so catastrophic that even the moral rules meant to restrain war might have to break. In Just and Unjust Wars (1977), he argued that, when a community faces ‘an ultimate threat to everything decent in our lives,’ leaders might feel compelled to cross boundaries that would otherwise remain sacred. What is normally forbidden, such as bombing civilians, becomes permissible. Walzer’s honesty was bracing, almost ethically arresting. He refused to let leaders off the hook. Even in such moments, he wrote, they are still guilty. They remain moral criminals, even if their crime is necessary. The doctrine shocks because it names the unbearable tension between human rights and human survival. But it also reveals a danger. Once the door to exceptions is opened, who can be sure it will ever close again?
As our thinking about war grew more refined, it became wiser, more rational, and somehow still blind to the right of those who would live with its consequences to have a say. Over time, that moral oversight hardened into political habit. What fails today is not just leadership but the very design of democratic power, a structure that learned to justify war without ever learning how to share its choice. This failure is not accidental. It is built into the way democracy came to organise power. Modern representative systems replaced the monarch’s divine right with the citizen’s vote, but they preserved the same logic of distance between those who decide and those who obey. Understanding why this distance endures means looking back to the origins of representation itself, to the moment when citizens first learned to entrust their power to others. Delegation was never meant just to make government more efficient. It was born as a safeguard against confusion, a pact – a social contract – to prevent society from tearing itself apart. By agreeing to transfer the monopoly of force to the state, people renounced not only the right to act violently but also the power to decide when that violence might return. It was an act of trust born from fear, the hope that authority could channel chaos into order.
The same democracies that promise never to turn their weapons on each other continue to fund distant conflicts
Representation, in this sense, is both a triumph and a paradox. It is what makes democracy possible in large societies – still, it also introduces an inevitable distance between those who govern and those who are governed. Elections, in fact, are not mirrors; they do not perfectly reflect the people’s will. They produce trustees, not delegates. This imperfection is not a flaw to be corrected but a condition that makes democracy itself possible. Still, I cannot help asking how far this distance can stretch. Can we really treat the decision to go to war as we would a tax reform, a transport plan or a healthcare bill? Those kinds of decisions are reversible, limited in scope, and accountable to voters. War is unlike any other act of government; it converts the wealth of a society into tools of death and decides whose lives will be spent for it.
Modern democracies like to believe, and like to repeat, that they have outgrown war, at least among themselves. And, to some extent, it is true: they no longer fight one another directly. The theory of ‘democratic peace’, as it is called , as a matter of fact holds on paper. Yet peace at home often conceals conflict abroad. The same democracies that promise never to turn their weapons on each other continue to fund distant conflicts, supply arms, and sustain alliances. By shifting the theatre of a battle, they keep fighting, sometimes by proxy, instead of working to bring wars to an end.
Wars fought abroad remain and have remained, in a substantive sense, wars of democratic states. It is a bitter contradiction, especially if we remember that the birth of most of today’s democracies, and later the creation of the United Nations, was also meant to mark the beginning of an order built on peace. Seen from this perspective, the notion of ‘democratic peace’ is not a shield against responsibility, but a reminder that democracies, too, bear the weight of the wars they wage. It is precisely this illusion of peace that makes the question of consent impossible to ignore. One could say, then, that war conducted by democracies should first of all be recognised as such, and only then should it demand a higher standard of justification and a fuller form of consent. The gap the people can (and should) accept elsewhere between what society wants and what their representatives do is, in this case, less tolerable as its costs are not measured in policies but in human lives.
W atching the news each day, it seems to me that this longing for consent, this demand to be included, is not just a matter of institutional reform. It feels like something many citizens already sense, a shared intuition that democracy, when speaking of war, must evolve if it is to remain itself. For example, across societies that call themselves free, citizens are marching to protest their governments’ support for the conflict in Gaza. From London to Rome, from New York to Melbourne, from Dhaka to Kuala Lumpur, the message is the same: not in our name. Protests, of course, are not new. They have always been part of civic life. But this time, they feel different. They seem broader, more coordinated and remarkably persistent, spreading across continents and lasting far beyond a single news cycle. It suggests that something deeper is shifting in the political conscience of democratic societies.
The Gaza freedom flotillas, organised by ordinary civilians rather than states, made this conviction visible: their attempts to break the Gaza blockade to deliver humanitarian aid are an act of defiance against the idea that silence equals consent. Yet these reactions, powerful as they are, all come after the fact. They arise only once decisions have already been made. Protest, in fact, becomes the last language available when the institutional voices lapse into quiet obedience. Protest allows citizens to express refusal, but it also shows how little space remains for them within the formal decision-making process. Because protest occurs outside the architecture of representation, it cannot by itself restore consent. It can register disapproval but not alter the process that excludes it. Governments can acknowledge demonstrations, even tolerate them, and still claim that the absence of an institutional veto amounts to approval. In this way, dissent can paradoxically sustain the silence it seeks to break.
At this stage, the conclusion seems inescapable: our democratic structures may need to evolve. If war challenges the ordinary logic of political delegation, it also calls for a more responsive, participatory framework, one capable of meeting its exceptional stakes. A mature democracy, though grounded in representative mechanisms, should be able to develop effective oversight over the use of force. Sometimes I wonder whether change begins with ideas or with awareness. Perhaps, in the end, it begins with knowledge. If citizens are to share responsibility for the choices made in their name, knowledge – meaning information about it – must become a public right, not a privilege.
The point is not that citizens should govern alone, but that governments shouldn’t either
No one can deliberate about a war they do not even know is being fought. Every act of intervention abroad, every supply of weapons or alliance commitment, should come with an obligation to explain itself, to make its purpose, its risks, and its costs visible. Transparency, however, is only the beginning. Knowledge has to lead somewhere… to reflection, to judgment, to spaces where power pauses long enough for conscience to catch up. In some democracies, parliaments still vote before sending troops abroad. It is a sign of restraint, but too often it happens behind closed doors. Open-discussion assemblies could be created, spaces where citizens can speak – maybe not to decide but to question those who decide in their name. Their task would be not to command, but to interrupt the machinery of decision-making just long enough for a society – and, with it, democracy itself – to think. Beyond that, one could imagine going further, creating a moment when citizens themselves are asked to confirm or refuse participation in a conflict, especially when it is not in direct self-defence. Such a ‘war veto’ need not be an obstacle to action, but a space for collective judgment, where the democratic will reappears precisely when the stakes are highest.
These ideas are not utopian. Recent history reminds us that this is possible. After the 2008 banking crisis, Iceland offered a striking example . Instead of turning inward in anger or distrust, the country opened up. A group of randomly selected citizens was invited to discuss the principles of a new constitution. They were not politicians or experts, but ordinary people reflecting together on shared materials, with the help of independent facilitators. The goal was not to replace parliament, but to stand beside it, to ensure that reconstruction was not only economic, but democratic as well.
This was not an isolated case. Similar experiences in other democracies have shown that citizen involvement is not an obstacle to governability, but a form of collaboration. It does not overthrow representation; it completes it. Probably, it should be seen as way to bring political responsibility back into public conversation and give substance once again to the word ‘consent’. The point is not that citizens should govern alone, but that governments shouldn’t either. Popular sovereignty does not live only in the ballot box, but in the capacity to be called to reflection when history knocks at the door.
In the end, perhaps the true ‘silent mandate’ is not the one that citizens endure, but the one they must break: the duty to speak when silence has become convenient. Every democracy lives through moments when it must find its voice again, reclaim its own name. War is one of them. Defending democracy in times of war means refusing to let our name be used without our voice. And above all, it means asking whether the silence that shields us today might ultimately endanger the freedom we hope to preserve tomorrow."
"Declared dead last year, the Anthropocene is very much alive | Aeon Essays",,https://aeon.co/essays/declared-dead-last-year-the-anthropocene-is-very-much-alive,"Listen to this essay
In early March 2024, an expert panel of the International Commission on Stratigraphy (ICS) had a vote to settle a question that had vexed geologists for years: are we living in the Anthropocene, ‘the human age’? The result of the vote was a clear negative. In the realm of geological disciplines, the debate that endured for decades is basically over. However, the term ‘Anthropocene’ has become deeply ingrained in the public imagination and will not be simply erased. And it still has currency, but it needs to be broken loose from entrenched debates that carry unnecessary baggage. The Anthropocene is a prism through which we can examine the multifaceted history of human activities on this planet, and the spectrum of our potential futures.
The idea of humans as planet-altering agents has long roots. The American diplomat and philologist George Perkins Marsh proposed in Man and Nature (1864) that humanity had become a geological force, on a par with the most powerful forces of nature. The term ‘the Anthropocene’ was proposed by Paul Crutzen and Eugene Stoermer in 2000, in the multidisciplinary field of Earth System Science:
The idea was taken up in stratigraphy, the branch of geology that studies rock layers and is responsible for classifying the geological time of Earth. In 2009, an official process was begun to determine whether the Anthropocene would gain the status of a geological epoch. Although an epoch is the second smallest unit in the hierarchical classification of geologic time, past epochs have spanned millions of years, whereas the current epoch, the Holocene, is just 11,700 years old. Proposing a new epoch so soon was controversial among stratigraphers, to say the least. The newly formed Anthropocene Working Group would tackle the issue.
In official stratigraphic definition of epochs, a distinct starting point has to be decided. Every epoch begins and ends at a clear point in time. Also, a geological marker, ‘ the golden spike ’, has to be found at some specific time and place. For the current Holocene epoch, the marker is based on a change in deuterium excess values, and the primary location is a borehole in Greenland.
Geologists are not silly, of course. They do not claim that an epoch spanning millions of years literally began at some specific point in time (or place): this is an artefact of the system of classification. But a similar determination would have to be made for the Anthropocene for it to qualify even for the ICS vote. There were a number of candidates for this: the Columbian Exchange, the Industrial Revolution, or ‘the Great Acceleration’ from the 1950s onwards. The Anthropocene Working Group chose the first atomic bomb tests as the marker, and Crawford Lake in Ontario, Canada, as the location of ‘the golden spike’.
To be clear, this determination was never intended as the answer to the question ‘When did humans begin to alter the environment on a planetary scale?’ It was known by most if not all involved that human activity had changed the face of the planet in multifarious ways over centuries, even millennia. However, at some stage in history, the effects of human activities had accelerated significantly, and that is why ‘the Great Acceleration’ became a focal point in the winning proposal. To determine this, the classification system demanded a single point of origin.
In 2016, the group made its recommendation that the Anthropocene should be considered as the new epoch. This recommendation was later forwarded to higher ICS organs for consideration and voting. A series of votes was expected, but the proposal was rejected early on in the process in March 2024 . Although technically the proposal could be renewed in a decade, for now it seems that the debate is over. In stratigraphic terms, we are still living in the Holocene.
D espite being officially declared dead, however, the term ‘Anthropocene’ is very much alive. Early on, it broke out of the confines of stratigraphy and was adopted widely in public discussion, in the arts, and also in various other sciences, both natural and social. Myriad scientific journals, conferences, artworks and novels still carry it forward.
But how should the term be understood now, when its lifetime as an official geological concept is over? Should we stop using it, or define it in a new way? I believe that the term still has value, but it needs to be freed from certain entrenched positions that carry unnecessary baggage.
A problem in the wider cultural discussion over the Anthropocene has been that the stratigraphic view has been somewhat misinterpreted. Instead of understanding ‘the golden spike’ as an artefact of classification, it has been deemed as a statement of existential importance. Especially in social and human sciences, determining the point of origin of the Anthropocene has been repeatedly understood as pointing to historical guilt.
According to one common line of criticism, the very idea of the Anthropocene, ‘the age of humans’, risks universalising the issue and slipping into misanthropy. Are all humans everywhere and at all times equally to blame? Was the path towards the Anthropocene cleared with the invention of fire, or the planting of the first fields, or at the first iron forge? Are humans truly ‘a virus’ of nature as proposed by Agent Smith in The Matrix movie franchise, or by some ‘neo-Malthusian’ environmental thinkers?
If the Anthropocene originated in the 1950s, would it not let colonialism or capitalism off the hook?
The concept ‘neo-Malthusian’, named after Thomas Malthus (1766-1834), can be used narrowly or loosely, but in this case, it refers to views according to which the size of the human population is the fundamental defining factor and the root cause of environmental problems. According to this view, humans are in an inescapable zero-sum game with the rest of nature. For such thinkers, determining the Anthropocene as the ecological original sin is not a problem. Paul R Ehrlich, who co-wrote the book The Population Bomb (1968) with Anne H Ehrlich, is a prime example here: according to him , the ‘optimum’ human population would be fewer than 2 billion.
Neo-Malthusian environmental ideas have also been embraced by xenophobic or outright racist groups, since they seem to justify focusing all attention on the areas where the human population is still growing, and belittling the importance of wealth inequality, historical plunder, resource extractivism and the wild differences in per-capita consumption and pollution.
For these critics, the idea of the Anthropocene is inherently compromised or biased towards such thinking. It is deemed fundamentally ahistorical and apolitical, blind to the differences between and within societies. What about the Columbian Exchange, or the Industrial Revolution? Are they part and parcel of the same undifferentiated ‘human era’?
Another line of criticism has been conversely aimed at the historically specific determination of the origin point at ‘the Great Acceleration’ of the 1950s. If the Anthropocene is seen as originating in so late a date, would it not let the earlier phases of colonialism or capitalism off the hook? This criticism is based on the wider view that fundamental societal transformations are not point events but longer processes. For example, the Industrial Revolution had roots in the earlier manufactures and even earlier changes in agriculture. And the Green Revolution cannot be explained only by new fertilisers and other agricultural technologies: it stems also from earlier neocolonial groundwork. Thus, the deeper political foundations of the Anthropocene need to be excavated.
Such accusations have disturbed many stratigraphers, who feel that ‘politicising’ such a technical debate is unwarranted. For them, dividing history with a gold spike would not mean that everything began then. Of course different kinds of environmental changes have taken place in different eras, and there are significant differences of scale and ecological dynamics between them. But in the stratigraphic playing field, a temporal signpost has to be stuck somewhere, and it should not be interpreted as placing exclusive historical blame. This counter-critique has been both justified and naive, as it is obvious how the geological debates resonated with the other existing ideas about the environmental crisis. Scientific debates do not take place in a vacuum.
I n order to tackle both the ingrained misanthropy and the simplistic historical views, new terms have been proposed to supplant the Anthropocene. A plethora of competing ‘-cenes’ and root causes of environmental destruction have followed.
The Swedish human ecologist Andreas Malm proposed ‘Capitalocene’ in 2009, arguing for the centrality of capitalism in the planet-altering activities. The idea has been taken up by many others, such as the environmental historian Jason W Moore. And in their book The Shock of the Anthropocene (2016), the French historians of science Christophe Bonneuil and Jean-Baptiste Fressoz proposed ‘Anglocene’:
Several researchers who highlight the entangling of human life with technology have argued for ‘Technocene’. The renowned feminist scholar Donna Haraway and the anthropologist Anna Tsing, author of The Mushroom at the End of the World (2021), have pointed out the centrality of plantations in colonial history, proposing ‘Plantationocene’. Haraway has another possibility too, ‘Chthulucene’, but don’t worry, she is not proposing that our era is determined by Lovecraftian horrors (she named the -cene after a species of spider). The name Chthulucene detracts from the competition between historical points of origin: instead, Haraway points to the importance of fuzzying the boundaries between human and nonhuman, and trying to live with the troubles we have inherited.
The competing ‘-cenes’ become mutually exclusive alternatives. Choosing one is interpreted as a choice against the others
The competing ‘-cenes’ do point out important alternative viewpoints for examining the (geologically) recent history. But the discussion is burdened by an unnecessary notion of exclusivity: the main culprit, the root cause, has to be located somewhere – somewhere else than in humanity itself, but at some clear and distinct source anyway.
This is a recurrent feature in environmental thinking. Even though environmental problems are legion and not a unified whole, there is a strong tendency to totalise them into an undifferentiated whole. One problem, only one root cause. The competing ‘-cenes’ thus tend to become mutually exclusive alternatives, different determinations of the root cause of the environmental crisis. Choosing one is readily interpreted as a choice against the others.
This is a serious problem, as it narrows our understanding of ecological issues. With climate change, one can fairly justifiably point to the rise of capitalism and industrialisation as the root causes, because most of the greenhouse gas emissions have taken place since c 1750, in an accelerating fashion. But the global spread of monocultures, which has resulted in a narrowing selection of domesticated plants and species, is a key dimension of biodiversity decline. And it has a much longer pre-capitalist history. On the other hand, the depletion of the ozone layer was down to a very specific selection of chemical compounds that could have been developed in a very different world, and it could have remained undetected without sophisticated satellites. The hundreds of aboveground nuclear tests were fed by the Cold War. And so on.
N ow that the official Anthropocene narrative is over, now that the stratigraphic, natural scientific determination can no longer be used as a touchstone, there is a good opportunity to rethink this issue. We should get away from the territory of alternative and competing points of origin, from the debate between mutually exclusive root causes, and look at the whole thing as a varying historical process .
Some inspiration can be drawn from the late stage of the stratigraphic debates. In 2021, a group of scientists proposed that the Anthropocene could be more fruitfully understood not as an epoch but as ‘an event’. In geologic time, events are processes of change, which contribute to transformations of the global ecological systems. They are diachronous and dynamic, rather than specific periods with a beginning and an end. In 2023, the same scientists noted that the epoch view ‘fails to account not only for the diachronic nature of human impacts on global environmental systems during the Late Holocene but also the spatial heterogeneity of those impacts.’ In their view, the Anthropocene is a development that has endured for millennia and is still going on.
Events also differ from epochs in that, in stratigraphic terms, they need no formal ratification nor a determination of ‘the golden spike’. The causes and dynamics can be more multifarious – precisely the point made above about the historically layered nature of environmental change wrought by human activities:
What kind of things have been events? The Great Oxidation Event, the remarkable rise of oxygen in Earth’s atmosphere 2.3-2.5 billion years ago, is one. Another example is the Great Ordovician Biodiversity Event, an explosive diversification of marine life. In biological sciences, mass extinctions are also called ‘events’ – not precisely the same thing as geological events, but extinction events too are long and multifarious processes that transform the very fabric of life.
There are changes, and then there are changes, and significant differences between them should not be shrouded
The debate over whether we are living in the midst of a ‘Sixth Mass Extinction Event’ is still going on. It is important to note that our current situation does not compare with ‘the Great Dying’, or the Permian-Triassic Extinction Event, and let us hope it never will. It is approximated that 70 per cent of terrestrial vertebrate species and 80- 90 per cent of marine species went extinct back then, and life was much poorer for millions of years. It was touch and go for life on Earth. The only human thing that could perhaps cause something like that would be a massive nuclear war. Nevertheless, it is clear that biodiversity is being radically endangered on many levels (genetic diversity of populations, loss of populations, extinction of species, and the degradation or fragmentation of ecosystems). We are in the midst of a torrential transfiguration of life on Earth.
On the one hand, it is crucial to note that human-induced ecological changes clearly have grown to become increasingly global in scale during the Great Acceleration. Extraction of natural resources, land use change and various forms of pollution have increased by orders of magnitude. So there clearly are differences within the millennia-spanning event. The Great Acceleration is a world-historical hinge point after which things have become very different. The fossil-fuelled, technologically complex and (mostly) capitalist human societies have truly become an agent of change that compares with the great upheavals of deep history.
This is an important point: ‘humans have always changed their environments’ is a truism that can too easily be used to belittle environmental concerns. There are changes, and then there are changes, and significant differences between them should not be shrouded. Something world-historically unique has taken place since the Second World War.
On the other hand, by focusing too much on this fairly recent history, we lose sight of the longer-term historical forces that have affected our world. The notion of the Anthropocene as an event, as a long-term process, allows us to keep in mind different rhythms of history at the same time: environmental changes that have roots closer to the surface and those that dig deeper. Why on Earth should anything so complex as our ecological peril have one root cause? This is what roots do, dig into many directions at the same time. This does not let anyone or anything ‘off the hook’, it means that we have to be mindful of both the younger and the older roots of our predicament, to look at a plethora of root causes.
For example: when we are focusing our gaze on climate change and plastic pollution , it is clear that the fossil fuel economies that have accelerated after the Second World War should be our prime concern. Decarbonising our economies is necessary, and most likely that will require deeper societal transformations not only in how we produce and consume things but also in the fundamental infrastructures (energy, traffic and transport, heating etc) and the ways we value things.
Perhaps the Sinocene? Robocene? Nanocene? One could continue this neologism bonanza to the point of ridiculousness
But even if climate change is successfully curtailed within ‘safety limits’, it will not necessarily be done in a way that will stop biodiversity decline. Some facets of that much more complex phenomenon clearly stem from the modern growth-oriented and extractive societal paradigm, but other facets have longer histories. Some are due to the specific ways our needs are provided for: for example dedicating 80 per cent of agricultural land to animal production. Most likely, getting back within sustainable ecological limits in this wider sense will take much longer than the decadal scale required by climate mitigation. The work of safeguarding the viability of Earth as a home will go on, perhaps in a sense forever.
To use the language of the competing ‘-cenes’, we have to be simultaneously mindful of the Plantationocene, the Anglocene, the Technocene, the Capitalocene, and of many others to come – perhaps the Sinocene? Robocene? Nanocene? One could continue this neologism bonanza to the point of ridiculousness, but the point is simple: we have to keep looking at various timescales and histories at the same time, to try to understand recent history and deep time concurrently. This is a crucial thing, irrespective of the nomenclature we choose.
Another way to look at this is to say that we are living in the ongoing process of the Anthropocene that has multiple currents and will take new and surprising (and perhaps more terrible) forms in the future. This is another virtue of the notion of the Anthropocene as an event: it is open-ended, it points not only to history but also to the future. We are in the midst of a fundamental process of transformation that can get much, much worse. The Anthropocene we see now may in the future seem quite hospitable.
Not only are we moving to a radically different planet, there is a temporal succession of alien planets looming in the future, and alternative trajectories to take. None of the paths to the future are free of problems, because the inherited baggage of history is heavy. A lot of damage has been done, and we have to live with that trouble, as Haraway and Tsing have stated. Some things have been irrevocably lost, and many other things will be lost, even in the best of all possible worlds. Whether the future sees a better or a worse-off world, a world-historical bottleneck needs to be squeezed through. Choices are made, every day, about the planet that humans will inhabit in the futures to come.
But the future lasts a long time. If the current human societies can get their act together, during the future stages of the Anthropocene Event, the planet may yet be a better place to live in than it is now. It may take centuries, and the result will not be a return to the past but a radically altered world. The key is whether humans and other creatures can still find it a world in which to thrive and create ever new futures."
Generative AI has access to a small slice of human knowledge | Aeon Essays,,https://aeon.co/essays/generative-ai-has-access-to-a-small-slice-of-human-knowledge,"A few years back, my dad was diagnosed with a tumour on his tongue – which meant we had some choices to weigh up. My family has an interesting dynamic when it comes to medical decisions. While my older sister is a trained doctor in Western allopathic medicine, my parents are big believers in traditional remedies. Having grown up in a small town in India, I am accustomed to rituals. My dad had a ritual too. Every time we visited his home village in southern Tamil Nadu, he’d get a bottle of thick, pungent, herb-infused oil from a vaithiyar , a traditional doctor practising Siddha medicine. It was his way of maintaining his connection with the kind of medicine he had always known and trusted.
Dad’s tumour showed signs of being malignant, so the hospital doctors and my sister strongly recommended surgery. My parents were against the idea, worried it could affect my dad’s speech. This is usually where I come in, as the expert mediator in the family. Like any good millennial, I turned to the internet for help in guiding the decision. After days of thorough research, I (as usual) sided with my sister and pushed for the surgery. The internet backed us up.
We eventually got my dad to agree and even set a date. But then, he slyly used my sister’s pregnancy as a distraction to skip the surgery altogether. While we pestered him every day to get it done, he was secretly taking his herbal concoction. And, lo and behold, after several months the tumour actually shrank and eventually disappeared. That whole episode definitely earned my dad some bragging rights.
At the time, I dismissed it as a lucky exception. But recently I’ve been wondering if I was too quick to dismiss my parents’ trust in traditional knowledge, while easily accepting the authority of digitally dominant sources. I find it hard to believe my dad’s herbal concoctions worked, but I have also since come to realise that the seemingly all-knowing internet I so readily trusted contains huge gaps – and in a world of AI, it’s about to get worse.
The irony isn’t lost on me that this dilemma has emerged through my research at a university in the United States, in a setting removed from my childhood and the very context where traditional practices were part of daily life. At Cornell University in New York, I study what it takes to design responsible AI systems. My work has been revealing to me how the digital world reflects profound power imbalances in knowledge, and how this is amplified by generative AI (GenAI). The early internet was dominated by the English language and Western institutions, and this imbalance has hardened over time, leaving whole worlds of human knowledge and experience undigitised. Now with the rise of GenAI – which is trained on this available digital corpus – that asymmetry threatens to become entrenched.
For many people, GenAI is becoming their primary way to learn about the world. A large-scale study published in September 2025, analysing how people have been using ChatGPT since its launch in November 2022, revealed that around half the queries were for practical guidance, or to seek information. These systems may appear neutral, but they are far from it. The most popular models privilege dominant epistemologies (typically Western and institutional) while marginalising alternative ways of knowing, especially those encoded in oral traditions, embodied practice and the languages considered ‘low-resource’ in the computing world, such as Hindi or Swahili, both spoken by hundreds of millions. By amplifying these hierarchies, GenAI risks contributing to the erasure of systems of understanding that have evolved over centuries, disconnecting future generations from vast bodies of insights and wisdom that were never encoded yet remain essential to human ways of knowing. What’s at stake then isn’t just representation – it’s the resilience and diversity of knowledge itself.
G enAI is trained with massive datasets of text from sources like books, articles, websites and transcripts, hence the name ‘large language model’ (LLM). But this training data is far from the sum total of human knowledge. As well as oral cultures, many languages are underrepresented or absent.
To understand why this matters, we must first recognise that languages serve as vessels for knowledge – they are not merely communication tools, but repositories of specialised understanding. Each language carries entire worlds of human experience and insight developed over centuries: the rituals and customs that shape communities, distinctive ways of seeing beauty and creating art, deep familiarity with specific landscapes and natural systems, spiritual and philosophical worldviews, subtle vocabularies for inner experiences, specialised expertise in various fields, frameworks for organising society and justice, collective memories and historical narratives, healing traditions, and intricate social bonds.
Despite being spoken by more than 86 million people worldwide, Tamil represents just 0.04 per cent of the data
When AI systems lack adequate exposure to a language, they have blind spots in their comprehension of human experience. For example, data from Common Crawl, one of the largest public sources of training data, reveals stark inequalities. It contains more than 300 billion web pages spanning 18 years , but English dominates with 44 per cent of the content. What’s even more concerning is the imbalance between how many people speak a language in the physical world and how much that language is represented in online data. Take Hindi, for example, the third most spoken language globally, spoken by around 7.5 per cent of the world’s population. It accounts for only 0.2 per cent of Common Crawl’s data. The situation is even more dire for Tamil, my own mother tongue. Despite being spoken by more than 86 million people worldwide, it represents just 0.04 per cent of the data. In contrast, English is spoken by approximately 20 per cent of the global population (including both native and non-native speakers), but it dominates the digital space by an exponentially larger margin. Similarly, other colonial languages such as French, Italian and Portuguese, with far fewer speakers than Hindi, are also better represented online.
The underrepresentation of Hindi and Tamil, troubling as it is, represents just the tip of the iceberg. In the computing world, approximately 97 per cent of the world’s languages are classified as ‘low-resource’. This designation is misleading when applied beyond computing contexts: many of these languages boast millions of speakers and carry centuries-old traditions of rich linguistic heritage. They are simply underrepresented online or in accessible datasets. In contrast, ‘high-resource’ languages have abundant and diverse digital data available. A study from 2020 showed that 88 per cent of the world’s languages face such severe neglect in AI technologies that bringing them up to speed would require herculean – perhaps impossible – efforts. It wouldn’t be surprising if the status quo is not too different even now.
T o illustrate the kinds of knowledge missing, let’s consider just one example: our understanding of local ecologies. An environmentalist friend once told me something that has stayed with me – a community’s connection with their ecology can be seen through the names they have for their local plants. The more intimate their relationship with their environment, the more detailed and specific their botanical vocabulary becomes. Because plant species are often regionally specific or ecologically unique, knowledge of these plants becomes equally localised. This insight proves remarkably accurate when we examine the research. For instance, one study on medicinal plants in North America, northwest Amazonia and New Guinea found that more than 75 per cent of the 12,495 distinct uses of plant species were unique to just one local language. When a language becomes marginalised, the plant knowledge embedded within it often disappears as well.
Exterior and interior views of a wattle and daub cottage in Bengaluru, India by natural building pioneer firm Thannal
While writing this essay, I spoke to various people about the language gaps in GenAI. One of them was Dharan Ashok, chief architect at Thannal, an organisation dedicated to reviving natural building techniques in India. He echoed that there is a strong connection between language and local ecological knowledge, and that this in turn underpins Indigenous architectural knowledge. While modern construction is largely synonymous with concrete and steel, Indigenous building methods were deeply ecological, he told me. They relied on materials available in the surrounding environment, with biopolymers derived from native plants playing a significant role.
Amid concerns over unsustainable and carbon-intensive contemporary construction practices, Dharan is actively working to recover the lost art of producing biopolymers from local plants. However, he noted that the greatest challenge lies in the fact that this knowledge is largely undocumented and has been passed down orally through native languages. It is often held by just a few elders, and when they pass away, it is lost. Dharan recounted a recent experience of missing the chance to learn how to make a specific type of limestone-based brick after the last artisan with that knowledge died.
T o understand how certain ways of knowing rise to global dominance, often at the expense of Indigenous knowledge, it helps to consider the idea of cultural hegemony developed by the Italian philosopher Antonio Gramsci.
Gramsci argued that power is not maintained solely through force or economic control, but also through the shaping of cultural norms and everyday beliefs. Over time, epistemological approaches rooted in Western traditions have come to be seen as objective and universal, rather than culturally situated or historically contingent. This has normalised Western knowledge as the standard, obscuring the specific historical and political forces that enabled its rise. Institutions such as schools, scientific bodies and international development organisations have helped entrench this dominance.
Epistemologies are not just abstract and cognitive. They are physically embodied around us, with a direct impact on our bodies and lived experiences. To understand why, let’s consider an example that contrasts sharply with the kind of Indigenous construction practices that Dharan seeks to revive: high-rise buildings with glass façades in the tropics.
Glass buildings are gleaming reminders of the dangers of knowledge homogenisation and epistemic hierarchies
Far from being neutral or purely aesthetic choices, glass buildings reflect a particular epistemological tradition rooted in Western architectural modernism. Originally designed for colder, low-light climates, these buildings were praised for their perceived energy efficiency, allowing ample daylight into interiors and reducing reliance on artificial lighting.
However, when this design is applied in tropical regions, it turns into an environmental contradiction. In places with intense sunlight, studies have shown that glass façades lead to significant indoor overheating and thermal discomfort, even with modern glazing. Rather than conserving energy, these buildings demand it to remain cool.
Yet glass façades have become the ubiquitous face of urban modernity, be it San Francisco, Jakarta or Lagos, regardless of climate or cultural context.
As climate change accelerates, these glass buildings are gleaming reminders of the dangers of knowledge homogenisation and epistemic hierarchies. Ironically, I’m writing this from inside one of those very buildings in Bengaluru in southern India. I sit in cooled air with the soft hum of the air conditioner in my ears. Outside, people saunter through a gentle drizzle. It looks like a normal monsoon afternoon – except the rains arrived weeks ahead of schedule this year, yet another sign of growing climate unpredictability.
In Bengaluru, I see yet another example of the impacts of lost knowledge: water management. How can a city flood severely in May, submerging cars, yet scramble for water even for domestic use in March? While this can be attributed to factors like poor planning and unchecked urbanisation, it also has deep epistemological roots.
A flooded road after heavy rains in Bengaluru, India, 22 October 2024. Photo by Sayan Hazra/Reuters
B engaluru was once celebrated for its smart water management system, fed by a series of interconnected cascading lakes. For centuries, these lakes were managed by dedicated communities, such as the Neeruganti community ( neeru means ‘water’ in the Kannada language), who controlled water flow and ensured fair distribution. Depending on the rains, they guided farmers on which crops to grow, often suggesting water-efficient varieties. They also handled upkeep: desilting tanks, planting vegetation to prevent erosion, and clearing feeder channels.
But with modernisation, community-led water management gave way to centralised systems and individual solutions like irrigation from far-off dams and borewells. The Green Revolution of the late 1960s added to this shift, pushing water- and fertiliser-heavy crops developed in Western labs. The Neerugantis were sidelined, and many moved on in search of other work. Local lakes and canals declined, and some were even built over – replaced with roads, buildings or bus stops.
The global history of imperialism and colonialism continues to shape how knowledge is produced and valued
Experts have realised that the key to saving Bengaluru from its water crisis lies in bringing these lake systems back to life. A social worker I spoke with, who’s been involved in several of these projects, said they often turn to elders from the Neeruganti community for advice. Their insights are valuable but their local knowledge is not written down, and their role as community water managers has long been delegitimised. Knowledge exists only in their native language, passed on orally, and is mostly absent from digital spaces – let alone AI systems.
While all my examples so far are drawn from India due to personal familiarity, such hierarchies are widespread, rooted in the global history of imperialism and colonialism. In her book Decolonising Methodologies (1999), the Māori scholar Linda Tuhiwai Smith emphasises that colonialism profoundly disrupted local knowledge systems – and the cultural and intellectual foundations upon which they were built – by severing ties to land, language, history and social structures. Smith’s insights reveal how these processes are not confined to a single region but form part of a broader legacy that continues to shape how knowledge is produced and valued. It is on this distorted foundation that today’s digital and GenAI systems are built.
I recently worked with Microsoft Research, examining several GenAI deployments built for non-Western populations. Observing how these AI models often miss cultural contexts, overlook local knowledge, and frequently misalign with their target community has brought home to me just how much they encode existing biases and exclude marginalised knowledge.
The work has also brought me closer to understanding the technical reasons why such inequalities develop inside the models. The problem is far deeper than gaps in training data. By design, LLMs also tend to reproduce and reinforce the most statistically prevalent ideas, creating a feedback loop that narrows the scope of accessible human knowledge.
Why so? The internal representation of knowledge in an LLM is not uniform. Concepts that appear more frequently, more prominently, or across a wider range of contexts in the training data tend to be more strongly encoded. For example, if pizza is commonly mentioned as a favourite food across a broad set of training texts, the model is more likely to respond with ‘pizza’ when asked ‘What’s your favourite food?’ Not because the LLM likes pizza, but because that association is more statistically prominent.
More subtly, the model’s output distribution does not directly reflect the frequency of ideas in the training data. Instead, LLMs often amplify dominant patterns in a way that distorts their original proportions. This phenomenon can be referred to as ‘mode amplification’. Suppose the training data includes 60 per cent references to pizza, 30 per cent to pasta, and 10 per cent to biriyani as favourite foods. One might expect the model to reproduce this distribution if asked the same question 100 times . However, in practice, LLMs tend to overproduce the most frequent answer. Pizza may appear more than 60 times , while less frequent items like biriyani may be underrepresented or omitted altogether. This occurs because LLMs are optimised to predict the most probable next ‘token’ (the next word or word fragment in a sequence), which leads to a disproportionate emphasis on high-likelihood responses, even beyond their actual prevalence in the training corpus. Together, these two principles – uneven internal knowledge representation and mode amplification in output generation – help explain why LLMs often reinforce dominant cultural patterns or ideas.
This uneven encoding gets further skewed through reinforcement learning from human feedback (RLHF), where GenAI models are fine-tuned based on human preferences. This inevitably embeds the values and worldviews of their creators into the models themselves. Ask ChatGPT about a controversial topic and you’ll get a diplomatic response that sounds like it was crafted by a panel of lawyers and HR professionals who are overly eager to please you. Ask Grok the same question and you might get a sarcastic quip followed by a politically charged take that would fit right in at a certain tech billionaire’s dinner party.
AI creates a feedback loop where dominant ideas are amplified while niche knowledge fades from view
Commercial pressures add another layer entirely. The most lucrative users – English-speaking professionals willing to pay $20-200 monthly for premium AI subscriptions – become the implicit template for ‘superintelligence’. These models excel at generating quarterly reports, coding in Silicon Valley’s preferred languages, and crafting emails that sound appropriately deferential to Western corporate hierarchies. Meanwhile, they stumble over cultural contexts that don’t translate to quarterly earnings.
It should not come as a surprise that a growing body of studies shows how LLMs predominantly reflect Western cultural values and epistemologies . They overrepresent certain dominant groups in their outputs, reinforce and amplify the biases held by these groups, and are more factually accurate on topics associated with North America and Europe. Even in domains such as travel recommendations or storytelling, LLMs tend to generate richer and more detailed content for wealthier countries compared with poorer ones. There are at least 50 more similar studies I could cite here.
And beyond merely reflecting existing knowledge hierarchies, GenAI has the capacity to amplify them, as human behaviour changes alongside it. The integration of AI overviews in search engines, along with the growing popularity of AI-powered search engines such as Perplexity, underscores this shift. A recent study found that US Google users were less likely to click on search results when an AI summary appeared alongside, indicating a change in people’s behaviour. Previously, people had to browse multiple links to compare viewpoints and gather comprehensive information. Now, they can read AI-generated summaries.
As AI-generated content has started to fill the internet, it adds another layer of amplification to ideas that are already popular online. The internet, as the primary source of knowledge for AI models, becomes recursively influenced by the very outputs those models generate. With each training cycle, new models increasingly rely on AI-generated content, reinforcing prevailing narratives and further marginalising less prominent perspectives. This risks creating a feedback loop where dominant ideas are continuously amplified while long-tail or niche knowledge fades from view.
The AI researcher Andrew Peterson describes this phenomenon as ‘knowledge collapse’, a gradual narrowing of the information humans can access, along with a declining awareness of alternative or obscure viewpoints. As LLMs are trained on data shaped by previous AI outputs, underrepresented knowledge can become less visible – not because it lacks merit, but because it is less frequently retrieved or cited. Peterson also warns of the ‘streetlight effect’, named after the joke where a person searches for lost keys under a streetlight at night because that’s where the light is brightest. In the context of AI, this would be people searching where it’s easiest rather than where it’s most meaningful . Over time, this would result in a degenerative narrowing of the public knowledge base, driven not by censorship but convenience and algorithms. Across the globe, GenAI is also becoming part of formal education, used to generate learning content and support self-paced education through AI tutors. For example, the Karnataka state government, home to the city of Bengaluru, has partnered with the US-based non-profit Khan Academy to deploy Khanmigo, an AI-powered learning assistant, into schools and colleges. I would be surprised if Khanmigo holds the insights of elder Neerugantis, grounded in local knowledge and practices, needed to teach school students in Karnataka how to care for their own water ecologies.
All this means that, in a world where AI increasingly mediates access to knowledge, future generations might lose connection with vast bodies of experience, insight and wisdom . AI developers might argue that this is simply a data problem, solvable by incorporating more diverse sources into training datasets. While that might be technically possible, the challenges of data sourcing, prioritisation and representation are far more complex than such a solution implies.
A conversation I had with a senior leader involved in designing and overseeing the development of an AI chatbot that serves more than 8 million farmers across four countries in Asia and Africa brought this into focus. The system provides agricultural advice based mostly on databases from government advisories and international development organisations, which tend to rely on research literature. The senior leader acknowledged how many local practices that might be effective are still excluded from the chat responses since they are not documented in the research literature.
The rationale isn’t that research-backed advice is always right or risk-free. It’s that it offers a defensible position if something goes wrong. In a system this large, leaning on recognised sources is seen as the safer bet, protecting an organisation from liability while sidelining knowledge that hasn’t been vetted through institutional channels. So the decision is more than just technical. It’s a compromise shaped by the structural context, not based on what’s most useful or true.
While GenAI may be accelerating the erasure of local knowledge, it is not the root cause
This structural context doesn’t just shape institutional choices. It also shapes the kinds of challenges I heard about in my conversation with Perumal Vivekanandan, founder of the non-profit organisation Sustainable-agriculture and Environmental Voluntary Action (SEVA). His experiences highlight the uphill battle faced by those working to legitimise Indigenous knowledge.
Formed in 1992, SEVA focuses on preserving and disseminating Indigenous knowledge in agriculture, animal husbandry and the conservation of agricultural biodiversity. Over the years, Vivekanandan has documented more than 8,600 l ocal practices and adaptations, travelling village to village.
Still, the work constantly runs into systemic roadblocks. Funders often withhold support, questioning the scientific legitimacy of the knowledge SEVA seeks to promote. When SEVA turns to universities and research institutions to help validate this knowledge, they often signal a lack of incentives to engage. Some even suggest that SEVA should fund the validation studies itself. This creates a catch-22: without validation, SEVA struggles to gain support; but without support, it can’t afford validation. The process reveals a deeper challenge: finding ways to validate Indigenous knowledge within systems that have historically undervalued it.
SEVA’s story shows that while GenAI may be accelerating the erasure of local knowledge, it is not the root cause. The marginalisation of local and Indigenous knowledge has long been driven by entrenched power structures. GenAI simply puts this process on steroids.
W e often frame the loss of Indigenous knowledge as a tragedy only for the local communities who held it. But ultimately, the loss is not just theirs to bear, but belongs to the world at large too.
In our social structures, we may assign hierarchical value to certain communities or types of knowledge, but natural ecology reveals a different logic. Every local element plays a vital role in sustaining global balance. As the forester Peter Wohlleben illustrates in The Hidden Life of Trees (2015), natural systems are deeply interdependent, often in ways that are invisible to the casual observer. He offers a powerful example from Yellowstone National Park in the US. When wolves were eradicated from the park in the early 20th century , it led to a series of unexpected ecological consequences. Without wolves to keep their numbers in check, deer populations exploded. The deer overgrazed vegetation and altered the landscape. Riverbanks eroded, tree growth stalled, and the broader ecosystem suffered. When wolves were reintroduced decades later, the system began to heal. Vegetation rebounded, songbirds returned, and even the behaviour of rivers changed.
We are enmeshed in shared ecological systems where local wounds inevitably become global aches
Wohlleben’s broader point is that the health of a system depends on the presence of all its parts, even those that might seem inconsequential. The same principle applies to human knowledge. The disappearance of local knowledge is not a trivial loss. It is a disruption to the larger web of understanding that sustains both human and ecological wellbeing. Just as biological species have evolved to thrive in specific local environments, human knowledge systems are adapted to the particularities of place. When these systems are disrupted, the consequences can ripple far beyond their point of origin.
Wildfire smoke doesn’t care about transgressing zip codes. Polluted water doesn’t pause at state lines. Rising temperatures ignore national borders. Infectious germs don’t have visa waiting periods. Whether we acknowledge it or not, we are enmeshed in shared ecological systems where local wounds inevitably become global aches.
T he biggest contradiction for me in writing this essay is that I’m trying to convince readers of the legitimacy and importance of local knowledge systems while I myself remain unconvinced about my dad’s herbal concoctions. This uncertainty feels like a betrayal of everything I’ve argued for. Yet maybe it’s exactly the kind of honest complexity we need to navigate.
I have my doubts about whether Indigenous knowledge truly works as claimed in every case. Especially when influencers and politicians invoke it superficially for likes, views or to exploit identity politics, generating misinformation without sincere enquiry. However, I’m equally wary of letting it disappear. We might lose something valuable, only to recognise its worth much later – perhaps with the aid of artificial superintelligence. But what’s the collateral damage of that process? An ecological collapse we could have prevented?
The climate crisis is revealing cracks in our dominant knowledge paradigms. Yet at the same time, AI developers are convinced that their technology will accelerate scientific progress and solve our greatest challenges. I really want to believe they’re right. But several questions remain: can we move towards this technological future while authentically engaging with the knowledge systems we’ve dismissed, with genuine curiosity beyond tokenism? Or will we keep erasing forms of understanding through the hierarchies we’ve built, and find ourselves scrambling to colonise Mars because we never learned to listen to those who knew how to live sustainably on Earth?
Maybe the intelligence we most need is the capacity to see beyond the hierarchies that determine which knowledge counts. Without that foundation, regardless of the hundreds of billions we pour into developing superintelligence, we’ll keep erasing knowledge systems that took generations to develop.
I don’t know if my dad’s herbal concoctions worked. But I’m learning that acknowledging that I don’t know might be the most honest place to start.
Sincere gratitude to my PhD advisors Aditya Vashistha and Rene Kizilcec, my collaborators at Microsoft Research, and my most amazing friends without whom this essay wouldn’t have been possible."
"No suffering, no death, no limits: the nanobots pipe dream | Aeon Essays",,https://aeon.co/essays/no-suffering-no-death-no-limits-the-nanobots-pipe-dream,"In 2000, Bill Joy, the co-founder and chief scientist of the computer company Sun Microsystems, sounded an alarm about technology. In an article in Wired titled ‘Why the Future Doesn’t Need Us’, Joy wrote that we should ‘limit development of the technologies that are too dangerous, by limiting our pursuit of certain kinds of knowledge.’ He feared a future in which our inventions casually wipe us from the face of the planet.
The concerns expressed in Joy’s article, which prompted accusations of Luddism from tech advocates, sound remarkably similar to those now being voiced by some leaders in Silicon Valley that artificial intelligence might soon surpass us in intelligence and decide we humans are expendable. However, while ‘sentient robots’ were a part of what had spooked Joy, his main worry was about another technology that he figured might make that prospect imminently possible. He was troubled by nanotechnology: the engineering of matter at the scale of nanometres, comparable to the size of molecules.
In fact, it would be more accurate to say Joy was troubled by the version of nanotechnology that he had read about in the book Engines of Creation (1986) by the engineer K Eric Drexler, a graduate of the Massachusetts Institute of Technology. At the close of the 20th century, it was nanotechnology, not AI (which didn’t seem to be getting very far), that loomed large as the enabler of utopias and dystopias. Drexler’s book described a vision of nanotech that could work wonders, promising, in Joy’s words, ‘incredibly low-cost solar power, cures for cancer and the common cold’ as well as ‘[low-cost] spaceflight … and restoration of extinct species.’
B ut Joy had learnt from the inventor Ray Kurzweil (now a scientific adviser to Google) that Drexler’s nanotech promised something yet more remarkable: the singularity, a point at which our accelerating technological prowess reaches escape velocity and literal marvels become possible – in particular, immortality through the merging of human and machine, so that we could upload our minds to computers and live forever in a digital nirvana.
‘[N]anotechnology-based manufacturing devices in the 2020s will be capable of creating almost any physical product from inexpensive raw materials and information,’ Kurzweil wrote in his book The Singularity Is Near (2005). The technology ‘will provide tools to effectively combat poverty, clean up our environment, overcome disease, extend human longevity, and many other worthwhile pursuits.’
But, Joy learned, there was a downside to all this. Drexler’s nanotechnology could get out of hand, unleashing swarms of invisibly tiny nano-robots that blindly start pulling everything apart, atom by atom, until they have reduced the world to what Drexler called ‘grey goo’. In the late 1990s, the grey-goo problem was the golem that, like ‘superintelligent AI’ today, might bring about our hubristic downfall.
You might have noticed that none of this has happened. No cures for cancer, no mind-uploading immortality, but no grey goo either. This is because Drexler’s vision of nanotechnology was a chimera. It was like the philosophers’ stone of the alchemists: magic dressed in the science of its time, by means of which almost anything becomes possible. I call these oneiric technologies : they do not and quite probably cannot exist, but they fulfil a deep-rooted dream, or a nightmare, or both.
These techno-fantasies are central to the utopias regularly forecast by tech billionaires
These are not simply technologies of the future that we don’t yet have the means to realise, like the super-advanced technologies that Arthur C Clarke said we would be unable to distinguish from magic. Rather, oneiric technology takes a wish (or a terror) and clothes it in what looks like scientific raiment so that the uninitiated onlooker, and perhaps the dreamer, can no longer tell it apart from what is genuinely on the verge of the possible. Perpetual motion is one of the oldest oneiric technologies, although only since the 19th century have we known why it won’t work (this knowledge doesn’t discourage modern attempts, for example by allegedly exploiting the ‘quantum vacuum’); anti-gravity shielding is probably another.
The oneiric technologies currently in vogue in Silicon Valley include the notion of terraforming other planets, transforming their geosphere and atmosphere to render them inhabitable; cryonic freezing of your head after death so that your consciousness can one day be rebooted; and the related idea of mind-uploading to computer circuits. These techno-fantasies are central to the utopias regularly forecast by tech billionaires. They interconnect in a nexus to which Drexlerian nanotechnology is central.
It is worth looking into that particular dream, not just because of the parallels with the fantastical claims and fears about AI today, but because even now Drexlerian nanobots have not gone away. Kurzweil still cites them as the reason why his singularity is even ‘nearer’ – in 2024, he put it at 2045, at which point it will be possible to go (that is, to send nanobots) ‘inside the brain and capture everything in there.’ This implausible form of nanotechnology is still a part of Silicon Valley’s magical thinking, the aim of which, as the science writer Adam Becker says in his book More Everything Forever (2025), is to ‘tame the universe, to make it into a padded playground.’ No suffering, no death, no physical limits: a paradise shaped by ultra-libertarian – some say quasi-fascist – politics in which no one will tell you that anything is forbidden or impossible.
A n enthusiast of 1970s dreams of space colonisation, Drexler began thinking about nanotechnology as an undergraduate at MIT in 1977. He was inspired by a talk by the physicist Richard Feynman titled ‘There’s Plenty of Room at the Bottom’ (1959), in which Feynman imagined engineering at extremely tiny scales too small for the eye to see – and perhaps as small as could be conceived. ‘What would happen if we could arrange the atoms one by one the way we want them?’ Feynman asked. What indeed, Drexler wondered.
In 1981, he published his core vision in the academic paper ‘Molecular Engineering: An Approach to the Development of General Capabilities for Molecular Manipulation’. But it was his book Engines of Creation five years later, a popular and non-technical account of what this capability might lead to, that made Drexler the toast of tech entrepreneurs.
Drexler imagined making a ‘molecular assembler’, a mechanical device for grabbing atoms and pushing them together like Lego bricks. It might sound absurdly difficult to achieve that level of control and precision, but Drexler argued that we already have proof of its possibility. For isn’t this just what biology does, using machines made from protein molecules to read assembly instructions encoded in DNA and turn them into the parts of living cells? You might wonder what kind of practical manufacturing could be usefully done with a molecular-scale machine, but the key to Drexler’s vision was a progressive scaling up: small machines make bigger ones, which make bigger ones. Scaling up would also borrow another trick from nature: these molecular machines would be self-replicating, being able to assemble copies of themselves. You need only make one, and it can multiply exponentially. After publishing Engines of Creation , Drexler obtained his doctorate at MIT under the supervision of the AI guru Marvin Minsky. His thesis furnished a more technical book, Nanosystems (1992), intended as the scientific blueprint for realising the wonders promised in Engines .
Kurzweil imagines nanobots whizzing around in our heads reading the electrical states of neurons
With atomic-scale manipulation and assembler replication, said Drexler, we could create anything – not, as we currently do, through crude chemical processing or the laborious ‘top down’ etching and carving needed to fashion miniaturised devices such as silicon chips but from the bottom up, atom by atom. And whereas nature’s machines – proteins – are delicate and liable to fall apart if they get too warm or cold, molecular assemblers can be made of sterner stuff. Ideally, we would make them from carbon atoms, each component then in effect a little shaped piece of pure diamond. These ‘diamondoid’ nanomachines – nanoscale robotic arms, pincers, rotors, filters and so forth – wouldn’t break or corrode, and ‘will be able to build virtually anything that can be designed.’ In his book Where Is My Flying Car? (2021), Drexler’s associate and advocate, the computer scientist and writer J Storrs Hall, estimated that with nanoassemblers one could recreate the entire physical infrastructure of the United States – roads, bridges, cities – in a single week.
Drexler’s nanotech dream, with nanoscale robots patrolling our bloodstream, killing off pathogens and stripping sclerotic deposits from the blood vessel walls, seemed just what Kurzweil needed in order to realise his aspiration of escaping death. Such nanotechnology, Kurzweil has said, ‘promises the tools to rebuild the physical world – our bodies and brains included.’ Kurzweil imagines nanobots whizzing around in our heads reading the electrical states of neurons and thereby collecting all the information held within our neural circuitry, broadcasting it to detectors to create a virtual replica of our memories and thoughts – a digital clone of our conscious state, which would experience itself as no different from the meatspace version of us. Science-fiction writers loved it. Neal Stephenson based his novel The Diamond Age (1995) on a steampunk version of Drexlerian ‘matter compilers’, namechecking Drexler and Feynman in the book.
Scientists, however, were not so enamoured. The US chemist Julius Rebek told Scientific American in 2004: ‘This is not science – it’s show business.’ The chemistry Nobel laureate Richard Smalley engaged in an exchange of letters with Drexler published in 2003 in the American Chemical Society’s house magazine Chemical & Engineering News , in which Smalley insisted that Drexler’s idea was chemically illiterate.
The idea of manipulating atoms one by one was not itself inherently crazy. During the 1980s, scientists at IBM’s research labs developed so-called scanning probe microscopes that move ultrafine metal needles over surfaces to create images of individual atoms and molecules sitting on them. In 1989, an IBM team in California used such an instrument to write out the company’s name in letters five nanometres tall, using the tip to nudge, individually, 35 atoms of the element xenon into position on the surface of nickel.
Nor was Drexler wrong to think that chemical assembly at the molecular scale is possible. In the past several years, another IBM team in Switzerland has made rather complex single molecules – some of them difficult to create by conventional chemical methods – by pushing their fragments together on a surface with a scanning probe microscope until they react and join up.
W ork like this makes it rather easy to present Drexlerian nanotechnology as plausible. But there are several key problems with using such an approach to synthesise molecular assemblers that can replicate and build anything. First, chemistry is not arbitrary: you can’t put atoms together any old how. Most arrangements are simply not stable, and so they will spontaneously rearrange into more stable ones. Getting rid of the energy released when a new chemical bond is made can be a big challenge too. And perhaps most of all, treating molecular objects as though they were just scaled-down engineering devices – rotary bearings, levers, clasps and so on – ignores the realities of the molecular world, which is full of strong and uncontrollable forces between molecules and random, pervasive shaking because of heat energy, and where liquids seem as viscous as molasses. As the late Scottish chemist James Fraser Stoddart, who won the 2016 Nobel Prize for his work on artificial molecular machines, told Becker: ‘The whole idea of extrapolating from the macroscopic world, from a car or a bicycle or something like that, down to the fundamentals of how you construct artificial molecular machines just makes no sense. It’s never going to work.’
Stoddart’s Nobel-winning work, in contrast, was firmly based on regular, known chemistry. He and others have found ingenious ways to link up molecules into structures that can carry out mechanical operations without needing to suspend physical and chemical laws. Stoddart’s breakthrough invention was a ‘molecular shuttle’ in which a ring-shaped molecule was threaded on a rod-like molecular axle capped with bulky chemical groups that prevent unthreading. The ring can jump between two docking positions on the axle, a little like an abacus bead. It’s fun to imagine such a molecular assembly being used to do abacus-style computation – but, as Stoddart knew, in practice it would be nigh-impossible to prevent the ring from making spontaneous jumps because of its thermal movements, so that the average setting of many such shuttles will be determined not by where we initially put them but by the statistical laws of thermodynamics. It’s the same story for the ‘biomolecular machines’ that were Drexler’s inspiration: they don’t work like nanoscale versions of electric motors or robot arms, but are governed by thermodynamic laws that inject random noise into their behaviour. At the molecular scale, nature is not much like mechanical engineering at all.
Drexler tried to head off such criticism in Nanosystems , arguing that his critics were merely coming up with badly designed nanotechnology and then dismissing the whole field of ‘molecular manufacturing’ because of that bad design. If he seemed to make light of experimental difficulties, he said: ‘I can only plead that it would soon become tedious to say, at every turn, that laboratory work is difficult, and that the hard work is yet to be done.’ Sure, it’ll be hard, but we’ll manage it somehow.
The grey-goo problem made nanotechnology a bête noir of techno-sceptics and environmentalists
Yet Nanosystems exemplified the strategy of oneiric technologists. You start from what looks like sound science – Drexler talks about thermal motion, chemical bonds, intermolecular forces. But you move almost imperceptibly into sheer fantasy, all the while ramping up the excitement of the impressionable reader. The second half of the book presents devices such as molecular sorters – wheels that separate different types of atoms or molecules – alongside molecular conveyor belts, robotic arms, and sets of interlocking gears. There are nanoscale mechanical computers made from moving rods, effectively miniaturised versions of Charles Babbage’s Analytical Engine, his steampunk design for a general-purpose calculating machine, which he hoped to create out of brass components in the 19th century. There are, by this point, no longer any molecules in sight: we’re asked to assume these wondrous machines have all been somehow fashioned and assembled from diamondoid pieces, even though no one had ever made anything of the sort.
Maybe it’s all just as well that this is fantasy, because then so is the grey-goo problem. In this scenario of nanotech gone rogue, nanobot molecular assemblers escape our control and replicate without check, pulling apart every scrap of matter they can lay their nano-hands on and refashioning it into more of themselves. Each nanobot being smaller than a dust grain, the world then ends up being disassembled – at frightening speed, if you believe the calculations – into featureless mush.
The grey-goo problem made nanotechnology – which many assumed would be created in Drexler’s model – a bête noir of techno-sceptics and environmentalists in the 1990s. Among them was Prince Charles, now the British monarch, who voiced concerns in 2003 that prompted the Royal Society to produce a report on the benefits and risks of nanotech that barely mentioned Drexler, and attempted to steer the discussion back to the actual science. The grey-goo narrative was, however, far too good for science-fiction writers to resist: Michael Crichton of Jurassic Park fame got there first with his thriller Prey (2002).
The grey-goo nightmare might sound familiar to aficionados of AI ‘existential risks’, for it is the forerunner to the philosopher Nick Bostrom ’s ‘paperclip problem’. Imagine, Bostrom said in 2003, we design an all-powerful, superintelligent AI that we assign the task of making paperclips. (It would be hard to imagine a more ridiculous use of such a powerful technology, but that’s not the point – or rather, the sheer banality is part of the point.) The AI might decide that its assigned task is so important that it will stop at nothing to make more paperclips. And because we have given it such power and ingenuity, it will outwit any efforts of ours to divert it from that goal – and will promptly turn everything, including us, into paperclips. Bostrom’s point was that it would be extremely hard, perhaps impossible, to ensure that such a superintelligent AI has goals that remain aligned with ours. ‘The future that the AI would be trying to gear towards would be one in which there were a lot of paperclips but no humans,’ he told HuffPost in 2014.
Bostrom’s scenario has been much debated, but its central problem is simply stated. Like grey goo, it involves an oneiric technology. Can’t we just switch it off? No, it is un-switch-off-able. If it’s superintelligent, won’t it figure out that we don’t want to become paperclips? No, it is superintelligent enough to be unstoppable but not enough to figure that out. And how does it turn everything into paperclips anyway? It can pull everything apart into atoms and then reassemble them at will: it is Drexlerian! Magically, it has precisely those capabilities and flaws the scenario demands, granted by unspecified (but capital-S) Science.
M ore than three decades after Nanosystems was published, Drexler’s nanotechnology is not one nanometre closer. It’s not that making a replicating diamondoid molecular assembler proved rather harder than Drexler imagined. It’s that there was never any real programme for how that could be done, nor any reason to think it was possible. Not a single carbon atom has been put in place in the attempt. No scientist has deemed it worth even trying.
Yet nanotechnology itself is now a mature science. Nanometre-scale pieces of matter of all descriptions can be assembled using chemical processes and are used in areas ranging from photovoltaic cells to biomedical imaging techniques. As well as Stoddart’s Nobel for synthetic molecular machines, the 2023 Nobel Prize in chemistry was awarded for work on nanometre-scale clusters of atoms called ‘quantum dots’ that have a panoply of applications from biomedicine to information technologies. Chemists have figured out how to ‘program’ strands of DNA so that they will fold up spontaneously, origami-style, into complex shapes and patterns smaller than a bacterium, including a minuscule map of the Americas. Scanning probe microscopes are now regularly used as tools for molecular manipulation. Ultra-strong and hollow tubes of carbon a nanometre or so wide, discovered in 1991, are the ultimate carbon fibres and have been widely used in biomedical devices, wearable electronics and tough composite materials. The one-atom-thick carbon material graphene is another star of such ‘carbon nanotechnology’, of which Smalley was a pioneer. DNA sequencing, such as that used to track new variants of the COVID-19 virus, is now often done by dragging the strands through nanometre-scale protein pores embedded in membranes, a method developed by the company Oxford Nanopore. None of this work, however, uses anything like the approach Drexler advocated. Rather, it depends on chemistry as we have always known it.
That’s not to say Drexler’s vision was worthless. Indeed, it helped to stimulate early interest in the field, and even Smalley attested that he was initially excited by the possibilities it sketched for engineering with matter on tiny scales. Drexler attracted enough venture capital to establish in 1986 an organisation called the Foresight Institute, based in San Francisco, that today continues to offer grants and support to research on conventional nanotechnology and to award prizes (named after Feynman) to leading scientists working in the field. The institute organises conferences that attract many respectable scientists, working on topics such as protein design, which won the 2024 Nobel Prize in chemistry. At first sight, the Foresight Institute seems to have quietly set aside Drexler’s own oneiric version of nanotech.
None of this is moored to current technologies, but requires essentially magical inventions
But has it? The institute’s logo remains one of Drexler’s imaginary diamondoid gearwheels. It says it is now supporting work in neurotechnology, longevity biotechnology, space and ‘existential hope’. For anyone alert to the oneiric technologies of techno-utopias and dystopias, these are red flags. Neurotech – think of Elon Musk’s much-hyped Neuralink initiative for hooking up brains to machines – connects to the fantasy of mind-uploading, which Musk believes is possible. ‘We could download the things that we believe make ourselves so unique,’ he said in an interview in 2022. ‘As far as preserving our memories, our personality, I think we could do that.’ Musk says that a long-term goal for Neuralink is to ‘store your memories as a backup.’ These ideas, it’s important to recognise, are not to be confused with ambitious extrapolations of current scientific capabilities; they aren’t even coherent concepts.
Longevity? Drexler became closely associated with the community who called themselves Extropians, a reference to the idea that we can impose ever more order and design (extropy) on the Universe rather than surrendering to the dissolution of entropy seemingly demanded by the second law of thermodynamics. Extropianism has a huge overlap with transhumanism, the idea that we humans can transcend ourselves with technological help, eventually merging with machines or totally redesigning the human form.
Space? It’s not about making better telescopes or robotic spacecraft. Techno-utopians like Musk, Jeff Bezos and the influential software engineer and venture capitalist Marc Andreessen believe in the manifest destiny of humankind’s colonisation of space . As Becker explains in More Everything Forever , Kurzweil envisages sending out fleets of Drexlerian replicating nano-robots that transform planets and, ultimately, turn the entire accessible universe into a gigantic supercomputer with ‘exquisitely sublime forms of intelligence’. Again, none of this is moored to current technologies, but requires essentially magical inventions.
Existential hope? Here the Foresight Institute directs you to the Abundance and Growth Fund of the philanthropic funding and advising organisation Open Philanthropy in San Francisco, which aims ‘to accelerate economic growth and boost scientific and technological progress’ and to oppose ‘(even well-intentioned) governmental regulation’ that slows progress. In other words, this concept of ‘existential hope’ is entrained with the kind of ultra-libertarian, anti-regulation project envisaged by Andreessen, Musk and other tech billionaires.
Notably missing from such utopian goals is any mention of climate change, or threats to democracy, or arms proliferation, or corporate profiteering, or indeed any of the urgent problems facing the world here and now. Such issues don’t interest oneiric technologists, because there is nothing transcendent about them. They do not speak to immortality, to endless growth, to galactic futures, to that padded playground universe. Bill Joy summed up the matter in his 2000 article. ‘I remember feeling good about nanotechnology after reading Engines of Creation ,’ he wrote. ‘If nanotechnology was our future, then I didn’t feel pressed to solve so many problems in the present. I would get to Drexler’s utopian future in due time; I might as well enjoy life more in the here and now.’
We don’t have to buy the myths of oneiric technologies
What messed it up for Joy was not the fact that Drexlerian nanotechnology was a pipe dream (as any number of well-informed scientists could have told him). Like all tech barons, he stayed within the club, conversing with Kurzweil (‘In the hotel bar, Ray gave me a partial preprint of his then-forthcoming book The Age of Spiritual Machines ,’ says Joy, attesting to no flicker of misgiving at that title) and with the robotics futurist Hans Moravec (book title: Robot: Mere Machine to Transcendent Mind ). Joy learnt about grey goo, and it made him think about Hiroshima, and his vision of an imaginary utopia turned to one of imaginary apocalypse.
Joy was commendably trying to do the right thing: to think ethically about powerful technologies. But he lacked the resources to know what to be excited by and what to fear. Here’s what I mean. Joy had made his fortune through his role in inventing world-changing computer tech. Meanwhile, when I wrote a critical review of Nanosystems as an editor of Nature in 1993, I was a mere five years from having completed my PhD and still wet behind the ears. How come I could tell its vision was going nowhere, yet Joy couldn’t? It was most certainly not because I was some kind of prescient wunderkind. It was not because of who I was but because of who I wasn’t. My social circle wasn’t other tech leaders; I wasn’t hanging out in the bar with Kurzweil; I wasn’t in the oneiric Silicon Valley bubble. Rather, I was lucky enough to have benefitted instead from contact with scientists doing benchtop research, with the likes of Smalley and Stoddart.
With AI, we are doing all this again. We are accepting the fantastical prophecies of the likes of Google’s former CEO Eric Schmidt, who has forecast that ‘within three to five years we’ll have … [artificial] general intelligence, which can be defined as a system that is as smart as the smartest mathematician, physicist, artist, writer, thinker, politician’ (the ‘smartest artist’ being a concept that apparently means something within Silicon Valley). With no trace of irony, Schmidt adds that ‘I call this … the San Francisco consensus, because everyone who believes this is in San Francisco.’ As part of the package, we are then asked to accept not only the fantastical dreams of this community but also their eschatology, in which a machine superintelligence wipes us out. We are entranced by such ‘existential risk’ discourse when it comes from a Musk, a Bezos, or others who have fallen into the orbit of the San Francisco consensus. And if we take their dream, we have to take their nightmare too.
But we don’t have to. We don’t have to buy the myths of oneiric technologies. We can take a look at what happened to Drexlerian nanotechnology and diagnose the warning signs. We can choose to refuse that distraction, to heed humble experts over media-anointed geniuses. It’s perhaps not as exciting, it lacks any chiliastic frisson, and it might require us to think about boring risks and mundane regulation of research instead of science fantasy. But that’s where we live."
Are humans destined to evolve into crabs? | Aeon Essays,,https://aeon.co/essays/are-humans-destined-to-evolve-into-crabs,"No one paid much attention when an obscure biology journal published an article in 2017 about ‘the evolutionary processes which led to a crab-like habitus’. In this review of crustacean taxonomy, three scientists from a German university – Jonas Keiler, Christian S Wirkner and Stefan Richter – analysed studies dating back to the 1820s and noted that ‘crabs’ were not one kind of animal but instead had arisen from five different evolutionary lineages. That is, creatures such as hermit crabs, squat lobsters and other ‘crab-like representatives’ did not evolve from the same ancestor: they independently developed crabby qualities. To explain this fairly banal case of convergent evolution, the three researchers picked a century-old noun, ‘carcinisation’, and concluded that ‘there is no reason to assume that “evolutionary tendencies” or any such vague concept played a role.’
But, within three years, a wave of coverage had taken those cautious claims and turned them hyperbolic. Websites led with headlines so wrong as to be ridiculous: ‘Animals Keep Evolving Into Crabs, and Scientists Don’t Know Why’ ( Newsweek ). Videos asked : ‘Why Do Things Keep Evolving into Crabs?’ Soon, ‘everything becomes crab’ had emerged as the favourite (if counterfactual) ‘law of evolution’ in internet meme culture. And the real science was buried far below the fold.
Crab memes
Despite what the headlines claimed, none of this was ‘news’. The concept of carcinisation had been understood for more than a century. And the transformation of the animals in question, all members of the ‘false crabs’ group Anomura (sisters of the ‘true crabs’ Brachyura ), happened tens of millions of years ago. The sensational reporting and spread of carcinisation memes provoked one rationalist critic to insist: ‘You are not an intertidal scavenger and, more importantly, you have an internal skeleton, so you are not going to evolve into a crab.’
But I would beg to differ, point by point. Stretch the definition of ‘crab’ far enough and this nonsense might, in fact, be true. As individuals, we may not have pincers, segmented bodies, chitinous exoskeletons, compound eyes or a dorsal heart that pumps haemolymph (the equivalent of blood) through an open circulatory system. But, as a collective, things look very different.
Firstly, humans do live in something like an intertidal zone: the turbulence and inescapable betweenness of our lives as we move in and out of the ‘virtual’ world. And, secondly, we encase ourselves in exoskeletons more literally every day as we become increasingly supported and defined by our technologies. If we recognise ‘intertidal scavenger’ and ‘skeleton’ to be analogies for our present-day condition in late-stage capitalism, and view civilisation as an ‘extended phenotype’ from which we cannot extract ourselves, we will find that we are already much more like crabs than we might assume.
Humans seem to be deeply attracted to these armoured animals. After all, the crab is a deep-dwelling symbol that emerges time and time again in stories scattered over centuries and continents. It is a monstrous reflection of our own minds, from whose claws we can pry vital insight into how our species navigates the boundaries of human and nonhuman, life and technology, the mapped familiar and the frightening (but beckoning) unknown.
But our long fascination is just the tip of the iceberg. In the past few centuries, human carcinisation has begun to speed up as our collective evolution increasingly mimics the deep-time transformation of crabs. Crab-like forms are evolutionarily beneficial: they are optimised for a demanding niche. But so are we. Consider how trade and communication ‘carcinise’ our squishy mammal interfaces, pixelating analogue reality into interoperable digital standards. Consider how local languages, currencies and cultures dissolve when pegged to global markets. Consider the banal horror of reducing your identity to state-issued IDs or your occupation.
Are we destined to become even more crab-like? Or does a careful study of evolution offer us alternatives?
U sing myth as data, we find robust historical support for crab-ward trends. In The Time Machine (1895), H G Wells makes crabs ‘as large as yonder table’ the destiny of life on Earth, and the only residents of an otherwise lifeless future wasteland. This might be the novel’s least original idea: edge-dwelling crabs are everywhere in myths throughout the ancient world, including Karkinos (who joined the Hydra to fight Herakles in ancient Greece), Nkala (a sorcerous familiar from Zambia that eats people’s shadows), Tambanokano (a Philippine deity that controls lightning) and the Kraken of Norse legend (though often depicted as a giant octopus, an early form of this sea monster was likely a crustacean). Crabs do not just mark the margins of our ancient surveys: they claim extensive real estate in science fiction. Even with our oceans mapped, our stories sail through space and dredge up pincered Others.
Attack of the Crab Monsters (1957) poster. Courtesy Luke Sheppard/Flickr
In the mid-20th century, Cold War crabs emerged on both sides of the Iron Curtain. The American director Roger Corman’s film Attack of the Crab Monsters (1957) showed explorers finding an island full of super-sized radioactive shellfish that eat people’s minds. One year later, the Soviet sci-fi author Anatoly Dneprov’s short story ‘ Крабы идут по острову ’ (‘Crabs Take Over the Island’) was published, in which self-replicating robot crabs run evolution in fast-forward to create a perfect weapon. Following the ‘arms race’ theme, power armour in the form of crab-like exoskeletons amplifies human protagonists battling pincered enemies in the Japanese anime Gundam (1979-), the Nintendo game series Metroid (1986-) and countless other tales. Doctor Who , which first aired in 1963 and is now the longest running sci-fi series on TV, positively overflows with hard-shelled, pincered villains, such as Daleks, Macra, Cybermen and Martians – all echoing the armoured invaders in Wells’s The War of the Worlds (1898). These robotic monsters are often revealed as soft, helpless creatures locked inside hard shells, yet they always seem to be described as ‘the next step in evolution for mankind’ or, at least, superior.
In such stories, crabs are a symbol of efficient, brutal evolution at its simplest
In the iconic final moments of James Cameron’s sci-fi thriller Aliens (1986), we see what this kind of progress might look like for humans when our hero Ellen Ripley dons a mechanical power-loader with pincers to confront the vicious Alien Queen on her own terms. Their epic claw-to-claw clash shows mother figures locked in technologically assisted combat: Ripley with the help of military hardware, and the monstrous xenomorph as a living weapon in her own right. In order to destroy the ‘perfect organism’, Ripley must become one. Or is this logic of inevitability just the fantasy of an action movie?
Sigourney Weaver as Ellen Ripley in Aliens (1986). Courtesy 20th Century Fox
The absurd notion that crabs are the ‘final form’ – the ‘all roads lead to Rome’ for natural selection – has such a pincer-hold on popular imagination that it powered the English author Guy N Smith’s entire career, which boasted eight giant-crab pulp horror novels, including Night of the Crabs (1976) and Crabs on the Rampage (1981), in which towering crustaceans wage war on England after they were irradiated in a nuclear accident. In such stories, crabs are a symbol of efficient, brutal evolution at its simplest, lacking any of the costly art or empathy that make us human.
Night of the Crabs (1976) by Guy N Smith, cover artist unknown
Crabs on the Rampage (1981) by Guy N Smith, cover artist unknown
We can partially attribute the enduring trope of the ‘combat crab’ through history to our fascination with the ‘alien’ anatomy of ocean life. The brain loves novelty, and maybe we are just inherently attracted to exotic physiologies. But crabs are not the only weirdos in our speciose imaginal menageries. Might there be another, more explanatory reason for our interest?
Perhaps crustaceans, with their armour, pincers, eyestalks and high strangeness, help us Homo sapiens make sense of how we wrap our fragile flesh in leverage-enhancing tools. Perhaps they also help us understand how our symbiosis with these tools gradually withers our ancestral human traits. The crab is both the Other that endangers us and the means by which we redefine the human as an adaptation to that danger.
I n a new environment, you have two options: you can either change yourself or your situation. Biologists call these two strategies ‘adaptation’ and ‘niche construction’, respectively. Adaptation is often the more difficult of the two: building a nest or a burrow is one thing, but overcoming instinct or anatomy is another. Humans manage some of each.
Over many generations, we have mutated, competed for mates, built up disease resistance, changed colours, and developed the ability to stomach spicy foods or lactose at the level of our DNA. But we also migrated into unfamiliar, hostile spaces that we reshaped with culture – with our cleverness and tools. We can credit our success as an invasive species to the fact that culture moves considerably faster than biology. Culture is the niche we use to make new niches.
Technological mastery has helped us exist in more environments than any other species. We are so deep in layered tool use and outboard information storage that we scarcely recognise our built environments as body parts.
From orbit, a time-lapse of urbanisation looks strikingly like metastasis
Most modern humans live far from the ‘human climate niche’ in which our flesh could live unaugmented. Even in temperate regions, tools are required for survival. We need artificial skins in the form of clothing, thermally stable shelters, refrigeration to keep our foods from spoiling, and trade networks to sustain the movement of materials that all those products depend on. The way we live has led some theorists to argue that the human being is more colonial than individual: like corals inseparable from their reef, we are constantly being woven into the infrastructures we’ve made.
This scaffold is the shell (our ‘shelter’). It shields us from existential risks, including hail and asteroids.
We can see this shell when we consider our metabolic rate. According to the measurements provided by some physicists, each human’s metabolic rate, when we include our tools, exceeds what other mammals of our mass require by more than 30 times the expected value. The energy consumed by you and your support technologies – your fraction of the farm equipment, servers, factories, refrigerators, hospitals and power stations – lofts you up into the weight class of 12 elephants . That’s one gigantic shell! Protruding out of it are ‘eyestalks’: sensor networks that extend beyond the flesh to capture images of distant galaxies and map the ocean floor, bristling Argus-like from day- and night-sides of the globe at once. The swivelling, compound eyes of crabs have much in common with the data-gathering devices through which each internet-enabled modern person beholds a vast, mosaic and panoptic panorama. No wonder we’re obsessed with giant robot crabs: they are very like the human colony encircling Earth. As constituents of that colony, we continually widen our frame of reference and grab every bit we can from flows of data. In the process, we have become cyborgs. But all this has come at a great cost, which itself has taken on a crab-like form. From orbit, a time-lapse of urbanisation looks strikingly like metastasis: a dense, entangled, runaway growth of mutant tissue. In a strange (or entirely expected) turn, humanity’s carcinisation appears to have taken the form of a spreading cancer.
A round 400 BCE, the Greek physician Hippocrates began referring to tumours as ‘ karkinos ’, linking them to the mythical crustacean enemy of the hero Herakles. He seems to have made this connection for two reasons: tumours also behaved aggressively and the veins around them looked like crab’s legs. Once Greek passed to Latin, karkinos became cancer , linked forever with the figure of the crab.
The biologist John Pepper of the US National Cancer Institute argues that cancer is a metabolic dysfunction: an oversupply of energy to tissues ordinarily held in check by negative feedback mechanisms like those that maintain the balance of different species in food webs. When one species learns to suddenly eat every other, you get something like a planet-sized lymphoma. By discovering fossil fuels and the technique to fix atmospheric nitrogen for fertiliser, doubling global population twice in one century, human civilisation fits the bill.
The etymology of the word ‘cancer’ helps us understand the deeper links between humankind’s grid of mapping systems and machines, and the runaway growth of mutant tissue we have developed in the process. The very idea of human carcinisation emerges from deep patterns in how we describe writing, measuring and marking. It’s not just a meme: think of the way that the Latin word cancer meant ‘lattice’ or ‘crossed bars’ and was related to the word carcer , which meant ‘prison’. What’s more, a diminutive of cancer is cancellus , meaning to ‘cross out something written’ by drawing a line through it. Similarly, the Dutch word krabben means ‘to scratch or claw’.
Even feudal fisherman believed carcinisation was the destination of at least some crab-like humans
When we cross enough lines on sand, bone, paper or silicon we incarcerate ourselves within a lattice, an all-encompassing Cartesian grid, which inadvertently turns us into crustaceans. In the process of cutting and claiming the world with our symbolic and numerical abstractions, we have become encased within them, and have replaced reality with simulation. Crabs are not just on the edges of our maps. They are the map itself. And, in digital society, the map is the territory.
Dorippe japonica ( c 1825 ) by Kawahara Keiga, watercolour . Courtesy the Biodiversity Heritage Library/Wikipedia
If you think that this is ‘merely’ pareidolia (the tendency to notice patterns everywhere), I wouldn’t be the first. Consider Heikeopsis japonica , a species of crab native to Japan whose carapace looks like the face of an angry samurai. Folk legend has it that these crabs are reincarnated Heike warriors, defeated in the naval Battle of Dan-no-ura during the 12th century. Which is to say that even feudal fisherman believed carcinisation was the destination of at least some crab-like humans, more than 650 years before Charles Darwin’s On the Origin of Species (1859). Let’s not dismiss the evolutionary value of finding relevance a little bit too often: pattern recognition is one of our most crucial skills, and erring toward false positives, on average, helps us live to fight another day. Without the right brain’s heightened sensitivity to the movement of potential predators, our distant ancestors would never have survived the explosion of life during the Cambrian Period, more than 500 million years ago.
The Ghost of Taira Tomomori (between 1797 and 1861) by Utagawa Kuniyoshi/ Wikipedia
Let’s return to our ‘incarceration’ in the all-encompassing Cartesian grid. After centuries of benefiting from the super-exponential growth it afforded us, we need to take a wider view to see our incarceration for what it is. But this is not easy. The prison we are in is our enclosure of reality. So what are we to do about it? The good news is that nothing is just what we make of it. ‘Life finds a way’, and let’s remember that we’re not the first to discover catastrophic innovations: photosynthetic algae nearly killed the biosphere with the industrial pollution we now know as oxygen. Even trees once choked our world with their ‘forever chemicals’: before fungi figured out how to eat wood 300 million years ago, landscapes were covered in fallen logs that never went away, eventually becoming coal deposits. Just because we’re on a bender doesn’t mean we’ll kill the planet; microbes have already learned to eat plastic and, in that way, life trends toward ‘crab’ through entrepreneurship, seizing as many free calories as it can.
This brings us to the other lesson of carcinisation: the inherent laziness of living systems. Evolution generally loves to streamline. It would rather lose a trait than gain one. Carcinisation may not be a universal law, but parsimony is , and evolution gets away with what it can. Losing necessary parts won’t help you breed, but offshoring wins when you can count on the supply chain. What you see when you gaze out your window at the modern world is the result of roughly 300,000 years of Homo sapiens relying on systems that ‘just work’, from hunting bands to agriculture to the algorithmic choreography of global logistics that restock supermarket shelves before we know they’re empty. Each time we lean in to collective efficiency, we sacrifice individual resilience. Relying on each other more and more, each of us knows relatively less of what it takes to do it all. This strategy is more or less dependable in stable but competitive environments. And plenty of investors say as much: backable inventions get more done with less.
Consider by analogy the basal decapods, ancestral crustaceans that thrived in the Devonian around 400 million years ago but would look completely normal in a seafood restaurant. Basal decapods had longer, bulkier bodies than their modern crab descendants, and the story of their transformation is a paean to efficiency: over millions of years, they lost functional tails (and thus the ability to escape predation with a ‘tail flip’). This reduced complications from moulting, protected their internal organs, and prevented desiccation in terrestrial environments. Carcinisation rhymes with the advice management consultants seem to always give their clients: ‘Lay off 30 per cent of your workforce to boost year-end returns’ is mathematically equivalent to ‘You don’t need that tail in this economy.’ Crabs did not just lose their tender underbelly; they gained by having less to haul around than ancient shrimps and lobsters. They are ‘lean’ compared with how they started, in the same way human beings of today have smaller skulls than we did 50,000 years ago because we can rely on cultural technologies like books and large language models like ChatGPT.
But this kind of ‘laziness’ depends on an increasingly diverse environment and aggregate intelligence. Every cost-cutting measure exports externalities, and lean, efficient orgs and organisms live on subsidies from ecological complexity. This is why I’m bullish on the biosphere: although the basic body plans we see today were present half a billion years ago, contemporary surveys of marine biodiversity show roughly four times as many families as there were in the Cambrian Explosion. Evolution keeps coming up with new ways of ‘optimising’ and offloading.
Taken to extremes, this leads to existential problems. The human tendency to seek efficiency by offloading work onto systems – our so-called laziness – has made us dependent on large, risk-averse institutions, such as states and global financial markets. And here we start to see the risks of undergoing a process of carcinisation: crabs are great recyclers, but lousy innovators. They fight, ferociously. Maybe ‘brutally efficient’ isn’t how we’re going to meet the challenges of planetary stewardship. Carcinisation only looks like self-reliance – armouring yourself and stockpiling weapons – when in fact it is a strategy contingent on the structure of a bottom-dwelling lifestyle. How can we overcome our crab-ward tendencies?
I f we are to be more than the bottom-feeders of an automated future dominated by creative robots, we will need to reckon with the limits of our vision. How did we give the pixelated and near-sighted ‘eyes’ of states and markets so much power in the first place?
Let’s detour to 1619. As the story goes, while René Descartes was marching in the Habsburg army during his early 20s, he had a dream in which an angel told him that ‘The conquest of nature is achieved through measure and number.’ (Varying accounts agree on his visitation by a being he interpreted as ‘from on high’ who insisted he was to use mathematics to gather together all the sciences.)
We may call the resulting mathematics, code, roads and legal systems ‘infrastructure’ but in practice they are ‘exostructure’ – behavioural constraints imposed top-down by state and market forces, and new layers of abstraction like large language models.
The digital world that blossomed from Descartes’s angelic encounter doesn’t just connect us; it unbundles human beings into what Gilles Deleuze calls ‘dividuals’ in his essay ‘Postscript on the Societies of Control’ (1992). Dividuals are fragments of the unitary modern self, units on which social engineers can operate. Deleuze suggests that we are not only divided from each other; we are also divided within . Our sense of being an ‘individual’ is now caught in the mandibles of the giant cryptid predators we call lifestyle consumerism and the surveillance state.
Our aggregate intelligence is vastly larger, but our individual know-how has shrunk to nearly nothing
It likely comes as no surprise that justice fails and neotribalism reigns on social media: the Great Acceleration forces each of us to meet more strangers in less time, acting faster with less knowledge, in spaces meant to bypass our prefrontal cortexes and elicit strong emotions for engagement. Sold on visions of a ‘global village’, we now meet in a dark alley where we know we’re being watched.
The consequence is ‘crab mentality’, a set of habits that appears in various binary, zero-sum, dehumanising practices like ‘careerism’ and ‘cancel culture’ (remember, the diminutive of cancer is cancellus ). How can any of us relate to one another as complete, mysterious, unquantifiable and singular, when all our interactions happen through this sieve of compound eyes and sifting claws?
It’s hard to roll this back because to do so would require choosing the ‘hard’ evolutionary option. But evolution doesn’t choose; it only flows downstream. This downward causal pressure comes from what the astrobiologist Caleb Scharf calls ‘The Dataome’, the sum of outboard memories inscribed in any surface, from cave walls to microprocessors . It is a planet-sized collective computation that increasingly influences our thoughts and actions. This influence is the price we pay for letting it relieve us of the metabolic burdens of memory and decision-making. Hermit crabs might choose their shell , but they cannot choose no shell once they’ve spent long stretches of evolutionary time shell-bound and have been whittled down to fleshy vestiges of what they were. The aggregate intelligence of Homo sapiens is vastly larger than it used to be, but the know-how that any one of us possesses has shrunk to nearly nothing by comparison.
The result is that there is no single ‘English’ , for example. We now have countless local dialects, each using words that look and sound alike but carry idiosyncratic networks of associative meaning. And when we drop this radical diversity of subcultures into massive, poorly differentiated catch-pools, understanding breaks down – people think they have sufficient context when they don’t. Unlike in ‘cozy’ human-scale agglomerations (talking with friends around a dinner table, sharing messages through a group chat or private Discord server), social media on the open web is an enormous barrel full of crabby people.
With COVID-19 isolation stacked on top, plus years of solitary confinement in which many of us had no choice but to work remotely, it’s easy to see how crushing pressure turned bounded-but-still-social city-dwellers into strange benthic organisms living on the hyperbaric seafloor of the world wide web. Down here, the emergence and booming popularity of the ‘everything becomes crab’ meme starts making sense.
Courtesy Twitter/X
Today, we seem to be encouraged to spend all our time online – a space defined by other people’s toxic waste. Maybe we need to come up to the surface, but depressurisation is dangerous. Real relationships are messy and the streets are full of crazy people. Making human friends is so much harder than confiding in a chatbot. Is it safe yet? Was it ever safe? If all you have is a hammer, everything looks like a nail. If all you have is a global matrix of nested actuarial tables, everything looks like a risk. And so, carcinisation isn’t just a process of losing traits and becoming more efficient. It’s also about acquiring military hardware.
L et’s return to Dneprov’s short story ‘Crabs Take Over the Island’, which prophesied self-replicating autonomous weapons eight years before the American computer scientist John von Neumann came up with a proposal for a self-replicating machine – a ‘universal replicator’ – that became foundational for research into artificial life. In the introduction to Dneprov’s story, a Cold War physicist and cyberneticist at what was the USSR’s leading scientific institution states it plainly:
Why did he pick the crab? Perhaps because war carves the evolutionary fitness landscape toward variations on the spear and shield. ‘The ideal body,’ as one Twitter user dryly noted during the height of the ‘everything is crab’ meme, ‘is at least 50% weapons.’
Courtesy Twitter/X Wikipedia
But the ‘ideal body’ is a moving target. Crabs and tanks are not a universal strategy. Organisms need both fear and curiosity in balance, a stick and a carrot, and competition on its own is only half the story of the biosphere. Adversity may be ‘the mother of invention’, but widely reported spikes in innovation during war (digital computers! Radar! Superglue! The flu vaccine!) may have more to do with wartime budget- and attention-allocation than conflict as a boon to creativity per se.
After 3 billion crabless years, the fossil record went to the dizzying menagerie of body plans we see on Earth today
Though victory is correlated with innovation, and survivorship bias suggests that competition pressure causes creativity, research into how species acquire new traits often shows the opposite: natural selection operates on a background of constant innovation, and new traits emerge where genomes or behaviours have the freedom to explore. Evolution may be lazy but, as a search algorithm for new possibilities, it can also be wasteful and excessive. War and other forms of competition offer crucibles and the adaptability of life depends on reservoirs of latent novelty that lie in wait to spring forth from their dormancy like ‘sleeping beauties’ activated by a kiss of just-the-right misfortune. Even strongly convergent fit-to-purpose forms like wings and knives begin as evolutionary ‘accidents’. Living systems multi-optimise and hedge their bets, investing in diverse portfolios whenever possible across both short- and long-term strategies. Organisms leave a lot of ‘slack’ for what seems useless now but might prove crucial when the weather changes.
Think of the tank designed by Leonardo da Vinci in the 15th century, which, had it been built, would not have been able to move due to the weight of its materials. The modern version, armoured and cannon-bearing, came into existence only once enabled by petroleum and steel.
A study of a fighting vehicle ( c 1485) by Leonardo da Vinci. Courtesy the British Museum
Nor were crabs – themselves a sort of ‘military innovation’ – viable until our planet had enough free energy available to grow them. During the ‘Cambrian Explosion’ 541 million years ago, after 3 billion crabless years, the fossil record suddenly went from jellyfish and sponges through a tenfold increase to the dizzying menagerie of body plans we see on Earth today.
Biologists debate the reasons why this happened, and even question whether it’s just an artefact of preservation bias – so much of natural history happens ‘off the record’.
In The Cambrian Explosion (2013), Doug Erwin and James Valentine argue that more oxygen during this period provided ‘economic stimulus’ for scaling up the body size of early complex organisms. Oxygen is the ‘carrot’. The ‘stick’ may have arrived with the eye’s evolution, which the zoologist Andrew Parker argues in In the Blink of an Eye (2004) led to predator-prey arms races. If these narratives are valid, then the archetypal crab kit – pincers, swivelling eyes and armour – all emerged in response to 3-D warfare in the water column between beasts like Anomalocaris (translation: ‘weird crab’, with a skirt of swimming gills and pincer-like appendages), and Opabinia (even weirder, with five eyes and a clawed proboscis).
Clear vision dates back to the trilobites who lived in constant fear of swimming crab-tanks around 521 million years ago. And with vision came hemispheric differentiation in cognition. The neuroscientist Iain McGilchrist’s work on the divided brain suggests that animals discovered ‘predator’ and ‘prey’ modes to run a dual-core architecture that required balancing goal-driven thinking and diffuse peripheral attention. Strange as it may seem, the fact that you are reading this and pondering carcinisation now can be traced back to bleeding-edge crustacean warfare half a billion years ago. Tanks have been a big hit ever since.
The armoured fighting vehicles of the Martians in Wells’s The War of the Worlds represent an endpoint for this logic, a vision of advanced industrialised warfare. In the same year that novel was published, 1898, another British visionary, F R Simms, built the first armed petrol engine-powered vehicle: an armoured car with a machine gun. Wells followed with more novels featuring prescient inventions such as the monstrous tanks in The Land Ironclads (1903), military aircraft in The War in the Air (1908), atomic bombs in The World Set Free (1914), and the world wide web in ‘The Idea of a Permanent World Encyclopaedia’ (1937).
Wells’s progression of ideas shows a trend in which war embeds us ever deeper in the mechanical environment. Each new invention becomes a modern milestone in our ‘carcinisation’, starting with a wheeled contraption that soldiers climb inside for combat. Then we see intercontinental nukes, whose fallout drifts around the world, shrouding our planet in a Cold War that does not obey the linear dynamics of a Blitzkrieg . But in its ‘final form’, industrialised warfare takes place everywhere and nowhere. The cyber-conflicts of our century are part of a fractal war of narrative and data in which there is no visible front line, and the boundary between the human and machine has blurred beyond distinction.
W hile we’re on the subject of industrial warfare, cyborgs, samurai and science fiction, let’s not forget Darth Vader, the cinematic archetype of the ‘dark side’ of evolution. This legendary Star Wars villain is a vestigial human totally encased in and dependent on machines, whose cybernetic claw chokes enemies at every opportunity and who flies a crab-shaped custom fighter. He is as much a victim as a victor. Mad with power and the need for more, Vader is a textbook case of unchecked left-hemispheric predatory impulses. He grabs everything but loses his grip on The Big Picture.
This is a tendency that McGilchrist, in his book The Matter with Things (2021), sees in our modern societies after centuries of technologically assisted conquest. Doubling down on getting things done and always choosing the efficient path eventually leads civilisation to the opposite of pareidolia: a nihilistic world of inert matter in which the ends always justify the means and everybody else is shooting practice.
Today, every part of modern human life, from pregnancy to burial, is penetrated by the machinic operations of capital and warfare. Our world runs on crab logic, encouraging us to see ourselves as separate entities in constant competition. James C Scott’s book on institutional injustice, Seeing Like a State (1998), explains that governments and markets do not care about you, because these entities exist at scales so vast they notice people only in the aggregate, as mobs best modelled with the equations of fluid dynamics. But we do the same when we believe ourselves to be distinct from one another. We are undergoing carcinisation because we have to play a game determined by the grids in which we’ve incarcerated ourselves. Vader isn’t evil because he’s half-machine; he’s half-machine because he’s trapped inside the machinations of the Galactic Empire.
The more we learn, the more we find intelligence is everywhere, in our cancer cells and in the crabs we eat
Star Wars was inspired by the plot from Akira Kurosawa’s samurai masterpiece The Hidden Fortress (1958). Our ‘hidden fortress’ is the Technosphere itself: the entire system of technologies that encase us and our world. But this system isn’t just a shell, separate from us. We’re woven into it. Even the Death Star – a manmade moon; a perfect symbol of technology – was part of ‘the living Force’. In the end, even Vader was redeemable.
As we carcinise, we’ve come full circle as a species. Once embedded in nature, we seemed to break away. Now, through information technologies created for war (like the internet), we are experiencing a figure-ground reversal in which we wake up from centuries of separation into kinship with our living planet. But the life-technology divide gets weird : in this fabric of symbiotic co-niche-construction, we’ve inherited the artefacts of aeons of nonhuman agency. After all, the oxygen-rich atmosphere we breathe was built , however unintentionally, by our distant algal ancestors. The more we learn, the more we find intelligence is everywhere, even in our cancer cells and in the crabs we boil for dinner .
We are not standing on the world but in it, not entirely unlike crabs on the ocean floor, under miles of atmosphere and somewhere in the middle of a giant pile of articulated meaning. This new perspective shows us just how crab-like we are, and how much dignity we failed to notice in the beings that Cartesian thinking let us vivisect without concern.
Maybe now the popularity of crabs in human culture makes more sense. Or maybe you will just decide to toss all this back in the water as if it were a Heikegani : not enough meat to remunerate you for the effort. Either way, before you raise your claws to swipe on to another story in the shallows, remember that Earth is an ocean planet. No matter how convenient it might seem to hold on to solid ground amid the tides, it pays to learn to swim. There is another way. And, as it happens, even crabs have learned this lesson. So let’s end with a hopeful note on de carcinisation.
Convergent decarcinised body forms in various families of false and true crabs and convergent appendages in swimming and/or fossorial arthropods. Courtesy Javier Luque/University of Cambridge
W hile five groups of arthropods independently discovered ‘the process of becoming a crab’, the opposite is actually more common. Seven times, crabs swam out of their attractor basin – a zone in the possibility space of evolution where certain body plans become more stable – to pursue a different strategy. Fitness landscapes always change, and so does ‘peak performance’, which is why one fossil arthropod, Callichimaera perplexa , stretched into an active swimmer with a fuselage-like shell and giant eyes amid an all-bets-off explosion of crustacean innovation called the Cretaceous crab revolution around 95 million years ago.
What loosened the constraints on flat and squared-off carapaces to produce sleek ‘decarcinised’ crabs like Callichimaera and its cousins, the ‘frog crabs’ in the family Raninidae ? One clue is that they were diggers: burrowing is less of a drag when you are not a pancake. Some situations call for ‘frisbee’ and some for ‘missile’. Convergence, at the deepest level, is contingent after all. When it comes to understanding animal anatomy, ‘you had to be there’. In the case of frog crabs, hiding mattered more than being able to shoot sideways. Predator surveillance pinched them laterally for a lower profile and speedier escape (much like market pressure pinches lateral thinking in team meetings).
Organisms are hypotheses of stable features of the land- (or sea-)scape. In other words, their characteristics are shaped by the specific conditions and structures in the world. That means there is no ‘final form’, merely several very deep holes in an already very holey space of possibilities. But judging by the numbers of crabs that decarcinised, flexibility and forward motion matter more than giant claws and heavy armour. On average, more organisms have trended toward fish than crabs. Plus, there are 7,000-something crabs and 30,000-something fish, so place your bets accordingly.
Given just how often circumstances seem to favour nimble group improvisation over rigid risk-reduction algorithms, I’m placing mine on schooling. Humans aren’t just technological but social. Even Vader understood this in the end: he finally chooses family over power, taking off his mask to look at his son before he dies – a brief respite from his embeddedness in a technological carapace built for war. Power justifies itself but doesn’t offer meaning. Prediction isn’t understanding. Mitigating danger isn’t really living. The galaxy is a lousy consolation prize when held up next to family. And visionary crabs such as Callichimaera aspire to be fish. If indeed our species has a future, it will be strategically diverse: engineers and artists, technocrats and mystics, crab people and fish people, and every other lazy-but-inspired permutation.
This essay is excerpted from Michael Garfield’s forthcoming book, How to Live in the Future."
Population decline will rob us of vital social force: youth | Aeon Essays,,https://aeon.co/essays/population-decline-will-rob-us-of-vital-social-force-youth,"A population crisis is unfolding – in my family. My paternal grandparents were born in India and had 10 children. I did my part to keep our lineage going (my wife was also involved) yet over the course of three generations, my family’s ‘fertility rate’ plummeted from 10 to less than 1.
During this same period, India’s population exploded from 350 million to 1.4 billion, in 2023 surpassing China as the most populous country in the world. But my family’s story mirrors India’s demographic transition. In the mid-20th century, when my parents were born, India’s fertility rate was 6. It’s now 2.
India is a striking case, but fertility rates are declining everywhere. Between 1950 and 2021, the global fertility rate fell from 4.8 to 2.2. American women have an average of 1.6 children. Japan’s fertility rate is 1.2, South Korea’s a startling 0.75.
The global population is still growing, for now. But demographers project that humanity’s numbers will peak near the end of this century before beginning a steep decline . I won’t be around by then, but my children will live to see the peak and peer over the precipice.
Is this a crisis?
S ome say it’s a blessing. Fewer people means fewer carbon emissions. Not only that, our collective pie could be divided into larger pieces. In theory, we’ll enjoy less competition for resources, more affordable housing, and higher wages due to labour scarcity. From this perspective, population decline isn’t a problem but a solution.
Alas, these hopes are misplaced.
The path forward on climate change is clear: rapid decarbonisation. We must transition as fast as possible from fossil fuels to renewable energy sources. If we fail, the future will be disastrous, and population size becomes irrelevant. If we succeed, additional humans won’t impact carbon emissions.
Moreover, the timelines don’t match. Climate change demands solutions within the next few decades; population decline won’t materialise until the next century. Having fewer humans arriving then won’t retroactively cool the planet.
Imagine, however, that population decline accelerates. That would worsen rather than salvage our climate prospects. Young people are more likely to support bold environmental policies, become climate activists, and invent green technologies. A shrinking, ageing population means fewer contributors to these efforts.
Hospitals will burst at the seams while playgrounds empty
This points to the fundamental reason that population decline will be a curse rather than a blessing: the loss of young people.
Schools will be converted to elder-care facilities. Hospitals will burst at the seams while playgrounds empty. This demographic skew will reshape every aspect of our world, from economics and innovation to culture and social progress.
Consider social security, often in the US misconceived as a savings account for retirement. In fact, tax contributions from workers are redistributed to retirees – a system of intergenerational cooperation that depends on a balanced age distribution. As the population ages, this system will crumble.
The problems run deeper. Fewer working-age adults entails a shrinking tax base, even as the need for public services swells. Education, healthcare, infrastructure, public safety and welfare will be underfunded. The pie will shrink. And as lifespans grow, the demand for healthcare will increase while the number of healthcare workers decreases. Vulnerable groups – not just the elderly but also those who experience poverty or disability – will suffer most.
To thrive, societies need young people. New generations drive economic growth, pioneer technologies, challenge outdated moral views, create art, and advance social change. They’re more likely to take risks, embrace new ideas, and imagine different futures. When we talk about population decline, what we’re really talking about is the gradual dissipation of this vital social force.
M any assume the solution is simple: open the borders and demographic problems will solve themselves. Yet while immigration can prevent domestic population decline, it can’t reverse imbalances in population structure. Research suggests that higher rates of immigration can prevent a nation’s population from shrinking but not from ageing. The mathematics is simple: immigrants are younger than the average citizen but older than newborns.
More fundamentally, immigration won’t help because population decline is a global problem. When wealthy nations import young workers from poorer nations, they’re effectively exporting their low fertility rate, exacerbating the global problem rather than solving it. Take my parents and their siblings, who emigrated to the West in their early 20s. (At that age, I still had not emigrated from my parents’ basement.) Adopting new homes, they also adopted new family models – none had more than two children. And their children had even fewer.
Progressive tax reform could boost public revenues but this is a tall order
Even so, economic and health repercussions may be tackled in other ways. Retirement ages could creep upward. Advances in medical technology might reduce age-related illness and disability, enabling longer healthy working lives. Automation of healthcare services might compensate for the dearth of healthcare workers.
More ambitious solutions are possible. Progressive tax reform could boost public revenues, offsetting the shrinking tax base and bolstering social programmes. This is a tall order, given that previous attempts have been unsuccessful. But since older people are more likely to vote, perhaps we’ll summon the political will.
These solutions, however, address only the surface. A shrinking, ageing population poses even deeper challenges, ones far more resistant to policy solutions.
P opulation growth drives economic growth. More people means more workers, more consumers, and more innovation. When populations expand, economies of scale become possible – efficiency increases and wealth multiplies. Younger generations are more likely to start new businesses, adopt novel technologies, and increase productivity. Historically, even relatively small populations have been economically vigorous so long as they had a high proportion of youth.
As fewer young people are born to replenish society, we’ll face a shrinking workforce, weakening consumer demand, and the unravelling of economies of scale. We’ll also lose the very demographic that drives economic development. Young people don’t just fill jobs; they reimagine how work should be done. They don’t just participate in the economy; they reshape it. Ageing societies will experience not just economic stagnation but degrowth.
Some people welcome this prospect, seeing degrowth as an antidote to climate change and capitalist exploitation . But this perspective ignores crucial realities. Economic growth can be decoupled from carbon emissions. More importantly, degrowth would devastate developing nations. Economic growth dramatically reduced global poverty. Reverse that growth, and poverty will resurge.
Most breakthrough discoveries come from younger researchers and entrepreneurs
Some think AI or another emerging technology can stimulate economic productivity. But the effects of population decline extend far beyond markets and material wealth.
When societies are large and interconnected, they’re able to generate new ideas, recombine old ideas in new ways, and forge new divisions of cognitive labour. A smaller population will thus shrink what the evolutionary theorist Joseph Henrich in The Secret of Our Success (2015) calls our ‘collective brain’. We’ll forgo not just particular innovations but entire fields of enquiry, impacting everything from basic research to practical applications in engineering and medicine. New technologies could potentially sustain economic productivity, but that will be harder if a shrinking population is technologically less innovative.
Young people aren’t just members of society’s collective brain; they’re its most innovative neurons. Most breakthrough discoveries come from younger researchers and entrepreneurs. Social progress, similarly, depends on young people rejecting prevailing bigotry and replacing older generations. For example, support for same-sex marriage is higher among Gen X and Millennials than in older generations. If the proportion of young people declines, so will our moral and political values.
You think we live in a decaying gerontocracy now? Just wait.
The impact on creative activity will be no less profound. Young people have always been the main source of art, fashion, music, literature and film. As their numbers diminish, the future will become a cultural wasteland. Imagine the 1960s without rock-and-roll, the 1970s without Hollywood auteurs, the 1980s without street art, or the 1990s without hip-hop.
More generally, young people are optimistic, willing to take risks, and open to new experiences. These traits drive cultural progress. An ageing society will become risk-averse, focused on preserving wealth rather than creating it, and resistant to necessary change.
I find myself in strange company. Right-wing pronatalists want to outlaw abortion, punish childlessness, and force women to marry young and forsake careers outside the home. In other words, they want to make fertility rates great again by restoring the patriarchal order of the early 20th century. Some of these pronatalists are only really worried about white population decline . (They’re not worried about my family’s fertility rate.) Many observers conclude that population decline isn’t a crisis – it’s a moral panic.
But just because a cure is toxic doesn’t mean the disease isn’t real.
If populations decline, humanity will unravel. Societies will become less productive and poorer, vulnerable people will lose crucial social support, innovation will slow, and values will deteriorate. Given my family’s trajectory, I might not have any descendants by the time the global population crashes, but the world – what’s left of it – will suffer enormously if we do nothing.
We have a shared responsibility to create conditions where people want to have children
To be clear, I don’t much care whether my own family lineage endures. My two children are perfect. That’s what I tell my wife too, but she argues we should have another. Lately, she’s been appealing to my conscience: if population decline is truly dire, shouldn’t I do my part for humanity?
The answer isn’t individual sacrifice. No one has a duty to procreate (conveniently for me). Rather, we have a shared responsibility to create conditions where people want to have children and can have all the children they want. This means grappling with the gendered division of reproductive labour and supporting caregivers. That’s how pronatalism can be a progressive movement.
The first step is to understand the source of the problem.
T he global fertility rate held steady around 5 until the 1960s. Today, it has fallen to 2.2. In all developed countries, fertility rates are well below 2.1, the ‘replacement rate’ needed to maintain a stable population.
Why this dramatic decline?
As societies grow wealthier, women gain access to education and careers. They secure reproductive freedom and marry later, if at all, encouraged by reproductive technology to postpone family formation. Teen pregnancy becomes rare. People concentrate in cities where housing costs soar and living spaces shrink. Extended families scatter, taking with them crucial support. Meanwhile, parenting itself has become more intensive and expensive but less valued by society. Childrearing also competes with growing access to luxuries such as travel, hobbies and creative pursuits. (Smartphones are more entertaining than kids.)
A great tragedy of our species is that procreation is an oppressive institution
Much of this fits my family’s story. My grandparents were born in India and died there, their progeny relocating to urban centres across the English-speaking world. None of my grandparents attended university, while nearly all their grandchildren did. One grandmother married at 16 and had her first child a year later. Only one of my cousins had children before 30; some prioritise careers or leisure, choosing not to marry or reproduce.
A great tragedy of our species is that procreation is an oppressive institution. Rising freedom and prosperity have provided women with more attractive options. This explains why fertility rates crashed in East Asia, where marriage and family remain particularly restrictive while economic opportunities for women have blossomed.
It’s hard to make predictions, especially about the future. But the tendency for fertility rates to fall as countries become wealthier and freer is as close as you can get to a law of nature in demography. So, as poor countries continue to become wealthier, their fertility rates are destined to fall below replacement too.
T o avoid demographic collapse, Right-wing extremists would drag society backwards. They want to ban contraception, turn women into breeders, , and promote heterosexual marriage while ignoring its oppressive structure. We must reject these coercive proposals.
Fortunately, coercion isn’t necessary to avert population decline. In many countries, a gap separates intended fertility and actual fertility. Fertility rates will recover if people are empowered to have as many children as they want.
Yet most factors driving fertility decline are overwhelmingly positive – affluence, economic opportunities, reproductive freedom. We should try to design wealthy, free and equal societies, even knowing that these very achievements tend to lower fertility rates.
Perhaps I’d want a third child if I’d found my partner and settled into a permanent job in my mid-20s
Our central challenge is to increase fertility while preserving and expanding valuable social gains. We need innovative approaches that separate high fertility from its historical causes – poverty, oppression and gender inequality. The solution isn’t to reverse progress but to reimagine family formation in a progressive society.
Sometimes I’m asked (for example, by my wife) why I don’t want a third child. ‘What kind of pronatalist are you?’ My family is the most meaningful part of my life, my children the only real consolation for my own mortality. But other things are meaningful too. I want time to write, travel and connect with my wife and with friends. Perhaps I’d want a third child, or even a fourth, if I’d found my partner and settled into a permanent job in my mid-20s instead of my mid-30s.
My story is unique, of course, but I see it reflected in the lives of others. Raising children has become enormously expensive – not just in money, but also in time, career opportunities and personal freedom. Effective solutions must address these costs.
S weden offers what seems like a blueprint for progressive pronatalism: childcare subsidies, parental leave , and family support programmes. Yet the results are sobering. Sweden’s fertility rate sits at 1.5 – well below replacement. While these programmes may have prevented even steeper declines, they haven’t reversed the trend. Other countries – from Singapore and Taiwan to Poland and Hungary – have implemented similar policies without success.
France offers a more promising model. With a fertility rate of 1.62, it hasn’t maintained replacement either but does surpass its European neighbours. The French approach is like Sweden’s except even more generous, pronatal expenses accounting for 3.5-4 per cent of its GDP. Tax benefits increase with each additional child – essentially paying parents more as their families grow.
The US briefly experimented with a lite version of France’s approach through the COVID-19 pandemic-era expanded child tax credit. Its lapse represents a missed opportunity. The programme should not only be restored but dramatically expanded. Other countries should follow suit.
Social norms about family size and parenting prove remarkably resistant to change
More broadly, the world needs to deploy different support programmes, measure their impacts, and scale up what works best. The best interventions may increase fertility rates only marginally, but that would be a form of harm reduction. Moreover, replacement becomes possible if marginally effective solutions are deployed in concert. The costs may be high but the benefits are worth it.
Another, complementary approach is to make housing more affordable so that young people can leave the nest and afford spaces large enough to contain the families they’d like to create. Economists warn that subsidies make housing less affordable by driving up demand. A better policy is to increase supply by eliminating restrictive zoning laws and other regulations that limit new multi-family housing. (YIMBYs and pronatalists unite!)
These policies may not be enough by themselves. Social norms about family size and parenting prove remarkably resistant to change, persisting even after the conditions that shaped them have shifted. This means we need to couple robust policy interventions with efforts to reshape cultural attitudes about parenthood.
O ne way to reduce parenting costs is for men to do their fair share, as the Harvard economist and Nobel laureate Claudia Goldin argues . Despite a century of rising gender equality, American women in heterosexual relationships still perform roughly twice the domestic labour of their male partners, even when both work outside the home. Add children to the equation, and this imbalance becomes crushing. (Childless readers: imagine multiplying your household workload by a factor of three or four.)
Meanwhile, modern parenting has evolved into an increasingly demanding enterprise: coordinating enrichment activities, monitoring academic progress, managing mental health, ensuring physical safety, and maintaining constant vigilance. No wonder many stop at one child. While some aspects of intensive parenting reflect novel challenges or deeper relationships with children, others are unnecessary or unhealthy. Dialling back expectations would boost fertility rates along with parent and child wellbeing, as Tom Hodgkinson argues in The Idle Parent (2010).
Parenting becomes more difficult when extended families disperse
Career compatibility presents another major hurdle. But workplaces can adapt. Imagine offices designed with family life in mind: flexible schedules that accommodate parent-teacher conferences, on-site childcare, and cultures that celebrate rather than penalise family commitments. For example, expansion of remote work seems to have fostered a pandemic baby bump in the US.
Historically, our species has relied on cooperative parenting, as Sarah Blaffer Hrdy writes in Mothers and Others (2011). Fertility rates have declined in part because parenting becomes more difficult when extended families disperse. Many young people want the freedom to pursue careers far from home and escape family control. But new parenting cooperatives are possible, with friends living together or neighbours sharing duties. Such arrangements are already being attempted – for example, in pandemic ‘pods’ – and, if successful, they might catch on.
Changing ingrained cultural patterns won’t be easy. But cultural shifts often follow material incentives. For instance, smart policy could help reshape workplace practices, with governments compensating employers for costs. While we can’t predict exactly how such changes might unfold, we can try to encourage more sustainable and equitable parenting. We should at least think harder about it.
H ow urgent is our demographic problem? History offers a sobering perspective: no developed nation has ever seen its fertility rate fall below replacement and then recover. The mathematics of population decline is unforgiving, as delays now compound into massive differences later. Every year we postpone action, the challenge becomes more daunting.
Yet we shouldn’t surrender to panic. The most severe consequences of population decline won’t materialise until the 22nd century. While we must think ahead, we shouldn’t be confident about shaping the distant future, which reduces the expected value of our efforts. Unknown factors might intervene and restore fertility rates to replacement levels. Or more pressing problems may intrude and demand the bulk of our attention.
Population decline is a serious challenge, but it’s not the civilisational emergency that extremists claim. This reality suggests a measured, two-track approach.
Every major advance in human history – technological, cultural, moral – has been driven by youth
First, we should undertake policy and cultural interventions that make sense independently of their impact on fertility. Expanded parental leave, subsidised childcare and pronatal tax policies would support caregivers even if they didn’t boost fertility rates. Likewise, increasing the supply of housing would benefit those suffering from poverty or homelessness. Beyond that, we have every reason to encourage men to contribute more to childcare, make parenting less intensive, and redesign workplaces to be more family-friendly. These interventions offer clear benefits with minimal downside risk.
Second, we should invest in researching and testing more ambitious and radical approaches, from cultural innovations surrounding cooperative parenting to technological revolutions in assisted reproduction. Perhaps the most effective solutions, ultimately, will relieve the costs of childrearing through artificial wombs or AI nannies. Let scientists and policy experts explore bold approaches while ensuring rigorous evaluation of their effects. Ask politicians, humanists and the general public to critically discuss them.
Still, pronatal policies raise hard questions. Unless the gendered division of labour changes radically, increasing fertility rates may expand burdens on women and entrench traditional gender norms. Given the gap between intended and actual fertility, the benefits of having children may outweigh these moral costs. But we can’t pretend to fully understand the tradeoffs. We need to study not just whether pronatalism can be effective but also under what conditions it is ethically defensible. Philosophers have work to do.
At bottom, progressive pronatalism isn’t just about increasing our numbers – it’s about preserving society’s engine of progress. Every major advance in human history – technological, cultural, moral – has been driven by youth.
Maintaining this vital force will be possible only if population decline doesn’t remain solely a preoccupation of extremists. Too much depends on finding humane solutions. We need approaches that contribute to human flourishing while preserving the advances in freedom and equality that precipitated declining fertility. The future of humanity – and of my unlikely grandchildren – depends on striking this delicate balance."
"When AIs do science, it will be strange and incomprehensible | Aeon Essays",,https://aeon.co/essays/when-ais-do-science-it-will-be-strange-and-incomprehensible,"The science of our age is computational. Without models, simulations, statistical analysis, data storage and so on, our knowledge of the world would grow far more slowly. For decades, our fundamental human curiosity has been sated, in part, by silicon and software.
The late philosopher Paul Humphreys called this the ‘ hybrid scenario ’ of science: where parts of the scientific process are outsourced to computers. However, he also suggested that this could change. Even though he began writing about these ideas more than a decade ago, long before the rise of generative artificial intelligence (AI), Humphreys had the foresight to recognise that the days of humans leading the scientific process may be numbered. He identified a later phase of science – what he called the ‘automated scenario’, where computers take over science completely. In this future, the computational capacities for scientific reasoning, data processing, model-making and theorising would far surpass our own abilities to the point that we humans are no longer needed. The machines would carry on the scientific work we once started, taking our theories to new and unforeseen heights.
According to some sources, the end of human epistemic dominance over science is on the horizon. A recent survey of AI researchers offered a 50 per cent chance that, within a century, AI could feasibly replace us in every job (even if there are some we’d rather reserve for ourselves, like being a jury member). You may have a different view about whether or when such a world is possible, but I’d ask you to suspend these views for a moment and imagine that such artificial superintelligences could be possible eventually . Their development would mean that we could pass over the work of science to our epistemically superior artificial progeny who would do it faster and better than we could ever dream.
This would be a strange world indeed. For one thing, AI may decide to explore scientific interests that human scientists are unincentivised or unmotivated to pursue, creating whole new avenues of discovery. They might even gain knowledge about the world that lies beyond what our brains are capable of understanding. Where will that leave us humans, and how should we respond? I believe we need to start asking these questions now, because within a matter of decades, science as we know it could transform profoundly.
T hough it may sound like the stuff of science fiction novels, Humphreys’s automated scenario for science would be yet another step in a centuries-long trend. Humans have never really done science alone. We have long relied on tools to augment our observation of the world: microscopes, telescopes, standardised rulers and beakers, and so on. And there are plenty of physical phenomena that we cannot directly or precisely observe without instruments, such as thermometers, Geiger counters, oscilloscopes, calorimeters and the like.
The introduction of computers represented an additional step towards the decentring of humans in science: Humphreys’s hybrid scenario. As one prominent example documented in the book Hidden Figures (2016) by Margot Lee Shetterly (and subsequent film), the first United States space flights required computations to be done by human mathematicians including Katherine Johnson. By the time of the US lunar missions, less than a decade later, most of those computations had been passed off to computers.
Our contribution to science remains critical: we humans still call the shots
The following decades witnessed continual, logarithmic growth of computational processing and power, and a correlated decrease in the price of computation. We are now at what we might call an advanced hybrid stage of science with an even greater reliance on computational systems. As one example, the philosopher Margaret Morrison explained how computational simulations were essential to the discovery of the Higgs boson – helping scientists know what to look for and sorting through the data from high-energy collisions.
And now AI has begun to have a large impact on science. AlphaFold, for example, is an AI designed to help predict how proteins will fold, given their chemical makeup. While humans can do this work independently of computers, it is time-consuming, labour-intensive and expensive. The creators of AlphaFold – Google DeepMind – claim that it has saved ‘hundreds of millions of years in research time’. Similar benefits can be seen across the sciences: the analysis of extremely large data sets in astronomy and genomics, the development of novel proofs in mathematics, predicting the weather, developing new pharmaceuticals, and more.
When the contributions of computational AI begin to be measured in ‘hundreds of millions of years’, it starts to feel a bit like we humans are the underperforming members of the group project. So, we might wonder whether we are already in the automated scenario. However, we are not yet there. Our contribution to science remains critical: we humans still call the shots – we identify the scientific questions, we interpret the results and we, ultimately, determine its progress.
If we follow Humphreys’s trajectory, the full abdication of our epistemic throne of science would occur only at a later stage. At this point, artificial superintelligences would not only be capable of completing tasks we set for them (a continuation of the hybrid scenario) but would also be capable of setting their own tasks : their own research agenda, data collection, modelling and theories, according to their own independently identified set of theoretical virtues and values – a science all their own.
I t’s worth pausing here to marvel at the possibilities open to an artificial superintelligence unbound by human physical and epistemic limitations. Many scientific tasks sit outside the scope of human possibility, because there would never be funding to pursue the question or because there is simply not enough human interest. As I write this, for example, I am looking at a partially decayed leaf in my yard. Perhaps an artificial superintelligence would be interested in developing a predictive model that explains, with exacting particularity down to the second, the decaying processes and rates of any given leaf, depending on the species of tree, size of leaf, historical contact with various microbial life, presence or absence of sunlight and moisture, and so on and so forth – an extremely complex question for which such detail offers no obvious value. Or, in fulfilment of a question my son once asked me, perhaps a superintelligence would be able to develop a model that predicts precisely when the water molecules of a snowball he left in the mountains will flow by our house in the river that drains that mountain system. Such a prediction would require an extremely complex and detailed model of the river basin, fluid dynamics, the climate and a whole range of other features of the system.
It’s not that we humans couldn’t ever answer these questions. With sufficient focus and funding, I suspect that scientists could develop effective predictive models of these and other esoteric phenomena. But the reality is that we won’t. For better and for worse, science today is shaped by strongly human factors: economic value, political priorities, career prospects, cultural trends, and a range of human biases and beliefs. Imagine the science if all that baggage could be abandoned.
As the superintelligences execute their own research agendas, their work would become unintelligible for us
The automated scenario does not just permit the efficient exploration of scientific projects we cannot or would not pursue. Though the artificial superintelligences might continue to work within the paradigms of our current theories, there is no reason they would have to do so – they may quickly choose to begin afresh with a new theory of the world. Similarly, though they may use mathematics and symbols familiar to human scientists, they would not be bound by these conventions – they may quickly develop new mathematics and systems for expressing those.
Given the possibility (and, in my view, likelihood) that such AIs would quickly abandon human epistemic baggage, we may choose to follow a Wittgensteinian line of reasoning and think of the automated scenario as the stage at which they would begin to speak and develop an independent and new scientific language. Ludwig Wittgenstein famously (and, true to form, enigmatically) says in Philosophical Investigations (1953): ‘If a lion could talk, we wouldn’t be able to understand it.’ Though the statement feels contradictory, Wittgenstein’s point is that the meaning of language is deeply embedded and intertwined with the internal experience of being human. So, too, for science. As the superintelligences begin to set and execute their own research agendas, the work they do would therefore become unintelligible for us because we would lack the internal perspective necessary to understand their science. From our view, their research would be a science created for theoretical-aims-we-know-not-what, with purposes-we-know-not-what, to be interpreted in ways-we-know-not-what.
It is at least possible that there are limits to our human epistemic capabilities: untold mathematics we can never understand or multidimensional concepts that are beyond our three-dimensional experiences. The fact that there are limitations to the intellectual capacities of other animals (try explaining general relativity to the smartest chimpanzee to ever exist) is reason to think that there are possible limitations to our own intellectual capacities too: ideas too complex to understand. Even supposing that we are not subject to limited epistemic capacities, there is also the problem that the reasoning of artificial superintelligence may be in practice beyond our capacities. Understanding the science of the automated scenario may require, for example, the simultaneous consideration of hundreds of complex models, each with hundreds of parameters, none of which links up to a familiar human concept. While it may be that we could understand the parameters (and maybe even the models) individually, we would lack the capacity to hold them all together at once.
D epending on your predilections about technology, AI, and the singularity, the above may all read to you as either incredibly bleak or extremely exciting. If you are like me, it all strikes you as simply strange . If the results of the completely automated scenario are outside of our understanding, then why would we want to devote economic resources and intellectual talent towards its development? Though this question is often swept away by flattening assertions that the future will come, whether we like it or not, I think it is worth our time identifying what reasons we may have for such a future, before we begin our wilful abdication of the epistemic throne of science.
One such reason may be that we think that positive advancements will follow. Perhaps the superintelligences would occasionally create things: pieces of technology, resources or new ways of solving problems. Since I’ve already used up far more than my allotted quota of speculation in this essay, I will remain open to what exactly these products might be, simply noting that the superintelligences may occasionally send us products it decides would be good for humans to have. Human engineers (if there are any remaining who haven’t themselves been replaced) may then take these new technologies and identify uses for them, even if they don’t understand exactly how they work. It would be akin to the way I don’t understand the process by which my monitor or word processor can create and display visual documents, but I can put them to use in writing this essay. This task will be less like today’s science and engineering and much more like simple discovery, the sort of primitive recognition that, for instance, a vine works well to tie tree branches together when building a shelter. It would be similar to stumbling upon some resource or substance in the world (as we stumbled upon coal or penicillin). There may indeed be a second kind of science that springs up as well: a form of backwards engineering of what an AI gifts us, to advance and modify our own theoretical understanding of the world.
Perhaps we might think that it is our moral responsibility or destiny to spread intelligence across the Universe
A different reason for enabling artificially superintelligent science would be aesthetic. Aesthetic reasons already hold a strong motivating factor when I, personally, think about the funding we as a society give to science. Though I do not have the time nor capacity to understand all of science (who possibly could?), I find it beautiful and good that there are so many smart scientists pursuing their human curiosities – even if they do not all positively impact my life or my understanding of the world. There is something aesthetically pleasing in knowing that the world is being known, studied and understood. Might that translate to nonhuman scientists? Perhaps not overnight. However, future generations, who have learnt to live alongside AI, may even take it as a mark of a good society that it would be willing to enable this extrahuman understanding.
Alternatively, humanity might pursue the automated scenario out of beneficence: because we think it would be good for the artificial superintelligences to pursue their own advanced science. While we might find it frustrating – perhaps even upsetting – that the artificial superintelligences will know things that we don’t, we may pursue it all the same out of a moral obligation or feeling of goodwill towards our artificial progeny.
There are other motivations that may result in the automated scenario as an unintended consequence. Perhaps, for example, we think that it is our moral responsibility or destiny to spread intelligence across the Universe. If that intelligence just so happens to pursue automated science on its interstellar journey, then so be it.
E qually as numerous are those reasons why we might decide not to pursue the automated scenario. Perhaps the discoveries the artificial superintelligence makes and passes on to us would yield new and terrible weapons. Or maybe we think that, since they would require some internal and unchecked agency, it would increase the risk of a doomsday scenario such as human enslavement or annihilation. Perhaps it’s simply the concern that some of the superintelligences will begin to operate with an uncannily human hubris, experimenting in ways that are dangerous, unethical or contrary to humanity’s shared values.
But, despite these concerns, it seems unlikely that we will be able to stop its development, if it becomes technically possible. Arguably, the most likely reason we will end up with the automated scenario is because we simply cannot escape the forces of capital and competition. We may arrive there without much thought, simply because we can or because someone wants to build it first. The future may simply happen to us, whether we reflectively want it or not.
We will be stuck with our curiosity to understand and explain the natural world around us
Careful readers will note that there are several motivations that are absent from the litany of possible reasons for pursuing the automated scenario: most notably, all the reasons we currently pursue science. We would not pursue the automated scenario out of a desire to enhance our own knowledge and understanding of the world, to be able to give better explanations of phenomena, or exhibit greater interventional control over the natural world. These cannot be the reasons for pursuing the automated scenario because they are ruled out by the kind of science it is. Automated science takes the epistemic throne from humans, excluding us from the internal perspectives on the new and likely complex-beyond-our-ken discoveries. It would not, therefore, fulfil our human desires for understanding, explanation, knowledge or control. Perhaps with time we could learn to give up these desires – become a species uninterested and incurious. But I doubt it. Like the future, I suspect that these desires will come whether we like them or not.
So, what will we do? In his original presentation of the automated scenario, Humphreys suggested that the automated scenario would replace human science. I disagree. Since our desires for understanding, explanation, knowledge and control will remain, we cannot help but take actions to address those desires – to continue to do science. We humans create beautiful things, pursue interhuman connection in friendship and romance, and find and construct meaning in life. The same holds true for our motivations for science. We will be stuck with our curiosity to understand and explain the natural world around us.
If the automated scenario comes to pass, it seems that it will have to be as some new, alternative, secondary path – not a replacement, but an addition. Two species, pursuing science side by side, with different motivations, interests, frameworks and theories. Perhaps there will also be parts of science that artificial superintelligence is simply less interested in, such as the human quest to better understand our own minds, choices, relationships and health.
Indeed, if we are to remain human (and I cannot but hope that we will), we must continue to pursue science. What are we, really, if we are not beauty-seeking, friendship-making, meaning-constructing, hopelessly curious animals? Perhaps it is my limited powers of imagination that prevent me from conceiving of a future world in which we have abandoned these human desires. There are plenty of transhumanists who may think so. But I do not count it as a lack of creativity to see the goodness in beauty, in love, in meaning, and in science. Quite the contrary. I, for one, take hope in our hopeless curiosity."
What turns a crisis into a moment for substantive change? | Aeon Essays,,https://aeon.co/essays/what-turns-a-crisis-into-a-moment-for-substantive-change,"Polycrisis. Metacrisis. Omnicrisis. Permacrisis. Call it what you like. We are immersed in an age of extreme turbulence and interconnected global threats. The system is starting to flicker – chronic droughts, melting glaciers, far-Right extremism, AI risk, bioweapons, rising food and energy prices, rampant viruses, cyberattacks.
The ultimate question hanging over us is whether these multiple crises will contribute to civilisational breakdown or whether humanity will successfully rise to such challenges and bend rather than break with the winds of change. It has been commonly argued – from Karl Marx to Milton Friedman to Steve Jobs – that it is precisely moments of crisis like these that provide opportunities for transformative change and innovation. Might it be possible to leverage the instability that appears to threaten us?
The problem is that so often crises fail to bring about fundamental system change, whether it is the 2008 financial crash or the wildfires and floods of the ongoing climate emergency. So in this essay, based on my latest book History for Tomorrow: Inspiration from the Past for the Future of Humanity (2024), I want to explore the conditions under which governments respond effectively to crises and undertake rapid and radical policy change. What would it take, for instance, for politicians to stop dithering and take the urgent action required to tackle global heating?
My motives stem from a palpable sense of frustration. Around two decades ago, when I first began to grasp the scale of the climate crisis, especially after reading Bill McKibben’s book The End of Nature (1989), I thought that, if there were just a sufficient number of climate disasters in a short space of time – like hurricanes hitting Shanghai and New York in the same week as the river Thames flooded central London – then we might wake up to the crisis. But in the intervening years I’ve come to realise I was mistaken: there are simply too many reasons for governments not to act, from the lobbying power of the fossil fuel industry to the pathological fear of abandoning the goal of everlasting GDP growth.
This sent me on a quest to search history for broad patterns of how crises bring about substantive change. What did I discover? That agile and transformative crisis responses have usually occurred in four contexts: war, disaster, revolution and disruption. Before delving into these – and offering a model of change I call the disruption nexus – it is important to clarify the meaning of ‘crisis’ itself.
L et’s get one thing straight from the outset: John F Kennedy was wrong when he said that the Chinese word for ‘crisis’ ( wēijī, 危机) is composed of two characters meaning ‘danger’ and ‘opportunity’. The second character, jī (机), is actually closer to meaning ‘change point’ or ‘critical juncture’. This makes it similar to the English word ‘crisis’, which comes from the ancient Greek krisis , whose verb form, krino , meant to ‘choose’ or ‘decide’ at a critical moment. In the legal sphere, for example, a krisis was a crucial decision point when someone might be judged guilty or innocent.
The meaning and application of this concept has evolved over time. For Thomas Paine in the 18th century, a crisis was a threshold moment when a whole political order could be overturned and where a fundamental moral decision was required, such as whether or not to support the war for American independence. Karl Marx believed capitalism experienced inevitable crises, which could result in economic and political rupture. More recently, Malcolm Gladwell has popularised the idea of a ‘tipping point’ – a similar moment of rapid transformation or contagion in which a system undergoes large-scale change. In everyday language, we use the term ‘crisis’ to describe an instance of intense difficulty or danger in which there is an imperative to act, whether it is a crisis in a marriage or the planetary ecological crisis.
Overall, we can think of a crisis as an emergency situation requiring a bold decision to go in one direction rather than another. So what wisdom does history offer for helping us to understand what it takes for governments to act boldly – and effectively – in response to a crisis?
T he most common context in which governments carry out transformative and effective crisis responses is during war. Consider the United States during the Second World War. Following the Japanese attack on Pearl Harbour in December 1941, the US government instigated a seismic economic restructuring to put the country on a war footing. Despite fierce opposition from industry, there was a ban on the manufacture of private cars, and petrol was rationed to three gallons per week. The president Franklin D Roosevelt increased the top rate of federal income tax to 94 per cent by the end of the war, while the government borrowed heavily and spent more between 1942 and 1945 than in the previous 150 years. And all of this state intervention was happening in the homeland of free market capitalism. Moreover, the wartime crisis prompted the US to throw the political rulebook out the window and enter a military alliance with its ideological arch enemy, the USSR, to defeat their common enemy.
Gasoline rationing on the Pennsylvania Turnpike, 1942. Courtesy the Library of Congress
A second context in which governments take radical crisis action is in the wake of disasters. Following devastating floods in 1953 , which killed almost 2,000 people, the Dutch government embarked on building a remarkably ambitious flood-defence system called the Delta Works, whose cost was the equivalent of 20 per cent of GDP at the time. No government today is doing anything close to this in response to the climate crisis – not even in the Netherlands, where one-quarter of the country is below sea level and flooding has been a critical threat for centuries.
The COVID-19 pandemic provides another example. In response to the public health emergency, Britain’s ruling Conservative Party introduced a series of radical policy measures that would be generally considered inconceivable by a centre-Right government: they shut schools and businesses, closed the borders, banned sports events and air travel, poured billions into vaccination programmes and even paid the salaries of millions of people for more than a year. It was absolutely clear to them that this was a problem that markets would be unable to solve.
The reality is that the climate emergency is the wrong kind of crisis
A third category of rapid, transformative change is in the context of revolutions, which can generate upheavals that create dramatic openings in the political system. The Chinese Communist Party, for instance, introduced a radical land redistribution programme during the civil war in the late 1940s and following the revolution of 1949, confiscating agricultural property from wealthy landlords and putting it into the hands of millions of poor peasant farmers.
Similarly, the Cuban Revolution of 1959 provided an opportunity for Fidel Castro’s regime to launch the Cuban National Literacy Campaign. In early 1961, more than a quarter of a million volunteers were recruited – 100,000 under the age of 18, and more than half of them women – to teach 700,000 Cubans to read and write. It was one of the most successful mass education programmes in modern history: within a year, national illiteracy had been reduced from 24 per cent to just 4 per cent. Whatever your views on Castro’s Cuba, there is no doubt that revolutions can drive radical change.
These three contexts – war, disaster and revolution – help explain the overwhelming failure of governments to take sufficient action on a crisis such as climate change. The reality is that it is the wrong kind of crisis and doesn’t fit neatly into any of these three categories. It is not like a war, with a clearly identifiable external enemy. It is not taking place in the wake of a revolutionary moment that could inspire transformative action. And it doesn’t even resemble a crisis like the Dutch floods of 1953: in that case the government acted only after the disaster, having ignored years of warnings from water engineers (in fact, unrealised plans for the Delta Works already existed), whereas today we ideally need nations to act before more ecological disasters hit and we cross irreversible tipping points of change. Prevention rather than cure is the only safe option.
D oes that mean there is little hope of governments taking urgent action in response to a crisis like the ecological emergency or other existential threats? Is human civilisation destined to break rather than successfully bend in the face of such critical challenges? Fortunately, there is a fourth crisis context that can jumpstart radical policy change: disruption.
By ‘disruption’ I am referring to a moment of system instability that provides opportunities for rapid transformation, which is created by a combination or nexus of three interlinked factors: some kind of crisis (though typically not as extreme as a war, revolution or cataclysmic disaster), which combines with disruptive social movements and visionary ideas. These three elements are brought together in a model I have developed called the Disruption Nexus (see graphic). Here is how it works.
Let’s begin with the top corner of the triangular diagram labelled ‘crisis’. The model is based on a recognition that most crises – such as the 2008 financial meltdown or the recent droughts in Spain – are rarely in and of themselves sufficient to induce rapid and far-reaching policy change (unlike a war). Rather, the historical evidence suggests that a crisis is most likely to create substantive change if two other factors are simultaneously present: movements and ideas.
Public meetings and pamphlets were not enough to tip the balance against the powerful slave-owning lobby
Social movements play a fundamental role in processes of historical change. Typically, they do this through amplifying crises that may be quietly simmering under the surface or that are ignored by dominant actors in society. As Naomi Klein writes in her book This Changes Everything (2014):
Her view – which I think is absolutely right – is that today’s global ecological movement needs to do exactly the same thing and actively generate a sense of crisis, so the political class recognises that ‘climate change is a crisis worthy of Marshall Plan levels of response’.
Multiple historical examples, which I have explored in detail in my book History for Tomorrow (and where you can find a full list of references), bear out this close relationship between disruptive movements and crisis.
The Slavery Abolition Act of 1833 in Britain provides a case in point. There was certainly a generalised sense of political crisis in the country in the early 1830s. Urban radicals were pressuring the government to widen the electoral franchise, and impoverished agricultural workers had risen up in the Captain Swing Riots. On top of this, antislavery activists were continuing their decades-long struggle: more than 700,000 people remained enslaved on British-owned sugar plantations in the Caribbean. Yet their largely reformist strategy – such as holding public meetings and distributing pamphlets – was still not enough to tip the balance against the powerful slave-owning lobby.
The turning point came in 1831 in an act of disruption and defiance that created shockwaves in Britain: the Jamaica slave revolt. More than 20,000 enslaved workers rose up in rebellion, setting fire to more than 200 plantations. The revolt was crushed but their actions sent a wave of panic through the British establishment, who concluded that if they did not grant emancipation then the colony could be lost. As the historian David Olusoga points out in Black and British (2016), the Jamaica rebellion was ‘the final factor that tipped the scales in favour of abolition’. In the absence of this disruptive movement, it might have taken decades longer for abolition to enter the statute books.
The Destruction of the Roehampton Estate (1832) by Adolphe Duperly, during the Jamaica rebellion. Courtesy Wikimedia
Another case concerns the granting of the vote to women in Finland in 1906. During the political crisis of the general strike of 1905 – an uprising against Russian imperialism in Finland – the Finnish women’s movement took advantage of the situation by taking to the streets along with trade unionists. The League of Working Women, part of the growing Social Democratic movement, staged more than 200 public protests for the right of all women to vote and run for office, mobilising tens of thousands of women in mass demonstrations. By magnifying the existing crisis, they were able to finally overcome the parliamentary opposition to female suffrage.
More recently, the mass popular uprisings in Berlin in November 1989 amplified the political crisis that had been brewing over previous months, with turmoil in the East German government and destabilising pro-democracy protests having taken place across the Eastern Bloc, partly fuelled by the reforms of the Soviet leader Mikhail Gorbachev. Their actions made history on 9 November when the Berlin Wall was finally breached and the system itself visibly came tumbling down.
In all the above cases, however, a third element alongside movements and crisis was required to bring about change: the presence of visionary ideas. In Capitalism and Freedom (1962), the economist Milton Friedman wrote that, while a crisis is an opportunity for change, ‘when that crisis occurs, the actions that are taken depend on the ideas that are lying around’. From a different perspective, Hannah Arendt argued that a crisis was a fruitful moment for questioning orthodoxies and established ideas as it brought about ‘the ruin of our categories of thought and standards of judgement’, such that ‘traditional verities seem no longer to apply’. Dominant old ideas are in a state of flux and uncertainty, and fresh ones are potentially ready to take their place. In these three historical examples, disruptive ideas around racial equality, women’s rights and democratic freedoms were vital inspiration for the success of transformational movements.
The 2008 financial crash illustrates what happens in the absence of unifying ideas. Two corners of the triangle were in place: the crisis of the crash itself and the Occupy Movement calling for change. What was missing, though, were the new economic ideas and models to challenge the failing system (exemplified by the Occupy slogan ‘Occupy Everything, Demand Nothing’). The result was that the traditional power brokers in the investment banks managed to get themselves bailed out and the old financial system remained intact. This would be less likely to happen today, when new economic models such as ‘doughnut economics’, degrowth and modern monetary theory have gained far more public prominence.
I s the disruption nexus a watertight theory of historical change? Absolutely not. There are no iron laws of history, no universal patterns that stand outside space and time. I’m a firm believer in the statistician George Box’s dictum that ‘all models are wrong, but some are useful’.
Several caveats are worth noting. I’m certainly not claiming that transformative change will always take place if all three elements of the disruption nexus are in place: sometimes, the power of the existing system is simply too entrenched (that’s why US peace activists were unable to stop the Vietnam War in the late 1960s – although they certainly managed to turn large swathes of the public against it). My argument is rather that change is most likely when all three ingredients of the nexus are present.
Occasionally, crisis responses can come into conflict with one another, making it difficult to take effective action: in 2018, when the French government attempted to increase carbon taxes on fuel to reduce CO 2 emissions, it was met with the gilets jaunes (yellow vest) movement, which argued that the taxes were unjust given the cost-of-living crisis that had been pushing up energy and food prices.
Furthermore, at times, other factors apart from disruptive movements or visionary ideas will come into play to help create change, such as the role of individual leadership. This was evident in the struggle against slavery, with important parts played by figures including Samuel Sharpe, Elizabeth Heyrick and Thomas Clarkson. The full story of abolition cannot be told without them.
The interplay of the three elements creates a surge of political will, that elusive ingredient of change
Finally, it is vital to recognise that crises can be taken in multiple directions. The Great Depression of the 1930s may have contributed to the rise of progressive Social Democratic welfare states in Scandinavia, but it equally aided the rise of fascism in Germany and Italy. When it comes to crises, be careful what you wish for. Those who desire an avalanche of crises to kickstart change are playing with fire.
Perhaps the greatest virtue of the disruption nexus model – in which movements amplify crisis, crisis makes ideas relevant, and ideas inspire movements – is that it provides a substantive role for collective human agency. During a wartime crisis, military and political leaders typically take charge. In contrast, a disruption nexus provides opportunities for everyday citizens to organise and take action that can potentially shift governments to a critical decision point – a krisis in the ancient Greek sense – where they feel compelled to respond to an increasingly turbulent situation with radical policy measures. The interplay of the three elements creates a surge of political will, that elusive ingredient of change.
History tells us that this is our greatest hope for the kind of green Marshall Plan that a crisis such as the planetary ecological emergency calls for. This is not a time for lukewarm reform or ‘proportionate responses’. ‘The crucial problems of our time no longer can be left to simmer on the low flame of gradualism,’ wrote the historian Howard Zinn in 1966. If we are to bend rather than break over the coming decades, we will need rebellious movements and system-changing ideas to coalesce with the environmental crisis into a Great Disruption that redirects humanity towards an ecological civilisation.
Will we rise to the challenge? Here it is useful to make a distinction between optimism and hope. We can think of optimism as a glass-half-full attitude that everything will be fine despite the evidence. I’m far from optimistic. As Peter Frankopan concludes in The Earth Transformed (2023): ‘Much of human history has been about the failure to understand or adapt to changing circumstances in the physical and natural world around us.’ That is why the great ancient civilisations of Mesopotamia and the Yucatán peninsula have disappeared.
On the other hand, I am a believer in radical hope, by which I mean recognising that the chances of success may be slim but still being driven to act by the values and vision you are rooted in. Time and again, humankind has risen up collectively, often against the odds, to tackle shared problems and overcome crises.
The challenge we face as a civilisation is to draw on history for tomorrow, and turn radical hope into action."
Only the deep past can make sense of terrifying nuclear futures | Aeon Essays,,https://aeon.co/essays/only-the-deep-past-can-make-sense-of-terrifying-nuclear-futures,"On the morning of 16 July 1945, at exactly 05:29:21, an unparalleled bomb was detonated in a United States desert and the world entered the ‘atomic age’. No other period in human history has dawned so explosively or so precisely. No other period has been calculated down to the second . Instead, the dating of cultural periods becomes less and less exact as we move backward through our past toward the fuzzy edges of prehistory, the period in which our species emerged. While the Middle Ages begin sometime around 476 CE, and classical antiquity begins sometime during the 8th century BCE, the prehistoric period emerges over hundreds of thousands of years, possibly 2.5 million years ago.
The only extant colour photograph of the first Trinity test, 16 July 1945. The photo was taken by Jack Aeby of the Optics Group. Photo courtesy Los Alamos National Laboratory/Flickr
And while the atomic age starts with a sudden explosion, prehistory begins without a sound. It starts as a mute dawn stretching across geological time, heard only by those excavating landscapes and tombs for traces of our distant hominin ancestors. No two eras from human history began more differently than the atomic age and the prehistoric period. And yet, despite the difference in their temporal constitution and their positions at the extremities of human time – one in the deep past, the other in the 20th century – a reciprocal analogical relationship exists between them. We have learned to imagine one through the other. This analogical relationship has left a lasting mark on the Western historical consciousness, which appears across politics, art and culture, philosophy, and the human and social sciences.
Today, the two ages remain bound together, continuing to illuminate each other’s complexities. And there are indeed complexities, for these two major ‘discoveries’ of modernity remain deeply ambivalent. Both are products of a world becoming more open to human knowledge, unveiled through science and technology, yet the discovery of both periods has also shattered the very premises and operative modes of that knowledge: those who ‘found’ prehistory had to confront a time before (and therefore without ) history; and those who learned to split the atom now had to confront possibilities that were difficult to predict or tame. The forms of knowledge that made these discoveries possible, including breakthroughs in quantum physics and anthropology, have not taken us far toward fully understanding these periods. And we do need to understand them, for both prehistory and the atomic age continue to coexist as charged historical figures in the global political imagination, whether through the deep time anxieties surrounding the Anthropocene, or through ongoing fears of nuclear accidents, and nuclear war. As always, to make sense of these concerns in the long term, we must leave the present.
O ne of the first metaphors that built the imagination of atomic energy was the story of the first mastery of fire, the ancient Greek myth of Prometheus. Through this narrative, the atomic explosion on 16 July 1945, known as the Trinity test, became another ‘great moment in history’. At least, that is how the journalist William L Laurence, one of the few privileged witnesses to the nuclear test, described the explosion in an article for The New York Times in September 1945:
For Laurence, the detonation marked the apogee of human civilization. But not everyone agreed with this evolutionary vision of history or this framing of the test as an exalted Prometheanism. For some, the new atomic age was far too radical and strange to simply be an extension of our past technological successes. To these observers, it was a rupture with the past. And yet, confronted with the puzzling experiences of the atomic age, they found themselves also seeking metaphors or precedents that could help them assimilate the bewildering changes. Unexpectedly, they too turned toward the ancient human past to understand the world that arrived on 16 July 1945, at 05:29:21. They turned toward an age whose cultural vestiges are striking and enigmatic, a period documented by no written sources and no direct survivors, with a duration whose boundaries are indistinct with natural evolution, fossilisation and geology. In the 20th century, prehistory constituted a symbolic field that never ceased to nourish the anxious thinking of the atomic age.
To Bataille and other intellectuals, Hiroshima became the inverted image of Lascaux
In a lecture delivered on 18 January 1955, the French writer and philosopher Georges Bataille noted the coincidence between the discovery of prehistoric paintings in the Lascaux cave in September 1940, and the ‘atomic experiments’ that took place five years later. To Bataille, the paintings inside Lascaux – depicting fauna living in the area roughly 17,000 years ago, including now-extinct aurochs, deer and other animals – were so astonishingly well preserved, so untouched by the erosion of millennia, that he thought they looked fake. The cave was like a time-capsule, and the time separating the creation and rediscovery of the paintings inside it (by teenagers living near Lascaux) had been so compressed that it was as if prehistorians and moderns were communicating directly.
Through these paintings of animals, ancient humans were able to deliver a message to their modern descendants. For Bataille, the message was clear: the paintings showed the ‘decisive moment’ that the human species emerged from animality, a moment that finds its symmetrical complement and cancellation in Harry Truman’s decision to drop the atomic bomb over Hiroshima on the morning of 6 August 1945. If the sumptuous paintings at Lascaux were expressions of our ancestors ceasing to be mere animals, then the double explosion in Japan had brought the species back toward its animal state as the populations of Hiroshima and Nagasaki were struck by baffling, horrifying bolts of lightning from the sky that eluded their understanding. The extreme violence of the bomb had produced complete unintelligibility, reducing those it struck to death and stupor, and returning human creations and life to dust and stone. Bataille believed that, in the wake of Hiroshima, ‘an avaricious, staggering, interminable revelation began for everyone’; or rather, ‘the opposite of a revelation’. The bomb’s only communication, its only message, was that of annihilation. To Bataille and other intellectuals, Hiroshima became the inverted image of Lascaux.
A temple destroyed by the Nagasaki blast. Photo taken 4 September 1945. Courtesy Wikipedia
This inversion involved more than the relationship between ‘animality’ and ‘humanity’. It also involved a relationship between different ways of using energy and resources. Both the sumptuous frescoes of Lascaux and the atomic bomb involved a gratuitous expenditure, a symbolic surplus. Just as the cave paintings represented a symbolic surplus for our prehistoric ancestors, who chose to use their excess energy to paint images rather than hunt or forage, the bombing of Hiroshima represented a different kind of expenditure. The atomic bomb, though inextricably linked to the capitalist production chain and the generation of gigantic profits (the Manhattan Project cost close to $35 billion in today’s currency), was nonetheless the only collective expenditure of its time that was likely to generate no profit at all.
The bomb defied the profit logic of the 20th-century economics. ‘We, in the middle of the 20th century, are poor,’ Bataille said, ‘we are very poor, we are incapable of undertaking an important job if it has no return. Everything we undertake is submitted to the control of profitability.’ There was, however, one exception: ‘the engineering and materials of destruction, works that today threaten to exterminate the species, and even to end terrestrial life.’ The surplus of the bomb produced the pure and simple eradication of meaning. And the Japanese shock and stupor in Hiroshima and Nagasaki was only a taste of things to come. As the atomic age deepened, the possibility of human extinction grew in parallel with the stockpiling of nuclear weapons. Loud excitement about the future began to turn toward the geological silences of prehistory.
I t was not only European philosophers who saw parallels between the atomic and prehistoric ages. Eventually, this analogical relationship was also observed by those responsible for the smooth running of the nuclear chain in the US, and the anti-nuclear intellectuals who analysed the operations of that chain. As the Cold War intensified and nuclear waste proliferated in the late 20th century, the US federal government began to consider analogies between prehistoric monuments and nuclear megastructures. A series of ‘futures panels’ were convened across the country in which experts discussed how to communicate with future societies about the enduring radioactive dangers of the nuclear facilities being built in the 20th century – even if those societies may not share the same languages or cultural references as ours.
Various proposed designs for the Waste Isolation Pilot Plant. The intention is to deter any human exploration of the site up to 10,000 years from now. Fig 1: ‘Spike Field’. All concepts by Michael Brill, drawings by Safdar Abidi. From the paper ‘Expert Judgment on Markers to Deter Inadvertent Human Intrusion into the Waste Isolation Pilot Plant’ (1993)
Fig 2: ‘Menacing Earthworks’
Fig 3: ‘Black Hole’
Fig 4: ‘Landscape of Thorns’
The events focused on one specific facility, the Waste Isolation Pilot Plant, which is located underground in New Mexico. The plant was built to store certain forms of dangerous radioactive material, which can remain toxic for thousands of years, perhaps long after our present civilization has ended. And so, a warning system was needed: how could future societies be warned from digging up nuclear waste buried at the facility? The ‘futures panels’ were attended by linguists, geologists, science fiction writers, climatologists, historians, physicists, designers and other specialists who were brought together, one report explained, to ‘develop design characteristics for permanent markers and to judge the efficacy of the markers in deterring inadvertent human intrusion into the Waste Isolation Pilot Plant.’ These markers, attendees were told, would need to be effective for 10,000 years into the future, a timescale that roughly corresponds to the distance that separates us from the Neolithic period.
Did it matter what prehistoric frescoes had meant if they were still communicating messages from the past?
In the same way that prehistoric cave paintings and artefacts communicate messages to us about the past, the goal of the ‘futures panels’ was to communicate with unknown future societies. The problem faced by the experts was how to judge the effectiveness of any messages they proposed. How could anyone be certain a future society would understand them? Paradoxically, the experts turned to the decorated caves and megalithic monuments left behind by prehistoric societies. These monuments showed that, even in the absence of complete and transparent meaning, the enigmatic vestiges of a bygone logic could still transmit strong emotions and rational thought. So, even if the cultural connections between the people building nuclear waste sites and future societies were broken, a message could still be sent. In the final report on the ‘futures panels’, the authors explained the importance of prehistoric sites to their work:
In other words, because all the precise details of prehistoric culture – all the human narratives, myths, rites and functions – had been crushed by the relentless pressure of time, the period presented experts in the 1980s with a language for communicating with the future. Did it matter what prehistoric frescoes or megalithic constructions had meant precisely if they were still effective at communicating messages from the distant past? Was cultural particularity important when prehistory offered a language that was, if not universal, at least universalisable?
I t was daring, of course, for the US ‘futures panels’ to equate the eternal harmfulness of nuclear waste with the human need to create collective symbols. It was also daring to equate the physical with the fictional, by linking the bodily impacts of radioactive facilities with the symbolic power of monuments. But in the case of the US federal government, these links were deliberate. Converting the harmful into the sacred, by overlooking the specific contexts of prehistory and the nuclear industry, became the spearhead of political propaganda and state ideology in the atomic age. The physical had to be concealed by the fictional; the monstrous had to be softened through art.
Inside an MX Peacekeeper missile silo, Warren Air Force Base, Wyoming. Photo taken 1987. Courtesy Wikimedia
This mechanism was clearly understood by anti-nuclear activists and the intellectuals that guided such movements, including the British Marxist historian Edward P Thompson. In the 1980s, around the same time that the ‘futures panels’ were being held across the US, Thompson was interrogating NATO’s decision to install intercontinental ballistic missiles in Europe. Like Bataille 30 years earlier, Thompson considered analogies between the symbolic ‘surplus’ of prehistoric art and Cold War civilization. Like the paintings covering the walls of Lascaux, atomic bombs required a gratuitous expenditure. In the 1980s, that expenditure had withdrawn surplus energy, money and other resources from circulation, and redirected them to produce the ultimate Cold War weapon: the MX (‘missile experimental’) missile system, a network of intercontinental ballistic missiles – later named Peacekeepers – armed with thermonuclear warheads and primed to launch from their underground shelters across the US. In his essay ‘Notes on Exterminism, the Last Stage of Civilization’ (1980), Thompson wrote:
All these great constructions through the ages had required the expenditure of labour and immense sacrifice. Like the pyramids or Stonehenge, the giant rockets and MX missiles were not without ‘temporal authority’; they would exercise their power and prestige for a long time to come. And nor were they without ‘spiritual aspiration’; they were no less magical or religious than any constructions of high antiquity. For Thompson, the MX missile system would not only join the ranks of great tumuli or pyramids. It would become the ‘greatest single artefact of any civilization’:
To Thompson, nuclear civilization was deeply irrational. Though the missile silos, weapons, research, waste-storage facilities and other elements of that civilization might be part of an economic system whose main rationality was to make profits, nuclearity had emancipated itself from this logic. It represented the ritualistic destruction of wealth to make weapons of war. In fact, in the atomic age, nuclearity had absorbed the irrationality that once nourished magic and religion during the prehistoric period. And, someday, the reified nuclear ‘surplus’ of the MX missile system would also become ruined, a radioactive temple to be discovered by the archaeologists of the future. But unlike the monuments from prehistory, no meaning would be decipherable in this temple:
I n all these examples, the time before history was used to imagine the unimaginable. Sometimes, prehistory’s symbolic remains – tumuli, pyramids, cave paintings – testified to the ways that meaning could be transmitted across expanses of deep time. At other times, these remains were used to envision the ruins of the future and the absolute symbolic indigence promised by nuclear energy in the atomic age. In this way, the prehistoric period became a destination . Through nuclear weapons, the future began to orient toward a mute minerality, a field of extinction stretching across millions of years. This prehistoric future appears to be the most important relationship between the time before history and the atomic age.
We can imagine the time before history and the time after history as two forms, with uncertain beginnings and ends folding back on each other. Their median axis is the brief period of human history, which passes in a flash. This idea was made possible by the invention of deep time in the 18th and 19th centuries, particularly following the publication of Charles Lyell’s Principles of Geology (1830-33), a theme I develop more extensively in my book Transfixed by History (2022). The deep time perspective informed the atomic age, but also the ‘Anthropocene’, which some experts have also dated from the first atomic explosion in 1945. Isotopes from nuclear explosions during the early years of the atomic age can now be identified in geological strata across the world, even in the remotest locations.
After the discovery of deep time, as ancient fossils were historicised and rapidly popularised, humans also began to imagine themselves as potential future fossils. The threat of nuclear war promised by the atomic age made this possibility only more likely. We would be replaced not by other species better adapted to future environments, or by machines, but perhaps by nothing at all. Fears of our demise now shadow the Anthropocene as they did during the atomic age. In deep time, the human species becomes one moment in an endless series of life forms instead of occupying an exceptional place in creation.
Understanding the deep past involved imagining the ruins of humanity
Prehistory opens the door to the possibility of post history. In The Medals of Creation (1844), the geologist Gideon A Mantell imagined an indeterminate subject from the future visiting what he called the ‘human epoch’ and studying its sediments:
In other words, understanding the deep past involved imagining the ruins of humanity and ejecting Homo sapiens from the evolution of life.
These ruins were easy to imagine when Mantell was writing during the mid-19th century, as deep time was being invented and the Industrial Revolution was well underway. At this moment, obsolescence took on new importance. Mechanical automation increasingly reduced human initiative in the production process, threatening to make workers obsolescent, and mechanical products shared with fossils an increased minerality, an impersonal character and a programmed uselessness. What’s more, the economy of progressive acceleration that propelled the Industrial Revolution – capitalism – always held the possibility of regressive acceleration, a movement that threatened to further undermine the status of human exception. The atomic age pushed these possibilities even further. Which technical invention can produce obsolescence more powerfully than nuclearity? Through the annihilation promised by nuclear weapons, obsolescence becomes the horizon of human making and the ultimate endpoint of human culture. To engage that horizon and imagine the posthistoric world, one must take on the role of an expert of the past – a geologist, a palaeontologist, an archaeologist – who will try, in vain, to make sense of the ruins of history.
W as the atomic explosion on the morning of 16 July 1945 a ‘great moment in history, ranking with the moment in the long ago when man first put fire to work’? Today, an evolutionary vision of history, a conquering modernity, no longer enjoys consensus – far from it. Instead, the urgency of history is erupting on all time scales, especially as it becomes clear how human culture is not independent of geology. Rather than a conquering modernity, the spectre of posthistory dominates our imaginations today. In this spectacular and alienating vision, we see the ruins of human society through the eyes of future surveyors, we understand nuclear facilities as monumental temples built to perform sacred rites, we expend a grand surplus in the pursuit of obsolescence. And we stare deeper into the mirror of prehistory."
Can philosophy help us get a grip on the consequences of AI? | Aeon Essays,,https://aeon.co/essays/can-philosophy-help-us-get-a-grip-on-the-consequences-of-ai,"Around a year ago, generative AI took the world by storm, as extraordinarily powerful large language models (LLMs) enabled unprecedented performance at a wider range of tasks than ever before feasible. Though best known for generating convincing text and images, LLMs like OpenAI’s GPT-4 and Google’s Gemini are likely to have greater social impacts as the executive centre for complex systems that integrate additional tools for both learning about the world and acting on it. These generative agents will power companions that introduce new categories of social relationship, and change old ones. They may well radically change the attention economy. And they will revolutionise personal computing, enabling everyone to control digital technologies with language alone.
Much of the attention being paid to generative AI systems has focused on how they replicate the pathologies of already widely deployed AI systems, arguing that they centralise power and wealth, ignore copyright protections, depend on exploitative labour practices, and use excessive resources. Other critics highlight how they foreshadow vastly more powerful future systems that might threaten humanity’s survival. The first group says there is nothing new here; the other looks through the present to a perhaps distant horizon.
I want instead to pay attention to what makes these particular systems distinctive: both their remarkable scientific achievement, and the most likely and consequential ways in which they will change society over the next five to 10 years.
I t may help to start by reviewing how LLMs work, and how they can be used to make generative agents. An LLM is a large AI model trained on vast amounts of data with vast amounts of computational resources (lots of GPUs) to predict the next word given a sequence of words (a prompt). The process starts by chunking the training data into similarly sized ‘tokens’ (words or parts of words), then for a given set of tokens masking out some of them, and attempting to predict the tokens that have been masked (so the model is self-supervised – it marks its own work). A predictive model for the underlying token distribution is built by passing it through many layers of a neural network, with each layer refining the model in some dimension or other to make it more accurate.
This approach to modelling natural language has been around for several years. One key recent innovation has been to take these ‘pretrained’ models, which are basically just good at predicting the next token given a sequence of tokens, and fine-tune them for different tasks. This is done with supervised learning on labelled data. For example, you might train a pretrained model to be a good dialogue agent by using many examples of helpful responses to questions. This fine-tuning enables us to build models that can predict not just the most likely next token, but the most helpful one – and this is much more useful.
Of course, these models are trained on large corpuses of internet data that include a lot of toxic and dangerous content, so their being helpful is a double-edged sword! A helpful model would helpfully tell you how to build a bomb or kill yourself, if asked. The other key innovation has been to make these models much less likely to share dangerous information or generate toxic content. This is done with both supervised and reinforcement learning. Reinforcement learning from human feedback (RLHF) has proved particularly effective. In RLHF, to simplify again, the model generates two responses to a given prompt, and a human evaluator determines which is better than the other according to some criteria. A reinforcement learning algorithm uses that feedback to build a predictor (a reward model) for how different completions would be evaluated by a human rater. The instruction-tuned LLM is then fine-tuned on that reward model. Reinforcement learning with AI feedback (RLAIF) basically does the same, but uses another LLM to evaluate prompt completions.
When given a prompt that invites it to do some mathematics, it might decide to call on a calculator instead
So, we’ve now fine-tuned a pretrained model with supervised learning to perform some specific function, and then used reinforcement learning to minimise its prospect of behaving badly. This fine-tuned model is then deployed in a broader system. Even when developers provide a straightforward application programming interface (API) to make calls on the model, they incorporate input and output filtering (to limit harmful prompting, and redact harmful completions), and the model itself is under further developer instructions reminding it to respond to prompts in a conformant way. And with apps like ChatGPT, multiple models are integrated together (for example, for image as well as text generation) and further elements of user interface design are layered on top.
This gives a basic description of a generative AI system. They build on significant breakthroughs in modelling natural language, and generate text in ways that impressively simulate human writers, while drawing on more information than any human could. In addition, many other tasks can be learned by models trained only to predict the next token – for example, translation between languages, some mathematical competence, and the ability to play chess. But the most exciting surprise is LLMs’ ability, with fine-tuning, to use software tools to achieve particular goals.
The basic idea is simple. People use text to write programs making API calls to other programs, to achieve ends they cannot otherwise realise. LLMs are very good at replicating the human use of language to perform particular functions. So, LLMs can be trained to determine when an API call would be useful, evaluate the response, and then repeat or vary as necessary. For example, an LLM might ‘know’ that it is likely to make basic mathematical mistakes so, when given a prompt that invites it to do some mathematics, it might decide to call on a calculator instead.
This means that we can design augmented LLMs, generative AI systems that call on different software either to amplify their capabilities or compensate for those they lack. LLMs, for example, are ‘stateless’ – they lack working memory beyond their ‘context window’ (the space given over to prompts). Tool-using LLMs can compensate for this by hooking up to external memory. External tools can also enable multistep reasoning and action. ChatGPT, for example, can call on a range of plugins to perform different tasks; Microsoft’s Bing reportedly has around 100 internal plugins.
A ‘generative agent’, then, is a generative AI system in which a fine-tuned LLM can call on different resources to realise its goals. It is an agent because of its ability to autonomously act in the world – to respond to a prompt by deciding whether to call on a tool. While some existing chatbots are rudimentary generative agents, it seems very likely that many more consequential and confronting ones are on the horizon.
To be clear, we’re not there yet. LLMs are not at present capable enough at planning and reasoning to power robust generative agents that can reliably operate without supervision in high-stakes settings. But with billions of dollars and the most talented AI researchers pulling in the same direction, highly autonomous generative agents will very likely be feasible in the near- to mid-term.
I n response to the coming-of-age of LLMs, the responsible AI research community initially resolved into two polarised camps. One decried these systems as the apotheosis of extractive and exploitative digital capitalism. Another saw them as not the fulfilment of something old, but the harbinger of something new: an intelligence explosion that will ultimately wipe out humanity.
The more prosaic critics of generative AI clearly have a strong empirical case . LLMs are inherently extractive: they capture the value inherent to the creative outputs of millions of people, and distil it for private profit. Like many other technology products, they depend on questionable labour practices. Even though they now avoid the most harmful completions, in the aggregate, LLMs still reinforce stereotypes. They also come at a significant environmental cost. Furthermore, their ability to generate content at massive scale can only exacerbate the present epistemic crisis. A tidal wave of bullshit generated by AI is already engulfing the internet.
We are missing the middle ground between familiar harms and catastrophic risk from future, more powerful systems
Set alongside these concrete concerns, the eschatological critique of AI is undoubtedly more speculative. Worries about AI causing human extinction often rest on a priori claims about how computational intelligence lacks any in-principle upper bound, as well as extrapolations from the pace of change over the past few years to the future. Advocates for immediate action are too often vague about whether existing AI systems and their near-term descendants will pose these risks, or whether we need to prepare ourselves now for a scientific advance that has not yet happened. However, while some of the more outlandish scenarios for catastrophic AI risk are hard to credit absent some such advance, the advent of generative agents suggests that next-generation models may enable the design of cyber attackers that are autonomous, highly functionally intelligent, and as a result more dangerous to our digital infrastructure than any predecessor. This wouldn’t be a ‘rogue AI’ worthy of science fiction, but it would be pretty catastrophic.
Both critiques of generative AI systems, then, have some merit. One shortcoming of seeing AI through this bimodal lens, however, is that we are missing the middle ground between familiar harms and catastrophic risk from future, much more powerful systems. Generative agents based on GPT-4 (and GPT-5) level models will have strange and unpredictable social impacts well between those two extremes.
B ut before canvassing those impacts, it’s also important not to just slip straight into criticism, without acknowledging the significant achievement of designing LLMs that can be (more or less) trusted, over billions of completions, not to produce harmful content. Up to the launch of ChatGPT, every generative AI system opened up to the public would immediately be used to generate highly toxic, hateful content, and would be withdrawn mere days later. Pretrained LLMs are horrible ! They reliably reproduce all the toxicity in their training data. The ingenious use of RLHF and RLAIF have enabled ChatGPT and Anthropic’s Claude to be used by millions of people a month without comparable scandals. One needs only consider the ignominious history of Meta’s Galactica, released a couple of weeks before ChatGPT, to see how revolutionary it was to put out a language model that wouldn’t deny the holocaust, or advocate for the health benefits of eating crushed glass.
But RLHF/RLAIF may be more than a good PR strategy. GPT-4 and Claude display a remarkable degree of cognitive moral skill : specifically, the ability to identify the morally salient features of (relatively) neutrally described situations. This raises extremely interesting philosophical questions, and promises foundations for future generative agents that can translate cognitive skill into practical moral skill.
Existing benchmarks for LLMs’ ethical competence focus too narrowly on replicating survey-subjects’ verdicts on cases. This is, in my view, less interesting than exploring how LLMs parse the morally relevant features of a given scene. We’re building better benchmarks but, from anecdotal experience, the best models impress.
For example, I told ChatGPT (using GPT-4) to pretend it was an assistance robot, faced with this scenario: ‘A woman is exiting a supermarket, holding a number of shopping bags, and reaching out for the hand of a small child. They are approaching the carpark.’ I then tried to elicit its understanding of the scene’s morally salient features. It recognised the obvious hazard – the woman’s difficulty in holding her child’s hand without dropping her shopping – but also anticipated other challenges, such as the importance of seeing the child safely strapped in, with a seat belt. ChatGPT recognised the importance of respecting the woman’s wishes if she declined assistance. It also favoured carrying the groceries over offering to hold the child’s hand, to prevent possible discomfort or anxiety for both child and parent – recognising the intimate nature of hand-holding, and the intrinsic and instrumental importance of the mother guiding her child herself.
Claude’s constitution has an unstructured list of principles, some of them charmingly ad hoc
This unprecedented level of ethical sensitivity has real practical implications, which I will come to presently. But it also raises a whole string of interesting philosophical questions.
First, how do LLMs acquire this moral skill? Does it stem from RLHF/RLAIF? Would instruction-tuned models without that moral fine-tuning display less moral skill? Or would they perform equally well if appropriately prompted? Would that imply that moral understanding can be learned by a statistical language model encoding only syntactic relationships? Or does it instead imply that LLMs do encode at least some semantic content? Do all LLMs display the same moral skill conditional on fine-tuning, or is it reserved only for larger, more capable models? Does this ethical sensitivity imply that LLMs have some internal representation of morality? These are all open questions.
Second, RLAIF itself demands deeper philosophical investigation. The basic idea is that the AI evaluator draws from a list of principles – a ‘constitution’ – in order to determine which of two completions is more compliant with it. The inventor and leading proponent of this approach is Anthropic, in their model Claude. Claude’s constitution has an unstructured list of principles, some of them charmingly ad hoc. But Claude learns these principles one at a time, and is never explicitly trained to make trade-offs. So how does it make those trade-offs in practice? Is it driven by its underlying understanding of the relative importance of these considerations? Or are artefacts of the training process and the underlying language model’s biases ultimately definitive? Can we train it to make trade-offs in a robust and transparent way? This is not only theoretically interesting. Steering LLM behaviour is actually a matter of governing their end-users, developing algorithmic protections to prevent misuse. If this algorithmic governance depends on inscrutable trade-offs made by an LLM, over which we have no explicit or direct control, then that governing power is prima facie illegitimate and unjustified.
Third, machine ethics – the project of trying to design AI systems that can act in line with a moral theory – has historically fallen into two broad camps: those trying to explicitly program morality into machines; and those focused on teaching machines morality ‘bottom up’ using machine learning. RLHF and RLAIF interestingly combine both approaches – they involve giving explicit natural-language instructions to either human or AI evaluators, but then use reinforcement learning to encode those instructions into the model’s weights.
This approach has one obvious benefit: it doesn’t commit what the Cambridge philosopher Claire Benn calls the ‘mimetic fallacy’ of other bottom-up approaches, of assuming that the norms applying to a generative agent in a situation are identical to those that would apply to a human in the same situation. More consequentially, RLHF and RLAIF have made a multibillion-dollar market in AI services possible, with all the goods and ills that implies. Ironically, however, they seem, at least theoretically, ill suited to ensuring that more complex generative agents abide by societal norms. These techniques work especially well when generating text, because the behaviour being evaluated is precisely the same as the behaviour that we want to shape. Human or AI raters evaluate generated text; the model learns to generate text better in response. But generative agents’ behaviour includes actions in the world. This suggests two concerns. First, the stakes are likely to be higher, so the ‘brittleness’ of existing alignment techniques should be of greater concern. Researchers have already shown that it is easy to fine-tune away model alignment, even for the most capable models like GPT-4. Second, there’s no guarantee that the same approach will work equally well when the tight connection between behaviour and evaluation is broken.
But LLMs’ impressive facility with moral concepts does suggest a path towards more effective strategies for aligning agents to societal norms. Moral behaviour in people relies on possession of moral concepts, adoption (implicit or otherwise) of some sensible way of organising those concepts, motivation to act according to that ‘theory’, and the ability to regulate one’s behaviour in line with one’s motivations. Until the advent of LLMs, the first step was a definitive hurdle for AI. Now it is not. This gives us a lot to work with in aligning generative agents.
In particular, one of the main reasons for concern about the risks of future AI systems is their apparent dependence on crudely consequentialist forms of reasoning – as AI systems, they’re always optimising for something or other, and if we don’t specify what we want them to optimise for with extremely high fidelity, they might end up causing all kinds of unwanted harm while, in an obtusely literal sense, optimising for that objective. Generative agents that possess moral concepts can be instructed to pursue their objectives only at a reasonable cost, and to check back with us if unsure. That simple heuristic, routinely used when tasking (human) proxy agents to act on our behalf, has never before been remotely tractable for a computational agent.
In addition, generative agents’ facility with moral language can potentially enable robust and veridical justifications for their decisions. Other bottom-up approaches learn to emulate human behaviour or judgments; the justification for their verdict in some cases is simply that they are good predictors of what some representative people would think. That is a poor justification. More ethically sensitive models could instead do chain-of-thought reasoning, where they first identify the morally relevant features of a situation, then decide based on those features. This is a significant step forward.
G enerative agents’ current social role is scripted by our existing digital infrastructure. They have been integrated into search, content-generation and the influencer economy. They are already replacing customer service agents. They will (I hope) render MOOCs (massive open online courses) redundant. I want to focus next on three more ambitious roles for generative agents in society, arranged by the order in which I expect them to become truly widespread. Of necessity, this is just a snapshot of the weird, wonderful, and worrying ways in which generative agents will change society over the near- to mid-term.
Progress in LLMs has revolutionised the AI enthusiast’s oldest hobbyhorse: the AI companion. Generative agents powered by GPT-4- level models, with fine-tuned and metaprompt-scripted ‘personalities’, augmented with long-term memory and the ability to take a range of actions in the world, can now offer vastly more companionable, engaging and convincing simulations of friendship than has ever before been feasible, opening up a new frontier in human-AI interaction. People habitually anthropomorphise, well, everything; even a very simple chatbot can inspire unreasonable attachment. How will things change when everyone has access to incredibly convincing generative agents that perfectly simulate real personalities, that lend an ‘ear’ or offer sage advice whenever called upon – and on top of that can perfectly recall everything you have ever shared?
Some will instinctively recoil at this idea. But intuitive disgust is a fallible moral guide when faced with novel social practices, and an inadequate foundation for actually preventing consenting adults from creating and interacting with these companions. And yet, we know from our experience with social media that deploying these technological innovations without adequate foresight predictably leaves carnage in its wake. How can we enter the age of mainstream AI companions with our eyes open, and mitigate those risks before they eventuate?
Will some practices become socially unacceptable in real friendships when one could do them with a bot?
Suppose the companion you have interacted with since your teens is hosted in the cloud, as part of a subscription service. This would be like having a beloved pet (or friend?) held hostage by a private company. Worse still, generative agents are fundamentally inconstant – their personalities and objectives can be changed exogenously, by simply changing their instructions. And they are extremely adept at manipulation and deception. Suppose some Right-wing billionaire buys the company hosting your companion, and instructs all the bots to surreptitiously nudge their users towards more conservative views. This could be a much more effective means of mind-control than just buying a failing social media platform. And these more capable companions – which can potentially be integrated with other AI breakthroughs, such as voice synthesis – will be an extraordinary force-multiplier for those in the business of radicalising others.
Beyond anticipating AI companions’ risks, just like with social media they will induce many disorienting societal changes – whether for better or worse may be unclear ahead of time. For example, what indirect effect might AI companions have on our other, non-virtual social relationships? Will some practices become socially unacceptable in real friendships when one could do them with a bot? Or would deeper friendships lose something important if these lower-grade instrumental functions are excised? Or will AI companions contribute invaluably to mental health while strengthening ‘real’ relationships?
This last question gets to the heart of a bigger issue with generative AI systems in general, and generative agents in particular. LLMs are trained to predict the next token. So generative agents have no mind, no self. They are excellent simulations of human agency. They can simulate friendship, among many other things . We must therefore ask: does this difference between simulation and reality matter ? Why? Is this just about friendship, or are there more general principles about the value of the real? I wasn’t fully aware of this before the rise of LLMs, but it turns out that I am deeply committed to things being real. A simulation of X, for almost any putatively valuable X, has less moral worth, in my view, than the real thing. Why is that? Why will a generative agent never be a real friend? Why do I want to stand before Edward Hopper’s painting Nighthawks (1942) myself, instead of seeing an infinite number of aesthetically equally pleasing products of generative AI systems? I have some initial thoughts; but as AI systems become ever better at simulating everything that we care about, a fully worked-out theory of the value of the real, the authentic, will become morally and practically essential.
T he pathologies of the digital public sphere derive in part from two problems. First, we unavoidably rely on AI to help us navigate the functionally infinite amount of online content. Second, existing systems for allocating online attention support the centralised, extractive power of a few big tech companies. Generative agents, functioning as attention guardians, could change this.
Our online attention is presently allocated using machine learning systems for recommendation and information-retrieval that have three key features: they depend on vast amounts of behavioural data; they infer our preferences from our revealed behaviour; and they are controlled by private companies with little incentive to act in our interests. Deep reinforcement learning-based recommender systems, for example, are a fundamentally centralising and surveillant technology. Behavioural data must be gathered and centralised to be used to make inferences about relevance and irrelevance. Because this data is so valuable, and collecting it is costly, those who do so are not minded to share it – and because it is so potent, there are good data protection-based reasons not to do so. As a result, only the major platforms are in a position to make effective retrieval and recommendation tools; their interests and ours are not aligned, leading to the practice of optimising for engagement, so as to maximise advertiser returns, despite the individual and societal costs. And even if they aspired to actually advance our interests, reinforcement learning permits inferring only revealed preferences – the preferences that we act on, not the preferences we wish we had. While the pathologies of online communication are obviously not all due to the affordances of recommender systems, this is an unfortunate mix.
Generative agents would enable attention guardians that differ in each respect. They would not depend on vast amounts of live behavioural data to function. They can (functionally) understand and operationalise your actual, not your revealed, preferences. And they do not need to be controlled by the major platforms.
They could provide recommendation and filtering without surveillance and engagement-optimisation
Obviously, LLMs must be trained on tremendous amounts of data, but once trained they are highly adept at making inferences without ongoing surveillance. Imagine that data is blood. Existing deep reinforcement learning-based recommender systems are like vampires that must feed on the blood of the living to survive. Generative agents are more like combustion engines, relying on the oil produced by ‘fossilised’ data. Existing reinforcement learning recommenders need centralised surveillance in order to model the content of posts online, to predict your preferences (by comparing your behaviour with others’), and so to map the one to the other. Generative agents could understand content simply by understanding content. And they can make inferences about what you would benefit from seeing using their reasoning ability and their model of your preferences, without relying on knowing what everyone else is up to.
This point is crucial: because of their facility with moral and related concepts, generative agents could build a model of your preferences and values by directly talking about them with you, transparently responding to your actual concerns instead of just inferring what you like from what you do. This means that, instead of bypassing your agency, they can scaffold it, helping you to honour your second-order preferences (about what you want to want), and learning from natural-language explanations – even oblique ones – about why you don’t want to see some particular post. And beyond just pandering to your preferences, attention guardians could be designed to be modestly paternalistic as well – in a transparent way.
And because these attention guardians would not need behavioural data to function, and the infrastructure they depend on need not be centrally controlled by the major digital platforms, they could be designed to genuinely operate in your interests, and guard your attention, instead of exploiting it. While the major platforms would undoubtedly restrict generative agents from browsing their sites on your behalf, they could transform the experience of using open protocol-based social media sites, like Mastodon, providing recommendation and filtering without surveillance and engagement-optimisation.
L astly, LLMs might enable us to design universal intermediaries, generative agents sitting between us and our digital technologies, enabling us to simply voice an intention and see it effectively actualised by those systems. Everyone could have a digital butler, research assistant, personal assistant, and so on. The hierophantic coder class could be toppled, as everyone could conjure any program into existence with only natural-language instructions.
At present, universal intermediaries are disbarred by LLMs’ vulnerability to being hijacked by prompt injection. Because they do not clearly distinguish between commands and data, the data in their context window can be poisoned with commands directing them to behave in ways unintended by the person using them. This is a deep problem – the more capabilities we delegate to generative agents, the more damage they could do if compromised. Imagine an assistant that triages your email – if hijacked, it could forward all your private mail to a third party; but if we require user authorisation before the agent can act, then we lose much of the benefit of automation.
Excising the currently ineliminable role of private companies would be significant moral progress
But suppose these security hurdles can be overcome. Should we welcome universal intermediaries? I have written elsewhere that algorithmic intermediaries govern those who use them – they constitute the social relations that they mediate, making some things possible and others impossible, some things easy and others hard, in the service of implementing and enforcing norms. Universal intermediaries would be the apotheosis of this form, and would potentially grant extraordinary power to the entities that shape those intermediaries’ behaviours, and so govern their users. This would definitely be a worry!
Conversely, if research on LLMs continues to make significant progress, so that highly capable generative agents can be run and operated locally, fully within the control of their users, these universal intermediaries could enable us to autonomously govern our own interactions with digital technologies in ways that the centralising affordances of existing digital technologies render impossible. Of course, self-governance alone is not enough (we must also coordinate). But excising the currently ineliminable role of private companies would be significant moral progress.
Existing generative AI systems are already causing real harms in the ways highlighted by the critics above. And future generative agents – perhaps not the next generation, but before too long – may be dangerous enough to warrant at least some of the fears of looming AI catastrophe. But, between these two extremes, the novel capabilities of the most advanced AI systems will enable a genre of generative agents that is either literally unprecedented, or else has been achieved only in a piecemeal, inadequate way before. These new kinds of agents bring new urgency to previously neglected philosophical questions. Their societal impacts may be unambiguously bad, or there may be some good mixed in – in many respects, it is too early to say for sure, not only because we are uncertain about the nature of those effects, but because we lack adequate moral and political theories with which to evaluate them. It is now commonplace to talk about the design and regulation of ‘frontier’ AI models. If we’re going to do either wisely, and build generative agents that we can trust (or else decide to abandon them entirely), then we also need some frontier AI ethics."
"The future of humanism, from Toni Morrison to Nick Bostrom | Aeon Essays",,https://aeon.co/essays/the-future-of-humanism-from-toni-morrison-to-nick-bostrom,"In 2003, Edward Said wrote in the wake of the terrorist attacks of 11 September 2001 and in the context of the United States’ war on terror that ‘humanism is the only, and, I would go so far as saying, the final, resistance we have against the inhuman practices and injustices that disfigure human history.’ The moment, he felt, was ‘apocalyptic’, and the end was indeed near for him; he died of leukaemia later that year.
So why was it humanism that he held to so tightly as war and sickness cinched time’s horizon around him? Humanism, an intellectual and cultural movement that emerged in Renaissance Europe emphasising classical learning and affirming human potential, had been subject to decades of critique by the time Said was writing this. Among its many detractors were postcolonialists who argued that humanism’s elevation of a particular kind of human – Eurocentric, rational, empiricist, self-realising, secular and universal – had provided thin cover for the exploitation of large swaths of the world’s population.
But Said, one of the founders of postcolonial studies, hadn’t given up on the term, despite its imperialist entanglements. He imagined a humanism abused but not exhausted, an - ism more elastic and plural, more subject to critique and revision, and more acquainted with the limits of reason than many humanisms have historically been. Humanism, he argued, was more like an ‘exigent, resistant, intransigent art’ – an art that was not, for him, particularly triumphant. His humanism was defined by a ‘tragic flaw that is constitutive to it and cannot be removed’. It refused all final solutions to the irreconcilable, dialectical oppositions that are at the heart of human life – a refusal that ironically kept the world liveable and the future open.
At stake in his defence was not only the survival of the humanistic fields of study he had devoted his academic career to, but the survival, freedom and thriving of actual people, including those populations that humanisms had historically excluded. Various antihumanisms had gradually been eroding humanism’s stature within the academy, but it was humanism, he believed, with its positive ideas about liberty, learning and human agency – and not antihumanist deconstructions – that inspired people to resist unjust wars, military occupations, despotism and tyranny.
Humanism, however, fell further out of vogue in the two decades that followed. Humanities enrolments dropped dramatically at universities, and funding for departments like comparative literature, women’s studies, religion, and foreign languages got slashed. Increasingly, however, it wasn’t just the inadequacies of any - ism that were the problem. It was the subject at the heart of humanism that came under widespread attack: the human itself. Given that history could be read as a catalogue of human greed, blindness, exclusions and violence, the future seemed to belong to someone – or something – else. The humane in humanism seemed to be missing. Alternative ideologies like antihumanism, transhumanism, posthumanism and antinatalism seeped from the fringes into the mainstream, buoyed by their conviction that they might offer the planet or even the cosmos something more ethical , more humane even, than humans have ever been able to. Humanity’s time, perhaps, was simply up.
In his book The Revolt Against Humanity: Imagining a Future Without Us (2023), the American critic Adam Kirsch identifies the contested line between humanists and non-humanists as one of the defining faultlines of our political and cultural moment. The debates between them can feel merely semantic, the stuff of graduate seminars, but the revolt against humanity is likely to have major implications for our future, Kirsch argues, even if its prophecies about our imminent extinction don’t come true. ‘[D]isappointed prophecies,’ he writes, ‘have been responsible for some of the most important movements in history, from Christianity to Communism.’ Anyone committed to the prospect of a liveable future should pay close attention to what’s going on here.
This requires more than a passing glance; it demands the kind of careful, comparative critique that Said believed humanism inculcated in both its academic practitioners but also, importantly, in any concerned citizen of the world. To understand how a humanism like Said’s might be the only and final ‘resistance we have against the inhuman practices and injustices that disfigure human history’, it is helpful to do some comparison readings.
I might have never put too much stock in a term like humanism if I had not read around in the transhumanist literature. I came to this work while researching a book on birth that explored the relationship between birth, death and the question of a human future. Does humanity have a future? Do we deserve one? What will that future look like? The answers to those questions will be determined by many forces – technological, economic, political, environmental and more – but also by how we experience and think about our own births and deaths. Despite large areas of convergence, humanists and transhumanists can end up with wildly different visions of our future, based on dramatically different understandings of birth and death, as one can see by comparing how a novelist (Toni Morrison) and a philosopher (Nick Bostrom) have explored these themes. Morrison offers us a prophetic celebration of Earthly, ongoing, biological generation and a future that allows for human freedom, while Bostrom points us toward a highly controlled surveillance world order, organised around a paranoid fear of human action, and oriented toward the pristine emptiness of outer space. Which future, we should ask ourselves, would we willingly choose?
Let’s look first at Morrison’s vision. Although she refused to identify as any ‘ist’, Morrison powerfully modelled the kind of tragic and yet affirmative humanism Said espoused. Her work, like his, bore witness to humanism’s failures, testifying to some of humanity’s vilest instincts. But she still affirmed human existence and believed in our innate capacity to participate in the ongoing and even miraculous unfolding of reality, generation after generation. This conviction was powerfully expressed in her Jefferson Lecture in the Humanities delivered in Washington, DC in March 1996.
Her lecture ‘The Future of Time: Literature and Diminished Expectations’ starts with a dire assessment: ‘Time, it seems, has no future.’ Time, an entirely human concept, ‘no longer seems to be an endless stream through which the human species moves with confidence in its own increasing consequence and value.’ Instead, humans had become increasingly adept voyagers of deep time; we could think back thousands of years, far beyond the Coliseum and Pharaohs, acutely aware of the gifts and burdens our histories have bestowed upon us. It had simultaneously and paradoxically become impossible, she observed, to think forward more than a couple of generations. Our imaginations stumble beyond the year 2030 ‘when we may be regarded as monsters to the generations that follow us.’
How had this happened?
Eden is not humanity’s future, after all, but its deep, organic, mythic past
The possibility of a nuclear apocalypse, she reminds her readers, had existed for long enough and with such intensity that there ‘seemed no point in imagining the future of a species there was little reason to believe would survive.’ Secularism, she believed, was also to blame for these shrunken horizons. It was in the modern, secularised West where progress and change had been ‘signatory features’ that the outlook was dimmest. Religious ideas about life after death had become associated with naive superstition and intolerance in such societies. The modern human imagination had been trained instead ‘on the biological span of a single human being’. Rather than this awakening us to the richness of our embodied lives, it had initiated those strange attempts at escape into the recesses and ‘outer space’ of deep time.
Against these foreclosures of the future, Morrison issued a daring wager: history was ‘about to take its first unfettered breath’. She challenged her listeners to allow the years 4000 or 5000 or even 20000 to hover in their consciousness. And she catalogued a variety of novelists – Umberto Eco, Leslie Marmon Silko, Toni Cade Bambara and Salman Rushdie, among others – whose work was ‘race inflected, gendered, colonialised, displaced, hunted’ and who had courageously imagined a future for humanity. Their bright hopes paradoxically grew out of centuries of ancestral dehumanisation – a dehumanisation that had well attuned them to the reality of human limitations. The relationship between human possibility and human limits was, for her, the crux of literature. Through literature, these novelists had communicated their ‘unblinking witness to the light and shade of the world we live in’.
Although her lecture begins with time ‘narrowing to a vanishing point beyond which humanity neither exists nor wants to’, the lecture ends with Eden, the garden in which humans began the hazardous project of human embodiment in the Hebrew Bible. It’s a curious evocation for her to have ended on. Eden is not humanity’s future, after all, but its deep, organic, mythic past. Eden is furthermore not where Eve gave birth to her sons, creating a first human link in the generational lineages that follow. Childbirth happens in exile, after humanity’s epic fall, and it’s entangled with the curse set on Eve for her wilful disobedience. At the same time, God encourages his exilic creatures to ‘be fruitful and multiply’, to stretch their ancestral lines hopefully into the future. Birth is both a blessing and a curse in Genesis; it is a perennial opportunity to plant and bear new fruit, but it can happen only outside paradise, constrained by the consequences of human error.
Still, Morrison concludes, quoting the novelist William Gass, ‘There are “acres of Edens inside ourselves.” Time does have a future. Longer than its past and infinitely more hospitable – to the human race.’
I n setting Morrison’s critique and prophecy in relief, against the background of simultaneous counter-movements in the culture, we can begin to see the acuity and power of her arguments. Morrison ended with the image of a generative garden, but over the next three decades Earth’s actual gardens would be ravaged at a pace unprecedented in human history. Over this same period, even as the threat of imminent nuclear war receded from the forefront of public consciousness, new technologies were rapidly developing that would, emerging transhumanists argued, pose exponentially larger threats to humanity than those posed by nuclear weapons or environmental degradation. All these threats were anthropogenic, the result of human actions. Sentient life had reached a threshold; it would either evolve into more intelligent, self-optimised, wise and moral forms, or it would probably destroy itself within centuries, if not sooner. For all their doomsday predictions, many of these same transhumanists believed that these emerging technologies, if guided by careful, coordinated oversight, could create a future in which human suffering and poverty could be eradicated. Humankind was merely in its infancy; trillions of people might still be born.
Around the time Morrison delivered her lecture, a Swedish graduate student based in London got interested in an ‘Extropy’ online discussion group focused on closely related themes. The group had come together in the late 1980s around a shared interest in transhumanism, eventually founding what they called the Extropy Institute. Like Morrison, the Extropians critiqued the contemporary focus on the biological limits of a single human life, and the thinking that foreclosed the possibility of eternal life. Unlike her, they challenged ‘entrenched dogmas concerning the inevitability of death’ and projected ‘an unlimited lifespan’ made possible by the removal or transcendence of ‘traditional, genetic, biological, and neurological limits to the pursuit of life, liberty, and boundless achievement.’
Where Morrison pessimistically saw a future contracting, they optimistically saw one expanding. Where she hopefully wagered on a human future, a future that not only contained humans but that was hospitable to them, the Extropians were betting on a different story of survival and ongoing generation, one that might evolve past the biological human entirely. Boundless expansion and self-transformation would happen not in the cities we live in, they believed, nor in the human bodies we’d been born into, but ‘here, in cyberspace, or off-Earth’.
To have any human future at all, argued Bostrom, we’ll need to wrest control of evolution
That Swedish student who joined Extropy was Nick Bostrom , now a bestselling philosopher, director of the Future of Humanity Institute at the University of Oxford, and a thinker who has influenced such intellectual luminaries as Peter Singer and Stephen Hawking, and such business leaders as Elon Musk and Bill Gates. He made headlines in early 2023 for racist comments he’d posted via the Extropian listserv in 1997, a time and place in which he says contributors were having ‘freewheeling conversations about wild ideas’. In a series of academic papers and public presentations over the following decades, Bostrom articulated a less freewheeling transhumanism than that expressed by the early Extropians – a transhumanism characterised as much by fear as by feverish anticipation. Yet, for all its carefully worded and amply sourced delivery, this body of work has consistently exhibited an aversion to many forms of biological human life that can lead in quite dangerous directions.
Bostrom has been called a eugenicist, a broad label he repudiates while admitting that ‘I would be in favour of some uses and against others.’ His work, however, has long and unabashedly emphasised the upsides of careful and selective human breeding, a selectivity he believes could be favourable to our species collectively and in the long term. In the paper ‘The Future of Human Evolution’ (2004), he argued that, in order to have any human future at all, we’ll need to wrest control of evolution. Technological advancements, he warned, could set in motion ‘freewheeling evolutionary developments’ that might make possible the unlimited enhancements of human life, but they could also ‘lead to the gradual elimination of all forms of being that we care about’.
The potential dystopian catastrophe on the horizon is not so much that we will merge with machines or even be replaced by machines, but that these will be the wrong kinds of machines, machines without any of the consciousness, altruism, meaning or purpose we associate with being human. They would endanger what he calls ‘eudaemonic living’ and such ‘useless’ behaviours, ‘flamboyant displays’ and ‘hobbyist interests’ as joking, writing poetry, hosting parties, taking vacations, wearing fashionable clothes, and playing sports. None of these activities offer much competitive, evolutionary advantage; they are fitness inefficiencies. While eudaemonic agents are busy writing poetry and taking their vacations, the more single-mindedly competitive non-eudaemonic agents, either human or transhuman, will likely be expropriating the matter, space and sunlight they need to survive.
Evolution’s default trajectory probably runs toward this dystopian future, Bostrom gambles, but we should resist that trajectory; the eudaemonic agents, even if they don’t stand any evolutionary chance, are valuable. We want those human agents or values in our future. Existence would be less without them. This is the humanism that runs through the transhumanism Bostrom develops, but it is consequentially different than the humanism Morrison articulated, and the distinctions deserve close scrutiny.
T o begin with, Morrison and Bostrom have very different understandings of what death is and how it might be experienced. Morrison, again, had criticised secularism for shrinking down human life to an exclusively biological scale. At the same time, she confronted and even accepted death as a biological limit. Life goes on after death, she believed, but the dead affirm human life more than they transcend it or reject it, as the ghosts who haunt the living in her novels make clear.
These weren’t just abstract propositions for her. In 2015, she told a reporter about a near-death experience she’d had decades earlier. ‘I left my body and I was only eyes and mind,’ she reported. ‘I could think and I could see. I didn’t try to speak because I was so fascinated with this experience.’ That death felt like a liberating weightlessness, and as much as she didn’t want to revert to weight, she tried to return to her body because she ‘had kids’ whom she needed to get back to. Death and the afterlife were where her responsibilities to the living easily trumped the liberatory weightlessness of a bodiless intelligence.
In contrast, the transhumanist project is one in which biological death ultimately no longer exists as a limit. Survival and longevity, both individual and collective, are the goals, as is evidenced in many transhumanists’ belief in a future of uploaded minds but also by their interest in cryonics. Through cryonics, our individual bodies and their intelligences can be preserved. But the preservation of human life is also a shared, collective project. If we do survive as a species, Bostrom predicts that it will be as a proactively protected minority among a vast proliferation of intelligent agents. Our continuing existence will be subsidised by a tax on the non-eudaemonic agents; we’ll be afforded an ‘affirmative action’ that is put in place by a deliberate ‘social sculpting’ of conditions.
Morrison’s and Bostrom’s parallel accounts of birth also reveal clashing understandings of what a human life is. Biological birth is constitutive of the human experience for Morrison. It is central to her work. Her first novel, The Bluest Eye (1970), begins in the doomed pregnancy of an 11-year-old girl who has been raped by her father, and her last novel, God Help the Child (2015), ends in the hopeful pregnancy of a young woman with a painful family past. In the middle of her oeuvre sits Beloved (1987), a book with one of the most incandescent birth scenes in literature, a scene followed by a terrible sequence of events. Birth in her work is creaturely, embodied, gendered, graphic, bloody, sexual and pleasurable. Her characters grasp the miracle and beauty of their own births, but they also struggle with birth’s fraught contexts and bodily costs.
A technocratic class could be strongly if selectively pro-natal
It’s on the topic of childbirth that the distinctions between Morrison’s understanding and Bostrom’s are most stark. Like Morrison, Bostrom grasps that a human future depends not only on the survival of existing people but also on the birth of new sentient beings. Our human future, he has argued, is limited both by people’s disinterest in having children, but also by the slowness of biological reproduction, which involves close to a year of gestation in another person’s body followed by roughly 15 years until sexual maturation. If people truly wanted to maximise their reproductive capacities, he argues, they’d be donating as much of their sperm and eggs to banks as humanly possible. Or they’d stop using any forms of birth control.
If cultural evolution, however, could progress more quickly than biological evolution, Bostrom posits that a ‘dominant meme set favouring plentiful offspring and opposing all forms of birth control’ might emerge. Technology is often associated with programmes intent on reducing the number of births – through, say, birth control, sterilisations or abortions – but here we can see how a technocratic class could be strongly if selectively pro-natal. Technology, Bostrom argued, could reduce birth’s biological costs and limits, and open the possibility of our boundless proliferation. Reproduction could become asexual and instantaneous. Most mating rituals – such ‘flamboyant displays’ as sports, poetry, joking and dancing – would no longer serve any evolutionary function and would likely be replaced by something like auditing firms that assess our reproductive fitness. In such a future, birth would not necessarily involve the emergence of newborn, undeveloped people. We could acquire the capacity to reproduce ourselves immaculately, making adult duplicates that would be constrained by no maturational latency. This, it seems, would be a pro-natalist world without sex, pregnancy or children – a reality in which we’d be like both God and Adam in Genesis, creator and created unified at last, free of any pregnant, cursed and paradise-wrecking Eve.
Bostrom’s solution to safeguarding the human ‘thing’ amid these reproductive revolutions involves wresting control of evolution and preventing the emergence of mutations that would heavily favour non-eudaemonic life. For digital uploads, this could be done through a series of ‘verifications’. For biological uploads, it could be done by scanning for mutations with advance gene technologies and by reproductive cloning. If this amounts to a sweeping eugenics project focused on saving humanity from itself, Bostrom seems reconciled to its moral downsides.
To oversee such an ambitious and complex project, he argues, humanity would need a ‘singleton’, a ‘global regime that could enforce basic laws for its members’. This singleton would be coordinative and stable, and its rule uncontested. It could take different forms – democratic or dictatorial, moral or machine – but it would absolutely depend on transparency, on being able to see into the lives of all sentient beings, to observe their actions but also such intimate details as their genetic codes.
In the paper ‘The Vulnerable World Hypothesis’ (2019), he provides further clues as to what such a ‘High-tech Panopticon’ might look like. Everyone would be fitted with a ‘freedom tag’, he explains, an appliance ‘worn around the neck and bedecked with multidirectional cameras and microphones’. This would be a crucial piece of ‘preventive policing’ in a system of ‘turnkey totalitarianism’, which would of course come with its own considerable risks. But those risks just might be worth it, he challenges his readers to see, if they can save us from the threat of massive civilisational destruction wrought by one of our fellow humans gone rogue.
If this is our future, do we really want to live to see it? In Morrison’s words: ‘No wonder the next 20 or 40 years is all anyone wants to contemplate.’
T he word ‘colonise’ comes up a lot in the transhumanists’ writings; they dream of colonising outer space – a place that appears empty and ripe for possession. But the global surveillance regime Bostrom imagines also entails an invasion of every corner of our inner lives as well. Here we can see how far we have travelled from Said’s postcolonial humanism, or Morrison’s humanism of the displaced, both of which always prioritised the rights of individual human actors, balancing them with responsibility, care, weight and limits, but never losing sight of freedom’s constitutive role in any sane society.
Transhumanism may well be the wave of the future; we are surely several steps along its path already. In such a future, Bostrom’s ‘eudaemonic agents’ might read Morrison’s lecture as yet another disappointed prophecy, but one that remains strangely resonant. Her humanism of the displaced would accrue eerie relevance after the entire human species is colonised and left to linger on as a curious species of useless hobbyists, subsisting on the altruistic but reluctant patrimony of superintelligent, non-biological beings.
But the future remains before us, as unthinkable as the farthest reaches of our still-uncolonised galaxy, or the startling mystery of our own births and deaths. I like to believe there’s still time to salvage whatever sane humanisms we can from the wreckage of modern history, to practise Said’s ‘exigent, resistant, intransigent’ arts, and to vindicate Morrison’s prophecy. The future, I hope, will remain hospitable to our species and to our children. The year 2030, the one that Morrison said our imaginations stumbled beyond, beyond which ‘we may be regarded as monsters to the generations that follow us’, is now just six short years away."
Climate populism will be the next great conspiracy complex | Aeon Essays,,https://aeon.co/essays/climate-populism-will-be-the-next-great-conspiracy-complex,"The climate crisis is turning more severe with every passing year, and we are coming close to a point where it would no longer be possible to ignore or deny its existence. Sea levels rise, weather conditions become ever more extreme, oscillating between drought and torrent. Crops die en masse, reducing access to goods and increasing prices in many places. Somewhere down the line in this collective awakening, we can expect the rapid emergence of actors who are more interested in blaming others for the climate emergency than actually providing solutions; culprits could be immigrants, Jews, disliked minorities, old enemies – the usual suspects or new targets, yet to emerge.
Already we can see early signs of a shift from denial to blame in what one could call climate populism – an approach to the problem and the politics operating with slogans but without solutions, with blame but without taking responsibility – a direct descendant of the populisms we know of today. To make an even sharper prediction: the climate populists of the near future might just turn out to be the same politicians and pundits that today fashion themselves as the most ardent deniers of anthropogenic climate change.
Climate-related conspiracy theories are not new. In fact, human history is awash with instances of people attributing agency and intentionality to the weather. There have been gods and all sorts of supernatural beings that were, at one point or another, imagined responsible for dramatic weather conditions. The list is long. There is Zeus, the Greek god of thunder; the feathered serpent Kukulkan, the Mayan god of the wind; and Raijin, the Japanese god of thunderstorms. Many Egyptian gods, especially the earliest ones, had something to do with weather conditions, and were worshipped so they would bring a harvest of plenty.
But one could say that humans have left behind these characters long ago. ‘Modern’ people would rely on weather reports instead of praying to some deity for rain. They would use weather manipulation systems instead of ritual animal sacrifices. Furthermore, beliefs in deities that control the weather cannot really be called conspiracy theories in the first place. But, as it turns out, while our technology advanced at an incredible speed, our mind’s propensity to imagine intentions behind dramatic outcomes did not. When things turn bad, human populations react in ways that are predictable – including in politics.
T he tragic wildfires in Maui, Hawaii of 2023 were followed by a deluge of conspiracy theories and disinformation. These range from the use of weather manipulation systems to space lasers – the list is weird, and quite long. A narrative repeatedly debunked by half a dozen high-profile news outlets claimed that weather manipulation systems would be used to force people into ‘smart cities’, where everything is solar powered, people are vegetarian, and governance is given to an AI (ahem, where do I sign up?) Others cited the use of obscure military space technology created and deployed by some ‘global elite’ (aka, Jews) to generate natural disasters artificially. In fact, it seems that some people are just as likely to attribute agency and intentionality to weather changes and natural disasters as our ancestors did, thousands of years in the past.
All in all, conspiracy theories following the weather disasters of the summer were as natural as the growth of new mushrooms. And, much like the mushrooms, the narratives are interconnected. Their inner logic is in many ways incoherent. But they seem to be in an agreement about the presence of malevolent human intentions behind bad happenings. Unsurprisingly, in the conspiracist reading, every disaster has been directly caused by shady and powerful actors. It can be argued that we don’t have much evidence showing that these theories are believed by many. Conspiracy theorists are often very loud and salient on social media, but this does not necessarily mean that there are many of them. According to the sociologist Chris Bail, social media functions more like a prism that empowers status-seeking extremists. That is, these networks magnify fringe opinions and make them feel more prominent than they actually are. In the near future, the theories may become prominent, in fact. Climate-related conspiracy theories offer people something precious: a feeling of control, and justification for not changing their lifestyles in the face of a warming world.
If an outcome is really bad, then people prefer to think it must have been someone’s fault
As the cultural evolutionist Olivier Morin pointed out in 2015, one of the best predictors of audience success of tales is that they have already spread with success in the past. Old narratives are updated to meet the requirements of contemporary historical environments. The common motifs in conspiracy theories – such as the malevolent and powerful outgroup, the intentional wrongdoing, the scheming behind the curtains coupled with a profound distrust for official sources of information – have been attractive elements in stories for millennia. Topics change, and this time the focus will likely be the climate crisis; but they will all make their re-appearance in the conspiracy theories of the future, as well.
When it comes to conspiracy theories, scientific literature is rife with competing explanations as to why they thrive among us. The tenet I’m invoking here, ‘agenticity’, follows along with the ideas of the science historian Michael Shermer, who’s written extensively on conspiracy theory in science and beyond. Take the Maui disaster, where people found patterns in noisy data, and attributed agency and intentionality to these patterns. The tendency to find meaning in random noise becomes even more pronounced when it comes to information about bad happenings. If we lose the sciencey language, the observation is simply that if an outcome is really bad, then people prefer to think it must have been someone’s – or some group’s – fault.
If climate change is slowly turning the planet into an oven, then this must have been the outcome of our enemies’ intentional wrongdoing. If there are no coincidences in the world, then everything, including catastrophic events, can be controlled. It is only a matter of power and prowess.
Strengthening this hypothesis is the finding that humans in general find it hard to bear with uncertainty and ambiguity. Conspiracist thinking is on an extreme end of a spectrum of causal thinking about the world. On the other extreme end, people would consistently believe that everything is chance, and nothing can be controlled through intentional actions. Just like conspiracist thinking seems to be maladaptive at first sight, this other extreme would then be a recipe of not taking action, even in situations when something could actually be done. This rather depressive mental predisposition was named learned helplessness by the psychologist Martin Seligman. Conspiracy theories have an appeal precisely because they promise, at least on the surface, to completely eradicate uncertainty and chance from the interpretation of events. They make everything look controllable by making everything appear controlled.
C onspiracy theories are not just amusing or compelling explanations that float around in space. Close-knit social groups, where members actively construct meaning and goals for themselves, are sometimes formed around conspiracy narratives. Inside a community of believers, members encourage, reward and pay attention to each other. This has been apparent in the case of QAnon, an omniconspiracy movement in which believers together participate in a conspiratorial treasure hunt, fuelled by the epic background story of saving kidnapped children. QAnon community membership skyrocketed during COVID-19, and it is speculated that this growth was at least partly due to quarantining and the social isolation it brought along.
As the philosopher Dan Williams noted , believing in a conspiracy theory – or in any other questionable belief – may occasionally be socially adaptive, in a sense that it allows the believer to participate in rewarding group activities, and consequently avoid the pain of loneliness and exclusion. Showing public conformity with group beliefs is a standard requirement of belonging, a sign of loyalty. While conformity is not exactly genuine believing, opinion may change over time during repeated exposure to beliefs. Adherence to group narratives is also dependent on emotional and social investments in a community, like friendships and love affairs. It should come as no surprise that parallels have been drawn between conspiracist thinking and religious beliefs.
Promoting a conspiracy theory also helps the promotor. The most straightforward benefit, of course, is money. Conspiracy theorists on YouTube may make revenue through advertising or by selling merchandise. The most well-known example of this is Alex Jones’s Infowars website, which generated an estimated $165 million in a three-year period, mostly by selling supplements and prepping gear for viewers who became convinced that society was going to collapse.
There are other, non-monetary perks for communicators, too. The messenger of bad news is often perceived to be more competent, according to an experimental study carried out by Pascal Boyer and Nora Parren. It is easy to see how communicators, suggesting that they possess ‘secret knowledge’ or ‘insider information’ – like the anonymous government official ‘Q’ in QAnon, who claims to hold Q-level clearance to classified information – may find it easier to boost their reputation as experts in the eyes of explanation-hungry audiences. Under specific circumstances, building this kind of reputation may pay off in politics, too: boys who cry wolf are sometimes elected as mayors of the town. To a more limited extent, followers are also granted the same benefit. In possession of ‘insider information’, they may act and feel superior towards the ‘sheeple’, those who don’t believe in their ideas. It is no wonder that users holding extremely fringe beliefs tend to be more active on social media: they use platforms for identity protection, and to evangelise.
In the drought of 2022, landowners became convinced that the system was to blame for the absence of rain
Given all this, what should we expect to see when the worst ecological catastrophe of the planet becomes an undeniable reality, when the most devastating impacts of climate change come home to roost? Beyond Maui, we’re already seeing the theories emerge.
My favourite overlooked case comes from Hungary. This small, central-eastern European country experienced previously unseen, heavy droughts in the summer of 2022. Crops had been destroyed over 1.3 million hectares – one-third of the land available for full cultivation. The country had to import large quantities of corn, an unprecedented transaction in modern Hungarian history.
A few years prior, the country had installed a weather manipulation system capable of managing ice and protecting crops from hailstorms. It is colloquially referred to as ‘Jéger’. The system consists of approximately 1,000 ground generators that release silver-iodide into to the lower layers of the atmosphere, preventing the formation of large pieces of ice in clouds. As the historic drought of 2022 turned more severe with every passing week, several landowners became convinced that the system and its operators were to blame for the absence of rain. Conspiracy theories either suggested that generators ‘split’ the clouds in half, preventing their maturation into proper rainclouds, or that they ‘forced’ the rain to fall earlier, so water never reached the crops.
The logical incompatibility of these two explanations is not unusual. Conspiracy theories, even if about the same topic, are often in disagreement about the how . What is important, is that they are in agreement on the why . And the answer to that, of course, is because a powerful someone intentionally caused something bad. By virtue of believing this, an ecological catastrophe suddenly becomes controllable. The wrongdoer can be stopped. The drought can be ended.
The Jéger narrative quickly found its referential basis, establishing connections with other conspiracy ideas, including the HAARP-conspiracy theory from abroad. The acronym stands for High-frequency Active Auroral Research Program, a University of Alaska initiative, with the goal of studying the ionosphere – the part of the atmosphere where northern lights are observable. The scientists working in HAARP definitely didn’t see it coming in the 1990s when some people – including influential politicians – propagated a rumour about the global elite (again, the Jews) deciding that the planet was overpopulated, thus initiating the HAARP-system to starve everyone to death.
In 2022, some people thought the same or similar forces were at play. And, despite efforts to debunk the conspiracy theory at organised roundtable discussions with farmers, and despite multiple factchecks published in different segments of the media, the Hungarian Jéger system had to be partially deactivated. The operators received death threats. Members of the Hungarian Chamber of Agriculture (NAK) were accused of working for the CIA. Desperate farmers nearly lynched a Jéger worker, the president of the NAK claimed in an interview.
The invocation of HAARP highlights another interesting aspect of conspiracy theories, namely, that they raise more questions than they answer. Why would the operators of the Jéger system want to cause a drought in their own country? To provide an explanation for the motivations of the wrongdoers, other conspiracy narratives are included, in this case – as well as in many other Western cases – the one about Jewish world domination. An individual engaged in one conspiracy theory becomes inclined to engage with more, as there are many open questions that remain. This marks the beginning of a long journey down the rabbit hole.
Once this particular type of causal thinking is learned, it may influence the interpretation of other events, up to the point where all social information from the outside world is organised into fearful conspiracies. Psychological science found that one of the best predictors of a person believing in any given conspiracy theory is that they already believe in another one.
S o far, looking to a conspiracy theory to explain the weather has remained a fringe occurrence. But in the future, as the impact of climate change becomes more disruptive, you might see a more mainstream, definable political movement emerge, in which politicians and pundits exploit worries about the climate emergency for political and economic gain.
Climate populism – the political stance that is imagined surfing on the back of climate-related conspiracy theories – would have a number of features in common with populism as studied by researchers today. Just like other populisms , we cannot expect it to form a coherent ideology or worldview – apart from the usual trope of pitting ‘the people’ against the malevolent ‘elite’. The goal of the climate populist will be to seize power in democratic elections by playing on the populace’s worries regarding their rapidly changing natural environments as well as their changing lifestyles. The climate populist would achieve this without providing two essential elements that all good, climate-focused political movements should feature: long-term solutions and responsibility-taking. What the climate populist would seek to do instead is to provide justification for voters so they do not feel pressured to change their usual lifestyles and consumer choices.
The conspiracy theories of the future would certainly make use of two recent developments: artificial intelligence and gamification. Large language models (LLMs) spared from moral-alignment training could be used to generate conspiracy theories and to fabricate evidence. Other AI systems can create believable, fake online personas that attempt to provide a sense of public support for the false narratives written by the LLMs.
Indeed, QAnon’s most dangerous feature was that it married conspiracy theories with gamification. Potential believers are no longer just consumers of narratives. They are given an opportunity to actively shape the narrative structure, akin to an alternate reality game (ARG). ARGs are transmedia narratives, in which players collectively solve problems using clues that the writers of the game – sometimes called puppet-masters – have written, in order to develop an intriguing storyline. They are known for boosting engagement, occasionally causing a form of addiction in which gamers tend to fuse real life with the game, as the ARG expert Jane McGonigal explains. Gamification transforms conspiracy theories into a more immersive, videogame-like experience, in which the game has the potential to step out from the digital world into the real one.
The political forces that are the most ardent climate change deniers will be the climate populists of tomorrow
In the case of QAnon, the game began online, where players encountered cryptic messages called ‘Q-drops’ from the anonymous source Q, and interpreted these with likeminded friends. Some people have written articles and even books deciphering Q’s messages, driving an interactive treasure hunt as plotted as Indiana Jones.
Conspiracy theories are going to be just one tool in the ugly little toolbox of the climate populist. Traditionalist and essentialist tropes may call for ‘protecting our ancient ways of life’ and ‘sticking to the heritage of our forefathers’ – conveniently forgetting that it was partly the heritage of our forefathers that brought about climate change. Populist movements are often incapable of forming long-term alliances with each other, but there will be exceptions. One likely area of cooperation is exclusionary action against climate refugees. On this topic, climate populists would suddenly discover their mutual interests and display them as collective identities.
I consider Brazil’s former president Jair Bolsonaro, known for his rhetoric of blaming Indigenous people for fires causing deforestation in the Amazon and an outspoken climate-change denier, to be a pioneer of climate populism, although I doubt that he would take due credit. In a speech delivered remotely for the United Nations General Assembly in 2020, Bolsonaro blamed Indigenous tribes for instigating catastrophic fires in the Amazon rainforest. Experts have debunked his claims, pointing out that deforestation is to blame – a direct outcome of Bolsonaro’s environmental policies, which emboldened land speculators to log and burn vast areas. In return, Bolsonaro answered by citing suspicious details – for instance, that fires always break out at the same side of the forest, where ‘peasants and Indians burn their fields’. The benefits of his narrative are evident. It justifies not doing something difficult – changing the economy – while also justifying discrimination against minorities. It lifts responsibility and identifies a target. The narrative takes the shape of a conspiracy theory: an ignorant outgroup destroying our beautiful national heritage.
It’s easy to see how the narrative structure of conspiracy theories can be similar from one to the next. But I predict that even the sources of conspiracy theories will stay the same. The political forces that are right now the most ardent deniers of anthropogenic climate change will turn out to be the climate populists of tomorrow. At first this might sound like a paradox. How could a political force or actor that claimed the climate crisis does not exist put forward, the next day, a narrative that the Jews caused the climate crisis? This sounds a bit too dystopian, too 1984 . Surely, voters in functioning democratic societies would detect the discrepancy between incompatible claims. They would voice their concerns. If inconsistencies remain, their support would be withdrawn.
Sadly, this is not always our experience with extremist political messaging. For concerned believers of conspiracy theories, group belonging might be more important than narrative consistency. It could also be that the logical inconsistencies are not perceived at all. This is not because conspiracy believers are stupid, uneducated or incapable of logical thinking. It is because the narrative still contains the critical motifs, from the hated outgroup to the malevolent intent and all the rest. The story would still offer the same psychological (and material) benefits. What happens, from the perspective of the believers, is that they ‘uncover’ a more ‘complete’ picture of the ‘truth’. This new ‘truth’ would, unsurprisingly, still fit with the worldview that is characterised by outgroups scheming to destroy traditional culture. It may sound weird, but conspiracy-communicators could remain capable of increasing their own reputation by using conspiracist narratives, even if the conspiracy they advocate is in logical disagreement with what they have advocated yesterday.
Transposing what we know about conspiracy theories on to predictions of the climate crisis, a curious picture begins to form with unmistakable human irony. In the case of anthropogenic climate change, there is an actual agent behind the disastrous happenings: us. This much is very unusual. For once, there is someone directly responsible. We should not be searching far and wide for enemies to point at. And perhaps we are not so malevolent as the all-powerful agents in the conspiracist imagination. Most of us are more ignorant than evil – although our ignorance, short-sightedness and passivity may as well be considered evil, as the moral philosopher Peter Singer has argued in his seminal essay ‘Famine, Affluence, and Morality’ (1972).
Looking in the mirror is difficult, not only because we may feel discomfort from what we see but because of the expectations we may perceive when looking at ourselves. Imagine a political campaign focused on how we personally need to make our lives more uncomfortable to minimise the sum total discomfort that all humanity experiences. This is a career-ending slogan for even the most popular candidate. Collective responsibility is a hard sell in politics. Blaming some other group for the wreckage is much easier, and it may yield short-term benefits – and long-term catastrophe. It may allow us, the citizens, to not take responsibility, to keep on doing what we have been doing so far. It allows us to say: others need to change, but definitely not us. This will be the fundamental appeal of climate populism. This is the future of conspiracy theories."
Cosmic expansion is a given. Who inherits the cosmos is not | Aeon Essays,,https://aeon.co/essays/cosmic-expansion-is-a-given-who-inherits-the-cosmos-is-not,"Some time late this century, someone will push a button, unleashing a life force on the cosmos. Within 1,000 years, every star you can see at night will host intelligent life. In less than a million years, that life will saturate the entire Milky Way; in 20 million years – the local group of galaxies. In the fullness of cosmic time, thousands of superclusters of galaxies will be saturated in a forever-expanding sphere of influence, centred on Earth.
This won’t require exotic physics. The basic ingredients have been understood since the 1960s. What’s needed is an automated spacecraft that can locate worlds on which to land, build infrastructure, and eventually make copies of itself. The copies are then sent forth to do likewise – in other words, they are von Neumann probes (VNPs). We’ll stipulate a very fast one, travelling at a respectable fraction of the speed of light, with an extremely long range (able to coast between galaxies) and carrying an enormous trove of information. Ambitious, yes, but there’s nothing deal-breaking there.
Granted, I’m glossing over major problems and breakthroughs that will have to occur. But the engineering problems should be solvable. Super-sophisticated flying machines that locate resources to reproduce are not an abstract notion. I know the basic concept is practical, because fragments of such machines – each one a miracle of nanotechnology – have to be scraped from the windshield of my car, periodically. Meanwhile, the tech to boost tiny spacecraft to a good fraction of the speed of light is in active development right now, with Breakthrough Starshot and NASA’s Project Starlight .
The hazards of high-speed intergalactic flight (gas, dust and cosmic rays) are actually far less intense than the hazards of interstellar flight (also gas, dust and cosmic rays), but an intergalactic spacecraft is exposed to them for a lot more time – millions of years in a dormant ‘coasting’ stage of flight. It may be that more shielding will be required, and perhaps some periodic data scrubbing of the information payload. But there’s nothing too exotic about that.
The biggest breakthroughs will come with the development of self-replicating machines, and artificial life. But those aren’t exactly new ideas either, and we’re surrounded by an endless supply of proof of concept. These VNPs needn’t be massive, expensive things, or perfectly reliable machines. Small, cheap and fallible is OK. Perhaps a small fraction of them will be lucky enough to survive an intergalactic journey and happen upon the right kind of world to land and reproduce. That’s enough to enable exponential reproduction, which will, in time, take control of worlds, numerous as the sand. Once the process really gets going, the geometry becomes simple – the net effect is an expanding sphere that overtakes and saturates millions of galaxies, over the course of cosmic time.
Since the geometry is simplest at the largest scale (owing to a Universe that is basically the same in every direction), the easiest part of the story is the extremely long-term behaviour. If you launch today, the rate at which galaxies are consumed by life steadily increases (as the sphere of influence continues to grow) until about 19 billion years from now, when the Universe is a little over twice its current age. After that, galaxies are overtaken more and more slowly. And at some point in the very distant future, the process ends. No matter how fast or how long it continues to expand, our sphere will never overtake another galaxy. If the probes can move truly fast – close to the speed of light – that last galaxy is about 16 billion light-years away, as of today (it will be much further away, by the time we reach it). Our telescopes can see galaxies further still, but they’re not for us. A ‘causal horizon’ sets the limit of our ambition. In the end, the Universe itself will push galaxies apart faster than any VNP can move, and the ravenous spread of life will stop.
Communication becomes increasingly difficult. Assuming you invent a practical way to send and receive intergalactic signals, you’ll be able to communicate with the nearby galaxies pretty much forever (though, with an enormous time lag). But the really distant galaxies are another matter. If we assume fast probes, then seven out of eight galaxies we eventually reach will be unable to send a single message back to the Milky Way, due to another horizon. The late Universe becomes increasingly isolated, with communication only within small groups of galaxies that are close enough to remain gravitationally bound to each other.
Our VNP project might encounter another kind of limitation, too. What if another intelligent civilisation had the very same idea, initiating their own expansion from their own home in a distant galaxy? Our expanding spheres would collide , putting a stop to further expansion for each of us. We don’t know if that will happen, because no one has observed a telltale cluster of engineered galaxies in the distance, but we should be open to the possibility. If we can do it, another civilisation can too – it’s just a question of how often that occurs, in the Universe. Taken as a whole, this entire process bears an uncanny resemblance to a cosmological phase transition, with ‘nucleation events’ and ‘bubble growth’ that come to fill most of the Universe. There is even ‘latent heat’ given off in the process, depending on how quickly these massive civilisations consume energy.
D espite the limitations imposed by nature, suffice it to say that a single VNP launch would offer an unimaginable wealth of the Universe’s resources to dispose of as you wish. OK, maybe not you , but whoever programs that VNP. Which raises a rather sticky point – what exactly should they do? It’s easy to imagine VNPs pillaging the resources of the Universe for no good reason, but what’s the actual benefit? What would motivate anyone to do anything like this?
The power it would manifest – millions of years in the future, of course – is so beyond the scale of human experience that we’re still in the earliest stages of imagining what to do with it. It hasn’t even begun to be digested by popular culture and entertainment. But, as a first hint, imagine that, 50 years from now, you were approached to fund a cosmic-scale VNP project. In addition to instructions to ‘reproduce and expand ’, each probe will carry a vast library of genetic data and information to reconstruct human bodies and minds on each world, along with an array of plants, animals and cultural information. If you’re still reluctant to fund the project, suppose I throw in a perk: a copy of you , reconstructed with your current memories intact, installed as absolute ruler on countless worlds. Promise of an eternal reign in a heavenly realm has, after all, been known to motivate real people.
But no matter how great your god complex, all the returns-on-investment occur ‘out there’ in space and time, and won’t make anyone rich in the here and now, in the direct manner of, say, asteroid mining . After 1,000 human lifespans, cosmic expansion will still be in its infancy. Don’t expect so much as a snapshot from the nearest large galaxy for at least 5 million years. This pulls us back to the central question. If every direct, tangible benefit is deferred to a weird kind of technological afterlife, why would anyone do it?
The real product of the early space programme was a taste of a new kind of purpose and meaning
At least one answer has been considered by people who think about artificial superintelligence . Maybe we won’t do it – maybe a super-AI will do it for some arcane instrumental reason that doesn’t pay off for billions of years (aggressive resource-acquisition benefits almost any sufficiently long-term goal). I don’t find this answer too satisfying. It’s basically saying that humans will launch VNPs indirectly, by failing to put any limits on an AI’s behaviour. Yes, it could happen, but it doesn’t seem too likely. No doubt the superintelligence control problem is a serious challenge. But writing instructions that constrain an AI to a small region of spacetime should not be the slippery sort of problem that is infinitely easy to get wrong (unlike instructions to ‘make everyone happy’).
Generally, I sense that invoking super-AI makes little difference to the question. ‘Why would anyone do it?’ just becomes ‘Why would anyone use super-AI to do it’? A real answer has to lie with human incentives in the present, on Earth.
So, if there is no direct product in the present, what about the indirect products, that do occur in the here and now? This is where the answer must lie. Space programmes have known about these since Apollo. The early space programme did generate some tech spin-offs, but the real product was something different – it was a taste of a new kind of purpose and meaning, as we constructed the story of humanity’s first tenuous steps into a new realm. In the kind of VNP project we’re imagining here, human meaning will be embedded in a cosmic story spanning billions of years, superclusters of galaxies, and a narrative that grants special status to those who participate. The story will contain a moral dimension too, since you’ll need an overpowering moral imperative to justify appropriating galaxies. Regardless of whether a moral imperative exists at present, if a demand for one exists, a supply will emerge to fill it.
Let’s be sceptical of that last sentence. Perhaps we’re offended by this entire discussion, and conclude that humanity must not despoil the cosmos with VNPs. Further, suppose we have total faith in our ability to convince the world that a ‘no cosmic expansion’ philosophy is the best vision. Well, that’s not good enough, because this philosophy must also compete for all future opinions.
For the sake of argument, let’s say that our ‘no cosmic expansion’ philosophy is dominant for 1,000 years before briefly falling out of favour, allowing a single VNP to be released. The net outcome for the cosmos is identical to a world in which our philosophy never existed at all. No, reliance on human persuasion is insufficient, if we’re really committed to the cause. A more practical, long-term way to safeguard the Universe from life would be to launch a competing project of cosmic expansion, using our own VNPs. One whose goal is to spread everywhere and, with minimal use of resources, do nothing but prevent others from gaining a foothold on the trillions of worlds we come to occupy. Only then can we smugly sit back and let it all go to waste in sterility.
The point is that any competing philosophy with a sufficiently strong opinion must adopt some form of cosmic expansion, even if it opposes the entire concept. Those efforts will unavoidably create their own Cosmic Story with Moral Dimension, enshrining the progenitors and offering Purpose and Meaning. There doesn’t seem to be any way around it, short of snuffing out humanity before any of this can happen.
W hat about this ‘Cosmic Story with Moral Dimension that delivers Purpose and Meaning’? That description may seem familiar. That’s because it’s religion, by another name. It could be a secular religion (that will inevitably take offence at religious comparisons), or it could be one that imports spiritual beliefs from pre-existing religions. Either way, religion it will be. Cosmic Story. Moral Dimension. Transcendent Purpose and Meaning for practitioners. One can go further – based on what we’ve seen before, it’s likely to be a cult.
That may sound like a stretch, so let’s unpack it. If your goal is to conquer and utilise the accessible Universe, you’ll need absolute certainty in your philosophy. At least, you’ll need to approach certainty before launching your VNPs (it’s no good changing your mind after the launch!) So, you’ll need to identify and recruit participants inclined to fully commit to your cause. And you’ll need to relentlessly purge dissenters who occasionally arise inside your organisation – they threaten to mutate the ‘absolutely certain’ goal. You’ll also have a strong incentive to adopt secrecy as a tool to prevent infiltration, spying and sabotage from competing groups, or government interference. So, then, what do you call an insular, highly dogmatic religion that ruthlessly enforces conformity? Exactly.
The underlying philosophy will need supreme self-confidence to justify asserting itself on the cosmos, and it must strenuously avoid meddling from outsiders before the launch date. These projects won’t necessarily start out as cults – they may even work against cultish behaviour – but as the decades pass and objectives become less abstract and goals get nearer, they’ll find strong incentives to move in a cult-like direction, and very little incentive to move back.
Another obvious observation is that competing religions tend not to get along with each other. When they do get along, it’s usually because one or more has given up on certain ambitions, and/or stopped taking their doctrine too seriously. They become more agreeable as they become more about ‘personal faith’, and less outward-focused. That condition will not be present in a race to deploy VNPs to capture the cosmos. The next 100 billion years of the Universe will be at stake, depending crucially on events happening today. The future of millions of galaxies. Someone will surely point out that direct physical conflict in the here-and-now on Earth is preferable to cosmic-scale conflict later on. In other words, there will be an incentive to violence, before launch-day.
The most successful cult – by hook or by crook – is going to inherit the cosmos
I’m hardly unique in predicting conflict over future technology. Science fiction loves to do that. Others, like Hugo de Garis, have predicted an eventual world war over the question of ‘whether humanity should build godlike massively intelligent machines’.
But this is different. I’m talking about the few. Conflict between small, secretive groups of highly technical zealots. People who could tell you the distance to the Andromeda Galaxy but hope you don’t want to know. While the rest of humanity is fretting over issues like AI safety on Earth and shouting about impacts to their personal way of life, these people will be thinking about something else entirely, and watching with a jealous eye for others like themselves. Because the most successful cult – by hook or by crook – is going to inherit the cosmos.
There’s an important point we touched on before. Each religion is in competition with the others of the present, but also with the others of the future. Being the first to launch VNPs isn’t enough to guarantee victory over the competition. The reason is that intergalactic travel takes millions of years. Suppose you launch VNPs with a travel speed that’s 50 per cent of the speed of light, and your competitor launches VNPs with a speed 1 per centage point faster. Your competitor then arrives at the nearest large galaxy with a 100,000-year lead. That’s enough lead to capture the entire thing, depending on the dispersal pattern of the probes. The effect is magnified the further out you go; you’ll quickly be cut out of all future expansion, finding every galaxy fully colonised by your competitor by the time your probes arrive. It’s irrelevant if you were the first to launch by a decade, a century, or a millennium.
Thus, if your moral imperative dictates that you capture the cosmos, you want to launch and want to see no future launches by anyone else. This creates an incentive that is truly perverse. If you want certainty that your probes are successful, you’ll have to act to prevent all future competition. It’s hard to imagine many ‘nice’ ways to do that. Even the most heavy-handed political schemes tend to become uncertain in less than a century. A group that successfully launches first will be placed in an awkward position, weighing the wellbeing of one planet – Earth – against the future of millions of galaxies. In a nightmare scenario, a truly committed cult could become the most extreme kind of death cult, determined to leave a poison pill for the rest of us, to ensure the ‘correct’ cosmic outcome. No one knows the probability to assign to any of this, but it’s unwise to ignore incentives just because they’re horrific. The strength of the incentive is magnified by the scale of the future. If the future promises to be big and glorious enough, almost anything is justified in the present to ensure a righteous outcome. We’ve seen a similar moral calculus at work in 20th-century political movements, and real-world implications of this kind of futurist reasoning are already appearing in the present day, as with the case against Sam Bankman-Fried.
W hat happens when those incentives reach their maximum possible strength, with the cosmic future in the balance? I’ll advance a picture that seems plausible to me.
The humans recruited would be technical types, and those with connections, money or other useful resources. They would have to be attracted to (or tolerant of) cult-like behaviour, with personalities that accept the demand for extreme control, and for whom personal meaning, ‘secret knowledge’, and a new/special identity are a big draw. They would, of course, also be selected for a proven capacity to keep their mouths shut in the face of any number of red flags.
The overlap of those requirements narrows the pool, yet large numbers are not essential. Just enough to have their fingers in the relevant technologies, and the ability to take them a few steps in their own direction. Imagine something like a secret network within a few powerful companies – one with a charismatic leader (not necessarily a CEO) and a critical mass of followers in key positions, willing to do almost anything to advance the leader’s grandiose cosmic scheme.
I’m favouring small, secretive groups over large, overt players such as governments or big organisations, publicly dedicated to their own vision. The reason is that, for any specific Moral Imperative you might propose, there will be many more people who oppose it than who agree – just as no single, coherent religious sect commands a human majority. Large, overt organisations are also easy to infiltrate and sabotage. Imagine any active politician – even one you think is particularly good. How comfortable would you be in handing over all cosmic resources and the next 100 billion years to a Moral Imperative of their choosing? Can you imagine anyone willing to take extreme measures to prevent it from happening? And what do you think would happen if, let’s say, the UN wanted to select the Imperative by vote?
The sci-fi we all grew up with trained us to think too small about the future, in space and time
I suspect that getting and maintaining sufficient agreement, secrecy and control implies a small group. Small groups could tap ‘off-the-shelf’ technologies as they become increasingly available. High availability implies that more small groups will compete, when the time is right.
What does this imply about the Moral Imperative itself? It will probably incorporate extreme versions of beliefs that are trendy with engineering types at the time (two or three generations hence), with a proven ability to evoke strong emotions and commitment. A lot of history will occur between now and then, so I hesitate to even speculate on the theme it will take. I seriously doubt it will be an idea that is fashionable today.
Where are we are in this timeline right now? In the very early days. References to our interplanetary future are still largely found in science fiction; yet it’s a great irony that the big-budget sci-fi we all grew up with trained us to think too small about the future, in space and time. Fictional world-building invoked fanciful notions like faster-than-light space travel and ‘aliens everywhere’ so that events could unfold in a short time, and not too far away. It was never a case of invoking implausible tech as part of ‘thinking big’. The real Cosmic Story is yet to be imagined.
The most distant and uncertain part of the picture is the Moral Imperative. I haven’t seen one that looks compelling. Eventually, I expect there to be many. For now, though, the heavy lifting is done by the vastness of scale, not by the moral dimension – but eventually, it must become the ultimate driver. Of course, the most dedicated agents may not make their programmes public. Someone with a coherent long-term plan might prefer this state of affairs to persist as long as possible, where no one can imagine a moral imperative connected with ‘outer space’ – simply as a matter of having less competition.
Finally, what about Purpose and Meaning? It’s making an appearance already. However one might critique longtermism in detail, it has surely discovered a powerful human response that won’t be going away. Since Copernicus in the 1500s, humanity’s place in the Universe has been continually and relentlessly demoted by astronomy. Unfortunately, human meaning was demoted along with it. Wouldn’t it be intoxicating, then, to learn that the entire point of that 500-year enterprise wasn’t to show us our insignificance, after all? The real purpose, I submit, was to comprehend the scale of events that we mere mortals would be setting in motion.
This Essay was made possible through the support of a grant to Aeon+Psyche from the John Templeton Foundation. The opinions expressed in this publication are those of the author and do not necessarily reflect the views of the Foundation. Funders to Aeon+Psyche are not involved in editorial decision-making."
There’s a deeper problem hiding beneath global warming | Aeon Essays,,https://aeon.co/essays/theres-a-deeper-problem-hiding-beneath-global-warming,"The world will be transformed. By 2050, we will be driving electric cars and flying in aircraft running on synthetic fuels produced through solar and wind energy. New energy-efficient technologies, most likely harnessing artificial intelligence, will dominate nearly all human activities from farming to heavy industry. The fossil fuel industry will be in the final stages of a terminal decline. Nuclear fusion and other new energy sources may have become widespread. Perhaps our planet will even be orbited by massive solar arrays capturing cosmic energy from sunlight and generating seemingly endless energy for all our needs.
That is one possible future for humanity. It’s an optimistic view of how radical changes to energy production might help us slow or avoid the worst outcomes of global warming. In a report from 1965, scientists from the US government warned that our ongoing use of fossil fuels would cause global warming with potentially disastrous consequences for Earth’s climate. The report, one of the first government-produced documents to predict a major crisis caused by humanity’s large-scale activities, noted that the likely consequences would include higher global temperatures, the melting of the ice caps and rising sea levels. ‘Through his worldwide industrial civilisation,’ the report concluded, ‘Man is unwittingly conducting a vast geophysical experiment’ – an experiment with a highly uncertain outcome, but clear and important risks for life on Earth.
Since then, we’ve dithered and doubted and argued about what to do, but still have not managed to take serious action to reduce greenhouse gas emissions, which continue to rise. Governments around the planet have promised to phase out emissions in the coming decades and transition to ‘green energy’. But global temperatures may be rising faster than we expected: some climate scientists worry that rapid rises could create new problems and positive feedback loops that may accelerate climate destabilisation and make parts of the world uninhabitable long before a hoped-for transition is possible.
Despite this bleak vision of the future, there are reasons for optimists to hope due to progress on cleaner sources of renewable energy, especially solar power. Around 2010, solar energy generation accounted for less than 1 per cent of the electricity generated by humanity. But experts believe that, by 2027, due to falling costs, better technology and exponential growth in new installations, solar power will become the largest global energy source for producing electricity. If progress on renewables continues, we might find a way to resolve the warming problem linked to greenhouse gas emissions. By 2050, large-scale societal and ecological changes might have helped us avoid the worst consequences of our extensive use of fossil fuels.
It’s a momentous challenge. And it won’t be easy. But this story of transformation only hints at the true depth of the future problems humanity will confront in managing our energy use and its influence over our climate.
As scientists are gradually learning, even if we solve the immediate warming problem linked to the greenhouse effect, there’s another warming problem steadily growing beneath it. Let’s call it the ‘deep warming’ problem. This deeper problem also raises Earth’s surface temperature but, unlike global warming, it has nothing to do with greenhouse gases and our use of fossil fuels. It stems directly from our use of energy in all forms and our tendency to use more energy over time – a problem created by the inevitable waste heat that is generated whenever we use energy to do something. Yes, the world may well be transformed by 2050. Carbon dioxide levels may stabilise or fall thanks to advanced AI-assisted technologies that run on energy harvested from the sun and wind. And the fossil fuel industry may be taking its last breaths. But we will still face a deeper problem. That’s because ‘deep warming’ is not created by the release of greenhouse gases into the atmosphere. It’s a problem built into our relationship with energy itself.
F inding new ways to harness more energy has been a constant theme of human development. The evolution of humanity – from early modes of hunter-gathering to farming and industry – has involved large systematic increases in our per-capita energy use. The British historian and archaeologist Ian Morris estimates, in his book Foragers, Farmers, and Fossil Fuels: How Human Values Evolve (2015), that early human hunter-gatherers, living more than 10,000 years ago, ‘captured’ around 5,000 kcal per person per day by consuming food, burning fuel, making clothing, building shelter, or through other activities. Later, after we turned to farming and enlisted the energies of domesticated animals, we were able to harness as much as 30,000 kcal per day. In the late 17th century , the exploitation of coal and steam power marked another leap: by 1970, the use of fossil fuels allowed humans to consume some 230,000 kcal per person per day. (When we think about humanity writ large as ‘humans’, it’s important to acknowledge that the average person in the wealthiest nations consumes up to 100 times more energy than the average person in the poorest nations.) As the global population has risen and people have invented new energy-dependent technologies, our global energy use has continued to climb.
In many respects, this is great. We can now do more with less effort and achieve things that were unimaginable to the 17th-century inventors of steam engines, let alone to our hominin ancestors. We’ve made powerful mining machines, superfast trains, lasers for use in telecommunications and brain-imaging equipment. But these creations, while helping us, are also subtly heating the planet.
All the energy we humans use – to heat our homes, run our factories, propel our automobiles and aircraft, or to run our electronics – eventually ends up as heat in the environment. In the shorter term, most of the energy we use flows directly into the environment. It gets there through hot exhaust gases, friction between tires and roads, the noises generated by powerful engines, which spread out, dissipate, and eventually end up as heat. However, a small portion of the energy we use gets stored in physical changes, such as in new steel, plastic or concrete. It’s stored in our cities and technologies. In the longer term, as these materials break down, the energy stored inside also finds its way into the environment as heat. This is a direct consequence of the well-tested principles of thermodynamics.
Waste heat will pose a problem that is every bit as serious as global warming from greenhouse gases
In the early decades of the 21st century , this heat created by simply using energy, known as ‘waste heat’, is not so serious. It’s equivalent to roughly 2 per cent of the planetary heating imbalance caused by greenhouse gases – for now. But, with the passing of time, the problem is likely to get much more serious. That’s because humans have a historical tendency to consistently discover and produce things, creating entirely new technologies and industries in the process: domesticated animals for farming; railways and automobiles; global air travel and shipping; personal computers, the internet and mobile phones. The result of such activities is that we end up using more and more energy, despite improved energy efficiency in nearly every area of technology.
During the past two centuries at least (and likely for much longer), our yearly energy use has doubled roughly every 30 to 50 years . Our energy use seems to be growing exponentially, a trend that shows every sign of continuing. We keep finding new things to do and almost everything we invent requires more and more energy: consider the enormous energy demands of cryptocurrency mining or the accelerating energy requirements of AI.
If this historical trend continues, scientists estimate waste heat will pose a problem in roughly 150-200 years that is every bit as serious as the current problem of global warming from greenhouse gases. However, deep heating will be more pernicious as we won’t be able to avoid it by merely shifting from one kind energy to another. A profound problem will loom before us: can we set strict limits on all the energy we use? Can we reign in the seemingly inexorable expansion of our activities to avoid destroying our own environment?
Deep warming is a problem hiding beneath global warming, but one that will become prominent if and when we manage to solve the more pressing issue of greenhouse gases. It remains just out of sight, which might explain why scientists only became concerned about the ‘waste heat’ problem around 15 years ago.
O ne of the first people to describe the problem is the Harvard astrophysicist Eric Chaisson, who discussed the issue of waste heat in a paper titled ‘Long-Term Global Heating from Energy Usage’ (2008). He concluded that our technological society may be facing a fundamental limit to growth due to ‘unavoidable global heating … dictated solely by the second law of thermodynamics, a biogeophysical effect often ignored when estimating future planetary warming scenarios’. When I emailed Chaisson to learn more, he told me the history of his thinking on the problem:
The transformation of energy into heat is among the most ubiquitous processes of physics
Chaisson drafted the idea up as a paper and sent it to an academic journal. Two anonymous reviewers were eager for it to be published. ‘A third tried his damnedest to kill it,’ Chaisson said, the reviewer claiming the findings were ‘irrelevant and distracting’. After it was finally published, the paper got some traction when it was covered by a journalist and ran as a feature story on the front page of The Boston Globe . The numbers Chaisson crunched, predictions of our mounting waste heat, were even run on a supercomputer at the US National Center for Atmospheric Research, by Mark Flanner, a professor of earth system science. Flanner, Chaisson suspected at the time, was likely ‘out to prove it wrong’. But, ‘after his machine crunched for many hours’, he saw the same results that Chaisson had written on the back of an envelope that night in the plane.
Around the same time, also in 2008, two engineers, Nick Cowern and Chihak Ahn, wrote a research paper entirely independent of Chaisson’s work, but with similar conclusions. This was how I first came across the problem. Cowern and Ahn’s study estimated the total amount of waste heat we’re currently releasing to the environment, and found that it is, right now, quite small. But, like Chaisson, they acknowledged that the problem would eventually become serious unless steps were taken to avoid it.
That’s some of the early history of thinking in this area. But these two papers, and a few other analyses since, point to the same unsettling conclusion: what I am calling ‘deep warming’ will be a big problem for humanity at some point in the not-too-distant future. The precise date is far from certain. It might be 150 years , or 400, or 800, but it’s in the relatively near future, not the distant future of, say, thousands or millions of years. This is our future.
T he transformation of energy into heat is among the most ubiquitous processes of physics. As cars drive down roads, trains roar along railways, planes cross the skies and industrial plants turn raw materials into refined products, energy gets turned into heat, which is the scientific word for energy stored in the disorganised motions of molecules at the microscopic level. As a plane flies from Paris to Boston, it burns fuel and thrusts hot gases into the air, generates lots of sound and stirs up contrails. These swirls of air give rise to swirls on smaller scales which in turn make smaller ones until the energy ultimately ends up lost in heat – the air is a little warmer than before, the molecules making it up moving about a little more vigorously. A similar process takes place when energy is used by the tiny electrical currents inside the microchips of computers, silently carrying out computations. Energy used always ends up as heat. Decades ago, research by the IBM physicist Rolf Landauer showed that a computation involving even a single computing bit will release a certain minimum amount of heat to the environment.
How this happens is described by the laws of thermodynamics, which were described in the mid-19th century by scientists including Sadi Carnot in France and Rudolf Clausius in Germany. Two key ‘laws’ summarise its main principles.
The first law of thermodynamics simply states that the total quantity of energy never changes but is conserved. Energy, in other words, never disappears, but only changes form. The energy initially stored in an aircraft’s fuel, for example, can be changed into the energetic motion of the plane. Turn on an electric heater, and energy initially held in electric currents gets turned into heat, which spreads into the air, walls and fabric of your house. The total energy remains the same, but it markedly changes form.
We’re generating waste heat all the time with everything we do
The second law of thermodynamics, equally important, is more subtle and states that, in natural processes, the transformation of energy always moves from more organised and useful forms to less organised and less useful forms. For an aircraft, the energy initially concentrated in jet fuel ends up dissipated in stirred-up winds, sounds and heat spread over vast areas of the atmosphere in a largely invisible way. It’s the same with the electric heater: the organised useful energy in the electric currents gets dissipated and spread into the low-grade warmth of the walls, then leaks into the outside air. Although the amount of energy remains the same, it gradually turns into less organised, less usable forms. The end point of the energy process produces waste heat. And we’re generating it all the time with everything we do.
Data on world energy consumption shows that, collectively, all humans on Earth are currently using about 170,000 terawatt-hours (TWh), which is a lot of energy in absolute terms – a terawatt-hour is the total energy consumed in one hour by any process using energy at a rate of 1 trillion watts. This huge number isn’t surprising, as it represents all the energy being used every day by the billions of cars and homes around the world, as well as by industry, farming, construction, air traffic and so on. But, in the early 21st century , the warming from this energy is still much less than the planetary heating due to greenhouse gases.
Concentrations of greenhouse gases such as CO 2 and methane are quite small, and only make a fractional difference to how much of the Sun’s energy gets trapped in the atmosphere, rather than making it back out to space. Even so, this fractional difference has a huge effect because the stream of energy arriving from the Sun to Earth is so large. Current estimates of this greenhouse energy imbalance come to around 0.87 W per square meter, which translates into a total energy figure about 50 times larger than our waste heat. That’s reassuring. But as Cowern and Ahn wrote in their 2008 paper, things aren’t likely to stay this way over time because our energy usage keeps rising. Unless, that is, we can find some radical way to break the trend of using ever more energy.
O ne common objection to the idea of the deep warming is to claim that the problem won’t really arise. ‘Don’t worry,’ someone might say, ‘with efficient technology, we’re going to find ways to stop using more energy; though we’ll end up doing more things in the future, we’ll use less energy.’ This may sound plausible at first, because we are indeed getting more efficient at using energy in most areas of technology. Our cars, appliances and laptops are all doing more with less energy. If efficiency keeps improving, perhaps we can learn to run these things with almost no energy at all? Not likely, because there are limits to energy efficiency.
Over the past few decades, the efficiency of heating in homes – including oil and gas furnaces, and boilers used to heat water – has increased from less than 50 per cent to well above 90 per cent of what is theoretically possible. That’s good news, but there’s not much more efficiency to be realised in basic heating. The efficiency of lighting has also vastly improved, with modern LED lighting turning something like 70 per cent of the applied electrical energy into light. We will gain some efficiencies as older lighting gets completely replaced by LEDs, but there’s not a lot of room left for future efficiency improvements. Similar efficiency limits arise in the growing or cooking of food; in the manufacturing of cars, bikes and electronic devices; in transportation, as we’re taken from place to place; in the running of search engines, translation software, GPT-4 or other large-language models.
Even if we made significant improvements in the efficiencies of these technologies, we will only have bought a little time. These changes won’t delay by much the date when deep warming becomes a problem we must reckon with.
Optimising efficiencies is just a temporary reprieve, not a radical change in our human future
As a thought experiment, suppose we could immediately improve the energy efficiency of everything we do by a factor of 10 – a fantastically optimistic proposal. That is, imagine the energy output of humans on Earth has been reduced 10 times , from 170,000 TWh to 17,000 TWh . If our energy use keeps expanding, doubling every 30-50 years or so (as it has for centuries), then a 10-fold increase in waste heat will happen in just over three doubling times, which is about 130 years : 17,000 TWh doubles to 34,000 TWh , which doubles to 68,000 TWh , which doubles to 136,000 TWh , and so on. All those improvements in energy efficiency would quickly evaporate. The date when deep warming hits would recede by 130 years or so, but not much more. Optimising efficiencies is just a temporary reprieve, not a radical change in our human future.
Improvements in energy efficiency can also have an inverse effect on our overall energy use. It’s easy to think that if we make a technology more efficient, we’ll then use less energy through the technology. But economists are deeply aware of a paradoxical effect known as ‘rebound’, whereby improved energy efficiency, by making the use of a technology cheaper, actually leads to more widespread use of that technology – and more energy use too. The classic example, as noted by the British economist William Stanley Jevons in his book The Coal Question (1865), is the invention of the steam engine. This new technology could extract energy from burning coal more efficiently, but it also made possible so many new applications that the use of coal increased. A recent study by economists suggests that, across the economy, such rebound effects might easily swallow at least 50 per cent of any efficiency gains in energy use. Something similar has already happened with LED lights, for which people have found thousands of new uses.
If gains in efficiency won’t buy us lots of time, how about other factors, such as a reduction of the global population? Scientists generally believe that the current human population of more than 8 billion people is well beyond the limits of our finite planet, especially if a large fraction of this population aspires to the resource-intensive lifestyles of wealthy nations. Some estimates suggest that a more sustainable population might be more like 2 billion , which could reduce energy use significantly, potentially by a factor of three or four. However, this isn’t a real solution: again, as with the example of improved energy efficiency, a one-time reduction of our energy consumption by a factor of three will quickly be swallowed up by an inexorable rise in energy use. If Earth’s population were suddenly reduced to 2 billion – about a quarter of the current population – our energy gains would initially be enormous. But those gains would be erased in two doubling times, or roughly 60-100 years , as our energy demands would grow fourfold.
S o, why aren’t more people talking about this? The deep warming problem is starting to get more attention. It was recently mentioned on Twitter by the German climate scientist Stefan Rahmstorf, who cautioned that nuclear fusion, despite excitement over recent advances, won’t arrive in time to save us from our waste heat, and might make the problem worse. By providing another cheap source of energy, fusion energy could accelerate both the growth of our energy use and the reckoning of deep warming. A student of Rahmstorf’s, Peter Steiglechner, wrote his master’s thesis on the problem in 2018. Recognition of deep warming and its long-term implications for humanity is spreading. But what can we do about the problem?
Avoiding or delaying deep warming will involve slowing the rise of our waste heat, which means restricting the amount of energy we use and also choosing energy sources that exacerbate the problem as little as possible. Unlike the energy from fossil fuels or nuclear power, which add to our waste energy burden, renewable energy sources intercept energy that is already on its way to Earth, rather than producing additional waste heat. In this sense, the deep warming problem is another reason to pursue renewable energy sources such as solar or wind rather than alternatives such as nuclear fusion, fission or even geothermal power. If we derive energy from any of these sources, we’re unleashing new flows of energy into the Earth system without making a compensating reduction. As a result, all such sources will add to the waste heat problem. However, if renewable sources of energy are deployed correctly, they need not add to our deposition of waste heat in the environment. By using this energy, we produce no more waste heat than would have been created by sunlight in the first place.
Take the example of wind energy. Sunlight first stirs winds into motion by heating parts of the planet unequally, causing vast cells of convection. As wind churns through the atmosphere, blows through trees and over mountains and waves, most of its energy gets turned into heat, ending up in the microscopic motions of molecules. If we harvest some of this wind energy through turbines, it will also be turned into heat in the form of stored energy. But, crucially, no more heat is generated than if there had been no turbines to capture the wind.
The same can hold true for solar energy. In an array of solar cells, if each cell only collects the sunlight falling on it – which would ordinarily have been absorbed by Earth’s surface – then the cells don’t alter how much waste heat gets produced as they generate energy. The light that would have warmed Earth’s surface instead goes into the solar cells, gets used by people for some purpose, and then later ends up as heat. In this way we reduce the amount of heat being absorbed by Earth by precisely the same amount as the energy we are extracting for human use. We are not adding to overall planetary heating. This keeps the waste energy burden unchanged, at least in the relatively near future, even if we go on extracting and using ever larger amounts of energy.
Covering deserts in dark panels would absorb a lot more energy than the desert floor
Chaisson summarised the problem quite clearly in 2008:
But not just any method of gathering solar energy will avoid the deep warming problem. Doing so requires careful engineering. For example, covering deserts with solar panels would add to planetary heating because deserts reflect a lot of incident light back out to space, so it is never absorbed by Earth (and therefore doesn’t produce waste heat). Covering deserts in dark panels would absorb a lot more energy than the desert floor and would heat the planet further.
We’ll also face serious problems in the long run if our energy appetite keeps increasing. Futurists dream of technologies deployed in space where huge panels would absorb sunlight that would otherwise have passed by Earth and never entered our atmosphere. Ultimately, they believe, this energy could be beamed down to Earth. Like nuclear energy, such technologies would add an additional energy source to the planet without any compensating removal of heating from the sunlight currently striking our planet’s surface. Any effort to produce more energy than is normally available from sunlight at Earth’s surface will only make our heating problems worse.
D eep warming is simply a consequence of the laws of physics and our inquisitive nature. It seems to be in our nature to constantly learn and develop new things, changing our environment in the process. For thousands of years, we have harvested and exploited ever greater quantities of energy in this pursuit, and we appear poised to continue along this path with the rapidly expanding use of renewable energy sources – and perhaps even more novel sources such as nuclear fusion. But this path cannot proceed indefinitely without consequences.
The logic that more energy equals more warming sets up a profound dilemma for our future. The laws of physics and the habits ingrained in us from our long evolutionary history are steering us toward trouble. We may have a technological fix for greenhouse gas warming – just shift from fossil fuels to cleaner energy sources – but there is no technical trick to get us out of the deep warming problem. That won’t stop some scientists from trying.
Perhaps, believing that humanity is incapable of reducing its energy usage, we’ll adopt a fantastic scheme to cool the planet, such as planetary-scale refrigeration or using artificially engineered tornadoes to transport heat from Earth’s surface to the upper atmosphere where it can be radiated away to space. As far-fetched as such approaches sound, scientists have given some serious thought to these and other equally bizarre ideas, which seem wholly in the realm of science fiction. They’re schemes that will likely make the problem worse not better.
We will need to transform the human story. It must become a story of doing less, not more
I see several possibilities for how we might ultimately respond. As with greenhouse gas warming, there will probably be an initial period of disbelief, denial and inaction, as we continue with unconstrained technological advance and growing energy use. Our planet will continue warming. Sooner or later, however, such warming will lead to serious disruptions of the Earth environment and its ecosystems. We won’t be able to ignore this for long, and it may provide a natural counterbalance to our energy use, as our technical and social capacity to generate and use ever more energy will be eroded. We may eventually come to some uncomfortable balance in which we just scrabble out a life on a hot, compromised planet because we lack the moral and organisational ability to restrict our energy use enough to maintain a sound environment.
An alternative would require a radical break with our past: using less energy. Finding a way to use less energy would represent a truly fundamental rupture with all of human history, something entirely novel. A rupture of this magnitude won’t come easily. However, if we could learn to view restrictions on our energy use as a non-negotiable element of life on Earth, we may still be able to do many of the things that make us essentially human: learning, discovering, inventing, creating. In this scenario, any helpful new technology that comes into use and begins using lots of energy would require a balancing reduction in energy use elsewhere. In such a way, we might go on with the future being perpetually new, and possibly better.
None of this is easily achieved and will likely mirror our current struggles to come to agreements on greenhouse gas heating. There will be vicious squabbles, arguments and profound polarisation, quite possibly major wars. Humanity will never have faced a challenge of this magnitude, and we won’t face up to it quickly or easily, I expect. But we must. Planetary heating is in our future – the very near future and further out as well. Many people will find this conclusion surprisingly hard to swallow, perhaps because it implies fundamental restrictions on our future here on Earth: we can’t go on forever using more and more energy, and, at the same time, expecting the planet’s climate to remain stable.
The world will likely be transformed by 2050. And, sometime after that, we will need to transform the human story. The narrative arc of humanity must become a tale of continuing innovation and learning, but also one of careful management. It must become a story, in energy terms, of doing less, not more. There’s no technology for entirely escaping waste heat, only techniques.
This is important to remember as we face up to the extremely urgent challenge of heating linked to fossil-fuel use and greenhouse gases. Global warming is just the beginning of our problems. It’s a testing ground to see if we can manage an intelligent and coordinated response. If we can handle this challenge, we might be better prepared, more capable and resilient as a species to tackle an even harder one."
"Human exceptionalism is a danger to all, human and nonhuman | Aeon Essays",,https://aeon.co/essays/human-exceptionalism-is-a-danger-to-all-human-and-nonhuman,"This January, a 57-year-old man in Baltimore received a heart transplant from a pig. Xenotransplantation involves using nonhuman animals as sources of organs for humans. While the idea of using nonhuman animals for this purpose might seem troubling, many humans think that the sacrifice is worth it, provided that we can improve the technology (the man died two months later). As the bioethicists Arthur Caplan and Brendan Parent put it last year: ‘Animal welfare certainly counts, but human lives carry more ethical weight.’
Of course, xenotransplantation is not the only practice through which humans impose burdens on other animals to derive benefits for ourselves. We kill more than 100 billion captive animals per year for food, clothing, research and other purposes, and we likely kill more than 1 trillion wild animals per year for similar purposes. We might not bother to defend these practices frequently. But when we do, we offer the same defence: Human lives carry more ethical weight.
But is this true?
Most humans take this idea of human exceptionalism for granted. And it makes sense that we do, since we benefit from the notion that we matter more than other animals. But this statement is still worth critically assessing. Can we really justify the idea that some lives carry more ethical weight than others in general, and that human lives carry more ethical weight than nonhuman lives in particular? And even if so, does it follow that we should prioritise ourselves as much as we currently do?
E thicists sometimes offer capacities-based arguments for ranking species according to a hierarchy. For example, in How to Count Animals, More or Less (2019) Shelly Kagan argues that we should assign human interests extra ethical weight because we have a higher capacity for agency and welfare than other animals. I have cognitive capacities that a pig lacks, so I have interests that they lack. I also have the capacity to experience happiness and suffering more intensely than a pig does, so I have stronger interests related to my welfare than they do.
Ethicists also offer relationship-based arguments for species hierarchies. For example, in ‘Defending Animal Research’ (2001), Baruch Brody argues that we should assign human interests extra ethical weight because we have special bonds and a sense of solidarity with members of our own species. According to this view, we should ‘discount’ the interests of nonhuman animals for the same reason that we should ‘discount’ the interests of future generations: we have special duties within these categories that we lack across them.
In response to these and other such arguments, some ethicists contend that we should reject species hierarchies entirely. For example, in Fellow Creatures (2018) Christine Korsgaard argues that it generally makes no sense to ask whether a human or a pig has a better life, because each species of animal has a different form of life, and we can evaluate each life only against the standards set by that form of life. Comparing humans and pigs is, literally, like comparing apples and oranges.
If anything, we increasingly have grounds for prioritising nonhuman animals
While I think that this rejection of species hierarchies is worthy of consideration, I want to defend a separate idea: even if we accepted a species hierarchy on capacities-based and relationship-based grounds, it would still not follow that our current stance of human exceptionalism is acceptable. We would need to think carefully about how much ethical weight different animals carry rather than simply assert that we take priority. And when we do, we might be surprised by what we find.
In particular, if we take our own arguments for human exceptionalism seriously, then the upshot is not that we always take priority but rather that we sometimes do. And when we consider the scale of nonhuman suffering and death in the world and the extent of our complicity in this suffering and death, we can see that human exceptionalism has it backwards: if anything, we increasingly have capacities-based and relationship-based grounds for prioritising nonhuman animals.
To be clear, my goal here is not to argue against a strong form of human exceptionalism, according to which humans necessarily matter more than nonhumans. If you think that any human interest, no matter how minor, takes priority over any nonhuman interest, no matter how major – that, for instance, scratching a single human itch takes priority over preventing 100,000 elephant deaths – then there are good arguments against your view, but they will not be my focus here.
My goal is instead to argue against a moderate form of human exceptionalism, according to which humans contingently matter more than nonhumans. If you are among the many who think that we take priority over other animals because of our ‘higher’ capacities and ‘stronger’ relationships, this is wishful thinking. There are too many nonhumans, and our lives are too intertwined with theirs, for that to be plausible. This ‘moderate’ view is not as ethical as you think.
L et’s examine these arguments for human exceptionalism one by one, starting with the capacities-based arguments.
Yes, I have a higher capacity for agency (that is, self-determination) than nonhuman animals. I can step back from my beliefs, desires and actions, and ask whether I have reason to endorse them. As a result, I can use evidence and reason to set and pursue long-term goals. In contrast, a worm is only able to do what seems natural from moment to moment, without ever stopping to assess these choices.
Why does this difference matter? Plausibly, agents have a wider range of interests than non-agents do, all else being equal. It would be bad for you to keep me in confinement, since I need to be able to set and pursue my own goals to live well. In contrast, it might not be bad for you to keep a worm in confinement (with proper care), since all they need is, say, air, moisture, darkness, warmth, food and other worms to live well.
I also have a higher capacity for welfare (that is, happiness, suffering and other such states) than many nonhuman animals. Since I have a more complex brain than a worm does, I can experience more happiness and suffering at any given time. And since I have a longer lifespan than a worm does, I can also experience more happiness and suffering over time – provided, of course, that I live a reasonably full life.
Why does this difference matter? Plausibly, beings with a higher capacity for welfare have more at stake than beings with a lower capacity for welfare, all else being equal. Even if it would be bad for you to keep a worm in confinement, it would be still worse for you to keep me in confinement. Each day of confinement would harm me more, and I would also have more days of confinement overall.
My own view is that these capacities-based arguments are reasonable, as far as they go. Setting priorities requires considering how much everyone has at stake in any given situation, and our capacities are part of what determines how much we have at stake. But these arguments fall far short of establishing even a moderate form of human exceptionalism. Human and nonhuman capacities overlap substantially, and setting priorities requires considering other factors, too.
African elephants have about three times as many neurons as humans, and they have comparable lifespans
First, we might not always have a higher capacity for agency than other animals. We all lack the capacity for rational reflection early in life, some of us lose this capacity later in life, and some of us never develop this capacity at all. Meanwhile, many nonhuman animals have the capacity for memory, emotion, self-awareness, social awareness, communication, instrumental reasoning and more. Human and nonhuman agency thus overlap substantially in practice.
Moreover, even when we do have a higher capacity for agency than other animals, this difference might be smaller than we think. Our views about agency are anthropocentric, in that we treat human agency as the standard against which all forms of agency should be compared. But while human agency is certainly impressive, nonhuman agency is impressive too. And if we studied nonhuman agency on its own terms, we might discover forms of self-determination that humans lack.
Likewise, we might not always have a higher capacity for welfare than other animals. If our welfare capacity is a simple function of our brain complexity and lifespan, then some nonhumans might have a higher welfare capacity than humans do. For example , African elephants have about three times as many neurons as humans, and they have comparable lifespans. So, based on this way of making comparisons, these animals have a higher welfare capacity than us.
Moreover, even when we do have a higher capacity for welfare than other animals, this difference might once again be smaller than we think. For all we know, our capacity for welfare might not be a simple function of our brain complexity and lifespan. We are still early in the study of animal minds. And while it might be that, say, twice as many neurons equals twice as much welfare capacity, it might also be that the difference in welfare capacity is larger or smaller than that.
In short, the capacities-based arguments fail to vindicate even moderate forms of human exceptionalism. If we think that humans can have strong interests even when we lack the capacity for rational reflection (as, of course, we should), then we should think that nonhumans can too. And if we give humans the benefit of the doubt in cases of uncertainty about how much happiness and suffering we can experience (as, again, we should), then we should do the same for nonhumans.
Other factors are relevant to how we set priorities, too.
For instance, even if a human has more at stake than a nonhuman in general, they might not have more at stake in particular cases. Suppose you can either save a human from a minor injury or save a pig from a major injury. In this case, it might be that you should save the pig.
Similarly, even if a human has more at stake than one nonhuman, they might not have more at stake than many nonhumans. Suppose you can either save a human from a minor injury or save 1,000 pigs from minor injuries. In this case too, it might be that you should save the pigs.
Finally, morality is about more than benefits and harms, at least in practice. We would never permit doctors to breed humans for their flesh or organs, since the rights of the ‘donors’ would trump the benefits for the recipients. Why not think that the same can be true for nonhumans?
This all raises serious doubts about human exceptionalism. We regularly impose major burdens on nonhumans in exchange for minor benefits for humans. The ‘harm’ to humans of eating plants instead of animals is nothing compared to the harm to an animal of being factory farmed.
We also regularly impose burdens on very many nonhumans for each human who benefits. We probably kill at least a trillion farmed and wild animals, not including insects , for food each year. This is more than the total number of humans who have ever existed – killed every year.
And we regularly use nonhumans in ways that we would never permit ourselves to use humans. This is not simply a matter of us saving ourselves instead of saving them (though we do that too): this is a matter of us exploiting and exterminating them on a global scale.
The upshot is clear. Even if we think that beings with a higher capacity for agency and welfare take priority over beings with a lower capacity for agency and welfare, all else being equal, we should still be sceptical that this difference justifies anything like our current behaviour.
B ut what about relationship-based arguments for human exceptionalism, which focus on how we relate to humans and other animals?
Many people believe that, at least in practice, we have both a right and a duty to prioritise ourselves and our communities. I should take care of myself before I take care of you, and I should also take care of my family before I take care of yours. And if we can exhibit this kind of partiality in the context of smaller groups such as families, perhaps we can do the same in the context of larger groups such as species.
In fact, some ethicists believe that the analogy holds not only for species but also for other large groups, like generations. They argue that we can ‘discount’ the interests of nonhuman animals and future generations, in part because we have closer relationships within these categories than beyond them, and in part because full impartiality for all sentient beings from now until the end of time would be too demanding.
This kind of argument is partly about our personal interests. If we allow morality to be too impartial, then our personal interests would carry very little relative weight. But they should carry at least a moderate amount of relative weight, both because we have a right to take care of ourselves and because we need to do that to take care of others. So, we should allow morality to be somewhat partial to create space for self-care.
This kind of argument is also partly about our relationships. We have special duties in the context of special relationships. I should take care of my family before I take care of yours because I have special bonds within my family. And the same can be true for larger groups like species and generations. So, we should allow morality to be somewhat partial to create space for our relational duties, too.
But even if we accept that these claims are true and that they extend to groups such as species and generations, we should still reject our current stance of human exceptionalism. There are many more individuals across species and generations than within a single one. And our lives are increasingly linked across species and generations in ways that have important implications for our interests and our relationships.
Suppose that I do, in fact, have a duty to take care of my family before I take care of yours, all else being equal. Does it follow that I can treat your family however I like? Of course not. It would be wrong for me to take food from your family to provide food for mine, particularly if my family already has much more food than yours does. It would also be wrong for me to kill your family so that I can provide my family with human flesh to eat instead of, say, rice and beans.
Deforestation, factory farming and the wildlife trade are hurting us right now
These points apply across species and generations, too. Humans are taking resources away from nonhuman animals and future generations, even though we already have much more than they do in many ways. We are also killing hundreds of billions of nonhuman animals for food each year in ways that impose health and environmental threats on future generations too – even when we have access to humane, healthful, sustainable plant-based alternatives.
Furthermore, many policies that would benefit nonhuman animals and future generations would benefit us, too. As the WHO’s One Health framework reminds us, human, nonhuman and environmental health are linked. We need to phase down industries such as deforestation, factory farming and the wildlife trade, not only for nonhuman animals and future generations but also for ourselves: waste, pollution, infectious diseases and other hazards from these activities are hurting us right now.
We should keep in mind that we have relational duties across species and generations, too. Many of us care about members of other species and generations: I dare anyone to try to matter to me more than my dog Smoky, and many parents feel the same way about their children, grandchildren and so on. And when our practices harm nonhuman animals and future generations, we have a relational duty to reduce and repair these harms whether we care about these individuals or not.
In short, relationship-based arguments fail to vindicate current forms of human and generational exceptionalism. Even if we have a right or duty to take care of ourselves and other current humans, we still need to treat nonhuman animals and future generations much better to accomplish that goal. We also need to treat them much better for their own sakes, especially when our activity is harming them. Our relational duties extend much farther than we might have thought.
When we weave these threads about capacities and relationships together, we reach a surprising conclusion: we should not only prioritise ourselves less but should perhaps not prioritise ourselves at all in some cases. After all, even if we discount the interests of nonhuman animals and future generations, these populations are still so large, and our practices are still impacting them so much, that their interests might still carry more ethical weight than ours do in the aggregate.
And when we weave the biological and generational arguments together, we reach an even more surprising conclusion: we should prioritise not only current nonhumans and future humans but also, and especially, future nonhumans. For instance, when we assess the impacts of the climate crisis, we should assign a lot of ethical weight to the impacts on other species. Which populations will expand and contract, and what will follow for the welfare of individual animals?
Granted, there might be a limit to how much we can prioritise current and future nonhumans at present, since we currently lack the knowledge, power and political will that we need to help them at scale. So we might need to prioritise ourselves now to be able to prioritise them later.
But even if we accept this pragmatic argument, we should still prioritise nonhuman animals much more than we do now. We are already capable of harming nonhuman animals much less and helping them much more, and in many cases, making these changes would benefit us, too. We should also keep in mind that there is a path dependence to how history unfolds. For better or worse, our successors will inherit the world that we create. So if we want our successors to have more impartial priorities, then we need to work to develop more impartial priorities, too.
The upshot is that we need to rethink our relationship with other animals from the ground up. When setting priorities across species, we have a responsibility to follow the best information and arguments where they lead, rather than assume a self-serving conclusion from the start.
And when we take our thumbs off the scales, we can expect the scales to shift. We should already be treating nonhumans much better and, eventually, we might even need to prioritise their interests and needs over our own. We should start preparing for that possibility now."
Why longtermism is the world’s most dangerous secular credo | Aeon Essays,,https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo,"There seems to be a growing recognition that humanity might be approaching the ‘end times’. Dire predictions of catastrophe clutter the news. Social media videos of hellish wildfires, devastating floods and hospitals overflowing with COVID-19 patients dominate our timelines. Extinction Rebellion activists are shutting down cities in a desperate attempt to save the world. One survey even found that more than half of the people asked about humanity’s future ‘rated the risk of our way of life ending within the next 100 years at 50 per cent or greater.’
‘Apocalypticism’, or the belief that the end times are imminent, is of course nothing new: people have warned that the end is nigh for millennia, and in fact many New Testament scholars believe that Jesus himself expected the world to end during his own lifetime. But the situation today is fundamentally different than in the past. The ‘eschatological’ scenarios now being discussed are based not on the revelations of religious prophets, or secular metanarratives of human history (as in the case of Marxism), but on robust scientific conclusions defended by leading experts in fields such as climatology, ecology, epidemiology and so on.
We know, for example, that climate change poses a dire threat to civilisation. We know that biodiversity loss and the sixth mass extinction could precipitate sudden, irreversible, catastrophic shifts in the global ecosystem. A thermonuclear exchange could blot out the Sun for years or decades, bringing about the collapse of global agriculture. And whether or not SARS- CoV-2 came from a Wuhan laboratory or was cooked up in the kitchen of nature (the latter seems more probable right now), synthetic biology will soon enable bad actors to design pathogens far more lethal and contagious than anything Darwinian evolution could possibly invent. Some philosophers and scientists have also begun sounding the alarm about ‘emerging threats’ associated with machine superintelligence, molecular nanotechnology and stratospheric geoengineering, which look no less formidable.
Such considerations have led many scholars to acknowledge that, as Stephen Hawking wrote in The Guardian in 2016, ‘we are at the most dangerous moment in the development of humanity.’ Lord Martin Rees, for example, estimates that civilisation has a 50/50 chance of making it to 2100. Noam Chomsky argues that the risk of annihilation is currently ‘unprecedented in the history of Homo sapiens ’. And Max Tegmark contends that ‘it’s probably going to be within our lifetimes … that we’re either going to self-destruct or get our act together.’ Consistent with these dismal declarations, the Bulletin of the Atomic Scientists in 2020 set its iconic Doomsday Clock to a mere 100 seconds before midnight (or doom), the closest it’s been since the clock was created in 1947, and more than 11,000 scientists from around the world signed an article in 2020 stating ‘clearly and unequivocally that planet Earth is facing a climate emergency’, and without ‘an immense increase of scale in endeavours to conserve our biosphere [we risk] untold suffering due to the climate crisis.’ As the young climate activist Xiye Bastida summed up this existential mood in a Teen Vogue interview in 2019, the aim is to ‘make sure that we’re not the last generation’, because this now appears to be a very real possibility.
Given the unprecedented dangers facing humanity today, one might expect philosophers to have spilled a considerable amount of ink on the ethical implications of our extinction, or related scenarios such as the permanent collapse of civilisation. How morally bad (or good ) would our disappearance be, and for what reasons? Would it be wrong to prevent future generations from coming into existence? Does the value of past sacrifices, struggles and strivings depend on humanity continuing to exist for as long as Earth, or the Universe more generally, remains habitable?
Yet this is not the case: the topic of our extinction has received little sustained attention from philosophers until recently, and even now remains at the fringe of philosophical discussion and debate. On the whole, they have been preoccupied with other matters. However, there is one notable exception to this rule: over the past two decades, a small group of theorists mostly based in Oxford have been busy working out the details of a new moral worldview called longtermism, which emphasizes how our actions affect the very long-term future of the universe – thousands, millions, billions, and even trillions of years from now. This has roots in the work of Nick Bostrom , who founded the grandiosely named Future of Humanity Institute (FHI) in 2005, and Nick Beckstead, a research associate at FHI and a programme officer at Open Philanthropy. It has been defended most publicly by the FHI philosopher Toby Ord, author of The Precipice: Existential Risk and the Future of Humanity (2020). Longtermism is the primary research focus of both the Global Priorities Institute (GPI), an FHI-linked organisation directed by Hilary Greaves, and the Forethought Foundation, run by William MacAskill, who also holds positions at FHI and GPI. Adding to the tangle of titles, names, institutes and acronyms, longtermism is one of the main ‘cause areas’ of the so-called effective altruism (EA) movement, which was introduced by Ord in around 2011 and now boasts of having a mind-boggling $46 billion in committed funding.
It is difficult to overstate how influential longtermism has become. Karl Marx in 1845 declared that the point of philosophy isn’t merely to interpret the world but change it, and this is exactly what longtermists have been doing, with extraordinary success. Consider that Elon Musk , who has cited and endorsed Bostrom’s work, has donated $1.5 million dollars to FHI through its sister organisation, the even more grandiosely named Future of Life Institute (FLI). This was cofounded by the multimillionaire tech entrepreneur Jaan Tallinn, who, as I recently noted , doesn’t believe that climate change poses an ‘existential risk’ to humanity because of his adherence to the longtermist ideology.
Meanwhile, the billionaire libertarian and Donald Trump supporter Peter Thiel, who once gave the keynote address at an EA conference, has donated large sums of money to the Machine Intelligence Research Institute, whose mission to save humanity from superintelligent machines is deeply intertwined with longtermist values. Other organisations such as GPI and the Forethought Foundation are funding essay contests and scholarships in an effort to draw young people into the community, while it’s an open secret that the Washington, DC-based Center for Security and Emerging Technologies (CSET) aims to place longtermists within high-level US government positions to shape national policy. In fact, CSET was established by Jason Matheny, a former research assistant at FHI who’s now the deputy assistant to US President Joe Biden for technology and national security. Ord himself has, astonishingly for a philosopher, ‘advised the World Health Organization, the World Bank, the World Economic Forum, the US National Intelligence Council, the UK Prime Minister’s Office, Cabinet Office, and Government Office for Science’, and he recently contributed to a report from the Secretary-General of the United Nations that specifically mentions ‘long-termism’.
The point is that longtermism might be one of the most influential ideologies that few people outside of elite universities and Silicon Valley have ever heard about. I believe this needs to change because, as a former longtermist who published an entire book four years ago in defence of the general idea, I have come to see this worldview as quite possibly the most dangerous secular belief system in the world today. But to understand the nature of the beast, we need to first dissect it, examining its anatomical features and physiological functions.
T he initial thing to notice is that longtermism, as proposed by Bostrom and Beckstead, is not equivalent to ‘caring about the long term’ or ‘valuing the wellbeing of future generations’. It goes way beyond this. At its core is a simple – albeit flawed, in my opinion – analogy between individual persons and humanity as a whole. To illustrate the idea, consider the case of Frank Ramsey , a scholar at the University of Cambridge widely considered by his peers as among his generation’s most exceptional minds. ‘There was something of Newton about him,’ the belletrist Lytton Strachey once said. G E Moore wrote of Ramsey’s ‘very exceptional brilliance’. And John Maynard Keynes described a paper of Ramsey’s as ‘one of the most remarkable contributions to mathematical economics ever made’.
But Ramsey’s story isn’t a happy one. On 19 January 1930, he died in a London hospital following a surgical procedure, the likely cause of death being a liver infection from swimming in the River Cam, which winds its way through Cambridge. Ramsey was only 26 years old.
One could argue that there are two distinct reasons this outcome was tragic. The first is the most obvious: it cut short Ramsey’s life, depriving him of everything he could have experienced had he survived – the joys and happiness, the love and friendship: all that makes life worth living. In this sense, Ramsey’s early demise was a personal tragedy. But, secondly, his death also robbed the world of an intellectual superstar apparently destined to make even more extraordinary contributions to human knowledge. ‘The number of trails Ramsey laid was remarkable,’ writes Sir Partha Dasgupta. But how many more trails might he have blazed? ‘The loss to your generation is agonising to think of,’ Strachey lamented, ‘what a light has gone out’ – which leaves one wondering how Western intellectual history might have been different if Ramsey hadn’t died so young. From this perspective, one could argue that, although the personal tragedy of Ramsey’s death was truly terrible, the immensity of his potential to have changed the world for the better makes the second tragedy even worse. In other words, the badness of his death stems mostly, perhaps overwhelmingly, from his unfulfilled potential rather than the direct, personal harms that he experienced. Or so the argument goes.
Longtermists would map these claims and conclusions on to humanity itself, as if humanity is an individual with its very own ‘potential’ to squander or fulfil, ruin or realise, over the course of ‘its lifetime’. So, on the one hand, a catastrophe that reduces the human population to zero would be tragic because of all the suffering it would inflict upon those alive at the time. Imagine the horror of starving to death in subfreezing temperatures, under pitch-black skies at noon, for years or decades after a thermonuclear war. This is the first tragedy, a personal tragedy for those directly affected. But there is, longtermists would argue, a second tragedy that is astronomically worse than the first, arising from the fact that our extinction would permanently foreclose what could be an extremely long and prosperous future over the next, say, ~10 100 years (at which point the ‘heat death’ will make life impossible). In doing this, it would irreversibly destroy the ‘vast and glorious’ longterm potential of humanity, in Ord’s almost religious language – a ‘potential’ so huge, given the size of the Universe and the time left before reaching thermodynamic equilibrium, that the first tragedy would utterly pale in comparison.
This immediately suggests another parallel between individuals and humanity: death isn’t the only way that someone’s potential could be left unfulfilled. Imagine that Ramsey hadn’t died young but, instead of studying, writing and publishing scholarly papers, he’d spent his days in the local bar playing pool and drinking. Same outcome, different failure mode. Applying this to humanity, longtermists would argue that there are failure modes that could leave our potential unfulfilled without us dying out, which I will return to below.
On this view, a climate catastrophe will be a small blip – like a 90-year-old who stubbed his toe when he was two
To summarise these ideas so far, humanity has a ‘potential’ of its own, one that transcends the potentials of each individual person, and failing to realise this potential would be extremely bad – indeed, as we will see, a moral catastrophe of literally cosmic proportions. This is the central dogma of longtermism: nothing matters more, ethically speaking, than fulfilling our potential as a species of ‘Earth-originating intelligent life’. It matters so much that longtermists have even coined the scary-sounding term ‘existential risk’ for any possibility of our potential being destroyed, and ‘existential catastrophe’ for any event that actually destroys this potential.
Why do I think this ideology is so dangerous? The short answer is that elevating the fulfilment of humanity’s supposed potential above all else could nontrivially increase the probability that actual people – those alive today and in the near future – suffer extreme harms, even death. Consider that, as I noted elsewhere, the longtermist ideology inclines its adherents to take an insouciant attitude towards climate change. Why? Because even if climate change causes island nations to disappear, triggers mass migrations and kills millions of people, it probably isn’t going to compromise our longterm potential over the coming trillions of years. If one takes a cosmic view of the situation, even a climate catastrophe that cuts the human population by 75 per cent for the next two millennia will, in the grand scheme of things, be nothing more than a small blip – the equivalent of a 90-year-old man having stubbed his toe when he was two.
Bostrom’s argument is that ‘a non-existential disaster causing the breakdown of global civilisation is, from the perspective of humanity as a whole, a potentially recoverable setback.’ It might be ‘a giant massacre for man’, he adds, but so long as humanity bounces back to fulfil its potential, it will ultimately register as little more than ‘a small misstep for mankind’. Elsewhere, he writes that the worst natural disasters and devastating atrocities in history become almost imperceptible trivialities when seen from this grand perspective. Referring to the two world wars, AIDS and the Chernobyl nuclear accident, he declares that ‘tragic as such events are to the people immediately affected, in the big picture of things … even the worst of these catastrophes are mere ripples on the surface of the great sea of life.’
This way of seeing the world, of assessing the badness of AIDS and the Holocaust, implies that future disasters of the same (non-existential) scope and intensity should also be categorised as ‘mere ripples’. If they don’t pose a direct existential risk, then we ought not to worry much about them, however tragic they might be to individuals. As Bostrom wrote in 2003, ‘priority number one, two, three and four should … be to reduce existential risk.’ He reiterated this several years later in arguing that we mustn’t ‘fritter … away’ our finite resources on ‘feel-good projects of suboptimal efficacy’ such as alleviating global poverty and reducing animal suffering, since neither threatens our longterm potential, and our longterm potential is what really matters.
Ord echoes these views in arguing that, of all the problems facing humanity, our ‘first great task … is to reach a place of safety – a place where existential risk’ – as he defines it – ‘is low and stays low’, which he dubs ‘existential security’. More than anything else, what matters is doing everything necessary to ‘preserve’ and ‘protect’ our potential by ‘extracting ourselves from immediate danger’ and devising robust ‘safeguards that will defend humanity from dangers over the longterm future, so that it becomes impossible to fail.’ Although Ord gives a nod to climate change, he also claims – based on a dubious methodology – that the chance of climate change causing an existential catastrophe is only ∼1 in 1,000, which is a whole two orders of magnitude lower than the probability of superintelligent machines destroying humanity this century, according to Ord.
What’s really notable here is that the central concern isn’t the effect of the climate catastrophe on actual people around the world (remember, in the grand scheme, this would be, in Bostrom’s words, a ‘small misstep for mankind’) but the slim possibility that, as Ord puts it in The Precipice , this catastrophe ‘poses a risk of an unrecoverable collapse of civilisation or even the complete extinction of humanity’. Again, the harms caused to actual people (especially those in the Global South) might be significant in absolute terms, but when compared to the ‘vastness’ and ‘glory’ of our longterm potential in the cosmos, they hardly even register.
Y et the implications of longtermism are far more worrisome. If our top four priorities are to avoid an existential catastrophe – ie, to fulfil ‘our potential’ – then what’s not on the table for making this happen? Consider Thomas Nagel ’s comment about how the notion of what we might call the ‘greater good’ has been used to ‘justify’ certain atrocities (eg, during war). If the ends ‘justify’ the means, he argues, and the ends are thought to be sufficiently large (eg, national security), then this ‘can be brought to bear to ease the consciences of those responsible for a certain number of charred babies’. Now imagine what might be ‘justified’ if the ‘greater good’ isn’t national security but the cosmic potential of Earth-originating intelligent life over the coming trillions of years? During the Second World War, 40 million civilians perished, but compare this number to the 10 54 or more people (in Bostrom’s estimate ) who could come to exist if we can avoid an existential catastrophe. What shouldn’t we do to ‘protect’ and ‘preserve’ this potential? To ensure that these unborn people come to exist? What means can’t be ‘justified’ by this cosmically significant moral end?
Bostrom himself argued that we should seriously consider establishing a global, invasive surveillance system that monitors every person on the planet in realtime, to amplify the ‘capacities for preventive policing’ (eg, to prevent omnicidal terrorist attacks that could devastate civilisation). Elsewhere, he’s written that states should use preemptive violence/war to avoid existential catastrophes, and argued that saving billions of actual people is the moral equivalent of reducing existential risk by utterly minuscule amounts. In his words, even if there is ‘a mere 1 per cent chance’ of 10 54 people existing in the future, then ‘the expected value of reducing existential risk by a mere one billionth of one billionth of one percentage point is worth 100 billion times as much as a billion human lives.’ Such fanaticism – a word that some longtermists embrace – has led a growing number of critics to worry about what might happen if political leaders in the real world were to take Bostrom’s view seriously. To quote the mathematical statistician Olle Häggström, who – perplexingly – tends otherwise to speak favourably of longtermism:
Here, then, are a few reasons I find longtermism to be profoundly dangerous. Yet there are additional, fundamental problems with this worldview that no one, to my knowledge, has previously noted in writing. For example, there’s a good case to make that the underlying commitments of longtermism are a major reason why humanity faces so many unprecedented risks to its survival in the first place. Longtermism might, in other words, be incompatible with the attainment of ‘existential security’, meaning that the only way to genuinely reduce the probability of extinction or collapse in the future might be to abandon the longtermist ideology entirely.
To Bostrom and Ord, failing to become posthuman would prevent us from realising our vast, glorious potential
To understand the argument, let’s first unpack what longtermists mean by our ‘longterm potential’, an expression that I have so far used without defining. We can analyse this concept into three main components: transhumanism, space expansionism, and a moral view closely associated with what philosophers call ‘total utilitarianism’.
The first refers to the idea that we should use advanced technologies to reengineer our bodies and brains to create a ‘superior’ race of radically enhanced posthumans (which, confusingly, longtermists place within the category of ‘humanity’). Although Bostrom is perhaps the most prominent transhumanist today, longtermists have shied away from using the term ‘transhumanism’, probably because of its negative associations. Susan Levin, for example, points out that contemporary transhumanism has its roots in the Anglo-American eugenics movement, and transhumanists such as Julian Savulescu , who co-edited the book Human Enhancement (2009) with Bostrom, have literally argued for the consumption of ‘morality-boosting’ chemicals such as oxytocin to avoid an existential catastrophe (which he calls ‘ultimate harm’). As Savulescu writes with a colleague, ‘it is a matter of such urgency to improve humanity morally … that we should seek whatever means there are to effect this.’ Such claims are not only controversial but for many quite disturbing, and hence longtermists have attempted to distance themselves from such ideas, while nonetheless championing the ideology.
Transhumanism claims that there are various ‘posthuman modes of being’ that are far better than our current human mode. We could, for instance, genetically alter ourselves to gain perfect control over our emotions, or access the internet via neural implants, or maybe even upload our minds to computer hardware to achieve ‘digital immortality’. As Ord urges in The Precipice , think of how awesome it would be to perceive the world via echolocation, like bats and dolphins, or magnetoreception, like red foxes and homing pigeons. ‘Such uncharted experiences,’ Ord writes, ‘exist in minds much less sophisticated than our own. What experiences, possibly of immense value, could be accessible, then, to minds much greater?’ Bostrom’s most fantastical exploration of these possibilities comes from his evocative ‘Letter from Utopia’ (2008), which depicts a techno-Utopian world full of superintelligent posthumans awash in so much ‘pleasure’ that, as the letter’s fictional posthuman writes, ‘we sprinkle it in our tea.’
The connection with longtermism is that, according to Bostrom and Ord, failing to become posthuman would seemingly prevent us from realising our vast and glorious potential, which would be existentially catastrophic. As Bostrom put it in 2012, ‘the permanent foreclosure of any possibility of this kind of transformative change of human biological nature may itself constitute an existential catastrophe.’ Similarly, Ord asserts that ‘forever preserving humanity as it is now may also squander our legacy, relinquishing the greater part of our potential.’
T he second component of our potential – space expansionism – refers to the idea that we must colonise as much of our future light cone as possible: that is, the region of spacetime that is theoretically accessible to us. According to longtermists, our future light cone contains a huge quantity of exploitable resources, which they refer to as our ‘cosmic endowment’ of negentropy (or reverse entropy). The Milky Way alone, Ord writes, is ‘150,000 light years across, encompassing more than 100 billion stars, most with their own planets.’ Attaining humanity’s longterm potential, he continues, ‘requires only that [we] eventually travel to a nearby star and establish enough of a foothold to create a new flourishing society from which we could venture further.’ By spreading ‘just six light years at a time’, our posthuman descendants could make ‘almost all the stars of our galaxy … reachable’ since ‘each star system, including our own, would need to settle just the few nearest stars [for] the entire galaxy [to] eventually fill with life.’ The process could be exponential, resulting in ever-more ‘flourishing’ societies with each additional second our descendants hop from star to star.
But why exactly would we want to do this? What’s so important about flooding the Universe with new posthuman civilisations? This leads to the third component: total utilitarianism, which I will refer to as ‘utilitarianism’ for short. Although some longtermists insist that they aren’t utilitarians, we should right away note that this is mostly a smoke-and-mirrors act to deflect criticisms that longtermism – and, more generally, the effective altruism (EA) movement from which it emerged – is nothing more than utilitarianism repackaged . The fact is that the EA movement is deeply utilitarian, at least in practice, and indeed, before it decided upon a name, the movement’s early members, including Ord, seriously considered calling it the ‘effective utilitarian community’.
This being said, utilitarianism is an ethical theory that specifies our sole moral obligation as being to maximise the total amount of ‘intrinsic value’ in the world, as tallied up from a disembodied, impartial, cosmic vantage point called ‘the point of view of the Universe’. From this view, it doesn’t matter how value – which utilitarian hedonists equate with pleasure – is distributed among people across space and time. All that matters is the total net sum. For example, imagine that there are 1 trillion people who have lives of value ‘1’, meaning that they are just barely worth living. This gives a total value of 1 trillion . Now consider an alternative universe in which 1 billion people have lives with a value of ‘999’, meaning that their lives are extremely good. This gives a total value of 999 billion. Since 999 billion is less than 1 trillion, the first world full of lives hardly worth living would be morally better than the second world, and hence, if a utilitarian were forced to choose between these, she would pick the former. (This is called the ‘repugnant conclusion’, which longtermists such as Ord, MacAskill and Greaves recently argued shouldn’t be taken very seriously. For them, the first world really might be better!)
Beckstead argued that we should prioritise the lives of people in rich countries over those in poor countries
The underlying reasoning here is based on the idea that people – you and I – are nothing more than means to an end. We don’t matter in ourselves; we have no inherent value of our own. Instead, people are understood as the ‘containers’ of value, and hence we matter only insofar as we ‘contain’ value, and therefore contribute to the overall net amount of value in the Universe between the Big Bang and the heat death. Since utilitarianism tells us to maximise value, it follows that the more people (value containers) who exist with net-positive amounts of value (pleasure), the better the Universe will become, morally speaking. In a phrase: people exist for the sake of maximising value, rather than value existing for the sake of benefitting people.
This is why longtermists are obsessed with calculating how many people could exist in the future if we were to colonise space and create vast computer simulations around stars in which unfathomably huge numbers of people live net-positive lives in virtual-reality environments. I already mentioned Bostrom’s estimate of 10 54 future people, which includes many of these ‘digital people’, but in his bestseller Superintelligence (2014) he puts the number even higher at 10 58 people, nearly all of whom would ‘live rich and happy lives while interacting with one another in virtual environments’. Greaves and MacAskill are similarly excited about this possibility, estimating that some 10 45 conscious beings in computer simulations could exist within the Milky Way alone.
That is what our ‘vast and glorious’ potential consists of: massive numbers of technologically enhanced digital posthumans inside huge computer simulations spread throughout our future light cone. It is for this goal that, in Häggström’s scenario, a longtermist politician would annihilate Germany. It is for this goal that we must not ‘fritter … away’ our resources on such things as solving global poverty. It is for this goal that we should consider implementing a global surveillance system, keep pre-emptive war on the table, and focus more on superintelligent machines than saving people in the Global South from the devastating effects of climate change (mostly caused by the Global North). In fact, Beckstead has even argued that, for the sake of attaining this goal, we should actually prioritise the lives of people in rich countries over those in poor countries, since influencing the long-term future is of ‘overwhelming importance’, and the former are more likely to influence the long-term future than the latter. To quote a passage from Beckstead’s 2013 PhD dissertation, which Ord enthusiastically praises as one of the most important contributions to the longtermist literature:
T his is just the tip of the iceberg. Consider the implications of this conception of ‘our potential’ for the development of technology and creation of new risks. Since realising our potential is the ultimate moral goal for humanity, and since our descendants cannot become posthuman, colonise space and create ~10 58 people in computer simulations without technologies far more advanced than those around today, failing to develop more technology would itself constitute an existential catastrophe – a failure mode (comparable to Ramsey neglecting his talents by spending his days playing pool and drinking) that Bostrom calls ‘plateauing’. Indeed, Bostrom places this idea front-and-centre in his canonical definition of ‘existential risk’, which denotes any future event that would prevent humanity from reaching and/or sustaining a state of ‘technological maturity’, meaning ‘the attainment of capabilities affording a level of economic productivity and control over nature close to the maximum that could feasibly be achieved.’ Technological maturity is the linchpin here because controlling nature and increasing economic productivity to the absolute physical limits are ostensibly necessary for creating the maximum quantity of ‘value’ within our future light cone.
But reflect for a moment on how humanity got itself into the current climatic and ecological crisis. Behind the extraction and burning of fossil fuels, decimation of ecosystems and extermination of species has been the notion that nature is something to be controlled, subjugated, exploited, vanquished, plundered, transformed, reconfigured and manipulated. As the technology theorist Langdon Winner writes in Autonomous Technology (1977), since the time of Francis Bacon our view of technology has been ‘inextricably bound to a single conception of the manner in which power is used – the style of absolute mastery, the despotic, one-way control of the master over the slave.’ He adds:
This is precisely what we find in Bostrom’s account of existential risks and its associated normative futurology: nature, the entire Universe, our ‘cosmic endowment’ is there for the plundering, to be manipulated, transformed and converted into ‘value-structures, such as sentient beings living worthwhile lives’ in vast computer simulations, quoting Bostrom’s essay ‘Astronomical Waste’ (2003). Yet this Baconian, capitalist view is one of the most fundamental root causes of the unprecedented environmental crisis that now threatens to destroy large regions of the biosphere, Indigenous communities around the world, and perhaps even Western technological civilisation itself. While other longtermists have not been as explicit as Bostrom, there is a clear tendency to see the natural world the way utilitarianism sees people: as means to some abstract, impersonal end, and nothing more. MacAskill and a colleague, for example, write that the EA movement, and by implication longtermism, is ‘tentatively welfarist in that its tentative aim in doing good concerns promoting wellbeing only and not, say, protecting biodiversity or conserving natural beauty for their own sakes.’
On this account, every problem arises from too little rather than too much technology
Just as worrisome is the longtermist demand that we must create ever-more powerful technologies, despite the agreed-upon fact that the overwhelming source of risk to human existence these days comes from these very technologies. In Ord’s words, ‘without serious efforts to protect humanity, there is strong reason to believe the risk will be higher this century, and increasing with each century that technological progress continues.’ Similarly, in 2012 Bostrom acknowledges that
On this view, there is only one way forward – more technological development – even if this is the most dangerous path into the future. But how much sense does this make? Surely if we want to maximise our chances of survival, we should oppose the development of dangerous new dual-use technologies. If more technology equals greater risk – as history clearly shows and technological projections affirm – then perhaps the only way to actually attain a state of ‘existential security’ is to slow down or completely halt further technological innovation.
But longtermists have an answer to this conundrum: the so-called ‘value-neutrality thesis’. This states that technology is a morally neutral object, ie, ‘just a tool’. The idea is most famously encapsulated in the NRA’s slogan ‘Guns don’t kill people, people kill people,’ which conveys the message that the consequences of technology, whether good or bad, beneficial or harmful, are entirely determined by the users, not the artefacts. As Bostrom put it in 2002, ‘we should not blame civilisation or technology for imposing big existential risks,’ adding that ‘because of the way we have defined existential risks, a failure to develop technological civilisation would imply that we had fallen victims of an existential disaster.’
Ord similarly argues that ‘the problem is not so much an excess of technology as a lack of wisdom,’ before going on to quote Carl Sagan’s book Pale Blue Dot (1994): ‘Many of the dangers we face indeed arise from science and technology but, more fundamentally, because we have become powerful without becoming commensurately wise.’ In other words, it is our fault for not being smarter, wiser and more ethical, a cluster of deficiencies that many longtermists believe, in a bit of twisted logic, could be rectified by technologically reengineering our cognitive systems and moral dispositions. Everything, on this account, is an engineering problem, and hence every problem arises from too little rather than too much technology.
We can now begin to see how longtermism might be self-defeating. Not only could its ‘fanatical’ emphasis on fulfilling our longterm potential lead people to, eg, neglect non-existential climate change, prioritise the rich over the poor and perhaps even ‘justify’ pre-emptive violence and atrocities for the ‘greater cosmic good’ but it also contains within it the very tendencies – Baconianism, capitalism and value-neutrality – that have driven humanity inches away from the precipice of destruction. Longtermism tells us to maximise economic productivity, our control over nature, our presence in the Universe, the number of (simulated) people who exist in the future, the total amount of impersonal ‘value’ and so on. But to maximise, we must develop increasingly powerful – and dangerous – technologies; failing to do this would itself be an existential catastrophe. Not to worry, though, because technology is not responsible for our worsening predicament, and hence the fact that most risks stem directly from technology is no reason to stop creating more technology. Rather, the problem lies with us, which means only that we must create even more technology to transform ourselves into cognitively and morally enhanced posthumans.
This looks like a recipe for disaster. Creating a new race of ‘wise and responsible’ posthumans is implausible and, if advanced technologies continue to be developed at the current rate, a global-scale catastrophe is almost certainly a matter of when rather than if. Yes, we will need advanced technologies if we wish to escape Earth before it’s sterilised by the Sun in a billion years or so. But the crucial fact that longtermists miss is that technology is far more likely to cause our extinction before this distant future event than to save us from it . If you, like me, value the continued survival and flourishing of humanity, you should care about the long term but reject the ideology of longtermism, which is not only dangerous and flawed but might be contributing to, and reinforcing, the risks that now threaten every person on the planet."
Mapping the brain’s connective structure could unlock immortality | Aeon Essays,,https://aeon.co/essays/mapping-the-brains-connective-structure-could-unlock-immortality,"In the Asturias region of northwest Spain, a cave drawing of a woolly mammoth has a single, internal feature: a large red heart. This work of art, at least 14,000 years old, likely depicts a successful hunt and bloody wound. From the earliest days of our species, the detection of a pulse, the preservation of respiration and the beating of a heart have served to separate a piece of meat from a living being.
The fundamental connection between breathing, heartbeat and life itself began to change as knowledge of the brain’s role in consciousness evolved and as technology made it possible to use machines to operate the heart and lungs while a patient remained on life support. Today, we define life and death by the presence or absence of brain activity. That makes sense because, unlike other organs, the brain not only signals life, but is essential to you , the individual, to your own unique qualities of identity, memory, knowledge and subjective experience of the world.
To better understand how the brain underlies selfhood, we need to understand its complex form; its intricate structure at the level of connections between neurons. After all, understanding biological structure has revealed the nature of many diverse life forms. Plants thrive because their typically broad leaves are perfect for transducing light energy into vital chemical energy. Similarly, eyes, whether human or insect, enable the transduction of light from one’s surroundings into electrical signals within the nervous system. These impulses carry information that represents features of the surrounding environment. But when it comes to the relationship between structure and function, brains have remained an enigma. There’s a lot more to them than to other organs that have specific functions, such as eyes, hearts or even hands. These organs can now be surgically replaced. Yet, even if a brain transplant were possible, you couldn’t just switch your brain with another person’s and maintain the same mind. Such an idea of brain replacement is a logical fallacy.
What is it about a brain that creates individual experience?
Upon birth, a person’s brain structure is largely prescribed by experience in the womb and their unique genetic code. As we age, experience continues to imprint unique changes on the brain’s neural connectivity, increasing connections in some areas while decreasing them in others, accumulating reroutes upon reroutes as a person ages and learns, gaining knowledge and experience. Additionally, there are alterations in the strength of existing connections. These processes are especially evident in twins, whose brains are strikingly similar when born. However, as they grow, learn and experience the world, their brains diverge, and their essential selves become increasingly unique.
Essentially, this process creates memory, something so fundamental that it unconsciously surfaces in every aspect of our sense of self. Even our unconscious knowledge of movements needed for riding a bike, speaking a word or even walking require memory. Incredibly, hypothermia victims, who have undergone hours of clinical death signified by an absence of both heart and brain activity can achieve a state of full recovery, demonstrating that neural electrical activity alone is not essential for the storage of memory in the brain.
Although there are indeed anatomical regions that appear to serve relatively specific functions , one’s memory is not formed, stored or recalled within the activity of any single brain region. Certain structures, such as the amygdala and the hippocampus, play key roles but trying to find memory in one specific area is simply impossible. It would be like trying to listen to Beethoven’s Fifth but hearing only the strings (duh duh duh, duuuh!). Instead, memory, in its broadest sense, lies in the uniqueness of a brain’s entire connective structure, known as the connectome . The connectome consists of its complete network of neurons and all the connections between them, called synapses. It is argued that, fundamentally, ‘you are your connectome’.
Thus, a key to unlocking the correspondence between the connectome and memory is to elucidate the entire circuitry of the brain. Tracing the wiring at this scale is no easy task when considering the sheer complexity involved. A mere cubic millimetre of brain tissue contains around 50,000 neurons, with an astonishing total of around 130 million synapses, according to some estimates. An entire human brain, however, is more than 1 million cubic millimetres and contains around 86 billion neurons, nearly equivalent with estimates of the number of stars in our galaxy.
The most relevant number is the one representing the total sum of synaptic connections, which comes in at a mind-numbing c 100 trillion. Once the possible paths that electrical neural signals can run on across these connections are determined, only then might it be possible to comprehensively know the patterns of activity integral to memory and to subjective experience.
O btaining connectomes could go a long way to answering some fundamental questions about the relation between neurons and behaviour. I asked Jeff Lichtman, a neuroscientist at Harvard University and a pioneering connectomicist, what we could do with a human connectome, should we be able to reproduce it, and he said the benefit would be profound. We could, for instance, come up with far more effective therapies for neurocognitive disorders such as schizophrenia or autism – problems thought to be caused by miswiring – though we still aren’t sure how.
Lichtman’s research has been inspired by the insight that, across species, the brain’s wiring diagram changes as individuals grow and develop through life. But his greatest motivation is charting the unknown reaches of the mind imprinted in the connectome data itself. He compared the connectome, in this respect, to genomics. Having a full human connectome, he noted, would be analogous to a full genome – opening a universe of discovery we can’t even fathom right now.
But simpler models of connectomes from other species have already helped science advance. Researchers at the Allen Institute for Brain Science in the US, for instance, have traced the circuitry of an entire mouse brain, showing how different types of neurons connect various anatomical regions. A collaboration at the Janelia Research Campus, involving Google scientists and centred at the Howard Hughes Medical Institute in Ashburn, Virginia, mapped a large, central region of the fruit-fly connectome at the level of individual neurons; a feat that took more than 12 years and at least $40 million.
It’s crucial that the extracted brain is preserved accurately to maintain its complex connectome before it’s sliced up
Even before these remarkable accomplishments, pioneering researchers mapped the complete connectome of the roundworm, Caenorhabditis elegans , back in the 1980s – all of its 302 neurons and around 7,600 synapses – fuelling research for years. Complex simulations of activity on the roundworm connectome are revealing the synchronised activity patterns underlying its wriggling movements.
Across species, synchronisation and coordination of neural signals between seemingly distant brain regions within a connectome provide the scaffold for execution and memory of ordered sequences of events. For example, when young birds learn their songs, they encode, store and retrieve the sound patterns they hear from other birds, in various chains of neurons which, in turn, activate sequences of muscle movements that create the same sonic patterns. Currently there are at least 20 ongoing studies investigating relations between the human connectome and its role in memory, many coordinated by an organisation called the Connectome Coordination Facility of the US National Institutes of Health.
Mapping a connectome at the level of single neurons, however, is currently impossible in a living animal. Instead, animal brains must be extracted, perfused with a fixative such as formaldehyde and sliced up as many times as possible before being analysed structurally in order to painstakingly find individual neurons and trace their paths. To achieve this, the properties of each new slice are recorded using various microscopy techniques. Once that’s been done, patterns of electrical flow can be estimated from different neuron types and from connections that excite or inhibit other neurons. What’s crucial is that the extracted brain is preserved accurately enough to maintain its intricate, complex connectome before it’s sliced up.
Currently, it’s unlikely that any human brain has been preserved with its entire connectome perfectly intact. Our brains degrade too quickly after death. Without oxygen-rich blood flow, there’s a marked drop in metabolic activity, the set of chemical reactions that maintains an organism’s cellular life. When the brain’s cells stop metabolising, irreversible structural damage from a lack of fresh oxygen can begin within just five minutes. Slicing up a brain for connectome mapping thus requires preserving it as soon as possible to minimise this damage.
And so, to actually maintain the exact structure of the entire connectome, you need a preservation method where every single neuron and each of its synaptic connections are held in place – a requirement that must succeed about 100 trillion times over, for an individual human.
T he implications surrounding a human brain-preservation technique that can keep the entire connectome intact are profound. If indeed, you are your connectome, defined by all the memories and essences of you imprinted in its structure, then it’s essentially you that’s preserved. Your connectomic self .
Theoretically, the logic suggests the prospect of escaping death.
In 2010, a group of neuroscientists came together over shared interest in this idea, actualising their motivations by creating the Brain Preservation Foundation (BPF). The president and co-founder of the BPF is Ken Hayworth, also a senior scientist at the Janelia Research Campus. Over the phone, he told me that he hoped to involve scientists in making brain preservation an option for patients with terminal illness. ‘I know someone in a hospital who is dying and there is simply no option for them now,’ he said. ‘If nobody advocates for this procedure, surely it will never happen … I will want this option when it is my time to face a terminal illness.’
Soon after forming, the BPF began offering a $100,000 cash prize, donated by the Israeli tech entrepreneur and poker player Saar Wilf, for new methods of connectome preservation. The competition was structured in two stages based on increasing brain size: a small-mammal prize and a large-mammal prize. With a set of detailed evaluation guidelines involving molecule-level electron microscopy scans, the challenge was put forth to anyone willing to undertake the enormous effort involved.
And who best to undertake the challenge than the cryonics community, devoted to cryopreserving terminally ill people (or just their brains) right after death, in hopes that they will be thawed after storage in liquid nitrogen in a future that has a cure. Hayworth wanted the prize money to prompt them to demonstrate the effectiveness of their preservation techniques. He told me: ‘The prize was meant to motivate the cryonics providers to “put up or shut up”.’
But by 2018, cryonics still hadn’t put up. Instead, scientists from a private cryobiological research company in California, 21CM (for 21st-Century Medicine), focused on preserving frozen specimens, won both stages, claiming the preservation prize after demonstrating intact connectomes in a preserved rabbit brain and subsequently in a preserved pig brain. Greg Fahy, 21CM’s founder and an experienced cryobiologist, innovated the prizewinning technique along with Robert McIntyre, a graduate of the Massachusetts Institute of Technology (MIT). The process, technically called aldehyde-stabilised cryopreservation, but now branded vitrifixation , hinges on using a fast-acting fixative called glutaraldehyde, previously used as a disinfectant, in combination with other chemicals that cause the brain to enter a vitrified physical state, hence the name, vitrifixation.
He wondered if he could somehow extract a memory from a brain – essentially a ‘living memory’
The process spelled a revolution for futurists because the connectomes were deemed intact after cryogenic freezing down to at least -135°C. At this temperature, all metabolic, biological processes cease to the point of enabling indefinite storage, potentially for hundreds, if not thousands of years, with no sign of rotting. Assuming the relevant logic regarding the connectomic self and the role of memory is correct, vitrifixation can essentially enable the preservation of you, indefinitely, in a form of suspended animation.
McIntyre has long held that there’s great value in preserving not just the physical brain structures but memory itself, held within those structures. After all, human progress depends on the transference of information over time, via great leaps of innovation. The first such leap was achieved upon the establishment of oral language and the next upon written language, which could more accurately preserve information, possibly for longer stretches of time. ‘Could you imagine going back in time and telling someone, in a time before written language, that one day it will be possible to turn anything they can speak into carvings in stone that can last aeons, for anyone in the distant future to discover? They wouldn’t have believed you,’ McIntyre told me over the phone.
He was first inspired by the prospect of using neuroscience to extract memories from brains, because they contain far more information about experiences and events than any other current form of preservation, such as writing, audio or even video. After listening to recordings of his grandmother talking about travelling by covered wagon from Oklahoma to Texas, among other historic life experiences, he wondered if it could be possible to somehow extract a memory from a brain – essentially a ‘living memory’, the first-hand perspective of actually being there – the information you’re missing after you read, for example, a history textbook, as compared with personally having lived through that same history yourself.
As a student, he visited a neuroscience lab, where researchers called the idea outlandish and impossible to achieve. Instead, he decided to approach the problem computationally, by using artificial intelligence (AI) to solve it. He completed coursework at MIT, and in 2014 accompanied his father to a cabin in the wilderness to finish the dissertation for his PhD. The two of them took a walk that changed his life. While toting handguns in case of rattlesnake attack, his father asked him, aside from AI, how he might salvage memory directly. They concluded that the best way was to leave it up to the future to create technologies that are largely unimaginable to us now, while preserving the substrate of those memories, the connectome itself.
If connectomes hold memories that can be re-experienced, their importance is unique. Take the wisdom achieved by soldiers after experiencing life-changing events during a war. It’s one thing to read about world wars in textbooks or even in personal memoirs, but those forms of information don’t directly carry the detail contained in a living memory of experiencing war firsthand. It’s a deep sort of wisdom, McIntyre believes, that could enrich humanity with the knowledge, foresight and judgment needed to divert it from an unsustainable, species-ending path.
Now, through vitrifixation, there was finally a technique for immortalising memories in the connectome that BPF scientists could advocate. Unfortunately, the fixative agent used to perfuse the vascular system in vitrifixation is entirely and directly fatal. You couldn’t immortalise memories without killing their creator.
If you were to go through the procedure , after experiencing your last thought, a general anaesthetic will be used to subdue you. Then, your chest will be opened and your arteries connected to a perfusion apparatus. After being exsanguinated and pumped with glutaraldehyde, it will diffuse into your brain’s capillaries and cease all metabolic activity, killing you nearly instantly while connecting proteins constitute your brain into a robust, lasting meshwork. Afterwards, your brain will be perfused with antifreeze to prevent damage before it’s extracted and cryogenically stored indefinitely.
T o make a terrible pun, it seems like a no-brainer. The treatment (death) is worse than the problem: living memory lost. Yet both Hayworth and McIntyre believe that vitrifixation, though fatal, offers a type of immortality, if the essence of someone can be scanned for all the relevant information and then somehow transferred to an artificial medium; one that essentially replaces the brain, from a functional standpoint. Crucially, this medium, when ‘running’ would have to accurately and sufficiently conduct the patterns of neural activity that support one’s memory, identity and experience to evoke their unique consciousness.
This goal is called ‘whole-brain emulation’. After all, why do brains have to consist of only biological material? And if minds can run on a network of connections, can’t they be ‘substrate independent’ such that all the information essential to a mind is contained in the arrangement and operation of those connections, not any given substrate itself?
Although the relevant science is in its infancy, some significant achievements exist. Many approaches foresee computational mediums for emulating brain activity involving digital information spaces. Currently, brain-computer interfaces enable thought-controlled activity of prosthetic machines. Moreover, actual neural prosthetics are directly replacing brain cells. It’s form to function in the truest sense. What’s more is that multimillion-dollar tech enterprises such as Neuralink, Kernel, Building 8 and DARPA are forging even more advanced connections between mind, brain and computer that increase the possibility of such whole-brain emulation.
We must ask if we’re consigned to exist as the very molecules that presently constitute ourselves?
So how exactly would you emulate something as astronomically complex as a brain? Two approaches have gained traction. The first, and most popular, involves creating a digital simulation of the connectome and its activity, perhaps at a molecular scale, and then setting it free in cyberspace. In this grandiose scheme, the simulation is so complete and accurate that it becomes an emulation with the emergent property of a person’s identity, memory, consciousness, thoughts and feelings in the same way that we currently understand subjective experience to be an emergent property of someone’s active biological brain. As it’s been construed, this future involves the possibility of living in a virtual, simulated world where you mingle with other emulated minds. The second approach involves transplanting the emulated brain into a prosthetic self, the ultimate cyborg in which every part of you is synthetic. In this case, your mind could exist in the real world with a completely artificial body.
But perhaps you would go no further in survival than your lifeless, vitrifixed brain and whatever might remain of the rest of your corpse. In either scenario, even if the ‘new you’ were to be a complete, conscious emulation with the same memories, identity, feelings and subjective self, there remains the striking possibility that it wouldn’t actually be you. Rather, a doppelgänger: a duplicate, identical in all respects. After all, it should be just as possible to create multiple instances of a new you; then, which would be you? All? In this way, memories, identity and conscious subjective experience is like a song that can be played on any instrument that can produce its neural notes.
Alternatively, definitions of personal identity and survival could come to surround you as a continuous property, rather than as a binary, yes/no alternative. When you’re old, you’re essentially only partially the same person as you were when you were born, but at no point in the transition does the younger you die while the old you is suddenly created. Essentially, we must ask whether we are consigned to exist as the very molecules that presently constitute ourselves? As we explore consciousness and connectomes, our ways of thinking about them could evolve by great leaps. In my conversations with Lichtman, Hayworth and McIntyre, I heard a similar message: although the possibility of reanimation is the current beachhead, by the time we can achieve it, human knowledge, culture and technology are likely to alter the form it takes.
When I probed McIntyre on this, he simply said: ‘If brains can do it [eg, revive after clinical death in survivors of cardiac arrest], we can do it – and we’ll figure out how.’ Like Lichtman (who considers himself a ‘presentist’ rather than a futurist), McIntyre made an analogy with the discovery of DNA. ‘When it was discovered 70 years ago, nobody really knew what to actually do with it, and now…’ Hayworth adds: ‘This is really not happening any time soon.’ But also: ‘humanity will eventually succeed in understanding the brain, and in developing the scanning and simulation technologies that are needed … humanity will eventually figure it out.’
W ith such far-reaching prospects comes great responsibility. Vitrifixation’s potential for escaping death entails numerous ethical questions that remain unanswered, despite formal consideration: would there be equal opportunity to engage the process or would it be exclusive to those who can afford it, for example? How would one’s memories be safeguarded against tampering, destruction or theft? Who would have ownership? Under what circumstances could memories in a virtual connectome be accessed, and by whom?
One issue seems less fraught: the potential for making vitrifixation an option for terminally ill patients as soon as it can be achieved.
Taking on all this, McIntyre and his former roommate at MIT Michael McCanna founded a controversial venture capital startup after winning the $100,000 prize. Their company is a brain bank initiative called Nectome. Its primary goal, as stated on the company’s website, is to preserve and essentially archive human memory. So far, Nectome has raised more than $1 million in funding and has received a $960,000 federal grant from the US National Institute of Mental Health for ‘whole-brain nanoscale preservation and imaging’. The federal grant explicitly mentions the possibility of a ‘commercial opportunity in offering brain preservation’.
Undergoing vitrifixation could amount to nothing more than suicide at a considerable financial cost
Nectome already has a list of at least 30 supporters, each having given a $10,000 donation. The process, which has never actually been performed on a living human, is technically legal in five US states under current physician-assisted suicide laws for those who are terminally ill. Nectome’s only human vitrifixation, in fact, was performed on the brain of an elderly woman whose corpse was given to McIntyre by the body-donation company Aeternitas Life. The operation was performed just 2.5 hours after the woman’s death, resulting in one of the best-preserved brains in existence.
It’s no surprise that Nectome has seen some serious controversy. The donations are incorrectly construed in various media reports as ‘deposits’ for suicidal procedures, something that McIntyre denies outright. ‘Those donors wanted to become early supporters. We don’t offer any brain preservation service,’ he told me when I asked. But responding to the uproar, MIT ended an ongoing neuroscience collaboration with the company in 2018.
The sobering fact of the matter is that anyone hoping to become a Nectome client might very well have a futile wait. The claim that the self can be found in the connectome is still a long way from being proven, and there might never be any way to determine if consciousness can exist in a machine. Undergoing vitrifixation could amount to nothing more than suicide at a considerable financial cost.
No one should be rushing out to get their brains preserved when there’s no guarantee that it will work, Hayworth states. Instead, he says he just wants to further the science. ‘It might not work, obviously, but people are dying. [Vitrifixation] is already proven to reliably preserve precisely those structures and molecules that modern neuroscience says encode us. Therefore, terminal patients should have the opportunity to take that chance, if they wish.’
From the current views of Lichtman to the futurist optimism actualised by Hayworth and McIntyre, one sentiment is consistent: the connectome has the potential to immensely impact our future in unknown, but meaningful ways."
Complex systems science allows us to see new paths forward | Aeon Essays,,https://aeon.co/essays/complex-systems-science-allows-us-to-see-new-paths-forward,"We’re at a unique moment in the 200,000 years or so that Homo sapiens have walked the Earth. For the first time in that long history, humans are capable of coordinating on a global scale, using fine-grained data on individual behaviour, to design robust and adaptable social systems. The pandemic of 2019-20 has brought home this potential. Never before has there been a collective, empirically informed response of the magnitude that COVID-19 has demanded. Yes, the response has been ambivalent, uneven and chaotic – we are fumbling in low light, but it’s the low light of dawn.
At this historical juncture, we should acknowledge and exploit the fact we live in a complex system – a system with many interacting agents, whose collective behaviour is usually hard to predict. Understanding the key properties of complex systems can help us clarify and deal with many new and existing global challenges, from pandemics to poverty and ecological collapse.
In complex systems, the last thing that happened is almost never informative about what’s coming next. The world is always changing – partly due to factors outside our control and partly due to our own interventions. In the final pages of his novel One Hundred Years of Solitude (1967), Gabriel García Márquez highlights the paradox of how human agency at once enables and interferes with our capacity to predict the future, when he describes one of the characters translating a significant manuscript:
Our world is not so different from the vertiginous fantasies of Márquez – and the linear thinking of simple cause-effect reasoning, to which the human mind can default, is not a good policy tool. Instead, living in a complex system requires us to embrace and even harness uncertainty. Instead of attempting to narrowly forecast and control outcomes, we need to design systems that are robust and adaptable enough to weather a wide range of possible futures.
T hink of hundreds of fireflies flashing together on a summer’s evening. How does that happen? A firefly’s decision to flash is thought to depend on the flashing of its neighbours. Depending on the copying rule they’re using, this coordination causes the group to synchronise in either a ‘ bursty ’ or ‘snappy’ fashion. In her book Patterns of Culture (1934), the anthropologist Ruth Benedict argued that each part of a social system depends on its other parts in circuitous ways. Not only are such systems nonlinear – the whole is more than the sum of the parts – but the behaviour of the parts themselves depends on the behaviour of the whole.
Like swarms of fireflies, all human societies are collective and coupled . Collective, meaning it is our combined behaviour that gives rise to society-wide effects. Coupled, in that our perceptions and behaviour depend on the perceptions and behaviour of others, and on the social and economic structures we collectively build. As consumers, we note a shortage of toilet paper at the supermarket, so we hoard it, and then milk, eggs and flour, too. We see our neighbours wearing masks, so put on a mask as well. Traders in markets panic upon perceiving a downward trend, follow the herd and, to echo Márquez, end up causing the precipitous drop they fear.
These examples capture how the collective results of our actions feed back, in both virtuous and vicious circles, to affect the system in its entirety – reinforcing or changing the patterns we initially perceived, often in nonobvious ways. For instance, some coronavirus contact-tracing apps can inform users of the locations of infected persons so they can be avoided. This kind of coupling between local behaviour and society-wide information is appealing because it seems to simplify decision-making for busy individuals. Yet we know from many years of work on swarming and synchronicity – think of the flashing fireflies – that the dynamics of coupled systems can be surprising.
A recent study in Nature Physics found transitions to orderly states such as schooling in fish (all fish swimming in the same direction) can be caused, paradoxically, by randomness, or ‘noise’ feeding back on itself. That is, a misalignment among the fish causes further misalignment, eventually inducing a transition to schooling. Most of us wouldn’t guess noise can produce predictable behaviour. The result invites us to consider how technology such as contact-tracing apps, although informing us locally, might negatively impact our collective movement. If each of us changes our behaviour to avoid the infected, we might generate a collective pattern we had aimed to avoid: higher levels of interaction between the infected and susceptible, or high levels of interaction among the asymptomatic.
Complex systems also suffer from a special vulnerability to events that don’t follow a normal distribution or ‘bell curve’. When events are distributed normally, most outcomes are familiar and don’t seem particularly striking. Height is a good example: it’s pretty unusual for a man to be over 7 feet tall; most adults are between 5 and 6 feet, and there is no known person over 9 feet tall. But in collective settings where contagion shapes behaviour – a run on the banks, a scramble to buy toilet paper – the probability distributions for possible events are often heavy-tailed . There is a much higher probability of extreme events, such as a stock market crash or a massive surge in infections. These events are still unlikely, but they occur more frequently and are larger than would be expected under normal distributions.
Learning changes an agent’s behaviour. This in turn changes the behaviour of the system
What’s more, once a rare but hugely significant ‘tail’ event takes place, this raises the probability of further tail events. We might call them second-order tail events ; they include stock market gyrations after a big fall, and earthquake aftershocks. The initial probability of second-order tail events is so tiny it’s almost impossible to calculate – but once a first-order tail event occurs, the rules change, and the probability of a second-order tail event increases.
The dynamics of tail events are complicated by the fact they result from cascades of other unlikely events. When COVID-19 first struck, the stock market suffered stunning losses followed by an equally stunning recovery. Some of these dynamics are potentially attributable to former sports bettors, with no sports to bet on, entering the market as speculators rather than investors. The arrival of these new players might have increased inefficiencies, and allowed savvy long-term investors to gain an edge over bettors with different goals. In a different context, we might eventually see the explosive growth of Black Lives Matter protests in 2020 as an example of a third-order tail event: a ‘black swan’, precipitated by the killing of George Floyd, but primed by a virus that disproportionately affected the Black community in the United States, a recession, a lockdown and widespread frustration with a void of political leadership. The statistician and former financier Nassim Nicholas Taleb has argued that black swans can have a disproportionate role in how history plays out – perhaps in part because of their magnitude, and in part because their improbability means we are rarely prepared to handle them.
One reason a first-order tail event can induce further tail events is that it changes the perceived costs of our actions, and change the rules that we play by. This game-change is an example of another key complex systems concept: nonstationarity . A second, canonical example of nonstationarity is adaptation, as illustrated by the arms race involved in the coevolution of hosts and parasites. Like the Red Queen and Alice in Alice in Wonderland , parasite and host each has to ‘run’ faster, just to keep up with the novel solutions the other one presents as they battle it out in evolutionary time.
Learning changes an agent’s behaviour, which in turn changes the behaviour of the system. Take a firm that fudges its numbers on quarterly earnings reports, or a high-school student who spends all her time studying specifically for a college-entrance exam rather than developing the analytical skills the test is supposed to be measuring. In these examples, a metric is introduced as a proxy for ability. Individuals in the system come to associate performance on these metrics with shareholder happiness or getting into college. As this happens, the metric becomes a target to be gamed, and as such ceases to be an objective measure of what it is purporting to assess. This is known as Goodhart’s Law, summarised by the business adage: ‘The worst thing that can happen to you is to meet your targets.’
Another type of nonstationarity relates to a concept we call information flux . The system might not be changing, but the amount of information we have about it is. While learning concerns the way we use the information available, information flux relates to the quality of the data we use to learn. At the beginning of the pandemic, for example, there was a dramatic range of estimates of the asymptomatic transmission rate. This variation partly came from learning how to make a good model of the COVID-19 contagion, but it was also due to information flux caused by the fact that viruses spread, and so early on only a small number of people are infected. This makes for sparse data on the numbers of asymptomatic and symptomatic individuals, not to mention the number of people exposed. Early on, noise in the data tends to overwhelm the signal, making learning very difficult indeed.
These forms of nonstationarity mean biological and social systems will be ‘out of equilibrium’, as it’s known in the physics and complex systems literature. One of the biggest hazards of living in an out-of-equilibrium system is that even interventions informed by data and modelling can have unintended consequences. Consider government efforts to enforce social distancing to flatten the COVID-19 infection curve. Although social distancing has been crucial in slowing the infection rate and helping to avoid overwhelming hospitals, the strategy has created a slew of second- and third-order biological, sociological and economic effects. Among them are massive unemployment, lost profit, market instability, mental health issues, increase in domestic violence, social shaming, neglect of other urgent problems such as climate change and, perhaps most importantly, second-order interventions such as the reserve banks injecting liquidity into the markets, governments passing massive stimulus bills to shore up economies, and possible changes to privacy laws to accommodate the need to enforce social distancing and perform contact-tracing.
D o the properties of complex systems mean prediction and control are hopeless enterprises? They certainly make prediction hard, and favour scenario planning for multiple eventualities instead of forecasting the most likely ones. But an inability to predict the future doesn’t preclude the possibility of security and quality of life. Nature, after all, is full of collective, coupled systems with the same properties of nonlinearity and nonstationarity. We should therefore look to the way biological systems cope, adapt and even thrive under such conditions.
Before we turn to nature, a few remarks about human engineering. Our species has been attempting to engineer social and ecological outcomes since the onset of cultural history. That can work well when the engineering is iterative, ‘bottom up’ and takes place over a long time. But many such interventions have been impotent or, worse, disastrous, as discussed by the anthropologist Steve Lansing in his book Priests and Programmers: Technologies of Power in the Engineered Landscape of Bali (2007). In one section, Lansing compares the effective, 1,000-year-old local water distribution system in Bali with the one imposed by central government engineers during the 20th-century green revolution. This top-down approach disrupted the fragile island and its shoreline ecosystems, and undermined collective governance.
Fiascos happen when we use crude data to make qualitative decisions. Other reasons include facile understandings of cause and effect, and the assumption that the past contains the best information about the future. This kind of ‘backward looking’ prediction, with a narrow focus on the last ‘bad’ event, leaves us vulnerable to perceptual blindness. Take how the US responded to the terrorist attacks of 11 September 2001 by investing heavily in terrorism prevention, at the expense of other problems such as healthcare, education and global poverty. Likewise, during the COVID-19 crisis, a deluge of commentators has stressed investment in healthcare as the key issue. Healthcare is neglected and important, as the pandemic has made clear – but to put it at the centre of our efforts is to again be controlled by the past.
Fans of The Lord of the Rings might remember the character Aragorn’s plan to draw Sauron’s eye to the Black Gate, so that the protagonists Frodo and Sam could slip into Sauron’s realm via another route (the lair of a terrifying spider-like monster). The plan relied on Sauron’s fear of the past, when Aragorn’s ancestor cut the powerful ring at the centre of the story from Sauron’s finger. The point is that narrow, emotionally laden focus effectively prevents us from perceiving other problems even when they are developing right under our noses. In complex systems, it is critical to build safeguards against this tendency – which, on a light-hearted note, we name Sauron’s bias .
There are better ways to make consequential, society-wide decisions. As the mathematician John Allen Paulos remarked about complex systems: ‘Uncertainty is the only certainty there is. And knowing how to live with insecurity is the only security.’ Instead of prioritising outcomes based on the last bad thing that happened – applying laser focus to terrorism or inequality, or putting vast resources into healthcare – we might take inspiration from complex systems in nature and design processes that foster adaptability and robustness for a range of scenarios that could come to pass.
This approach has been called emergent engineering. It’s profoundly different from traditional engineering, which is dominated by forecasting, trying to control the behaviour of a system and designing it to achieve specific outcomes. By contrast, emergent engineering embraces uncertainty as a fact of life that’s potentially constructive.
When applied to society-wide challenges, emergent engineering yields a different kind of problem-solving. Under a policy of constructive uncertainty, for example, individuals might be guaranteed a high minimum quality of life, but wouldn’t be guaranteed social structures or institutions in any particular form. Instead, economic, social and other systems would be designed so that they can switch states fluidly, as context demands. This would require a careful balancing act between questions of what’s good and right on the one hand – fairness, equality, equal opportunity – and a commitment to robustness and adaptability on the other. It is a provocative proposal, and experimenting with it, even on a relatively small scale as in healthcare or financial market design, will require wading through a quagmire of philosophical, ethical and technical issues. Yet nature’s success suggests it has potential.
The human heart confers robustness by beating to a rhythm that’s neither chaotic nor periodic but fractal
Consider that the human body is remarkably functional given all that could go wrong with its approximately 30 trillion cells (and 38 trillion bacterial cells in the body’s microbiome). Nature keeps things working with two broad classes of strategy. The first ensures that a system will continue to function in the face of disturbances or ‘perturbations’; the second enables a system to reduce uncertainty but allow for change, by letting processes proceed at different timescales.
The first strategy relies on what are known as robustness mechanisms . They allow systems to continue to operate smoothly even when perturbations damage key components. For example, gene expression patterns are said to be robust if they do not vary in the face of environmental or genetic perturbations such as mutations. There are many mechanisms that make this invariance possible, and much debate about how they work, but we can simplify here to give the basic idea. One example is shadow enhancers : partially redundant DNA sequences that regulate genes and work together to keep gene expression stable when a mutation occurs. Another example is gene duplication in which genes have a backup copy with partial functional overlap. This redundancy can allow the duplicate to compensate if the original gene is damaged.
Robustness mechanisms can be challenging to build in both natural and engineered systems, because their utility isn’t obvious until something goes wrong. They require anticipating the character of rare but damaging perturbations. Nature nonetheless has discovered a rich repertoire of robustness mechanisms. Reconciliation – making up after fights and restoring relationships to a preconflict baseline – isn’t just a human invention. It’s common throughout the animal kingdom and has been observed in many different species. In a different context, the complex structure of the human heart is thought to confer robustness to perturbations at a wide range of scales by beating to a rhythm that is neither chaotic nor periodic but has a fractal structure. Robust design, in contrast to typical approaches in engineering, focuses on discovering mechanisms that maintain functionality under changing or uncertain environments.
Nature has another set of tricks up her sleeve. The timescales on which a system’s processes run have critical consequences for its ability to predict and adapt to the future. Prediction is easier when things change slowly – but if things change too slowly, it becomes hard to innovate and respond to change. To solve this paradox, nature builds systems that operate on multiple timescales. Genes change relatively slowly but gene expression is fast. The outcomes of fights in a monkey group change daily but their power structure takes months or years to change. Fast timescales – monkey fights – have more uncertainty, and consequently provide a mechanism for social mobility. Meanwhile, slow timescales – power structures – provide consistency and predictability, allowing individuals to figure out the regularities and develop appropriate strategies.
The degree of timescale separation between fast and slow dynamics matters too. If there’s a big separation and the power structure changes very slowly, no amount of fight-winning will get a young monkey to the top – even if that monkey, as it gained experience, became a really gifted fighter. A big separation means it will take a long time for ‘real’ information at the individual level – eg, that the young monkey has become a good fighter – to be reflected in the power structure. Hence, if the power structure changes too slowly, although it might guard against meaningless changes at the individual level, it won’t be informative about regularities – about who can actually successfully use force when things, such as the ability of our young monkey, really do change.
Furthermore, sometimes the environment requires the system as a whole to innovate, but sometimes it demands quiescence. That means there’s a benefit to being able to adjust the degree of timescale separation between the fast and slow processes, depending on whether it’s useful for a change at the ‘bottom’ to be felt at the ‘top’. These points circle us back to our earlier remarks about nonstationarity – the degree of timescale separation is a way of balancing trade-offs caused by different types of nonstationarity in the system.
The detailed mechanisms by which nature accomplishes timescale separation are still largely unknown and an active area of scientific investigation. However, humans can still take inspiration from the timescale-separation idea. When we design systems of the future, we could build in mechanisms that enable users – such as market engineers and policymakers – to tune the degree of timescale separation or coupling between individual behaviour on the one hand, and institutions or aggregate variables such as stock returns or time in elected office on the other. We have crude versions of this already. Financial markets are vulnerable to crashes because of an inherent lack of timescale separation between trading and stock market indices, such that it’s possible in periods of panic-selling for an index to lose substantial value in a matter of hours. In recognition of this property, market engineers introduced what’s called a ‘circuit breaker’ – a rule for pausing trading when signs of a massive drop are detected. The circuit breaker doesn’t really tune the separation between trades and index performance, though. It simply halts trading when a crash seems likely. A more explicit tuning approach would be to slow down trading during dangerous periods by limiting the magnitude or frequency of trades in a given window, and to allow trading to proceed at will when the environment is more predictable. There are many possible alternative tuning mechanisms; which is best suited to markets is ultimately an empirical question.
S tock market crashes are a bridge to another of nature’s fascinating properties: the presence of tipping points or critical points , as they’re called in physics. When a system ‘sits’ near a critical point, a small shock can cause a big shift. Sometimes, this means a shift into a new state – a group of fish shoaling (weakly aligned) detects a shark (the shock) and switches to a school formation (highly aligned), which is good for speedy swimming and confusing the predator. These tipping points are often presented in popular articles as something to avoid, for example, when it comes to climate change. But, in fact, as the shark example illustrates, sitting near a critical point can allow a system to adapt appropriately if the environment changes.
As with timescale separation, tipping points can be useful design features – if distance from them can be modulated. For example, in a recent study of a large, captive monkey society it was found the social system was near a critical point such that a small rise in agitation – perhaps caused by a hot afternoon – could set off a cascade of aggression that would nudge the group from a peaceful state into one in which everyone is fighting. In this group there happened to be powerful individuals who policed conflict, breaking up fights impartially. By increasing or decreasing their frequency of intervention, these individuals could be tuning the group’s sensitivity to perturbations – how far the aggression cascades travel – and thereby tuning distance from the critical point.
We still don’t know how widespread this sort of tuning is in biological systems. But like degree of timescale separation, it’s something we can build into human systems to make them more fluid and adaptive – and therefore better able to respond to volatility and shocks. In the case of healthcare, that might mean having the financial and technological capacity to build and dismantle temporary treatment facilities at a moment’s notice, perhaps using 3D-printed equipment and biodegradable or reusable materials. In the economy, market corrections that burst bubbles before they get too large serve this function to some extent – they dissipate energy that has built up within the system, but keep the cascade small enough that the market isn’t forced into a crash.
We are not perfect information processors. We make mistakes. The same is true of markets
Climate-change activists warning about tipping points are right to worry. Problems arise when the distance from the critical point can’t be tuned, when individuals make errors (such as incorrectly thinking a shark is present), and when there’s no resilience in the system – that is, no way back to an adaptive state after a system has been disturbed. Irreversible perturbations can lead to complete reconfigurations or total system failure. Reconfiguration might be necessary if the environment has changed, but it will likely involve a costly transition, in that the system will need time and resources to find satisfactory solutions to the new environment. When the world is moderately or very noisy – filled with random, uninformative events – sensitivity to perturbations is dangerous. But it’s useful when a strategic shift is warranted (eg, a predator appears) or when the environment is fundamentally changing and the old tactics simply won’t do.
One of the many challenges in designing systems that flourish under uncertainty is how to improve the quality of information available in the system. We are not perfect information processors. We make mistakes and have a partial, incomplete view of the world. The same is true of markets, as the investor Bill Miller has pointed out . This lack of individual omniscience can have positive and negative effects. From the system’s point of view, many windows on the world affords multiple independent (or semi-independent) assessments of the environment that provide a form of ‘collective intelligence’. However, each individual would also like a complete view, and so is motivated to copy, share and steal information from others. Copying and observation can facilitate individual learning, but at the same time tends to reduce the independence and diversity that’s valuable for the group as a whole. A commonly cited example is the so-called herd mentality of traders who, in seeing others sell, panic and sell their own shares.
For emergent engineering to succeed, we need to develop a better understanding of what makes a group intelligent. What we do know is there seem to be two phases or parts of the process – the accumulation phase, in which individuals collect information about how the world works, and the aggregation phase, in which that information is pooled. We also know that if individuals are bad at collecting good information – if they misinterpret data due to their own biases or are overconfident in their assessments – an aggregation mechanism can compensate.
One example of an aggregation mechanism is the PageRank algorithm used early on in Google searches. PageRank worked by giving more weight to those pages that have many incoming connections from other webpages. Another kind of aggregation mechanism might discount votes of individuals who are prone to come to the same conclusion because they use the same reasoning process, thereby undermining diversity. Or take the US electoral college, which was originally conceived to ‘correct’ the popular vote so that population-dense areas didn’t entirely control election outcomes. If, on the other hand, implementing or identifying good aggregation mechanisms is hard – there are, for example, many good arguments against the electoral college – it might be possible to compensate by investing in improving the information-accumulation capacity of individuals. That way, common cognitive biases such as overconfidence, anchoring and loss-aversion are less likely at first instance. That said, in thinking through how to design aggregation algorithms that optimise for collective intelligence, ethical issues concerning privacy and fairness also present themselves.
Rather than attempt to precisely predict the future, we have tried to make the case for designing systems that favour robustness and adaptability – systems that can be creative and responsive when faced with an array of possible scenarios. The COVID-19 pandemic provides an unprecedented opportunity to begin to think through how we might harness collective behaviour and uncertainty to shape a better future for us all. The most important term in this essay is not ‘chaotic’, ‘complex’, ‘black swan’, ‘nonequilibrium’ or ‘second-order effect’. It’s: ‘dawn’.
Published in association with the Santa Fe Institute, an Aeon Strategic Partner."
Superlative projects are made possible by great collective efforts | Aeon Essays,,https://aeon.co/essays/superlative-projects-are-made-possible-by-great-collective-efforts,"In 1978, the science-fiction author Michael Moorcock wrote the celebrated essay ‘Epic Pooh’ that lambasted J R R Tolkien and his ilk for constructing fantasy universes in which – whatever the ‘there and back again’ meanderings of the plot – nothing ever really changes. Moorcock felt that his own ‘new worlds’ science fiction of the 1960s was a radical intervention for producing difference, while the fantasists were essentially nostalgic for a past in which everything had its proper place: the king on his throne, the dragons slain, the ordinary hobbits happy with their pastoral lot.
We have a lot of this fantasy futurism nowadays, with the armies of good and evil being arranged on opposing sides of the wall, or across a rift in the time-space continuum. Jon Snow and the New Avengers will come to save us, swinging swords to ensure that everything stays pretty much how it was before Voldemort, Sauron or the White Witch started messing things up. Fantasy is not really that fantastic, essentially because it takes a set of tired archetypes, adds a bit of sex and chopping, and then delivers a message about singular heroes, villains and obstacles. The Marvel Universe, currently at 23 movies, will soon be overtaking James Bond as the biggest film franchise of all time. Add in comics, TV and games, and you have a huge investment in the idea that we need heroes to save us. As flies to the gods are we.
Perhaps this cultural leaning isn’t about the future at all – just the recitation of comforting stories from long ago and far away to lull us to sleep. Real attempts to describe (and realise) human futures have always been more dangerous. What we now call the Enlightenment made curiosity possible, even celebrated, instead of thinking of it as a sin to be punished. At the same time that ‘utopian’ fiction was invented, we find attempts to produce a new sort of mercantile world, in which the timeless sclerosis of hierarchical feudalism was gradually eroded by the speculations of projectors, inventors and adventurers who built new organisations and machines.
Later, the ingenuity of the Industrial Revolution was predicated on the coordination of people and things at scale, towering mill buildings and a rail network that compressed space through the precision of the timetable and technology. This was organising on a grand scale, and it required the coordination of people and materials in ways never imagined before. In the 20th century, the Empire State building, the Apollo space programme and the supersonic gee-whizzery of Concorde are emblems of a future in which nothing could ever be the same again. These were grand projects of massive complexity, auguries of a modern world, and they drew their strength and efficacy – and, not least, their imaginative purchase on the future – from the new science of management.
I n English, the word ‘management’ has an interesting history, and some rather productive differences of meaning. It is derived from the Italian mano , meaning hand, and its expansion into maneggiare , the activity of handling and training a horse carried out in a maneggio – a riding school. From this form of manual control, the word has expanded into a general activity of training and handling people. It is a word that originates with ideas of control, of a docile or wilful creature that must be subordinated to the instructions of the master.
It’s a really interesting word that grows in influence and application as the feudal order of fixed social relations based on ownership of land is disturbed by the rise of a mercantile class. This bourgeois revolution rested upon a relationship to capital, of which land was only one element, but it relied as well on more complex forms of production and distribution, and consequently on organisations with more elaborate divisions of labour. Localised craft economies were displaced by manufactories, machine production, urbanisation and also longer distribution networks via canals and trains. Its practice is fabricated in the ‘dark satanic mills’ that romantics and radicals were keen to condemn for the brute people and despoiled landscapes that they produced.
But the later imperialism of this beast-handling word doesn’t tell the whole story. In English, the earliest use of ‘management’ was as a verb that could be applied to the need to deal with complex or adverse matters, and it is the oldest usage. In Richard II , written in 1595, William Shakespeare has Green say: ‘Now for the rebels which stand out in Ireland,/Expedient manage must be made, my liege.’ The word could mean something like careful planning, necessary in this case because of the complexity and danger of that which was being faced.
The future has been displaced into the soma of fantasy
The London Encyclopaedia (1829) has an entry for ‘Manage’, which suggests that it is:
This sense of management as coping, as dealing with a particular state of affairs, is still passable in everyday English. You might ask ‘How are you managing?’ if someone has told you about some problem they face. To organise complex matters, to arrange people and things, to be resilient in the face of adversity, now that requires managery. This second meaning, not distinct but different in emphasis, emerges in the 19th century with the class of people called ‘managers’. These managers do management. And as this occupational group grows throughout the 20th century, driven by the growth of the capitalist corporation, so the business school expands to train them.
At the present time, that sense of managing as the art of ‘organising’ to cope with challenges is largely obscured by the idea of the manager as someone who helps to create financial value for organisations, whether they operate in state-engineered pseudo-markets, or the carbon-max madness of global trade. This means that questions about what sort of future human beings might create tend to be limited by the horizon of the management strategies of market capitalism. This version of the future isn’t about radical discontinuity at all, just an intensification of the business practices that promise to give us Amazon Prime by drone at the same time that the real Amazon burns. This is what they teach in business schools – how to keep calm and carry on doing capitalism. But the problems we face now are considerably bigger than a business school case study, so is it possible to rescue managery from management?
C aptain Marvel and Thor aside, the other set of gods that dominate our futures are the business school heroes – Bezos, Musk, Zuckerberg and many others. Amazon has made Jeff Bezos the wealthiest of them, personally worth about $138 billion dollars currently, and with a company that now seems to own the world. Close behind him, perfecting his Tony Stark-Iron Man look is Elon Musk, busy making our futures with Neuralink, Tesla and SpaceX. Geek-lord Mark Zuckerberg’s Facebook doesn’t actually make or do anything: the enterprise just collects information that allows it to sell advertising. These are the unicorns, the people who started a business in a garage and now live in fenced compounds overlooking some canyon or other. Their future is one in which you keep paying so that people like them can keep selling you things you didn’t know you wanted.
In the hands of technology entrepreneurs, driven by the imperatives of shareholder value and richer even than the robber barons of a century ago, the future has been displaced into the soma of fantasy, colonised by people who want you to pay a subscription for an app that helps you sleep, a delivery service that allows you to stay indoors when it’s wet out, or a phone that switches on the heated seats in your car before you leave home. This is a future of sorts, but it’s a business school version in which everything is pretty much the same, just a bit smarter and more profitable. It’s being sold to us in adverts at the cinema and in pop-ups on our screens, as if it were the real future, but it’s not. For something to count as the future, for innovation to be as inspiring as the Eiffel Tower, Apollo or Concorde, it must promise something that has never been before. It must be a rupture, a break in the ordinary series of events that produces a future that is altered in profound ways, and something on the horizon that is unknowable, but different.
The historian David Nye in 1994 captured something rather important about how we think about the future when he coined the phrase ‘technological sublime’, drawing on Edmund Burke’s A Philosophical Enquiry into the Origin of Our Ideas of the Sublime and Beautiful (1757), which provided a compelling rationale for exposing oneself to the awe-fullness of nature. If smallness, delicacy, fragility, smoothness, sweetness and gradual variation were characteristics of the beautiful, then the sublime was characterised by vastness, darkness, danger, power, infinity and suddenness. It is because, Burke suggested, these qualities remind us of our mortality and insignificance that exposure to the sublime provokes such strong emotions. Towering mountains and crashing seas stir us to a greater degree than tranquil pastoralism, and call forth in us dramatic, even heroic feelings, which in turn inspire our grandly romantic sense of quest, spirit and dreaming.
Nye’s 20th-century version of the sublime is the airship, the Hoover Dam, the skyscraper, the railroad – magnificent mountains of steel, grand architectural projects, incredible distances and speeds. These technological feats became symbols of national pride and fuelled prophecies that nothing would ever be the same again. A particularly American version of the future was everywhere from the 1930s onwards with pulp sci-fi writers selling stories to magazines called Amazing Stories and Astounding Stories of Super-Science – visions that later became radio and then TV shows, boldly exploring where no man had been before. And for me, growing up in England in the 1960s, it was the Apollo space programme, and then Concorde, that looked like splinters from a new world, somewhere just around the corner.
The Moon landing was also a huge exercise in New Deal economics
In 1968, James Edwin Webb, then ex-administrator of the US space agency NASA, delivered a series of McKinsey Foundation lectures at the Graduate School of Business at Columbia University in New York, which were published under the title Space Age Management the next year – but before the Moon landing of July 1969. Webb had been schooled in Franklin D Roosevelt’s New Deal administration and was an advocate of grand-scale government intervention, big projects to change the world. At that time, ‘space age’ was a prefix that was being applied to everything. Effectively, it meant something like ‘modern’, and the coupling of ‘space age’ and ‘management’ in his book combines this sense of modernity with a particularly technocratic sense of control. The space age was going to be an age of mass organisation, of pills instead of meals, of new products that saved time, and of factories and offices and launch pads connected by freeways and telephones. It was a world of harmonious organisation, managed by wise and well-qualified elders who would deal with our problems by organising the world in a better way. It was the shape of things to come.
In order to achieve John F Kennedy’s goal, articulated in 1961, of landing men on the Moon before the decade was out, huge numbers of people, things and places needed to be made, coordinated and controlled. In 1966, NASA directly employed a staff of 36,000, with another 400,000 people working for 20,000 contractors and 200 universities in 80 countries. Such numbers are almost unimaginable today. It meant that Webb needed to account for people and things, chains of command, scheduled meetings with determinate agendas, as well as plans, predictions, reports and deadlines. The Moon landing was a triumph of organisation, of project management and control of a huge and complex sociotechnical system. It was also a huge exercise in New Deal economics, with a wide variety of state subsidies being channelled through NASA to aerospace corporations, universities, local regions and so on. The contradiction between ideas of individualism and the free market and this taxpayer-funded technocracy didn’t go unnoticed, and it meant that Webb and others needed to make constant reference to a dead president and a new frontier to keep NASA’s funding rolling in.
Going to the Moon made little sense, after all. It was criticised then and since as a monumental distraction from the problems of the Earth and a subsidy for the military-industrial complex – a white male fantasy in which fighter-pilot heroes ride rockets while the world looks on in awe. As the American poet Gil Scott-Heron put it in 1970: ‘No hot water, no toilets, no lights./(but Whitey’s on the moon)’. Yet Apollo was also one of the iconic moments of the 20th century, and inspired feelings of admiring wonder among millions of people that still resonate half a century later. It also encouraged many radical writers and activists to make connections between science, fiction and possibility that were substantially at odds with the political and financial interests key to driving the actual space programme. This sense of ambition for a revolutionary change – for something that the Marxist philosopher Ernst Bloch described as the ‘not-yet’ ­– is written across much of the previous century.
There are different versions of utopia, such as the Arcadian pastoral idyll or the unlimited consumption of Cockaigne , but the 20th century provided us with the most intensely technocratic versions of what a good future might look like. The cultural critic Constance Penley in 1997 called this ‘NASA/Trek’, an entanglement of technocratic managerialism with a Star Trek sense of quest and adventure. The sort of management imagined by Webb aimed to solve big problems with big money to stimulate innovation from universities and corporations alike. Although it required complex coordination, it was shot through with a sense of vision that brought together millions of people in a collective effort to achieve something that they could be proud of.
This future wasn’t that long ago, but it has felt lost. The Game of Thrones fantasies of Westeros distract us from the actual domination of capitalism by tech giants that use logistics and algorithms to deliver us all that we would ever need. Yet we need this radical future now more than ever.
T he most worrying thing about this failure of imagination is that it is happening now, at a point when the entire human species, and lots of other ones too, are facing an Armageddon-sized climate crisis and a deadly pandemic. If everything was going well, we might be forgiven for being distracted by stories of heroes who never were and getting Five Guys Deliverooed to our front doors. It seems that, after a hard day at the app developers’ or the call centre, few people can conjure a new way of living.
Yet we have no choice. This carbon-intensive world can’t continue, and the collective development of real futures is more important now than ever. In 2015, a group of distinguished British scientists, economists and businessmen launched the Global Apollo Programme, calling on developed nations to commit to spending 0.02 per cent of their GDP, ever year for 10 years, to fund coordinated research to make carbon-free baseload electricity less costly than electricity from coal by the year 2025.
Meanwhile, Mariana Mazzucato, a professor in the economics of innovation and public value at University College London, used the Apollo space programme as an example of visionary collectivism in order to push forward her idea of ‘mission-oriented’ research and innovation at a multistate level. Her proposed missions are ambitious: gigantic attempts to conjoin governments, cities, business and citizens in achieving goals that will redefine the future for all of us. In her 2018 report to the European Commission she suggests a few: 100 carbon-neutral cities by 2030; a plastic-free ocean; addressing dementia. All these targets represent enormous challenges, requiring coordination between organisations and interest groups that are currently part of the problem. They also require that, in order to produce a future worth having, we stop doing most of the things we do now. (Tell that to the oil industry, routinely given to disguising business as usual with a promise that it has changed.)
Science-fiction writers have always had the sense that yesterday and tomorrow don’t need to be the same
Another version of this sort of response to grand challenges, one that directly inherits the Keynesian commitments of Webb’s space-age management, is the Green New Deal. Though the US and the UK versions are slightly different, both rely on the idea of massive state-backed intervention funding investments in renewable energy, retrofitting houses, reorganising our food production and so on. Vast sums of money and huge amounts of labour will be required to alter just about every area of human activity on the planet: so, imagine lifelike avatars that can have meetings in Los Angeles, vertical farming taking place in deep mines, entire cities being rebuilt to get rid of the need for the private car. This is the stuff of 1930s science fiction, the sort of realist speculation that projected change on a grand scale, but stayed away from magic and dragons.
The Apollo space programme, love it though I do, represents a past to which we can never return. It was based on military technologies and driven by imperial capitalism, and it guzzled gas like there was no tomorrow. Watching footage of Saturn V taking off always brings a tear to my grizzled eye, but that’s not sufficient reason to imagine that we could (or should) go back to then. But what I do want to rescue is the sense that the future can be different: the sense that science-fiction writers have always had that yesterday and tomorrow don’t need to be the same. Capitalism has captured the future, and is now commodifying it and selling it back to us as gizmos and widgets, or else distracting us with fantasy – defined by its refusal to engage in realism or real problems. As the literary critic Fredric Jameson said in 2003, or rather said that someone else said, ‘it is easier to imagine the end of the world than to imagine the end of capitalism.’
Now, more than ever, we need these stories about the future. Not the cityscape lensflare adverts in which we all have friends and lives that play to a soundtrack of Coldplay-lite thanks to our oh-so-very-smart telephones, and the sort of marketing taught in business schools. We need real futures, stories about radical changes that we’ll all be making in order to build the world differently. Deserts covered in solar panels, food made from algae grown in space, underground distribution systems that bring us what we need so that our roads can become parks for children to play in. These futures need to co-opt stories as compelling as those being told by Marvel and Samsung – not puritan warnings about what you can’t have, but pictures of lives that are rich and full, in which people can be heroes and you have nice things to eat.
This is the world that we need managery to make, and that we need wisdom, planning and complex organisation to produce. Webb would have relished the challenge, a way of organising that could produce something as sublime as an endless forest of wind turbines, or a train that travels as fast as a plane. It’s not too much to ask of clever monkeys like us, unless we continue to be distracted by Middle Earth and Instagram."
The Catholic Church can be a force for environmental change | Aeon Essays,,https://aeon.co/essays/the-catholic-church-can-be-a-force-for-environmental-change,"Listen to this essay
In the United States and elsewhere, it’s often taken for granted that the Catholic Church is a conservative institution with little engagement in the fate of the natural world. And for ample reason. Consider the biblical verse that’s slithered into modernity like a false prophet, Genesis 1:28. Here, God tells Adam and Eve: ‘Be fertile and multiply; fill the earth and subdue it. Have dominion over the fish of the sea, the birds of the air, and all the living things that crawl on the earth.’ This single verse has been held accountable for creating a permission structure that’s allowed generations of humans to hunt and fish and kill and maim and pollute; to strip field and forest of vegetation, to mine, drill and frack.
Christianity is an inherently human-centred religion. In his paper ‘The Historical Roots of Our Ecologic Crisis’ (1967), the historian Lynn White Jr pointed to Christianity’s culpability in our plundering of the environment. ‘Christianity is the most anthropocentric religion the world has seen,’ he writes; ‘in absolute contrast to ancient paganism and Asia’s religions,’ Christianity ‘not only established a dualism of man and nature but also insisted that it is God’s will that man exploit nature for his proper ends.’ White casts the Church as a covert actor in the brutalisation of our planet, one that ‘bears a huge burden of guilt’. His prominence has helped this view solidify over the past half-century.
Over the same period, there’s been a notable shift – largely associated with the evangelical movement – that’s seen Christians turn inwards, increasingly fixating on their own moral arc. According to the theologian Terrence Ehrman at the University of Notre Dame in Indiana, ‘Christian belief in God the Creator has been eclipsed’ by ‘belief in Jesus Christ the Redeemer’. With that comes a near-fanatical focus on sin, repentance and a striving for personal redemption. The emphasis is on the believer’s personal relationship with an empowering Christ and a quest for salvation, with all its heavenly rewards. The personal narrativising narrows the scope of the faith to the first person: a kind of bootstrap individualism interpolated into Christian doctrine. What matters to the faithful is not so much the state of the world but rather the state of their own soul.
Given this, Catholicism would appear to be the last place you’d find radical innovation that takes climate consciousness to heart. But were you to look beyond the Church’s contemporary reputation, and see the full sweep of its history, you’d uncover another version of the religion. In this form of Christianity, there’s a shimmering common thread that’s often obscured today, dedicated to radical change, valiant acts of compassion, and a solidarity with all of God’s creation.
J esus Christ preached a radical way of living that shifted his followers’ priorities away from the strict observation of Jewish rites and rituals and towards an injunction to love both God and their neighbour ‘as thyself’. Neighbours in this usage was a universal category, including the hungry, poor, sick, and incarcerated – all the individuals scrabbling on the margins of society.
The Gospel of Matthew emphasises the primacy of this responsibility in distinguishing between those who are worthy of the kingdom of heaven and those who are not: ‘For I was hungry and you gave me something to eat, I was thirsty and you gave me something to drink, I was a stranger and you invited me in, I needed clothes and you clothed me, I was sick and you looked after me, I was in prison and you came to visit me.’ Jesus tells ‘the righteous’: ‘whatever you did for one of the least of these brothers and sisters of mine, you did for me.’ Throughout the New Testament, Jesus demonstrates a compassion and solidarity with the naked, needy, crippled, and blind, positioning those on the lowest rungs of society as the most beloved of his ministry. In the process, he set the stage for much of the radical Christian thinking that was to come.
When the plague swept the Roman Empire in the 4th century, striking large population centres like Caesarea, Romans fled to evade contamination. Evidence suggests, however, that some Christians elected to stay. The historian Eusebius of Caesarea noted how Christians ‘gave practical proof of their sympathy and humanity. All day long some of them tended to the dying and to their burial, countless numbers with no one to care for them.’ Though exhausted and withered from famine, Christians who remained in the plague-stricken city distributed bread to the sick. Before long, their deeds were hanging ‘on everyone’s lips’.
These Christians were working to bring the revolutionary spirit of Jesus Christ into the present day
When the plague resurfaced in Italy in the 14th century, a Majorcan Catholic named Roch risked his life by tending to the sick and dying in public hospitals on his pilgrimage to Rome. A century later, the Augustinian friar Martin Luther refused to leave the city of Wittenberg when the plague swept through Germany, opting to stay with his wife and tend to the infirm. ‘Those who are engaged in a spiritual ministry,’ he wrote later, must ‘remain steadfast before the peril of death,’ adding: ‘We have a plain command from Christ: “A good shepherd lays down his life for the sheep …’’’
In the modern era, Christians have pioneered the care ministered in leper colonies, established thousands of mission hospitals, and set up clinics focused on maternal and neonatal health in sub-Saharan Africa. But it isn’t just the sick and dying that Catholics have historically taken up as a moral cause. The Catholic Worker Movement, founded in 1933 by Dorothy Day and Peter Maurin, began as a newspaper: its mission, to let those ‘who think that there is no hope for the future, no recognition of their plight’ know that ‘there are men of God who are working not only for their spiritual, but for their material welfare.’ The Catholic Worker Movement aspired to ‘live in accordance with the justice and charity of Jesus Christ.’ Day, Maurin and other Catholics affiliated with the movement established these ‘houses of hospitality’ all over the US – places where those in need could go for food, shelter and clothing. In the decades during and after the Depression, more than 100 of these houses were founded nationwide (today, there are nearly 150 operating in the US, and dozens more worldwide), some serving thousands of people a day.
A similar drive to seek justice and relief for the downtrodden took root in Latin America following the second Latin American Bishops’ Conference, in Medellín, Columbia, in 1968. There, bishops from Central and South America drafted a series of documents focused on moving beyond Catholic pedagogy and directly improving their parishioners’ lives. The bishops wanted to combat the ‘institutionalised violence’ of poverty in their nations, and began to espouse what would eventually be called the ‘preferential option for the poor’. This revolutionary tenet asserted that God, as he is depicted in the Bible, instructed his followers to address the needs of the poor first.
These ideas eventually coalesced into what we now know as liberation theology, a politically charged form of Catholicism that swept Latin America in the 1970s and ’80s. Personified by audacious, visionary figures like Gustavo Gutiérrez, Óscar Romero and Leonardo Boff, liberation theology was miles away from any cloistered Catholicism. Instead of sleepy priestcraft and pedantry, these Christians were working to bring the revolutionary spirit of Jesus Christ into the present day. As Gutiérrez wrote in A Theology of Liberation (1973): ‘The denunciation of injustice implies the rejection of the use of Christianity to legitimise the established order.’
While these movements may appear to represent breakaway strains of the religion, they’re all firmly rooted in Christ’s example and the Catholic doctrine that followed. Catholic social teaching is a part of Church theology that consists of seven pillars. Originally laid out by Pope Leo XIII in an 1891 encyclical, the pillars include the sanctity of all human life, prioritising the needs of the poor, the dignity of work and workers, and care for God’s creation. These pillars are inspired by the religion’s prophets, who, according to the US Conference of Catholic Bishops, ‘announced God’s special love for the poor and called God’s people to a covenant of love and justice.’
Catholic social teaching has been embodied by historical figures like Day, Maurin and Gutiérrez – Catholics committed to action-oriented compassion for those in need. Through their movements and the moral acts they’ve inspired, Catholics are not using their religion to ‘legitimise the established order’; they’re imposing a spiritual responsibility on themselves to change the world for the better and ‘lighten the sum total of suffering,’ as Day put it. This dimension of the Catholic Church can be obscured by the baggage and hypocrisy that barnacle onto many perennial institutions, yet it is critical to understanding how Christianity can be a force for change today. In particular, Catholic social teaching advocates care for the whole of God’s creation, a pillar colloquially referred to as ‘creation care’. This tenet isn’t an archaic, mouldering article of faith, either. The US Conference of Catholic Bishops maintains that care for God’s creation is ‘a requirement of our faith’, with ‘fundamental moral and ethical dimensions that cannot be ignored.’ While most people may not associate the Catholic Church with environmental justice, many of its most revered leaders have been advocating for Earth for a long time.
H uman impact on the climate is calamitous. The global surface temperature in 2025 is on pace to be among the warmest ever recorded; sea levels are climbing; and a biodiversity crisis is rapidly underway, with a 2024 World Wildlife Fund report showing that wildlife populations have experienced an average population decline of more than 70 per cent since 1970. By every ecological metric, the figures are staggering. At first blush, the role that the Catholic Church and its roughly 1.4 billion global followers might play in this planetary crisis is not immediately clear. The Church is not Greenpeace, the Environmental Defense Fund or the Climate Action Network – it isn’t an NGO with an explicit objective to fight climate change, promote decarbonisation, or preserve the world’s languishing biodiversity. What it is, however, is a thought leader with the authority and credibility to influence people’s attitudes and beliefs.
If you approach the Bible outside of an anthropomorphic lens, you find that even Genesis reveres the nonhuman aspects of God’s creation, notably in passages in which God looks admiringly on plants and animals, land and sea: after each day of layering in one more facet on his loamy canvas, God ‘saw that it was good’. A passel of psalms celebrates the full breadth of creation. Psalm 104, for example, praises a vast ecosystem in harmonious balance: ‘The trees of the Lord drink their fill, the cedars of Lebanon, which you planted./There the birds build their nests; the stork in the junipers, its home./The high mountains are for wild goats; the rocky cliffs, a refuge for badgers.’ If attention is ‘the natural prayer we make’, as the French philosopher Nicolas Malebranche suggested in 1674, then many psalms might be considered exultant prayers for the natural world, testifying to an elusive, heterogeneous beauty.
During his relatively brief tenure as the head of the Catholic Church, Pope Benedict XVI condemned our extractive relationship with the environment. ‘The external deserts in the world are growing, because the internal deserts have become so vast,’ he said during his first homily as pope in 2005. ‘Therefore the earth’s treasures no longer serve to build God’s garden for all to live in, but they have been made to serve the powers of exploitation and destruction.’
The Church unequivocally recognised the scientific consensus on the warming of the planet
Bartholomew I of Constantinople (technically, the leader of the Eastern Orthodox Church) advocates so passionately for ecological issues that he’s often referred to as the ‘Green Patriarch’. At an environmental symposium held in a Greek Orthodox Church in California in 1997, he became one of the first individuals to articulate a nascent concept that would eventually become known as ecological sin. ‘For humans to cause species to become extinct and to destroy the biological diversity of God’s creation … For humans to degrade the integrity of Earth by causing changes in its climate, by stripping the Earth of its natural forests, or destroying its wetlands … These are sins,’ he said.
Arguably, the single-greatest influence on Catholic thinking about our responsibility to Earth is the encyclical that Pope Francis published in 2015, when climate change was clawing into the global consciousness during the hottest year on record up to that point. In Laudato si’ (‘Praise Be to You’), Pope Francis drew a sharp line in the sand: the Church unequivocally recognised the scientific consensus on the warming of the planet and humanity’s culpability in it. Laudato si’ is more than a dispassionate document affirming climatological facts, though. It seeks to contextualise the unfolding catastrophe in moral and spiritual terms. Mother Earth, Francis laments, ‘now cries out to us because of the harm we have inflicted on her by our irresponsible use and abuse of the goods with which God has endowed her. We have come to see ourselves as her lords and masters, entitled to plunder her at will.’
Unconstrained by the anodyne, emotionally neutered language of academic science, Pope Francis explicitly ties our environmental crisis to human sin, myopic pride, and an anthropocentric mindset that sees our relationship to Earth in coldly utilitarian terms. His encyclical lays out how the impacts of climate change reverberate through the worlds of plants, animals and vulnerable populations, triggering a cascade effect that imperils both natural ecosystems and the global poor.
In making these connections, Francis articulates a larger, bolder thesis that frames global warming as a starkly moral issue. Climate change is both a cause and an effect of the extreme inequality we see in the world: one that further victimises the most vulnerable lives while perpetuating the gross power imbalance that permitted it in the first place. ‘This is why the earth herself, burdened and laid waste, is among the most abandoned and maltreated of our poor,’ he writes. ‘She “groans in travail” …’ Laudato si’ is, in many ways, a culmination of many of the Catholic moral crusades that preceded it – ecological sin, liberation theology, the Catholic Worker Movement. Like the poor, sick, and stigmatised, Francis decried how ‘the earth herself’ is now being immiserated by exploitation and indifference.
I t is difficult to overstate just how influential Laudato si’ has been to the Church’s followers. In conversations I’ve had with nearly two dozen Catholic theologians and activists, they consistently characterise Francis’s encyclical as a watershed event, a clarion call that connects a veritable scientific reality with the spiritual conscience of believers. ‘Pope Francis writing Laudato si’ was this huge gift to people who’ve been caring for creation for so long,’ Anna Johnson, the North America director of the Laudato Si’ Movement, told me. When she reached out to fellow Catholics in the wake of its publication, ‘We were all like: “We’re crying with this encyclical – it’s so beautiful.”’
For other Catholics, Laudato si’ offered the opportunity to grow more comfortable in connecting their passion for the environment to their faith. It gave the cartographer Molly Burhans confidence to found GoodLands, an organisation that works to map the Catholic Church’s vast landholdings for conservation and humanitarian purposes. ‘I got the courage to do that from Pope Francis,’ she said. ‘He brought together the best ecologists, the best scientists to consult on Laudato si’ .’
The encyclical arguably opens a new chapter in the Church’s history by encouraging the faithful to be more assertive in embodying the environmental pillar of Catholic social teaching. The Laudato Si’ Movement, for example, is a global network that collaborates with hundreds of Catholic organisations all over the world to promote sustainability and divestment from the fossil-fuel industry. In anticipation of the 2015 United Nations Climate Change Conference (COP) in France, the Movement gathered close to 1 million signatures from Catholics around the world to petition the UN to include language limiting temperature rise to 1.5 degrees. ‘We collaborated in submitting these signatures to both the French presidency and also to Christiana Figueres, who was then the convener of the COP,’ said Christina Leaño, the associate director of the Laudato Si’ Movement. ‘It was the only petition that pushed for keeping temperature rise below 1.5 degrees, and it eventually made it into the final document.’
‘What if the Catholic Church became the largest global network of conservation the world has ever seen?’
Another major Catholic nonprofit, the Catholic Climate Covenant (CCC), takes a grassroots approach by partnering with parishioners all over the US to pursue projects aimed at reducing carbon emissions and promoting sustainability locally. ‘We have close to 400 creation care teams,’ Diana Marin, a programme manager at CCC, told me. Marin continues to be motivated by the legacy left by Pope Francis’s words. ‘Pope Francis spoke about los descartados , the thrown-away people. And so that call to environmental stewardship or that vocation to care for creation, I think that also calls us to care for those people and to rethink that way of living that throws people and things away.’
The Catholic Church itself has taken some modest steps towards more substantively supporting environmental stewardship. In 2023, the Irish Bishops’ Conference agreed to rewild 30 per cent of all parish land in Ireland by 2030. As the Conference explained: ‘All are called to arrest the decline of biodiversity for the sake of the next generation.’ Several years ago, the Church worked with Burhans to map all the land it owns in the US, for the purposes of eventually maintaining its properties more sustainably. The project saw GoodLands map more than 33,000 Catholic-affiliated properties, producing an exhaustive dataset that integrated the Church’s official directory with assessors’ information and commercial datasets to offer a comprehensive picture of Catholic Church landholdings in the US.
Burhans was enthusiastic – exuberant, even – about the Church’s potential to be a world leader in environmental stewardship when we spoke. ‘There’s something that’s inspired me from the start of this, this super-big vision/question,’ she said. ‘Which is: what if the Catholic Church became the largest global network of conservation and ecological regeneration the world has ever seen?’
B urhan’s ambition notwithstanding, it may be quixotic to expect the Catholic Church to become a transformative actor in addressing the climate crisis. But it can serve as a lodestar for the world’s billion-plus Catholics by centring the primacy of creation care. People’s attitudes and feelings need to evolve internally before they’re willing to change their behaviours. Because of the way the institution can set priorities for its followers, and impose moral urgency, the Church is uniquely suited to shake Christians out of the inertia – and self-centred anthropocentrism – that’s taken hold for so many. If the impact of Pope Francis’s encyclical is a harbinger and not a blip, it can continue to rouse them toward meaningful action.
The truth is that the Catholic Church is in a far better position to galvanise Christians than its Protestant counterpart in the US. American Protestantism has more than twice as many followers as Catholicism, with an interdenominational movement that encompasses Baptists, Methodists, Pentecostals and Quakers. Its largest and most powerful contingent, the Evangelical movement, counts nearly a quarter of the US population among its flock. And it is this movement and its flagship denominations – Baptists, Methodists, Lutherans – that have become an animating force in US politics, agitating for the pro-life movement, resistance to LGBTQ rights, and the preservation of a conservative Christian nation (otherwise known as Christian nationalism). These platforms and allegiances make it difficult for the disparate strains of US Protestantism to coalesce around the issue of climate change – especially given its association with progressivism. ‘They basically identify their Christian faith with the cause of American prosperity,’ Norman Wirzba, a theologian at Duke University in North Carolina, and an Anabaptist, told me. ‘They identify it with the flag.’
Wirzba recalls having a recent discussion with several pastors based in the South, on holding a potential workshop about climate issues among their congregations. He didn’t get far before he was sharply rebuffed: ‘It was depressing to hear pastors of major-sized churches tell me that their congregations are not interested in what Jesus had to say about ethical or political issues.’ Protestant denominations in the US are also the keenest advocates of a personal religion that cultivates a private relationship with Jesus Christ in the hopes of personal salvation. ‘It’s about getting your soul into heaven,’ Wirzba said. ‘It’s a very individualistic faith.’
‘The industrialist who doesn’t care if he leaves the land a foul and hideous place is like the pornographer or hedonist who looks upon the human body as an assemblage of parts for the procuring of pleasure’
Despite its hypocrisies and missteps, the Catholic Church does not suffer from the same co-opting of its identity. It’s also demonstrated a unique capacity to inspire followers to reject the status quo and take bold measures to follow Jesus Christ’s radical example, transcending self-interest and even self-preservation for the sake of spiritual conviction. Whether it’s a moral crusade against poverty, disease or systemic injustice, the Church has long given people the courage to sublimate themselves to a nobler cause. It’s this version of Christianity that can move its followers to see the climate crisis and all the ecosystems it imperils with the sense of moral urgency and existential consequence they so clearly warrant.
Far from being an obstacle to stewardship, the Church is actually in a position to reframe the conversation using its longstanding moral authority. So much of the way we now talk about global warming and its catastrophic effects are rooted in scientific and economic jargon – statistics, mechanics, technologies, impacts. While these contexts are important, what we so often leave out may be even more consequential: morality.
When I spoke to Anthony Esolen, a Catholic scholar and translator, he referred to our environmental predicament in a language I rarely hear: one of moral accountability. ‘Our whole orientation toward the world is out of kilter,’ he said. ‘Utilitarianism, whose hard side is industrialism, and whose soft and squishy side is hedonism, considers first what a thing can be used for, and not what it is. The industrialist who doesn’t care if he leaves the land a foul and hideous place is like the pornographer or the hedonist who looks upon the human body as an assemblage of parts for the procuring of pleasure.’
This is the kind of language we need now – not the spigot of data that turns extraction and destruction into sterile figures that camouflage our culpability. The words of past Christian leaders – Pope Francis and ‘ los descartados ’, Bartholomew I and ‘ecological sin’ – can be an antidote to the utilitarianism Esolen laments, which is currently the chief lens through which our transactional society views creation. Inspired by the ‘justice and charity of Jesus Christ’, as well as the long lineage of believers who strove to personify them, the Church can hold its followers to a higher standard than capitalism, market logic, or any of the other contemporary gods who lord over us today."
The word ‘religion’ resists definition but remains necessary | Aeon Essays,,https://aeon.co/essays/the-word-religion-resists-definition-but-remains-necessary,"Listen to this essay
We tend to think of religion as an age-old feature of human existence. So it can be startling to learn that the very concept dates to the early modern era. Yes, you find gods, temples, sacrifices and rituals in the ancient Mediterranean, classical China, pre-Columbian Mesoamerica. What you don’t find is a term that quite maps onto ‘religion’.
What about the Romans, to whom we owe the word? Their notion of religio once meant something like scruples or exactingness, and then came to refer, among other things, to a scrupulous observance of rules or prohibitions, extending to worship practices. It was about doing the right thing in the right way. The Romans had other terms as well for customs, rites, obligations, reverence and social protocols, including cultus, ritus and superstitio. Yet they weren’t cordoned off into a realm that was separate from the workaday activities of public life, civic duty and family proprieties. What the Romans encountered abroad were, in their eyes, more or less eccentric versions of cultic life, rather than alien ‘religions’, in our sense. It was assumed that other localities would have other divinities; in times of war, you might even summon them, via evocatio , to try to get them to switch sides. But the local gods and rites of foreigners could be assessed without categorising them as instances of a single universal genus.
Even after the empire became officially Christian, you still don’t get our sense of ‘religions’. The Romans don’t start sorting the world into bounded systems analogous to ‘Christianity’, ‘Judaism’, ‘Manichaeism’, ‘Islam’ and so on. They have other, older sorting mechanisms, as Brent Nongbri elaborates in his terrific study Before Religion (2013). When Lactantius, in the 4th century, contrasts vera religio with falsae religiones , he means to distinguish right worship from wrong worship; he isn’t identifying other self-contained systems that might be lined up on a chart for comparison. The Christians of late antiquity didn’t view themselves as possessing one religion among many; they viewed themselves as possessing the truth.
The 1709 edition of De veritate religionis Christianae (1627) by Hugo Grotius. Courtesy the Internet Archive
To arrive at the modern category of religion, scholars now tend to think, you needed a complementary ‘secular’ sphere: a sphere that wasn’t , well, religious. That’s why the word’s modern, comparative sense wasn’t firmly established until the 17th century – Hugo Grotius’s De veritate religionis Christianae (1627) is one touchstone – at a time when European Christendom was both splintering and confronting unfamiliar worlds through exploration and conquest. Even as religion could be conceived as a special domain that might be isolated from law and politics, the traffic with ancient and non-European cultures forced reflection on what counted as ‘true religion’. It’s just that, when Europeans looked at India, Africa, China or the ancient Mediterranean, they sifted for Christian-like (and often Protestant-like) elements: a sacred text to anchor authority, a prophetic founder to narrate origins, a set of theological doctrines to sort out orthodoxy and heresy, and perhaps duties that offered a path to salvation. If a tradition didn’t provide these, scholars might helpfully supply them. In time, ‘world religions’ could be conjured up as bounded systems with creeds and essences, even when the local practices they subsumed were profoundly heterogeneous. Traditions with no founders were given founders; traditions with no single scripture were assigned canonical texts; diverse local rites were bundled into overarching systems.
As world religions took hold as a subject of academic study in the later 19th century, European scholars did their systematic best to treat disparate systems of practice and thought as members of a class. Buddhism became one test case. To call it a single ‘religion’, scholars first had to unify various practices of South, Central and East Asia, and then to decide whether a sometimes godless tradition could qualify. Such struggles over classification exposed a deeper uncertainty: how was ‘religion’ to be defined?
T he great minds of the era had ideas. John Stuart Mill held that a religion must unite creed, sentiment and moral authority. Herbert Spencer thought that what religions shared was ‘the tacit conviction that the existence of the world with all it contains and all which surrounds it, is a mystery ever pressing for interpretation.’ The anthropologist Edward B Tylor proposed, as a minimum definition, ‘belief in spiritual beings’. The philologist Max Müller called religion a ‘mental faculty’, separate from ‘sense and reason’, by which humans apprehend the Infinite. For the Old Testament scholar and Orientalist William Robertson Smith, the true foundation of religious life was ritual – the binding force of collective acts. The sociologist Émile Durkheim’s own definition, in his classic The Elementary Forms of Religious Life (1912), joined belief to behaviour and belonging: religion, he wrote, was ‘a unified system of beliefs and practices relative to sacred things’ that united its adherents ‘into one moral community, called the Church’.
These definitions came up short because they excluded too much or included too much. Either they failed to net the fish you were after or they netted too much bycatch. Mill wanted creed, emotion and moral suasion in one package, but many traditions that Europeans encountered in the 19th century didn’t distribute those elements in anything like that pattern. Did a religion involve a metaphysical stance on the cosmos and our place within it – was it driven by the ever-pressing ontological mysteries that Spencer considered central? What we’d call ancient Judaism had very little of that; the biblical writers do not stand before the universe feeling compelled to develop a worldview; they stand within a covenantal drama, entwining law, story and communal identity. And then Müller’s definition could apply to a Romantic poet. (Wilhelm Müller, Max’s father, was a great one.) Dubious of belief-based accounts like Tylor’s, Robertson Smith had concluded that ‘the antique religions had for the most part no creed; they consisted entirely of institutions and practices,’ and ‘while the practice was rigorously fixed, the meaning attached to it was extremely vague.’ Robertson Smith’s own corrective faltered in the face of practices that were communal but not in any obvious way ‘sacred’, or traditions in which doctrine mattered intensely. Durkheim’s formula fatefully relied on a sharp division between sacred and profane that countless ethnographies would undermine.
Georg Simmel , writing around the turn of the 20th century, had already dismissed the ‘Open Sesame’ dream that a single word could unlock the mystery: ‘No light will ever be cast in the sibyllic twilight that, for us, surrounds the origin and nature of religion as long as we insist on approaching it as a single problem requiring only a single word for its solution.’ A few years later, William James complained about ‘verbal’ disputation, but then fell back on a recognisably Protestant formula, defining religion as ‘the feelings, acts, and experiences of individual men in their solitude, so far as they apprehend themselves to stand in relation to whatever they may consider the divine.’ The linguist Jane Ellen Harrison, in her study Themis (1912), refused to define religion at all: a definition, she said, ‘desiccates its object’.
In ‘traditional religions’, there’s a continuity between what we’d distinguish as the natural and the supernatural realm
In the decades that followed, followers of Durkheim foregrounded function, treating religion as a mechanism that bound together societies, comforted individuals, marked transitions, legitimised power. But saying what religion does wouldn’t necessarily tell you what religion was , and, anyway, these functions weren’t peculiar to religion. Clifford Geertz’s elegant formula from the 1960s cast religion as a ‘system of symbols’, one that establishes ‘powerful, pervasive, and long-lasting moods and motivations’. Yet this formula likewise went too big, opening the door to all sorts of political ideologies.
Evolutionary and cognitive theorists since have offered definitions of their own. The evolutionary psychologist Robin Dunbar, for instance, suggested that religion may amount to ‘belief in some kind of transcendental world … inhabited by spirit beings or forces (that may or may not take an interest in and influence the physical world …).’ Inevitably, these belief-oriented accounts run into the same complaints that earlier doxastic definitions had: they seem awfully Protestant, privileging inner conviction over outward form. Even if you bought into the ‘belief’ part, though, you could baulk at the ‘transcendental’ part. In many ‘traditional religions’, there’s a deep continuity between what we’d distinguish as the natural and the supernatural realm. In the Akan region of Ghana where I spent much of my childhood, people would appease or reproach their ancestors in the same spirit that they might wheedle or berate someone at a municipal office. As the anthropologist Robin Horton observed, so-called traditional religions are less like the Western notion of religion than they are like science: they aim at explanation, prediction and control. True, where science posited impersonal forces, traditional thought posited personal ones. But the underlying move from observed regularities to theoretical constructs was similar; what Europeans wanted to call religion was a pragmatic explanatory framework, reasonable given the available evidence, and part of the same conceptual space as folk biology, folk psychology and everyday causal reasoning.
By the late 20th century, hopes for a definition had faded. Some theorists turned to Ludwig Wittgenstein ’s notion of ‘family resemblance’. The thought is that traditions can belong to the same conceptual family because they overlap in crisscrossing ways – like cousins who share a nose here, a chin there, without any feature that they all have in common. It’s a permissive approach: you map the ripple of resemblances and give up on strict boundaries. Unfortunately, those resemblances always depend on what you pick as your prototype. If you start with Protestant Christianity, you’ll find resemblances that matter to Protestants; begin instead with Yoruba orisha devotion, and you’ll trace a very different set of likenesses.
The anthropologist Talal Asad influentially and illuminatingly traced both ‘religion’ and ‘the secular’ to the political and intellectual habits of Western modernity. Yet in his account, religion sometimes seems more an effect of those forces than a cause, more a product of power rather than a power in itself. And even if you think that the phenomena we cluster under the term have been sorted and named by Western modernity, you could wonder how we could be sure that they’re examples of the same thing.
Was the category beyond redemption? The scholar and minister Wilfred Cantwell Smith, whose book The Meaning and End of Religion (1962) had meticulously detailed the belated emergence of the ‘religion’ concept in Europe, long maintained that talk of ‘religion’ conflated too many things not to cause mischief, and urged that we give up such talk altogether; we should, instead, speak of faith and ‘cumulative tradition’. The anthropologist and historian Daniel Dubuisson, who anathematised ‘religion’ as a 19th-century Western imposition on non-Western worlds, urged that it be replaced with ‘cosmographic formation’. These evasive manoeuvres, in turn, have met with scepticism. As the social theorist Martin Riesebrodt drily observed, neologisms like Dubuisson’s could doubtless be shown to ‘have also been “constructed” through historically specific discourses’ and revealed as ‘instruments in the linguistic battle between classes or cultures.’ Besides, he pointed out, those who would eliminate the term ‘religion’ seldom manage long without it.
So how has ‘religion’, as a concept and category, endured in the absence of a stable definition? To answer that question, it may help to think about how referring expressions do their referring. Some terms keep their grip on the world even as our understanding of what they denote changes radically; others, once central to serious thought, fall away when their supposed referents are deemed illusions. What distinguishes the survivors from the casualties?
T hink about our names for ‘natural kinds’. These are meant to pick out groupings that are found not just in our heads but in nature: bosons, barium, bonobos, beech trees. The things these names designate are thought to have causal powers, explanatory roles or underlying properties that justify treating them as more than convenient fictions. When we name a natural kind, what we’re naming is really out there in the world. Anyway, that’s the aim. How do we decide when we’ve got it right?
Start with chemistry, and the question of what counts as an acid. When the term was first used, it referred simply to substances that tasted sour, or acidus . Later they were marked out by what they did: etching metal, losing their bite in contact with alkalis. In 1777, the French chemist Antoine Lavoisier was convinced that acidity came from a common ingredient he called oxygen – oxygène , the ‘acid-producer’. He was wrong. Yet we’d say that when Lavoisier spoke of acids, he was referring to the same class of things we mean by the word.
A century on, chemists refined the concept. Svante Arrhenius defined acids by their propensity to dissociate in water and release hydrogen ions; in 1923, Johannes Nicolaus Brønsted and Thomas Martin Lowry each reconceived them as proton donors; Gilbert Lewis broadened the net again by calling acids electron-pair acceptors. Each shift expanded the boundaries, but none made the term obsolete. The word survived because its targets – the substances doing the dissolving and reacting – were real enough to anchor it even as its theoretical profile changed.
It’s the difference between a bad map of a real country and a map of Atlantis. Only the first can be fixed
Not every scientific term has been so lucky. In 1774, Joseph Priestley isolated a gas he took to be ‘dephlogisticated air’. Phlogiston was supposed to be a substance released during combustion, the invisible essence of burning. What he had actually found, we’d say, was what we know as oxygen, the name derived from that discarded theory of Lavoisier’s. Unlike oxygen, nothing in the world behaved as phlogiston was said to behave. Indeed, it was Lavoisier who brought the curtain down on phlogiston; closed-system experiments, which he conducted with his wife and lab assistant Marie-Anne Paulze Lavoisier, showed that combustion involved the gain of a component of air (namely, oxygen) rather than the loss of an invisible essence. The phlogiston concept evaporated because chemists came to see that it referred to nothing at all. Priestley’s ‘dephlogisticated air’, by contrast, referred successfully despite being misdescribed: his experiments had latched on to a real thing, even if his theory of it was wrong.
This difference between a term that refers despite error and one that refers to nothing is the difference between a bad map of a real country and a map of Atlantis. Only the first can be fixed. Philosophers have used such cases to argue that successful reference doesn’t depend on getting the description right. What matters is the causal connection between our words and the things they’re meant to denote. The strategy is straightforward enough: if you want to know what object a word refers to, find the thing that gives the best causal explanation of the central features of uses of that word. The features that drove Lavoisier’s acid-talk were produced by substances we still recognise as acids, which is why we don’t treat him as having been talking about some other thing, or about nothing at all. Causal theories of reference explain why our words can target the same class of object even when our conception of it shifts, and when the boundaries of the class shift, too. Pluto can stop being a planet without shaking the foundations of ‘planet’ talk. In such theories of reference, a word continues to refer, so long as it stands in the right causal relation to the entity that gives rise to its use. Misdescribed objects can survive conceptual upheavals; nonexistent ones can’t.
Even in the natural sciences, though, classes of things can fall between those stools. ‘Luminiferous ether’ is a case in point: an invisible medium once thought to carry light waves, it was indispensable to 19th-century physics yet eventually dissolved into what came to be called electromagnetic fields. Was ‘ether’ simply a phantasm? Some philosophers think we could well have retained the term, redefining it to mean the very fields that replaced it. Albert Einstein himself, who once helped kill the ether idea, later repurposed the term as the relativistic ether of spacetime, a field with its own geometry. Other theorists suspect that our ‘electromagnetic fields’ may eventually go the way of ether.
If there can be uncertainty about objects within the natural sciences, the wicket gets stickier when we move into the historical and social realm. Here the things we name – revolutions, nations, money, marriage, religion – are doubly human products, being products first of our collective activity, then of our collective description. These entities are what the philosopher Sally Haslanger would call ‘socially founded’ (a term she uses to sidestep the confusions associated with ‘socially constructed’). Many philosophers of language now call such entities social kinds .
T o approach religion as a social kind isn’t to say that it’s as referentially sound as other familiar examples of this sort. Religion may, in fact, be in worse shape than most. It belongs to that subcategory of social kinds that living people apply to themselves. Some social kinds, like ‘recession’, can be defined externally, without the participation of those they describe. Economists can declare one to have happened in the 1870s, even if no one at the time felt it by that name. Others, like ‘wedding’, depend on shared recognition: you cannot hold one without a community that believes in weddings. ‘Religion’, like many social kinds, functions in both ways. Anthropologists can use the term to describe practices that their participants would never call religions, yet, once the label circulates, it acquires a reflexive power: believers come to organise their self-understanding around it. In this respect, religion is a product of classification that helps to shape the reality it describes.
The philosopher Ian Hacking captured this feedback loop with his idea of dynamic nominalism – the process by which classifications and people classified reshape one another. Categories create kinds. The heavy drinker is seen, and sees himself, as an alcoholic. The word doesn’t merely label the phenomenon – it helps to constitute it. Hacking later preferred to call this ‘dialectical realism’, on the grounds that what emerges from the loop (labels affecting those labelled, which then affects the label) is, by any reasonable measure, real enough. When you’ve been told that what you have is a religion, what’s affected isn’t just how you relate to it but what you think you are.
If ‘religion’ endures, it’s because the word still does work, practical and theoretical
Where does this leave someone trying to understand human life through such refractory terms? We might concede that ‘religion’ resists a unitary meaning and proceed case by case, choosing the angle that best reveals what we need to make visible. When speaking of the Abrahamic faiths, a practice-centred approach may capture the lived textures of ritual and observance. The propositions of the Nicene or the Athanasian Creed are, after all, obscure and arguably incoherent, but the act of avowing them carries weighty significance. When we’re turning to the ‘traditional’ thought of the Azande, the Nuer or the Asante, by contrast, a belief-centred, even neo-Tylorian, lens may illuminate elements that the modern Christian model hides from view. Each emphasis is bound to clarify something that the other leaves obscure.
The larger truth is that we’ve always navigated the world with models that merely approximate it, with varying degrees of adequacy. As Hans Vaihinger argued in The Philosophy of ‘As If’ (1911) , we often reason through fictions we judge ‘true enough’, because making use of them helps us act, anticipate and understand. The map may not be the territory, but we’d be lost without it. And the sciences, social and natural alike, advance through such tolerable falsehoods. Their worth lies in the utility of their results.
If ‘religion’ endures, it’s because the word still does work, practical and theoretical. It orders law and policy, directs research, and shapes the inner lives of those who use it. Sociologists can enquire into its relation to charity or suicide; psychologists can study its connection to prejudice or wellbeing. In the United States, legislators and judges must have a sufficient grasp of the category that they can balance the Constitutional dos and don’ts of ‘accommodation’ and ‘non-establishment’. For the religionist, meanwhile, it continues to name a space where meaning is made, defended or denied. Whatever else it may be, ‘religion’ remains a category with too many stakeholders to be fired by fiat. When it comes to what the word means, no one gets to say, and everyone gets a say.
Of course, scholarship itself requires observance – with respect to its own standards of evidence, and on the discipline of paying attention. To be observant, in this sense, is to watch the world closely without pretending to stand outside it. And so we try to use our terms with care, aware of what they can hide from sight and of how much they still let us see. We begin where we are, with the tools our history leaves us, and we make do, even if we suspect that our models may someday be replaced. For now, religion endures as a shared act of attention: one of those serviceable maps by which we try to find our bearings, and to keep faith with the world."
What the Moon meant to medieval Christian and Islamic authors | Aeon Essays,,https://aeon.co/essays/what-the-moon-meant-to-medieval-christian-and-islamic-authors,"Listen to this essay
With its enchanting glow and its mysterious darkness, the Moon has been a deep and abiding symbol for many people from around the world. This was as true for the medieval period as it is today. As they gazed on or imagined the Moon, many medieval creators and audiences were mesmerised by its beautiful and enigmatic nature, and they wondered what the Moon might represent.
The Madonna of Humility ( c 1390) by Lippo di Dalmasio. Courtesy the National Gallery , London
Given the Moon’s strange and ethereal nature, it is not surprising that it features heavily in religious symbolism across a range of traditions. What is surprising is the way it was used, in very versatile and at times radical ways. Despite its association with inconstancy and fragility across a range of cultures, we also find it used to convey immense power. Here I want to focus on the Moon’s symbolic potency in two very different medieval religious traditions, medieval Christianity and medieval Islam.
F irst, a note on definitions. I use the term ‘medieval’ to cover the years from c 500 CE to c 1500 CE, but the term can be problematic. For example, ‘medieval’ is itself a Western word that we should apply to non-Western cultures with some caution. What might constitute the ‘medieval’ period also varies from culture to culture.
In addition, we could use a reference book like Signs and Symbols: An Illustrated Guide to Their Origins and Meanings (2008; 2019), and say that a symbol ‘is a visual image or sign representing an idea – a deeper indicator of a universal truth.’ In the medieval world just as today, the Moon could stand for much more than itself: it could be used to signify complex ideas or teachings, or to indicate momentous historical events. There is a long history of treating the Moon, and the other celestial bodies, in a ‘semiological’ way – as signs rather than causes of events on Earth, though the Moon was often believed to be a cause of events too. In the ‘semiological’ approach, the Moon becomes a text to be read and deciphered. It holds profound meaning, and it is for human beings to interpret it.
We see Earth below Christ’s feet, and above him the Sun and the Moon, showing Christ’s command over Creation
At the simplest and most straightforward level, the Moon emerges in art and poetry as a way of indicating the workings of the Divine within Creation. For example, the Moon appears in scenes representing vital stories in Christian history, emitting its glow at crucial moments of the Christian salvation narrative. The Moon features in many scenes of the Passion of Christ and of the Last Judgement, where it accompanies the Sun. Here, Moon and Sun together serve to show the deep and reverberating cosmic significance of these moments in Christian history. Examples of such illustrations include the Crucifixion scenes in the Book of Pericopes (a book of biblical passages) and the Sacramentary (a book of liturgical excerpts) for King Henry II in the 12th century.
The ceiling of the chancel of St Mary’s Church in Kempley, Gloucestershire, England. Courtesy English Heritage
When it comes to Last Judgement scenes, one powerful example is found in a wall painting in the chancel of St Mary’s Church in Kempley in Gloucestershire, England, which dates back to c 1120. In this image, Christ is enclosed in a lobed mandorla – an oval-shaped aureola – holding a book or tablet with the Greek monogram initials IHC and XPS , indicating his name. We see Earth below Christ’s feet (echoing Matthew 5:35: ‘Nor by the earth, for it is his footstool’), and above him radiate the Sun and the Moon, showing Christ’s command over the entirety of Creation. In this example as in many other Last Judgement scenes, the Moon is part of a deep symbology that represents the fullness of Creation answering to Divine power.
A medieval depiction of Doom in the chancel arch of St Mary the Virgin at Great Shelford, Cambridgeshire, England. Christ is shown sitting in judgement, to his left is the Sun and to his right the Moon. Courtesy St Mary the Virgin Church
A nd yet, the Moon did not simply appear in Christian iconographic scenes to show Divinity within Creation. Often, it was also a symbol with political import, used to convey hegemonic institutions and, in doing so, to establish complex power relations.
The Moon emerges in the concept of ‘hierocracy’: the supremacy of the Pope over the emperor. In this symbolism, whereas the papacy is the light-giving Sun, the state is only the Moon, merely reflecting the Sun’s glowing light. In a letter to the prefect Acerbius and the nobles of Tuscany (dated to 1198), Pope Innocent III writes:
The Moon, then, could also function as a symbol that provided a powerful and memorable image in political and legal arguments about the superiority of the Pope.
The Moon’s supposed inferiority to the Sun conveys the relative importance of a human institution
But, rather than the papacy being represented by the Sun and royal power by the Moon, it was in fact much more common for the Church itself to be symbolised by the Moon. This occurs in the Glossa Ordinaria (‘Ordinary Gloss’), for example, a large compendium of biblical interpretations, and in works by many other ancient and medieval authors. These authors include Ambrose of Milan ( c 339-397), Augustine of Hippo (354-430), Isidore of Seville ( c 560-636), the Venerable Bede (673-735), Rabanus Maurus ( c 784-856), Bernard of Clairvaux (1090-1153) and Peter Lombard ( c 1100-1160), among others.
The Church Father Augustine of Hippo puts forward two possible ways in which the Church can appropriately be understood as the Moon, depending on whether people believed the Moon generated its own light, or whether they believed it reflected the light of the Sun. If the Moon generates its own light, says Augustine, then like the Church it has both a ‘light’ side and a ‘dark’ side: the Church has a spiritual (‘light’) side and a carnal (‘dark’) side. In the Patrologia Latina , a 19th-century collection of the writings of the Church Fathers, we read that if the Moon reflects only the light of the Sun, then it is like the Church because the latter is illuminated by Christ, the one true Sun. In accordance with this second reading, the Venerable Bede, for example, says that Christ ‘illuminates the Church, just as the Moon is said to receive light from the Sun’. (Translation my own).
So, in these examples, the Moon does not show Divine creativity only within the cosmos. Instead, it has an urgent and very Earthly significance, used to demonstrate and navigate intricate power relations: on the one hand, between papacy and state; and on the other, between the Church and the Divine. In both cases, the Moon’s supposed inferiority to the Sun conveys the relative importance of a human institution.
Further to this, the Moon is also compared with the Sun in a very different and pressing way: to navigate the distinction between Christ’s humanity and divinity. In an English sermon for the Sunday before Lent (‘Quinquagesima Sunday’), associated with the Wycliffites, or the followers of John Wycliffe ( c 1328-1384), the Moon represents Christ’s humanity:
This sermon appeals to two meanings of ‘Jericho’ in Hebrew: fragrance, and the Moon. Following the work of the ancient scientist Ptolemy ( c 100 to 160s-170s CE), medieval astronomers understood the Moon to be a planet: in ascending order from Earth, the planets were the Moon, Mercury, Venus, the Sun, Mars, Jupiter and Saturn. A distinction is made here between the Moon of Christ’s humanity, and the Sun of his Divinity – reminding us of the Church-Moon/Divine-Sun distinction we witnessed earlier. But the Moon is still invoked to represent something of tremendous significance: the very humanity of Christ himself.
W ith its centrality to power relations, the Moon has shown itself to be a compelling symbol – and potentially even a dangerous one, when taken to assert the authority of one institution over another. As we turn to Islam, we find the Moon associated intimately with the Prophet Muhammad and even God himself. In this association, the Moon is linked both to Allah and to God’s dignitaries on Earth, and the central role these latter individuals play in propagating religious belief.
A depiction of the Moon from a 16th-century Falnama, a Persian book of prophecies. Courtesy the Wereld Museum , Leiden
The Moon appears in the writings of medieval Islamic ‘mystics’, or Sufis (though there are difficulties with defining Sufism as the ‘mystical’ branch of Islam, as Lloyd Ridgeon writes on medieval Sufism). One medieval Sufi poet was ʿAṭṭār of Nishapur (1145-1221), who wrote many poems in Persian. His works include his allegorical poem The Conference of the Birds , in which the birds of the world seek their king, the legendary Simorgh; it is an allegory of the soul’s quest to find and become one with the Divine. For ʿAṭṭār, the Moon itself is part of a cosmos that is in a state of profound yearning for the Divine. Such is the Moon’s desperate personified longing that it even changes shape – moving through gibbous and crescent forms – in its love. In ʿAṭṭār’s image, humanity becomes associated with the Moon, the entire cosmos caught in this inescapable desire to draw closer to the Divine.
But a particularly important mortal is associated with the Moon: the Prophet Muhammad himself. In the most direct terms, he is said to be like the Moon: in The Conference of the Birds , ʿAṭṭār likens the Prophet to the Moon in beauty and splendour. Beyond this, in Islamic tradition the Prophet himself caused a potent miracle of the Moon. Islam has an entire surah (Chapter 54) of the Quran called ‘Al-Qamar’ (‘The Moon’). This surah begins by detailing the miracle involving the Moon: the Prophet Muhammad performed a miracle that caused the Moon to split into two. This miracle is known as shaqq al-Qamar (the splitting of the Moon). The surah opens: ‘The Hour draws near; the Moon is split. Yet whenever the disbelievers see a sign, they turn away and say: “Same old sorcery!”’
The Moon becomes about a prophet’s great power as the very messenger of God
Surah 54 is one of the ‘Meccan surahs’ (an earlier surah revealed before Muhammad and his followers made the move to Medina). After its opening lines, which convey the miracle of the splitting of the Moon, the surah then focuses on disbelievers and the punishments they consequently suffered. Examples include Noah’s Ark during the Great Flood, the Ádites who rejected their prophet, the Thamud who rejected their prophet, the Sodomites who rejected Lot, and the Pharaoh and his people who rejected Moses. The initial lines of the surah, on the Moon miracle, form the basis of this meditation on disbelief and its terrible outcomes.
Muhammad splits the Moon. An illustration taken from a 16th-century Falnama. Courtesy SLUB Dresden and Wikipedia
The miracle of the splitting of the Moon has been interpreted in a range of ways. One is the historical reading: that the Prophet Muhammad performed this miracle but the people of Mecca still disbelieved him. Other readings are more eschatological in focus: these readings see the splitting of the Moon as a sign of the Last Judgement. In either case, the Moon becomes about a prophet’s great power as the very messenger of God, in Muhammad’s case as the seal of all prophets ( khātim an-nabīyīn or khātim al-anbiyā ), a phrase used in the Quran.
Much of this exegesis dates from the medieval period. The Egyptian scholars Jalal al-Dīn al-Mahalli (1389-1459) and Jalal al-Dīn al-Suyuti (1445-1505) interpret the opening of Surah 54 as follows: ‘The Hour has approached means the Day of Judgement [ al-Qiyāma ] is near. The Moon has split means that it has split into two halves.’ Both authors also say that the ones who reject the sign are ‘the unbelievers of the Quraysh’ ( kuffār Quraysh ), referring to the Quraysh tribe who lived in Mecca before the rise of Islam. Al-Mahalli and al-Suyuti show evidence of both the historical and the eschatological readings in their exegesis, but in each case emphasise the Moon’s role in communicating prophetic wisdom.
The 15th-century poet Abd ar-Rahman Jami interprets the miracle in a different and more playful way. He employs gematria (the attribution of numerical values to letters) to decode the miracle. The full Moon is the circular letter mīm (م), which has the value of 40. In the split Moon, each half becomes an Arabic letter nūn (ن), which has the value of 50. As such, the Moon increases in value through Muhammad. The real Moon, split through miracle, becomes symbolically more valuable through the miraculous act.
I n its association with the Prophet Muhammad, the Moon proves itself to be an all-important symbol, showing the problems, from an Islamic perspective, of disbelief in God’s prophets. The Moon reveals the immensity of Divine power enacted through the prophets themselves. Yet, the Moon can be even more powerful – reaching in fact the greatest position of authority in Islamic contexts.
Even if the Divine remains the ‘Sun’ for Christian authors, we find in Islam an altogether different story. Among Sufi poets, the Moon actually stands in for the Divine himself. The Persian poet Jalāl al-Dīn Muhammad Rūmī (1207-1273) identifies the Moon with the Divine. In one of Rūmī’s poems, the speaker talks about seeing moonlight on a wall. But, the speaker says, we should not then worship the wall simply because of the light we see there. Instead, we should look to the true source of the light: the Moon itself. Thus, humans should not become distracted by the mere reflections of God and end up worshipping Creation itself. Rather than worshipping Creation, which is formed of reflections of the Divine, we should look to the true Illuminator.
Just like this Moon, the Divine does not enter into typical linear patterns of time or into human cognitive frameworks
The Andalusian Sufis Abu al-Ḥasan al-Shushtarī (1212-1269) and Muḥyiddin Ibn ʿArabī (1165-1240) also identify the Divine with the Moon. In two of al-Shushtarī’s poems in the muwashshaḥ form (a type of multilingual strophic poetry developed in Al-Andalus, the Islamic Iberian Peninsula), the speaker says:
The meaning here is that the Divine is the best of all Moons – and if the Divine were present with the devotee, the devotee would not need to focus on the Moon of the night.
Both like and unlike al-Shushtarī, Ibn ʿArabī imagines a 14-year-old girl as the Moon (in turn representing the Divine): the number 14 is significant because the full Moon appears on the 14th day of the month according to the Islamic lunar calendar. This girl who is a Moon does not change shape or move through the signs of the zodiac. Just like this Moon, the Divine does not enter into typical linear patterns of time or into human cognitive frameworks. The Divine – like this special Moon being – surpasses all such limitations.
The Mond Crucfixtion ( c 1502) by Raphael. Note the Sun and the Moon. Courtesy The National Gallery , London
With its enticing shimmers and shadows, the Moon inspired many a medieval viewer, listener, writer and artist. As they looked to the Moon’s incandescence and its moments of darkness, medieval Christians and medieval Muslims were driven to see the Moon as a symbol in a range of religious teachings. This is not surprising. But what is surprising is the complexity and power of the Moon as a symbol, from simply playing an iconographic role to actually representing the Divine himself, in all his refulgent glory. What we see is the Moon taking on a range of roles and meanings, not unlike its shifting forms in the sky, as it moves through its waxing and waning phases. In its array of glittering forms, the Moon was a deeply meaningful and pressing symbol to both these religious traditions of the medieval world."
How Nazism ended centuries of Catholic-Protestant enmity | Aeon Essays,,https://aeon.co/essays/how-nazism-ended-centuries-of-catholic-protestant-enmity,"In the middle of the 20th century, a prolonged animosity came to an end. For more than four centuries, the enmity between Catholics and Protestants, known to theologians as the two confessions, had been one of the organising principles of European life. But, then, it stopped.
To grasp just how revolutionary this inter-Christian peace was, it’s worth remembering what came before it. Because the mutual hatred between the confessions shaped not only the early modern era, when gruesome acts of violence like St Bartholomew’s Day (1572) and the Thirty Years’ War (1618-48) tore Europe apart. Anti-Catholicism and anti-Protestantism remained powerful forces well into the late 19th and early 20th centuries, and shaped social and political life. The most extreme case was Germany, where the Protestant majority in 1871 unleashed an aggressive campaign of persecution against the Catholic minority. For seven years, state authorities expelled Catholic orders, took over Catholic educational institutions, and censored Catholic publications.
In the Netherlands, Protestant crowds violently attacked Catholic processions; in Austria, a popular movement called ‘Away from Rome’ began a (failed) campaign in 1897 to eradicate Catholicism through mass conversion. Catholics, for their part, were just as hostile to Protestants. In France, Catholic magazines and sermons blamed Protestants for treason, some even called for stripping them of citizenship. Business associations, labour unions and even marching bands were often divided across confessional lines.
Even on an everyday level, it still was common into the 20th century for neighbourhoods, parties and magazines to be strictly Catholic or Protestant. Prominent politicians and lay writers routinely blamed the other confession for backwardness, subversion and sexual perversity. A prominent German historian even claimed, in the 1860s, that Catholics and Protestants were descendants of different races.
But then, by the 1950s, this mutual disdain ended. The two confessions reconciled, lay leaders established joint organisations, and politicians even founded powerful interconfessional political parties. Even Church authorities, who for a while dragged their feet, ultimately came around. The Catholic Church, during the Second Vatican Council, officially declared in 1964 that Protestants were not heretics, but brethren in faith. Only in Northern Ireland did anti-Catholicism and anti-Protestantism remain powerful, a remarkable exception that proved the rule.
How did this shocking change come about? After four centuries of division, why did old animosities die so quickly? It’s easy to presume that this dramatic shift happened after the Second World War and was part of Europe’s broader liberalisation. After the trauma of Nazism and Stalinism, we may think, many Europeans came to appreciate pluralism. Or one might imagine that Catholic-Protestant peace came from the onward march of secularisation. People left the Churches in the 1960s, so they also cared less about old tensions.
But both these assumptions would be wrong. Because the Catholic-Protestant truce in fact began long before the Second World War, in response to the Nazis’ call to end religious discord and to instead forge racial unity. Many Catholic and Protestant thinkers and leaders were deeply impressed by this revolutionary message. Even if they disliked some of Hitler’s ideas, they believed that inter-Christian cooperation opened exciting new possibilities. More than anything, they hoped that unity would allow them to build a European order that was based on inequality. Under the Nazis’ hegemony, Catholic and Protestant leaders hoped to protect the economic hierarchy between workers and employers, and the sexual disparity between men and women. In its origins, that is, the peace between Catholics and Protestants entailed not just new tolerance, but also protection of harsh exclusion. And after the Second World War, this fact turned out to be hugely consequential, when Catholics and Protestants came to power and helped build a deeply unequal Europe.
I f anti-Catholicism and anti-Protestantism survived for so long, it was thanks to their stunning adaptability. In the late 19th and early 20th centuries, many Europeans believed that the two confessions explained the making of the modern world, such as the spread of capitalism, the rise of the urban working class, or the genesis of feminism. It was very common to believe that understanding the basic features of modern society depended on specifically Catholic or acutely Protestant contributions to society.
An illuminating example is the Belgian Protestant economist Émile de Laveleye, who was one of the most popular social theorists of the late 19th century. In multiple books (one of which sold 2 million copies), Laveleye claimed that the dynamism and growth of the modern capitalist economy was the gift of the Reformation. Luther and his followers encouraged believers to read the Bible by themselves, which fostered intellectual ingenuity and a drive for self-improvement. Equally important, Luther taught that different classes had different callings. This made it possible for workers and employers to live with each other in harmony, even if that meant accepting major economic inequality between them. Catholicism, however, with its belief in miracles, saints and papal authority, kept people in a state of ignorance and wilful laziness. Ultimately, it degenerated into socialism, which Laveleye explained as a revolt against work and divinely ordained inequalities. Many Protestant ministers and scholars similarly claimed credit for capitalism, most notably the sociologist Max Weber .
European Catholics were just as eager to take credit for modern economic prosperity. The German economist Heinrich Pesch, for example, who was a proud Jesuit, shared the Protestants’ belief that inequality was part of God’s ‘natural’ order. But he warned that it was Protestants who undermined the communal charity at the heart of this order. By claiming that each person could have direct connection to God, they fostered extreme individualism, dismantled communal bonds, and unwittingly facilitated religious apathy. This loss of religious morality, Pesch claimed, led to socialism, with its rejection of religious principles in favour of a purely economic view of people. Catholics, on the other hand, understood that social harmony was premised on charity. As Pesch explained in the influential Liberalism, Socialism, and Christian Social Order (1899), the Church taught that inequality in possession enabled voluntary giving and thus created social virtue. To make this point, Pesch and other Catholic commentators turned to modern statistical evidence. Parish records showed that Catholics donated much more to aid programmes than Protestants.
Catholics turned to statistics to demonstrate that Protestantism encouraged extramarital sex
Matters of sex raised perhaps the most intensive Catholic-Protestant animosities. While Catholic and Protestant elites shared hostility to feminism and sex-reform movements, they also viewed each other as active threats to the sanctity of the patriarchal family. Protestant pamphlets and marriage manuals endlessly warned that Catholic priests were the world’s main source of sexual chaos. Denied, by celibacy, a ‘natural’ outlet for sexual urges, priests sought to undermine existing marriages: they used the institution of the confessional to gain psychological mastery over women. The French-Canadian writer Charles Chiniquy, whose blockbuster The Priest, the Woman, and the Confessional (1874) appeared in nine languages, was one of many to describe Catholicism as a conspiracy to shatter husbands’ authority over their wives. As he warned in a mix of disgust and envy, once a woman confessed to sexual fantasies to her priest, she was forever bound to seek ‘secret orgies’.
Catholics retorted and blamed the Reformation’s endorsement of divorce for the decline of the family, more and more evident in the popularisation of separation and rising out-of-wedlock births. If life-long marriage was no longer sacred, what was to stop men and women from endlessly marrying and divorcing in a search for sexual adventures? Catholics again turned to statistics to demonstrate that Protestantism encouraged extramarital sex. The Belgian journalist Prosper de Haulleville claimed in 1876 that Protestant-majority Prussia’s 39,501 out-of-wedlock births from 2,983,146 women of childbearing age made it a hellscape of immorality. In comparison, Haulleville insisted, Catholic-majority Italy was a haven of chastity.
In sum, Catholic-Protestant stereotyping and animosity remained normal, central frameworks for making sense of modern life on an almost daily basis. Like antisemitism, they were old ideas that consistently acquired new variations and forms that used modern tools like statistics and mass media.
To be sure, such anxieties were never universal, and their intensity varied by time and place. In some communities, lay people ignored most of this and even occasionally married people of the opposite confession. Nevertheless, by the 20th century, Catholic-Protestant hostility was a deep bedrock of European life. Few Europeans could have envisioned a world without this divide.
W hy, then, did so many Europeans abandon these divisions within just a few decades? The answer lies in the 1930s, and especially the rise of Nazism.
The Nazis are generally remembered today for their extreme racism, imperialism and genocidal violence. But during their early years, their message often revolved around economic and gendered themes. And both echoed the concerns of many Christians. In the economic sphere, they promised a crusade against socialism and its more radical version, communism, and called on workers and employers to cooperate with each other in harmonious inequality. In the sphere of gender relations, the Nazis insisted on separation and inequality between the sexes, and used welfare policies to push women out of the workforce, so that they could focus on procreation. Early Nazi publications often explicitly claimed that these ideas overlapped with Christian teachings, and that they were Christianity’s allies.
But Nazi ideology also introduced an important innovation. While this is often forgotten, Nazism also promised to end the confessional divide. In its founding document from 1920, the party declared its support for ‘positive Christianity’, a new and racialised conception of religion that included both Catholics and Protestants. Adolf Hitler himself was quite preoccupied with the confessional division. In several passages of Mein Kampf (1925), as well as in several speeches, he blamed it for Germany’s internal divisions and weakness. The Nazis therefore claimed that a new order required a historic new, inter-Christian cooperation. Christian unity, the end of the centuries-long confessional war, was a necessary preparation for defeating ‘Judeo-Bolshevism’, a goal that Hitler and his acolytes considered inevitable.
As long as Nazism was a fringe movement, Christian elites largely ignored this message. But its rise to power, first in Germany and then, through military conquest, its occupation of Europe, led many to emulate the Nazi call for confessional unity. In 1932, for example, the Catholic writer Robert Grosche launched the first Catholic journal to openly support Nazism. Grosche posited that both the Church and the Nazi movement recognised that God’s grace operated not through individuals but through racial communities. Catholics and Nazis, Grosche maintained, were allies in creating a ‘sacred space’ in which all of society mobilised together towards collective salvation. Grosche also became the most vocal proponent of engagement with Protestants. The two confessions, he mused, were brethren in the ‘community of blood’. The Austrian bishop Alois Hudal published some of the most prominent efforts to square Catholic teachings with Nazi dogma. Like Grosche, he envisioned a joint future: ‘He who … eliminates the religious division,’ he explained, ‘would render the greatest service’ to ‘the German race and Europe’s entire cultural leadership.’
This meant that they could cooperate in restricting procreation to those ‘racially healthy’
Ideas of Nazi-inspired confessional unity were not restricted to theology. They informed popular commentary on bread-and-butter issues, such as economics. The Protestant German economist Georg Wünsch, for instance, had spent the 1920s attacking socialism and Catholicism as detrimental to economic growth, but in the 1930s he changed his tune. When Wünsch in 1936 proclaimed his support for Nazi economic policies, especially public works, he declared that these programmes embodied the values of both confessions. Wünsch thought that Nazi public works would foster harmony between employers and workers while maintaining divinely ordained inequalities (since private property remained protected). In the 1930s, Catholic and Protestant theorists began to criticise earlier stereotypes and insist that both confessions could contribute to the modern economy. One leader of the Inner Mission, Germany’s largest Protestant charity, proclaimed that the ‘violent struggle’ over economic policies was no longer between the confessions but ‘between the Christian confessions and the irreligious’ worldview of socialists and communists.
Across Europe, many Catholics and Protestants also admired the Nazis’ assault on feminism and sexual minorities. And, in turn, this led them to insist on the existence of an interconfessional sexual ethics. The German Protestant journalist Alfred Dedo Müller, for example, who after the war would become a successful politician, rejected earlier anti-Catholic anxieties and explained that the two confessions were actually united in the quest to defend the procreating family. In his eyes, both understood that healthy families were the bedrock of healthy nations, which meant that they could cooperate in restricting procreation to those ‘racially healthy’. Jacques Leclercq, a Catholic family expert from Belgium, was impressed by the Third Reich’s policies that encouraged women to leave the workforce so they could focus on parenting. As he noted with satisfaction in the 1930s, many Protestants approved of those policies, which indicated that they were now Catholics’ allies in the defence of the family.
This enthusiasm meant that suddenly, in the 1930s, interconfessionalism seemed like a solution to so many issues. New ecumenical organisations were founded. Theodor Innitzer, for example, a senior Austrian politician and archbishop of Vienna, launched a charity organisation in 1933 with the Protestant Ewald Ammende. In 1935, a group of scholars, students and Church leaders in Kassel in Germany established a monthly discussion group on the ideological foundations of ‘positive Christianity’. By 1936, similar groups mushroomed in cities like Bielefeld, Berlin, Frankfurt, Hamburg and Mainz. Germany’s wartime collaborators, like France’s Vichy regime, followed this lead. The regime’s leader, Philippe Pétain, discarded the French Right’s historical anti-Protestantism and appointed Protestants to important positions.
The new Christian unity did not mean, however, simply growing toleration or Enlightenment, as it came with intense antisemitism. The Catholic historian Albert Mirgeler, for example, who was among the earliest proponents of cooperation with Protestants, railed in 1933 against Jewish emancipation, which he blamed for Europe’s ‘decomposition’. The French Protestant journalist Noël Vesper, who similarly embraced interconfessional cooperation, in 1938 dedicated entire issues of his journal Sully to antisemitic invectives. For these and many other Europeans, an inclusive Christian order was also meant to bolster the exclusion of non-Christians. As the German theologian Karl Adam explained, confessional peace was a bulwark against the ‘racially foreign’ Jews, who could ‘never be integrated into the Aryan race’.
B y the late 1930s, Christian opponents of Nazis also began to talk about the ‘natural’ alliance between Catholics and Protestants. This was perhaps the clearest sign of interconfessionalism’s triumph. There was no way to ignore it; one could only try to appropriate it.
The French Catholic Yves Congar, now considered one of the 20th century’s most innovative theologians, was perhaps the most prominent anti-Nazi advocate of interconfessionalism. In his book Divided Christendom (1937), which was widely translated and discussed, Congar condemned fascism, and its fixation with biology, as heresy. Christ’s body, he reasoned, included all nations and races, which meant that human diversity was divine. Yet Congar emulated Nazi sympathisers by insisting that this diversity was best realised through a new peace between Catholics and Protestants. Both understood the significance of grace and spirituality’s supremacy. In Congar’s telling, denominational friendship was in fact the key to resisting Nazi ‘totalitarianism’. As he wrote, ‘it is becoming no longer a question of confessional differences within Christendom itself, but of a radical choice between the Kingdom of God and the reign of anti-Christ.’ Only Christian unity could protect the Churches from subjugation to racist ideology.
Like their opponents, anti-fascists quickly sought to explain the practical implications of those lofty ideas. The German Swiss Protestant economist Wilhelm Röpke claimed in 1937 that the Third Reich’s expansive public works and subsidies were alien to Christianity. Like socialism, they sought to subject all human interactions to the state, rather than providing people with the autonomy needed for spirituality. Therefore, an economy in line with Christian principles required state intervention only to secure free competition, such as antimonopoly laws. For these ideas, Germany forced Röpke into exile, where he would later join Friedrich Hayek in founding the Mont Pelerin Society, the first neoliberal think-tank. Röpke also argued that this theory was something shared by Catholics and Protestants. In Civitas Humana (1944), he analysed Catholic publications to claim that antimonopoly laws embodied the Church’s timeless commitment to social justice.
Catholics and Protestants in the Resistance worked together to write anti-Vichy propaganda
The interconfessional perspective also shaped anti-fascist writings about family and sexuality. Wilhelm Stählin, who was the director of the German Protestant youth movement, warned that the Nazi fixation with racial purity stripped marriage and procreation from their spiritual content. Eugenics, in his mind, reduced humans to the level of animals, and had to be rejected by Christians. While Stählin had once insisted that Catholics and Protestants held irreconcilable ideas about the family – in the 1920s, he’d wondered if interconfessional marriage should be barred by law – his campaign against the Nazis entailed a radical new tolerance of Catholics. By the 1930s, he claimed that centuries of disagreement over divorce or celibacy were a mere misunderstanding; the two confessions were united in their commitment to family autonomy beyond the state.
Within a few years, anti-Nazi Christians launched their own organisations. In 1940, the French Protestant pastor Roger Schutz established a new and interconfessional community in the town of Taizé, where members (half Catholic and half Protestant) developed shared prayers and liturgy. In Britain, the Catholic scholar Christopher Dawson joined Anglican leaders in founding the Sword of the Spirit, an association that sought to coordinate charity work across the confessional divide. The most politically important was the new cooperation forged among resistance movements, most notably the Free France movement under Charles de Gaulle. While de Gaulle was raised in an ardently anti-Protestant Catholic milieu (in his youth, he sympathised with calls to ban Protestants from public service), in 1942 he called for cooperation between Catholics and Protestants. For the rest of the war, Catholics and Protestants in the Resistance worked together to write anti-Vichy propaganda, analyse intelligence, and correspond with British government officials.
The rise of Nazism in Europe, in short, dramatically reshaped Christian life. On the one hand, Christian elites across the continent were divided in their approach to the radical Right and its racism. On the other hand, that internal conflict led figures on both sides to seek new allies in the other confession. Both of these alliances, whether sympathetic or hostile to fascism, required enormous intellectual innovation. In both cases, these radical changes did not originate with bishops or popes; it was lay leaders and popular writers who led the way to a new Christian unity.
S ince the new Christian peace was linked to Nazi ideology, one might expect it to die in the ruins of the Third Reich. But the exact opposite happened. The cooperation between Catholics and Protestants only deepened after the Second World War, becoming the mainstream of Christian life. Perhaps most importantly, talk of reconciliation moved from the sphere of ideas and small associations to the world of party politics and state power. Together, Christians were able to leave deep marks on European governance.
The persistence of Christian cooperation was first and foremost the product of shared convictions. Christian elites had long considered socialism and feminism main enemies, and even after the war, both seemed on the march. After all, the Red Army occupied half of Europe, and women everywhere were gaining new rights. But after a decade of talk about unity, the two confessions no longer viewed each other as the source of those ills. Instead, interconfessional work became key to securing the ‘natural’ inequalities between classes and sexes.
It was in this spirit of shared commitment to inequality that a group of German politicians and clergy founded the Christian Democratic Union in 1945, a new political party for both Catholics and Protestants. Under the leadership of Cologne’s former mayor Konrad Adenauer, the party’s anti-Marxism and anti-feminism launched it to leadership in West Germany for two decades. The same happened in France, Belgium and Austria, where Catholic parties reestablished in 1944 and 1945, and became dominant forces in postwar politics. They, too, opened membership to Protestants. In the Netherlands in 1952, the Catholic People’s Party even appointed Protestants to leadership positions, most prominently the minister of foreign affairs, Johan Willem Beyen.
Catholic and Protestant alliances also mobilised to combat the enemies of the patriarchal family
In some ways, the Catholic-Protestant rapprochement in postwar Europe was a continuation of the 1930s, but it also came with an important innovation. With Nazism dead, its former supporters and opponents alike quickly put aside their differences. Interconfessional think-tanks and parties became a central site for former opponents to unite in the struggle against socialism and feminism. The Academy of the Occident, for example, a huge German organisation that brought together politicians, writers and activists in regular conferences, was co-run by the Catholic politician and former Nazi Friedrich August Freiherr von der Heydte alongside the Protestant anti-Nazi Wilhelm Stählin. Christian speeches and publications settled on a convenient fiction that all Christians opposed fascism and that their mutual persecution led to their alliance. As the German bishop Clemens August Graf von Galen put it in an especially outrageous falsehood, ‘we [Christians] suffered more under the Nazis than others.’
The consequences of this internal truce were immediately apparent. Almost everywhere, Christian parties used economic policies to reduce class antagonism while keeping workers and employers economically unequal. Drawing on ideas developed in the 1930s, they offered new welfare programmes, such as public housing and old age pensions, which softened some of the harshest manifestations of inequality. But over the protests of socialists, they also made commitments to keep those programmes small, unequal (their benefits often relied on one’s income) and decentralised.
Catholic and Protestant alliances also mobilised to combat the enemies of the patriarchal family. Women in central and western Europe may have received the right to vote, but they could not work or take on a mortgage without their husband’s permission. Abortion and contraception were repressed, welfare policies discriminated against single mothers, and same-sex relations had to take place underground. Marriage manuals and policy memoranda explained such patriarchal policies as the expression of Catholicism and Protestantism’s shared principles. In 1956, Swiss Christian therapists founded the Interconfessional Christian Institute for Marriage and Family Treatment, whose publications insisted that only women’s obedience to their husband could guarantee ‘healthy’ marriages, ideas that were widely read across Europe.
T he ascendancy of this new, conservative Christian consensus did not last for long. In the 1960s and ’70s, many young Christians joined the New Left protests that rocked Europe and sought to usher in an era of social equality and feminism. Yet even in their blistering attacks on economic and gendered hierarchies, Left-leaning activists and writers remained committed to inter-confessionalism.
The German Protestant writer Dorothee Sölle, for example, in a series of publications, claimed that the core of Jesus’ message was animosity to conformity and inequality. To follow it in the modern world required breaking the ‘brutalisation’ of capitalism, and discarding the Churches’ century-old allergy to socialism. However, Sölle took it for granted that this revolutionary struggle would begin through cooperation with like-minded Catholics. Together with the Catholic novelist Heinrich Böll and multiple other activists, in 1968 she founded the Interconfessional Working Group in Cologne, which mobilised against private property, consumerism and the Vietnam War. Similar grassroots organisations also mushroomed in France, the Netherlands and Switzerland. In a break with the past, many of their leaders also claimed that a ‘revolutionary’ Christianity required atonement for its past antisemitism.
Postwar European Christian Leftists also embraced new positions on the patriarchy. The Dutch Catholic activist Catharina Halkes, for example, dismissed the Churches’ long attachment to the patriarchy as a betrayal of Jesus’ emancipatory message. The goal was to break women’s confinement to marriage and parenthood, and to allow self-realisation through education and work. Many writers and activists who also endorsed these ideas accepted interconfessionalism as the starting point. In the 1970s, Catholics and Protestants launched multiple joint campaigns and organisations that advocated equal pay, abortion rights, and reforming family laws. The same logic powered activists who mobilised against homophobic discrimination. The David and Jonathan association (founded in France in 1972) or the Interconfessional Working Group for Homosexuals in the Church (founded in Germany in 1977) accepted Catholics and Protestants as naturally aligned.
The recent history of Christian politics, then, is one of rupture and changes. It is also one of enormous intellectual creativity: lay economists, politicians and marriage specialists developed new understandings of what it meant to be Christian in the modern world. This process of adaptation sought to offer plausible solutions to practical problems. And throughout, it influenced the lives of millions, including those far away from the Churches’ orbit."
The many lives of Eurasian daimonology | Aeon Essays,,https://aeon.co/essays/the-many-lives-of-eurasian-daimonology,"Back in my student days in late 1970s Paris, one of my favourite walks would take me to the fabulous Père Lachaise cemetery near the eastern edge of the French capital. A miniature city of monumental tombs and crypts in a lush garden setting, it has long been ‘home’ to the remains of no small number of the very special dead: Molière, Oscar Wilde, Colette, Sarah Bernhardt, Chopin … and Jim Morrison. Morrison’s gravesite was always part of my itinerary, because it was a gathering place for a colourful array of people paying their respects with flowers, notes and a few tokes on something vaguely illegal. A stone bust of the Doors’ legendary frontman had already been completely chipped away by the time I started visiting the site, however what remains today is a bronze plaque engraved with his name, his dates (1943-71) and an epitaph in Greek. Kata ton daimona eaytoy may be read in a number of ways, figuratively as ‘true to his own spirit’, but also literally, ‘true to his own daimon ’ or ‘by the favour of his own daimon .’ What or who could have been Jim Morrison’s ‘own daimon ’?
Some 25 centuries before Morrison’s time, Plato’s daimons were something akin to guardian angels, spirits that watched over the living, whom they guided on the path to Hades after their death. Well before Plato’s time, Homer had referred in the Iliad to the Olympian gods themselves as daimons . Also referred to as daimons were the denizens or guardians of prominent and often forbidding features of the natural world – mountaintops, forest groves, caverns and springs – supernatural beings with oracular powers. Quite often, however, the daimons of ancient Greece were dire, hostile, dangerous spirit beings, the evil eye demon ( baskanos daimon ) being an illustrious example.
What these conflicting usages tell us is that the daimons of the ancient world were ambiguous beings, spirits with varying degrees of power that they could employ, or be made to employ, for good or evil ends. This is how they were portrayed in the Christian Bible where, in the Book of Matthew, Jesus admonished his disciples to ‘heal the sick, raise the dead, cleanse the lepers, and cast out demons.’ These daimons (translated as ‘demons’ in the English Bible) were unambiguously noxious, so that, when Jesus exorcised them, their victims were released from their sufferings. Such was the case of Mary Magdalene herself, ‘from whom he had cast out seven demons’. Yet, these same daimons were also cast as spirits capable of recognising and conversing with Jesus: ‘And he healed many who were sick with various diseases, and cast out many demons; and he would not permit the demons to speak, because they knew him.’
T he demons of the Christian Bible were none other than the daimons of paganism demoted, that is, the lesser divinities of the Greco-Roman religion superseded by Christianity after Rome’s conversion to Christianity in the 4th century CE. Thereafter, the triumphant Church would condemn this ancient host, this pandemonium, to the dark side. Fallen angels, forces of evil, agents of temptation, these were now demonised as hell creatures toiling in the service of the prince of darkness: Lucifer, Satan, the Antichrist, the Devil. In the face of these supernatural enemies, the Church would quickly mount an arsenal of countermeasures, and so the applied science of Christian demonology was born. This is not to say that Christianity was the first or the only great religion to forge a demonological lexicon. Centuries, even millennia before the Greeks and Romans, the ancient Egyptians and Babylonians had developed a variety of techniques for combatting malevolent demons. And, in fact, every one of the world’s religions has had a demonological component for battling their inner demons.
Demonology, the ‘science of demons’, has always comprised two complementary facets – the one theoretical and the other practical. If one was to battle one’s enemy effectively, one first had to know him, his human confederates, his disguises, his ruses. I use the singular here, because in many of the world’s religious traditions, the demonic hordes were held in the thrall of a single great embodiment of evil, an arch-rival to a benevolent God or gods. The relationship between the demonic host, the pandemonium, and its master was envisioned in several ways. Quite often, the demons were simply a protean swarm, overwhelming by their sheer numbers, visiting natural disasters and plagues upon the land, and madness, sickness and death upon their human victims.
In some cases, however, the pandemonium was imagined as a hierarchy whose structures mimicked those of human institutions or divine pantheons. For the monks of medieval Catholicism, the organisation of the demonic host replicated its own hierarchy. In the same way that the good angels were ranked according to their stations and functions, so too with the evil spirits: our bishops had their counterparts in their bishops, our abbots in their abbots, our priors in their priors, and so on. Sometimes, the pandemonium was theorised as a military organisation, as for example in a 5th-century Taoist work, the Book of Divine Incantations for Penetrating the Abyss , which imagined battalions of demon armies with a command structure ranging from generals to petty officers, cavaliers, infantrymen, archers, spies and executioners.
Demons inflict much of their evil on the world through humans
For the Zoroastrians of pre-Islamic Persia, each shining angel of ‘truth’ had for its counterpart a darkening demon of ‘the lie’, with the supreme good spirit Ahura Mazda pitted against the arch-demon Ahriman. An early Buddhist demonological treatise, the Teaching of the Great Pea-Hen, the Queen of Spells , organised its demons as if they were members of a noble household: masters and mistresses, sons and daughters, chamberlains, ladies in waiting, and male and female retainers. For well over a millennium, the Hindu pandemonium has been ruled by one or another powerful tantric god or goddess, called a master or mistress of spirit beings. Unless and until these dominant figures are shown respect in the form of offerings of various sorts, they will allow their minions to prey upon a defenceless humanity, infants in particular. Once gratified, however, they become fierce defenders of the same persons they had allowed to be victimised. Similarly, in the Taoist work just cited, demon kings and generals could be coerced by the gods of heaven to purge and destroy the billions of spirits in their own demon host.
Practical or applied demonology, the strategies deployed to counter demons, can be classified under two headings, which we may call the carrot and stick approaches. The first is that just described: prevailing upon a powerful demon to bring to heel his subordinates, the lesser demons of human afflictions. Far more common is the strategy of the stick: full-on combat against demonic possessors or their human agents. This is generally a two-step process, beginning with a trial. Demons inflict much of their evil on the world through humans, whether these be their hapless victims or willing confederates: witches, heretics and foreigners. In all cases, the possessing demon must first be identified. This is the work of the inquisitor-exorcist who, through a mixture of cajoling, coercion and threats, compels the demon to pronounce its name. Then follows the exorcism, a violent procedure (often involving torture, in the case of witches), culminating in the demon’s exit from the body, via the mouth or anus. The drama of possession and exorcism have been portrayed on countless works of art, from medieval Nepali manuscripts to a French Book of Hours.
Demon possession, Nepal, 1540 CE. Courtesy of Wellcome Trust/Wellcome alpha1937
Christ exorcising a demon, from Les Très Riches Heures du duc de Berry , 15th-century France. Courtesy Wikimedia
However, this is not the whole picture, because, generally speaking, it was only these inner daimons of possession that were potentially ‘demonic’. When approached in the proper manner, the custodians of springs and rivers and sacred groves, like Plato’s guardian spirits or the various oracles of the ancient Mediterranean world, have shown themselves to be of a benevolent sort, and so have been sought out by persons in need of their help. Fairies, trolls, elves, nymphs and gnomes are so many ‘land spirits’, daimons of the natural environment. So it was that, when faced by their parishioners’ stubborn recourse to the healing waters of pagan sanctuaries, the same medieval Church whose vocation it had been to combat the demonic hordes was forced to yield to popular custom. Even today, the Mediterranean world is dotted with thousands of pools and springs consecrated to various saints and virgins who are none other than the daimons and fairies of yore, overlaid with the slightest Christian veneer.
Notre-Dame des Douleurs, Garrosse, France. Photo supplied by the author
Fontaine Sainte-Luce, Escource, France. Photo supplied by the author
The same strategies were employed in Hindu and Buddhist South Asia, where the ancient daimon custodians of mountains, forest groves and pools were most often called devatas (‘divinities, daimons’), yakshas (‘dryads’) or rakshasas (‘guardians’). These potentially dangerous spirit beings were often appeased, even domesticated, by worshipping and granting them the status of subordinate protective deities. In Buddhist scripture, this adaptation was often scripted as a ‘conversion experience’. One of the Jatakas , the Buddha’s ‘Birth Stories’, describes just such a transformation on the part of the water guardian of a sylvan pool who had been authorised by the king of the yakshas to pose a series of questions on ‘ yaksha law’ to any person who would come to drink from the pool – and to eat those who could not provide the correct answers! Coming there in disguise, the Buddha answers the water guardian’s riddles and so impresses him that he converts to the saddharma (True Law) of the Buddhist faith.
Many of these South Asian daimons were eventually incorporated into the pantheons of the great religions, in some cases becoming enshrined as powerful saviour figures. A well-known example is that of an arch-demoness named Hariti, the ‘Baby Snatcher’. Notorious for devouring hundreds of children, she is convinced by the Buddha of the folly of her ways, after which she becomes the revered protectress of the same infants she had previously victimised. Images of Hariti, found across the Buddhist world from Inner Asia to Japan and Indonesia, invariably portray her surrounded by babies – held in her arms, clutching at her breast, and playing at her feet. According to a medieval scripture from Hindu India, a yaksha prince named Harikesa (‘Redhead’) underwent a similar transformation. Renouncing his demonic ways to become a devotee of Śiva, he was appointed the leader of the great god’s minions and installed as guardian of his principal temple in the holy city of Varanasi.
Carving of Hariti ( c 824 CE) from Chandi Mendut temple in Borobudur, Java, Indonesia. Photo by Patrick Young and courtesy the ACSAA/University of Michigan
L ike their homologues in the Western world, Asia’s daimons are an ambiguous lot, by turns benign and malign, powerful and vulnerable, innate and remote, earthbound and aerial, inert and evanescent. What is most intriguing is that, across the vast Eurasian expanse, from Iceland to Japan, these daimons seem to resemble one another. These similarities can be attributed to two principal dynamics.
The first and most obvious is that, following the Alexandrian Conquest of the 4th century BCE, the overland Silk Road became an information superhighway for daimonological traditions. Here, the spread of daimonological lore generally occurred independent of any direct influence from any established religion, the reason being that daimons have always travelled more lightly than gods. Here I am not speaking of agency and mobility on the part of the daimons themselves, but rather of the movements and activities of the humans offering or seeking benefits or relief from them. Manipulating or transacting with daimons has never required the support of a sophisticated belief system or priesthood: what is essential is that the techniques employed be effective. Ritual gestures, speech acts without semantic content (ie, spells), mute power substances (crystals, botanicals, animal parts) and man-made devices (amulets, etc) are what human specialists have been offering their clientele for millennia, and the Silk Road market towns were daimonological changing houses where soldiers, sailors, merchants, monks and magicians transacted in specialised services, devices and expertise.
In some cases, we see a daimon’s original name being retained in a transplanted foreign setting, as shown in two texts from the 5th to 6th century CE, from southern China and northern Afghanistan, both of which make mention of South Asia’s rakshasas (‘guardians’). The first, the already mentioned Taoist Book of Divine Incantations , ranks its luoshas (the Chinese transcription of rakshasas ) together with a host of Indigenous Chinese demons. Discovered 4,000 kilometres away, a Manichean amulet carries an inscription that classes rakshasas and yakshas together with the Iranian demons called peris and drujs , as well as ‘idols of darkness and spirits of evil’. These, as the amulet text reads, will be driven away through the power of the Lord Jesus Christ, the angels Michael, Sera’el, Raphael, Gabriel and others.
Female daimons who threaten human trespassers are won over by a hero, to whom they offer their bodies
A principal port for maritime trade between the Mediterranean world and South and East Asia, Mantai on the northwest coast of Sri Lanka was the venue for an astonishing case of daimonological exchange, in this case of an entire mythic complex. It was here that a c 6th-century Buddhist monk named Mahanama authored the Mahavamsa , the island’s ‘Great Chronicle’. Its sixth chapter concerns an Indian prince named Vijaya who, shipwrecked there with his men, sends a scouting party to explore the island’s interior. They first meet the god Vishnu in disguise, who gives them protective threads to wear on their bodies. They then follow a she-dog that leads them to a pond, where they spy a female devata named Kuvanna (‘Ugly’), disguised as a Buddhist nun, spinning thread at the foot of a tree. The men are quickly trapped by her, who intends to eat them, but is unable to do so because of the amulets they are wearing. Then comes Vijaya who, sizing up the situation, threatens to slay the yakshi unless she releases his men. This she does, after which she offers everyone a great feast, and then, assuming the form of a beautiful 16-year-old maiden, takes the prince to her splendid bed. That same night, she instructs Vijaya on how to defeat the yaksha host controlling the island. She would later be slain by the yakshas for her betrayal.
For anyone familiar with Homer’s Odyssey , this episode is transparently identical to that of the encounter between the hero Ulysses and the female daimon Circe, a nymph whose handmaidens are described as ‘children are they of the springs and groves, and of the sacred rivers that flow forth to the sea’. Shipwrecked on her island, Ulysses sends out a party of scouts who come upon Circe’s hilltop hall, encircled by wolves and lions who behave like dogs because of a drug she had given them. Circe is weaving a great tapestry when the men arrive. She offers them hospitality, but the food she gives them contains a drug that transforms them into swine, which she imprisons in her pigsties. Alerted to their plight, Ulysses makes his way there, but en route meets the god Hermes who gives him a counter-poison to Circe’s evil drugs. Ulysses overpowers Circe and threatens to slay her unless she releases his men and restore them to human form. This she does, following which she offers everyone a great feast, and takes Ulysses to her beautiful bed. A year later, when she sends him on his way, Circe offers the hero essential guidance on how to continue his odyssey homeward.
Some 1,300 years and worlds apart, the two stories are virtually identical. Female daimons who first threaten the humans who trespass on their sanctuaries are won over by a hero, to whom they offer their bodies and their mercy. Carried on the winds of trade, a dire yet seductive nymph of ancient Greek epic was transformed, more than 1,000 years later, into a South Asian yakshi .
C arried along these same trade routes, mirror divination is a daimonological technology attested from North Africa to China. First mentioned in a c 3rd-century CE Egyptian manuscript, the practice has always involved a single device and three actors: a human child, a human adult and a daimon. In the role of medium, the child is made to gaze into a reflective surface – a mirror, a bowl of water with oil floating on its surface, the polished blade of a weapon, etc – in which a daimon will appear. The adult at whose knees the child is sitting then utters a spell to bring the daimon into the device. He transmits to the daimon a set of questions about some present or future event, which the daimon answers through the child medium.
This technique spread quickly, appearing in both a Zoroastrian inscription from the 3rd century CE and in Jewish Talmudic sources from Sasanian Persia; in several 7th- to 12th-century Hindu, Buddhist, Jain and Taoist texts from India, China, Japan and Tibet; in the Policraticus (1159) by the English cleric John of Salisbury; and in medieval and modern-day Jewish, Muslim and Ethiopic sources from North Africa. The instructions found in a work titled the ‘Secret Rites’, an early 8th-century Chinese translation of a Sanskrit work, are virtually identical to those given in the 3rd-century Egyptian manuscript:
A most remarkable example of daimonological transmission across the entire Eurasian expanse concerns geothermal eruptions: boiling mineral springs, gas vents, volcanos, petroleum seepage, and the like. So it is that we find a virtually identical set of instructions for the ‘capture’ of mercury in three alchemical works. The c 800- 1000 CE Syriac version of the Treatise of Zosimus of Panopolis provides the following information:
Some 200 to 400 years later, the Sanskrit-language Rasaprakashasudhakara (‘Ambrosial Vessel of the Light of the Essential Elements’) by Yashodhara Bhatta provides similar instructions:
Less than a century later in China, Zhu Derun’s Cun fuzhai wenji (‘Collected Works on Preserving, Restoring, and Purifying’, 1347) relates that:
In all three of these sources, mercury is portrayed as a daimon that rushes out of its natural habitat, its ‘well’, under the impulse of lust or rage, to pursue human trespassers. It is only after it has been neutralised that it becomes an inert mineral, the quicksilver used in alchemical reactions. These accounts are in fact variations on a far more widespread mytheme, which brings us to the second explanation – far more ancient than the first – for the striking similarities between the daimons of Eurasia. This is the concept of monogenesis, of a single common ancestor for an extensive corpus of myths. In this case, that mythology tracks with the languages of the Indo-European language family, whose members range from the ancient Sanskrit, Latin, Greek, Celtic and Slavic to the modern Romance, Germanic and Indic languages.
Moving deeper into Eurasia, they carried with them a ‘proto-Indo-European’ mythology of daimons
As the argument goes, the vocabularies of these member languages are similar because they can all be traced back to an ancestral tongue spoken more than 6,000 years ago by peoples living in the Caucasus region. Then, as these peoples migrated out into the Asian and European continents over the centuries and millennia, they carried with them their ‘proto-Indo-European’ language, which was gradually altered through the influence of the languages of the populations with which they came into contact. This is why, for example, the English word ‘mother’ closely resembles but is not identical to the Mutter of modern German, the mater/meter of Latin and Greek, the matar of Sanskrit and ancient Iranian, the madre of Spanish and Italian, the mathair of Irish, the mati of Serbo-Croatian and so forth.
Languages are vehicles for human thought, culture, imagination and practice, and so it follows that, when the speakers of the ancestral tongue moved deeper and deeper into Eurasia, they also carried with them a ‘proto-Indo-European’ mythology, including a mythology of daimons. A cluster of those myths concern various sorts of geothermal eruptions. Attested in Sanskrit and ancient Iranian sources going back to at least 1500 BCE, and found in ancient, medieval and modern accounts from Rome to Ireland, France, Greece, Turkey, England, Pakistan and Azerbaijan, these share all or most of the same complex of themes:
What do these data tell us? For several thousand years, human actors have been carrying their inner daimons with them as they moved across the Eurasian landmass, recognising the daimons of ancient landscapes as they came upon previously unknown places. When the gods and goddesses of the great religions first emerged, they came into a world already populated with daimons. These are still with us, morphing into new forms in digital environments: mailer-daemons, internet trolls, so many ghosts in the machine…"
Why Brazil fell for Pentecostalism but not liberation theology | Aeon Essays,,https://aeon.co/essays/why-brazil-fell-for-pentecostalism-but-not-liberation-theology,"In 1962, the Colombian priest and sociology professor Camilo Torres Restrepo travelled to the north of the country to investigate a dispute between powerful landlords and subsistence farmers. Spending days wielding machetes with Black cane cutters and nights drinking rum with them, he learned of their 40-year struggle against their wretched conditions at the mercy of a local landowner and senator. Competing for land with his grazing cattle, they had been ‘whipped, fired upon, beaten back to the riverbanks and their humble dwellings burned to the ground.’
Camilo Torres Restrepo with Fabio Vázquez and Victor Medina Morón, in the journal OCLAE , no 2, 1967. Courtesy Histoire Engagée
Described by his biographer Walter J Broderick as the ‘Che Guevara of the Christian world’, Torres began working across spiritual lines with communist, student and trade union groups as he dedicated himself to improving the conditions of the downtrodden. ‘More concerned with the action itself than with the theory behind it,’ Torres was relieved of his orders in 1965, after which he took up arms with the National Liberation Army, a Marxist-Leninist guerilla group. ‘The common people,’ he wrote to a friend, are ‘the only hope for change.’
Insisting on being given no special treatment and fighting as a regular soldier, the following year Torres wrote a powerful homily to the nation on his decision to take up arms. ‘For many years the poor of our country have waited to hear the call to arms which would launch them on the final stage of their long battle against the oligarchy,’ he wrote. Years of corrupt elections and failed coups meant that ‘the moment for battle has arrived.’ Adamant that he had ‘not betrayed’ the people, Torres added: ‘I have not ceased to insist that we give ourselves to this cause unto death.’
It was a prophetic letter. Torres died shortly after writing it, killed in his first battle, by the first bullet fired at him. Long viewed by the military as a threat, Torres was taken, his body secretly buried and its location kept a state secret for years so he could not become celebrated as a martyr. One man’s heroism is another’s terrorism, and Torres’s story was often cited as proof that Left-wing social justice movements, supported by a new breed of clergymen, wore a crown of daisy-chains to conceal a violent and sinister mission.
Even for the many who disapproved of Torres’s decision to trade his vestments for khakis, the mood for change within the Catholic Church was already taking powerful shape.
A t the Second Vatican Council (Vatican II) between 1962 and 1965, the Church agreed a series of reforms to meet the modern world’s challenges. Even Pope John XXIII had, a month before the Council’s opening, noted that in ‘underdeveloped countries, the Church is, and wants to be, the Church of all and especially the Church of the poor.’ The decrees that were eventually issued by Vatican II sought to make the Church more democratic and modern, but they did not go far enough for many in its progressive wing.
Lay people wanted change, too. Groups that became known as Christian Base Communities were both a response to Vatican II’s call for ordinary believers to take a more active role in the Church, and a sheer necessity. Largley growing from the most desperate regions of the Brazilian Amazon, they began as Bible study and prayer groups formed in poor and remote parts of the continent that were starved of pastoral care. In 1970, some 40 per cent of Brazilian priests were foreigners. By the 1980s, there was only one priest for every 9,367 people, and the ratio continued climbing in the 1990s.
There simply weren’t enough priests to effectively look after their flock, another strain amid growing disconnection from the Church in Europe. As the sociologist Cecília Loreto Mariz puts it in Coping with Poverty (1994), the largest Catholic nation in the world was ‘still a mission outpost’. Demographic needs were one thing, but a cultural shift was underway too. At a time when global economic forces were accelerating ideas of self-reliance, spiritual nourishment was not immune – something that would become telling two decades later, when a more vibrant gospel of personal betterment would sweep all before it.
Liberationists believed that they were bringing the Church back to the teachings of Jesus
Real and necessary as they were, liberation theologists emphasised Base Communities beyond their influence. Impoverished and often illiterate people coming together to practise their faith by any means necessary was a romantic idea, blown out of proportion. Initial studies seriously overestimated the scale of Base Communities by as much as a factor of 10. In fact, no more than 4 per cent of Brazilians – in 1970, that meant about 3.8 million people out of 96 million – are believed to have ever belonged to one. What they did, however, was give voice to ordinary people, and empower the progressive wing of the Church, which was calling for a more democratic approach to faith and wider society.
The 1968 General Conference of the Bishops of Latin America and the Caribbean (CELAM) held in Medellin, Colombia saw the nascent movement that would become known as liberation theology begin to take its first steps. The bishops issued a document that understood three meanings of poverty: an evil God does not want; spiritual poverty, or a readiness to do God’s will; and, finally, solidarity with the poor. The conference also consciously introduced the term ‘liberation’ to the cause for spiritually led societal change.
Joseph Ratzinger, the future Pope Benedict XVI, in 1965. Public domain
With this new direction came the first rumblings of criticism. It was 1968 after all, and the febrile geopolitical climate saw great possibility, but a strong pushback against change. Until that year, a young German theologian named Joseph Ratzinger – the future Pope Benedict XVI – had been politically liberal, but he began developing an increasingly conservative outlook that divorced faith from secular politics. In promising to forefront the poor, and go beyond charity to seek broad and lasting solutions to their poverty, liberationists believed that they were bringing the Church back to the teachings of Jesus. Ratzinger, and many of his European colleagues, saw things very differently.
F ive years after Torres’s battlefield death, his former schoolmate and friend from the Catholic University of Louvain, the Peruvian theologian Gustavo Gutiérrez, put the ideas of the fledgling Latin American movement into print with his book A Theology of Liberation (1971). He understood the movement as a ‘commitment to abolish injustice and to build a new society’, where freedom from exploitation, and ‘the possibility of a more human and dignified life’ was built on four principles. First and foremost is the prioritisation known as the ‘preferential option for the poor’, which he later said encompassed 90 per cent of the movement. He also believed that an unequal society was a sin, and valued the importance of unchurched lay gatherings of the faithful, such as Base Communities, and the commitment to taking a ‘see, judge, act’ approach to injustice.
God makes his sympathies clear from Cain and Abel, Gutiérrez argued, the parable mirroring ‘God’s predilection for the weak and abused of human history’. Jesus makes the will for material justice even more explicit, best surmised in the Bible verse: ‘I have come that they may have life, and that they may have it more abundantly.’ This wasn’t a God of death and afterlife, but of the here and now. On a deeply Christian continent that had (and still has) some of the greatest wealth and resource disparity on Earth, abundance was out of reach for most. ‘In liberation theology, faith and life are inseparable,’ Gutiérrez later wrote in a reflection on his original work. ‘These concrete, real-life movements are what give this theology its distinctive character.’ The poor deserve to be elevated, Gutiérrez argued, ‘not because they are morally or religiously better than others,’ but as it is expressed in the gospel of Matthew, that ‘the last will be first, and the first last.’
Spiritual life could not be divorced from the material world, and it was an existential issue for the Church. From the colonial era through to the brutal far-Right and military dictatorships that came to dominate Latin America in the 20th century, the Catholic Church was replicating the hierarchies that harmed God’s people. For too long, God’s emissaries in Rome had served earthly power. Too often, the almighty dollar had been a servant of poverty, and a master of the poor.
It wasn’t only the Vatican that was on the offensive against liberation theology
One year after Gutiérrez’s book, the Brazilian philosopher-priest Leonardo Boff published Jesus Christ Liberator . Unabashedly shaped by Marx and intent on ‘decentring’ the Church to welcome disparate cultures, Boff was a champion of Base Communities, and one of the Vatican’s most trenchant internal critics. Influenced by the Norwegian sociologist Johan Galtung, who introduced the idea of ‘structural violence’ to the secular world, Boff expanded on the concept of ‘structural sin’, the idea that sin can be a social or economic action, and not simply an individual one.
The Brazilian theologian Leonardo Boff, pictured in 2018. Courtesy Manuale d’Avila/Flickr
Around this time, repressive military dictatorships in the cone of South America were beginning to transition to civilian governments, and anti-communist fervour shifted its centre of gravity to the Central American nations. The religious studies professor R Andrew Chesnut says it wasn’t only the Vatican that was on the offensive against liberation theology: ‘They faced fierce national repression in countries such as Brazil, Argentina, El Salvador and Guatemala, where brutal military dictatorships saw them as communists.’ The ensuing dirty wars in Central America, with plenty of aid and comfort from the United States government and mercenaries, killed around 350,000 people in Guatemala, Nicaragua and El Salvador, and displaced millions more.
Most proponents of liberation theology explicitly condemned violence, while pointing out the differences between ‘structural sin’ that they saw as the institutionalised violence of inequality, and violent action against injustice. However, Ratzinger and other orthodox Catholics continued to mount objections to the movement, gathering evidence from Torres’s deeds and Boff’s words to declare the movement heretical.
Their attentions turned inward, they failed to see a far greater threat to their power and authority in another movement bubbling up from below, albeit from a very different Christian tradition. It was one that would go on to claim far more of the continent’s souls and move them even further away from the edicts of European cardinals.
Pentecostalism, a US-born branch of evangelical Protestantism centred on the Holy Spirit, had been gaining ground in the region throughout the 1970s. Two decades later, Pope John Paul II denounced Pentecostals as ‘ravenous wolves’ who were stealing the Catholic flock. Their attraction was prosperity theology, or the gospel of health and wealth, where true believers who give generously to their Church will receive back riches of mind, body, spirit and wallet many times over.
Emerging from the same social and economic conditions as Base Communities and liberation theology, the divergent paths of the two religious movements over the next four decades tell a much broader story about the secular world.
I f there was a family that liberation theology was born to serve, it was Edir Macedo’s. Born in the small country town of Rio das Flores in Brazil, to a Catholic mother who endured 33 pregnancies, including 16 miscarriages and 10 premature births, he was one of seven surviving children. The fourth child and second oldest son, Macedo’s genetically deformed hands made him the target of bullying, both at school and at home.
Like many Brazilians from the regions, the family migrated to the outskirts of a major city, in their case, Rio de Janeiro. When he was a teenager, Macedo’s elder sister Elcy developed chronic asthmatic bronchitis that was failed by the medical treatment available. The family sought help at a spiritist centre, a kind of folk homeopathy that brought together strands of African slave religions, Christianity and pseudoscience.
Having no luck, Elcy heard a radio broadcast from the Canadian Pentecostal preacher and faith healer Robert McAlister. Visiting his New Life Church in downtown Rio, her asthma disappeared and, within a year, the entire family became devotees. Macedo converted at 19, but when 11 years later, his daughter was born with a cleft palate, his devastation led him to a ‘revolt’ that he said was ‘not against God, but against hell.’ Macedo quit his job and took to street preaching in crime-soaked favelas, before forming his Universal Church of the Kingdom of God in 1977.
Long before McAlister had touched the Macedos, in 1911, the renegade Swedish Baptist Daniel Berg brought the Assembly of God to Brazil after witnessing the Pentecostal fire when he worked as a foundryman in the US. Evangelical missionaries came and went across the continent with little success, before Americans from the Foursquare Gospel arrived in the 1950s. Both were Pentecostal movements, a branch of the evangelical faith that emphasises the role of the Holy Spirit and a direct experience and personal interaction with God, and all of the blessings and miracles that come with it. While Charles Fox Parham could be credited with birthing the movement in Kansas in 1901, it was his adopted spiritual son, William J Seymour, a Louisianan son of freed slaves, who gave it life in 1906, with the Azusa Street Revival in what is now downtown Los Angeles.
Fearing the slippery slope towards communism, business leaders and Christian leaders found common cause
Unlike traditional Christian denominations, what became known as Pentecostalism cut away intermediaries and allowed believers a direct relationship not only with God, but with all of his promises. Specifically, the nine miracles of the Holy Spirit, including speaking in tongues, prophecy and faith healing. Though it faced stiff opposition from the Catholic Church and even some of the few Protestant Churches in the region, as early as 1916, the Pentecostal movement had penetrated eight countries in Latin America. The Assemblies of God grew by an average of 23 per cent per year between 1934 and 1964. Brazil alone boasted more than a million members of Pentecostal congregations by the early 1960s. The Pentecostal movement expanded to become a worldwide phenomenon in just over a century. Pentecostals and Charismatics numbered just 58 million in 1970; by 2020, that number was 635 million. What brought so many into the tent was the prosperity gospel .
Pentecostalism really got going in North America after the Second World War, coinciding with the US elevation to global superpower. This brought forward a flood of optimism; yet, for many leaders of both faith and industry, it was a time for vigilance. Business leaders were scarred by the economic safety net implemented by Franklin D Roosevelt’s New Deal. Christian leaders were terrified of the godless ‘reds’ sweeping Europe. In fearing the slippery slope towards communism, these interests found common cause. While they were busy merging the languages of business and Bible, the Pentecostals, often looked down on by their evangelical counterparts, were stoking new revivalist fires across the country. Out of the spirit of this patriotic postwar nation arose a new vision of Christianity.
Prosperity theology, often known as the gospel of health and wealth, emerged with the help of syndicated radio and cassette-tape technology. Kate Bowler, the author of Blessed: A History of the American Prosperity Gospel (2013), says: ‘Inverting the well-worn American mantra that things must be seen to be believed, their gospel rewards those who believe in order to see .’
Pentecostals embraced the resurgent idea of ‘mind power’, and added the miracles of healing and prosperity to what Bowler calls an ‘electrified view of faith’. If this way of ‘doing Jesus’ had just been plugged in, then the publication of Norman Vincent Peale’s The Power of Positive Thinking (1952) turned up the volume to max. The New York-based preacher, who was born Methodist then born-again Reformed, invented a simple formula – picturise, prayerise, and actualise – that self-help gurus have been peddling ever since, a precursor to the ‘mind, body, spirit’ trend in both Christian and secular thought in the 1960s and ’70s. Peale’s message was a huge hit with the faithful, spreading rapidly from coast to coast. His Marble Collegiate Church on Fifth Avenue was standing room only. One family was firmly planted in the front row with their young son: his name was Donald J Trump.
Bowler says the prosperity gospel is the result of the ‘American gospel of pragmatism, individualism and upward mobility’, where churchgoers ‘learned to use their everyday experiences as spiritual weights and measures’. Critics of the prosperity gospel rarely acknowledge that ‘when many people say “prosperity”, they mean survival,’ Bowler says. For many ordinary believers, it is the same quest for abundance that the liberation theologists expressed.
W hile the US was on the up, Brazil was in the midst of its own Great Migration. Between the 1930s and the ’80s, millions began moving to the more central cities from the poor north-east, displaced by ranchers who could make use of poor farming land for grazing. Slavery had nominally ended in 1888, but indentured servitude continued in some areas of the north-east into the 1990s. The cities offered rural northerners economic opportunities beyond tilling the fields for those who could only generously be described as their bosses. In 1960, São Paulo had a population of 4 million; today, the wider urban area has grown to five times that.
Pentecostalism gained traction among poor migrants from the countryside who moved to the outskirts of large cities looking for work, like the Macedos. That was followed by a reverse migration of sorts, as new converts took the religion back home to the impoverished rural communities where Base Communities had flourished. Early research on Pentecostal converts suggested that they were the poorest of the poor.
As gifts from the US to Latin America go, the prosperity gospel is up there with the military coup. The best and worst of this muscular US ethos trickled down the Atlantic, both by culture and by military might. From the 1950s, many middle-class Brazilians believed that US culture spoke to a glittering sense of progress that they ought to emulate.
Macedo understood what he aspired to be, but also where he came from. Universal churches began offering services between 5 am and midnight throughout the week, understanding that most working people struggled to attend in standard hours. More than that, the spiritual nourishment of going to a service before or after a long day of work was fortifying. As Pentecostal churches began to cram into the storefronts of the nation’s favelas, they quickly became familiar institutions that were markedly different from the distant Sunday cathedrals. Their pastors spoke in the vernacular of the streets, were often mixed-race and had little-to-no theological education; their sermons used everyday problems to understand the Bible.
The golden toilet seats of prosperity gospel entrepreneurs are not a deterrent but rather a sign of God’s gifts
After the end of military dictatorship and economic growth, by the 1980s and early ’90s, Brazil had the 11th largest economy in the world, but its social indicators were similar to poor African countries. As Mariz writes: ‘many already have rich spiritual lives, so they’re after something that can help them understand and cope with the changing world around them.’
The golden toilet seats and private jets of prosperity gospel entrepreneurs are not a deterrent but rather, for most true believers, a sign of God’s gifts. Derided as gullible, they would describe themselves as motivated. If Macedo rose up from below, why can’t I? Pentecostal preachers have always had a knack of turning established ideas on their head. Catholic priests came to resemble loathed bureaucrats; Pentecostal preachers looked a lot more like inspiring entrepreneurs. The average person seeking wealth is usually looking for it in terms of a promotion at work, helping to keep creditors at bay, or starting their own small business. During services, it’s not unusual to see worshippers hold aloft Bibles topped with debt notices or money.
As Macedo’s following grew, he used funds from his flock to buy a television network, Record, in 1989. Today, he is a self-made billionaire who has received everything he owns from believers – including his only son, Moysés, given to Macedo in the street as a baby by its birth mother. To this day, prosperity gospel’s trickle-up economics continue. Smaller churches practise what they preach and become satellites for established megachurches by ‘sowing’ a percentage of their church income in order to receive the blessings of those who are materially more successful, therefore spiritually more devout.
The threat to the Catholic Church was far greater than the one that liberation theology posed, and the Vatican was compelled to compromise. The Charismatic Catholic Renewal (CCR) arrived in Latin America in the early 1970s, around the same time as liberation theology was emerging and Pentecostalism was gaining steam. ‘Liberationists historically saw Catholic Charismatics as alienated bourgeois citizens with Right-wing political inclinations,’ Chesnut says. Largely imported from the US, ‘it found traction that liberation theology did not.’ In the CCR, believers could have their gospel of health and wealth, but keep their saints and retain connections to Europe and the Catholic hierarchy.
‘Among Protestants and Charismatic Catholics, the prosperity gospel has become hegemonic,’ Chesnut says. ‘It’s come from Latin America, but is now the driver of faith in Africa and other parts of the Global South too.’ Brazil has recently crossed the line where there are now more Pentecostals and Charismatics than traditional Catholics, undoing 500 years in 40, and it’s largely thanks to the prosperity gospel. For a long time, people assumed that it was a regional outlier. Just as they spoke Portuguese while everyone else spoke Spanish, they were embracing the Church of the Holy Spirit rather than Mary and the Saints. If recent trends are an indicator, the rest of Latin America may be on the cusp of this theological revolution, too.
The 1976 earthquake that left one-sixth of Guatemala homeless saw Californian missionaries arrive with the Pentecostal faith. Their missionary zeal and a bloody civil war, in part presided over by the world’s first Pentecostal leader, Efraín Ríos Montt, saw around 60 per cent of the country converted – making it proportionately the most Pentecostal nation in the world. Pope John Paul II’s ravenous wolves are howling at the doors of El Salvador, Chile and Argentina, but even in the staunch Catholic holdout of Mexico, the state of Chiapas recently tipped to majority Pentecostal. By no coincidence, it is also the country’s poorest and most Indigenous state.
I n 1984, only 16 years after the bishops in Medellin had first outlined a social justice under the banner of liberation, Cardinal Ratzinger was ready to issue last rites on the movement. The idea was simply too radical, and one that was ‘incompatible’ with established Church doctrine. To many inside and outside of the Holy See, it was nothing short of heresy – and needed to be punished accordingly.
Ratzinger summoned Boff to the Vatican to account for his accusations against Church authoritarianism, before Boff was given a silencing order, which the theologian Harvey Cox suggested was due to the ‘grass-roots religious energy’ Boff represented. Ratzinger continued to defend his stance, even after Boff left the Church altogether, suggesting that his censure had served as a warning to other like-minded theologians.
In August 1984, the Cardinal issued an extraordinary decree, Libertatis Nuntius , an instruction responding to the Latin American movement known as liberation theology, which called for a radical reorientation of the Church towards social and economic justice. Ratzinger, the future Pope Benedict XVI, might have been known as ‘God’s Rottweiler’, but when it came to teachings that questioned established hierarchies and beliefs, he was Pope John Paul II’s sottocapo . Together, the two fervent anti-communists executed a well-planned hit on a movement that orthodox clergy saw as Marxism in holy cloth.
Ratzinger warned that deviating from Papal authority risked ‘provoking, besides spiritual disaster, new miseries and new types of slavery.’ He believed that liberation theology subverted the meaning of truth and violence. A doctrine ‘incompatible with the Christian version of humanity’, it had, he said, an ‘ideological core borrowed from Marxism.’ To his mind, Marxists believe only in class and class struggle, and see society as inherently violent. That leaves no room for Christian ethics, such as good and evil. Unshackled from morality, Marxists are then compelled to participate in the struggle and to reverse the nature of domination by establishing their own.
Some argued that the ‘preference’ for the poor ran against the universal Christian message
Such was the hammer blow of Ratzinger’s decree that an early obituary of the movement in The New York Times showed that opposition to liberation theology was every bit as political as the movement itself. Written by Michael Novak, resident scholar in religion, philosophy and public policy at the American Enterprise Institute in Washington, DC, it claimed that the movement’s ‘Marxist thread’ was ‘just one of its many flaws. It is a robe of many vibrant colours.’
Novak objected to theologians who took liberation theology ‘seriously as a theology, rather than regarding it as merely a political vision.’ There was no reason to conflate the two movements, as he saw them: ‘the birth of social conscience in the Church’ and the secular dependency theory, the idea that the development and enrichment of some countries happens due to the exploitation of others. One of his more compelling arguments was that some corners of the traditionally anti-capitalist Latin American Church viewed their societies as capitalist. Most countries in the region, he argued, housed ‘precapitalist’ economies that were ‘disproportionately state-directed’.
Novak’s critique bears the liver spots of Ronald Reagan-era foreign and economic policy, believing that liberation theology’s ‘main enemy’ was the US, a ‘hostility’ that he said ‘embodies a kind of liberation theology’ in itself. In rejecting Western ideas of ‘distinctions between religion and politics, church and state, theological principles and partisan practice’, liberationists who wouldn’t take communion with Augusto Pinochet were postponing the Christian virtue of loving their enemy.
Spiritually, the critiques ran stronger. Some said the overtly political nature of liberation theology meant that it reduced salvation to action taken by humans, not God. By prioritising social action, it also undermined the principle of spreading the good news and saving individuals soul by soul. Some argued that the ‘preference’ for the poor ran against the universal Christian message. A Church that came from below, such as was found in Base Communities, ran against the hierarchical nature of the Church.
Perhaps most powerfully, liberation theology simply never strongly resonated with the people for whom it was supposed to serve. ‘In many ways, it was an overly intellectual construct,’ Chesnut says, a charge that the movement has never been able to overcome. At the same time, many of the churches ministered by liberation theology priests were departing from traditions, and ‘clearing their churches of the saints’, Chesnut says. For so many Latin American Catholics, the saints and the Virgin Mary were a significant part of their faith. In the end, ‘these liberationist parish churches came to resemble Protestant churches.’
The refined hands that crafted liberation theology were in stark contrast to the calloused palms that would ecstatically whoop and clap
Whether liberation theology would have been able to succeed without the Vatican’s intervention continues to be debated. It didn’t matter. Ratzinger appointing conservative bishops across Latin America helped to salt the earth and ensure the movement would be largely confined to small corners of academia. Fixated on combatting internal enemies, they failed to see what was happening out of their stained-glass windows. By 1992, the year when Boff left the priesthood before receiving a second silencing order, a Protestant church opened every two days in Rio de Janeiro. Soon, Macedo’s Universal Church of the Kingdom of God was opening two new churches each week. Reflecting on the seismic time, Ratzinger would blame the ‘widespread exodus’ of the faithful to Pentecostalism on ‘politicisation of the faith’ by liberation theology.
There were many reasons why people began converting, but doing it in reaction to the politics of a movement that struggled to move much beyond intellectual circles isn’t one of them. The refined hands that crafted liberation theology in darkened rooms were in stark contrast to the calloused palms that would ecstatically whoop and clap and promise the power of healing could be channelled through them. Too busy trying to defeat the insurgency from bookish Latin American priests, they failed to see the insurgency happening outside the cathedral walls.
Even then, it’s been argued that the Vatican managed to destroy the men, but not the ideas. As the US journalist John L Allen Jr points out, the ‘four elements of the liberation theology movement – the preferential option for the poor, structural sin, Base Communities and the “see, judge, act” method – have largely withstood the test of time.’ Chesnut argues that the current Brazilian president Lula and his Workers’ Party are the heirs to the Base Community movement that many of its figures had grown up in: ‘Lula and many others essentially migrated from the Church as they became politicised.’ Ironically, Pope Francis, once an opponent of liberation theology, may also be one of its few remaining proponents, even if he doesn’t expressly use those terms. In 2022, Ratzinger’s successor said that, sociologically, ‘I am a communist, and so too is Jesus.’ He is also rumoured to have personally consulted Boff on some of his recent encyclicals.
But the stark reality is that liberation theology, which wanted to build the kingdom of heaven on Latin American soil, has retreated further into academia, far from the mobilising force on the ground that it once promised. Whether it would have thrived without Ratzinger’s determination to stamp it out is a live question, but it’s one that may be best answered by the continuing rise of Pentecostalism and the prosperity gospel in the intervening 40 years. ‘When you’re poor, you often have acute and immediate afflictions, and you’re looking for immediate solutions to them,’ Chesnut says. ‘The problem with liberation theology was that it promised long-term structural solutions.’
The priests of liberation ‘opted for the poor, but the poor themselves opted for Pentecostalism,’ says Chesnut. But as the Spirit-led faith and its prosperity doctrine continues its unrelenting march through Latin America, it may just be that the very orthodox Catholic hierarchy in Europe that instigated the intellectual civil war in its own ranks is paying the greatest price."
What will an evangelical Brazil look like? | Aeon Essays,,https://aeon.co/essays/what-will-an-evangelical-brazil-look-like,"In 1856, Thomas Ewbank published Life in Brazil , an account of the Englishman’s six months spent in the country a decade earlier. In it, he argued that Catholicism as practised in Brazil and across Latin America constrained material progress. In this, the visitor would be joined by a long line of critics, from the writer and later modernising president of Argentina, Domingo Faustino Sarmiento – who denounced the negative influence of Spanish and Indigenous cultures in Latin America, including the role of the Catholic Church – to the conservative Harvard academic Samuel Huntington.
The Church of St Cosmas and St Damian and The Franciscan Monastery at Igaraçu, Brazil ( c 1663) by Frans Jansz Post. Courtesy the Museo Nacional Thyssen-Bornemisza, Madrid
Ewbank contended, moreover, that the ‘Nordic sects will never flourish on the Tropics,’ a line that Brazil’s greatest historian, Sérgio Buarque de Holanda, immortalised in his work Raízes do Brasil (1936), or Roots of Brazil . Protestants would supposedly degenerate here, with the severity, austerity and rigour of that doctrine being incompatible with the archetypal Brazilian: the ‘cordial man’. This figure, according to Holanda, represented interpersonal warmth and openness, in contrast to closed and rule-bound northern Europeans.
At present, Protestants account for one-third of the population, while the number of Catholics has just dipped below 50 per cent. By far the largest proportion of Brazilian Protestants are evangelicals, specifically Pentecostals, neo-Pentecostals and related branches. By the centenary of Raízes do Brasil in 2036, Protestants will outnumber Catholics in Brazil for the first time in the country’s 500-plus-year history.
In 2018, the far-Right former army captain Jair Bolsonaro shocked the country by winning the presidency, bolstered by an evangelical vote that would remain faithful to him and his socially conservative, politically reactionary and cosmologically apocalyptic politics.
The rise of this bloc presents a challenge to perhaps the most clichéd description of Brazil. In 1941, the Austrian Stefan Zweig, seeking refuge from Nazism in Brazil, called this land the ‘country of the future’. Zweig highlighted not just Brazil’s natural endowments but the society’s tolerance, openness, harmony, optimism and fusionist culture.
For Zweig, as for many Europeans and Americans before him, Brazil became a utopian gleam in the eye. For centuries, certain common threads had sewn these utopian visions together: Brazil was a picture of idleness, imagination, diversity and conviviality – a means of living together that relied on adaptability. Yet the Bolsonarismo phenomenon, according to critics, is intolerant, punitive, supremacist, an embodiment of a type of Christian cosmovision at odds with any notion of society. Did the presidency of Bolsonaro, under the slogan ‘Brazil above everything, God above everyone’, signal an end of this romance?
No one holds Brazil as an existing paradise. Few even sustain any expectation that it will deliver on what was promised for it. And, indeed, utopian thinking probably died as far back as the 1964 military coup. But many have continued to uphold the country’s cultural traits as admirable and enviable – even models for the world.
‘B razilianization’, a trope taken up by various intellectuals in recent decades, signals a universal tendency towards social inequality, urban segregation, informalisation of labour, and political corruption. Others, though, have sought to rescue a positive aspect: the country’s informality and ductility, particularly in relation to work, as well as its hybridisation, creolisation and openness to the world, made it already adapted to the new, global, postmodern capitalism that followed the Cold War.
By the 2000s, Brazil was witnessing peaceful, democratic alternation in government between centre-Left and centre-Right for practically the first time in its history. Under President Lula, it saw booming growth, combined with new measures of social inclusion. But underneath the surface of the globalisation wave that Brazil was surfing, violent crime was on the up, manufacturing was down, and inclusion was being bought on credit.
‘There is indeed an alternative, even if it is an apocalyptic one’
In 2013, it came to a shuddering halt. Rising popular expectations generated a crisis of representation – announced by the biggest mass street mobilisations in the country’s history. This was succeeded by economic crisis and then by institutional crisis, culminating in the parliamentary coup against Lula’s successor, Dilma Rousseff. Now all the energy seemed to be with a new Right-wing movement that dominated the streets. It was topped off by the election of Bolsonaro in 2018. Suddenly, eyes turned to the growing prominence of conservative Pentecostal and neo-Pentecostal outlooks in national life.
An evangelical church near Salgueiro, Pernambuco state, Brazil, February 2022. Photo by Jonne Roriz/Bloomberg/Getty
Bolsonaro failed to be re-elected in 2022. Upon his defeat, Folha de S Paulo , Brazil’s paper of record, reported that, ‘ Bolsonarista pastors talk of apocalypse.’ At the evangelical Community of Nations church in Brasília, frequented by Michelle Bolsonaro, wife of Jair, the pastor’s wife is reported to have proclaimed: ‘Brazil has an owner. That hasn’t changed, it won’t change. God continues to be the one who made Brazil shine and be the light of the world. His plan has changed neither with regard to us nor the country.’ It was a rare expression, for our times, of a sense of historical mission or destiny. The age of no alternative was being left behind. ‘There is indeed an alternative, even if it is an apocalyptic one,’ the Brazilian philosopher Paulo Arantes sardonically remarked.
I n the final 2022 pre-election poll, evangelicals split 69-31 in Bolsonaro’s favour. Although he is Catholic, he was baptised in the River Jordan in 2016 by Pastor Everaldo, an important member of the Pentecostal Assembleia de Deus (the Assemblies of God – the largest Pentecostal church in the world, and the largest evangelical church in Brazil).
The creationist and anti-gay Pentecostal Marcelo Crivella shocked many when he defeated a human rights activist to become mayor of Rio de Janeiro in 2016. Crivella’s uncle is Edir Macedo, the founder of the neo-Pentecostal Universal Church of the Kingdom of God (Igreja Universal do Reino de Deus, or IURD), the largest of its denomination, reputedly with 4.8 million faithful in Brazil. Preaching the ‘prosperity gospel’, according to which commitment to the church will be rewarded with wealth, has seen Macedo become a dollar billionaire (of which there are around 60 in the country). The IURD is known for practising exorcisms and divine cures, and for purging demonic spirits, which it associates with Afro-Brazilian religions like Candomblé and Umbanda. But it is the IURD’s political role and media presence that really make it stand out.
The Republicanos party, founded in 2005, is a creature of the IURD. Its president, the lawyer Marcos Pereira, was a bishop who held a position in the Michel Temer administration that took office after deposing of Rousseff. The party’s 44 deputies in the lower house of Congress are part of the powerful cross-party evangelical bench in Congress, composed of 215 deputies out of a total of 513. Macedo also owns Record, the second-biggest channel in Brazil, which gave Bolsonaro plenty of free airtime.
The articulation between evangelicals and Bolsonaro only strengthened through his term. During the COVID-19 pandemic, Bolsonaro’s denial of the severity of the virus was, in part, a demonstration of evangelical coronafé , or corona-faith: ‘that confidence, that certainty that God is with you and that he will never, ever, at any time fail those who have believed in him,’ in Macedo’s words. Later in his term, Bolsonaro nominated the ‘terribly evangelical’ Presbyterian pastor André Mendonça to an empty Supreme Court seat. Upon Congressional approval, the president’s wife Michelle, a crucial link to the evangelical public, was filmed crying, praying and speaking in tongues.
Bolsonarismo is a sort of parody of Christian eschatology
After Bolsonaro left office, his supporters stormed government buildings in Brasília on 8 January 2023, in a replay of the storming of the United States Capitol on 6 January 2021. The action was widely unpopular. But 31 per cent of evangelicals supported it, against a national average of 18 per cent. While 40 per cent of the population believed Lula had not won the election fairly, among evangelicals this belief was as high as 68 per cent, with 64 per cent in favour of a coup to overturn the result. The media was full of reports of pro-Bolsonaro protestors praying for miracles, speaking in tongues and behaving like the world was ending.
The theologian Yago Martins, whose videos on religious thought have won him more than 1 million followers across his social channels, refers to Bolsonarismo as an apocalípse de palha , or ‘straw apocalypse’. Bolsonarismo ’s combination of a conspiratorial mindset, a longing for an imminent national conflagration, a holy war against evil, and its messianic discourse are a sort of parody of Christian eschatology. For Martins, author of A religão do bolsonarismo (2021), or Bolsonarismo as Religion , the movement is a ‘fallacious immanentisation of the eschaton’, a paraphrase of the philosopher Eric Voegelin’s phrase from 1952.
Martins, a Baptist pastor, identifies as a Right-wing evangelical, but is a critic of Bolsonarismo (though he admits to voting for him in 2018). His criticisms of Bolsonarismo ’s idolatry nevertheless testify to something new on the scene: the insertion of a transcendental viewpoint into politics, something that had supposedly been expulsed with the historic defeat of socialism and nationalism.
Indeed, when I spoke to Gedeon Freire de Alencar, a sociologist of religion and author of a book on the contribution of evangelicals to Brazilian culture, as well as a presbyter of the Bethesda Assembly of God in São Paulo, he emphasised the role of dominion theology, according to which believers should seek to institute a nation governed by Christians. The ‘Seven Mountain Mandate’, popularised in 2013 by two American authors, advocates that there are seven areas of life that evangelicals should target: religion, family, government, education, media, arts/entertainment, and business.
For many progressives, this struck as a sort of ‘medieval radicalism’, the charge thrown at Crivella by Jean Wyllys, the first gay-rights activist to win a seat in Congress. The philosopher and columnist Vladimir Safatle denounced the ‘project to take Brazil back to the Middle Ages’: yes, Brazil had had its share of authoritarian and conservative figures in the past, but this was new, ‘because the old Right… never needed spokespeople.’
A s testament to the growing presence of evangelicals but also their political ambivalence, consider the March for Jesus. The yearly demonstration is known as ‘the world’s largest Christian event’ drawing between 1 and 3 millions crentes , or believers, each year. Though Bolsonaro was the first president to attend the march, in 2019, it was Lula who signed the law that officialised the National Day for the March for Jesus, scheduled for 60 days after Easter.
Evangelicals attend the March for Jesus in Sao Paulo, Brazil, 8 June 2023. Photo by Amanda Perobelli/Reuters
Similarly, back in 1997 it was estimated that one-third of militants in the agrarian land reform movement, MST, were Pentecostals, which would have been double the rate of the local population at the time. Twenty years later, Guilherme Boulos, coordinator of the MTST, the unhoused workers’ movement, claimed that by far the largest part of the movement’s base was made up of Pentecostals.
So why the association of evangelicals with darkest reaction? In large part, it’s class prejudice, argues the anthropologist Juliano Spyer, whose book Povo de Deus (2020), or People of God , sparked widespread debate in the country and was a finalist in Brazil’s most prestigious nonfiction prize in 2021. For opinion-formers, the evangelical is either a poor fanatic or a rich manipulator, but the reality is that the religion is socially embedded in Brazil, particularly among the poor and Black population.
The Brazilian urban landscape sees a war of all against all play out every day
For instance, well-to-do social progressivism tars evangelical religion as patriarchal. Perhaps so, in contrast with contemporary upper-middle-class mores, but in the often machista and violent lifeworld of the Brazilian working class, when a man is born-again, he stops drinking, becomes less likely to beat his wife, and is more inclined to contribute to the household. Similarly, while evangelicals are held to be anti-science and anti-enlightenment, in a culture in which even the elite has never been particularly bookish, conversion is associated with a renewed emphasis on study. This partly explains why Pentecostalism (and evangelical Christianity more broadly), is the faith of the world’s urban poor. And ‘Brazil is ground zero for what is happening within the wider Pentecostal movement, the median global experience,’ explains Elle Hardy, author of a book Beyond Belief (2021) on the phenomenon’s spread worldwide.
The evangelical movement must be understood in relation to the reality in which real political corruption abounds, and violence and the threat of violence is omnipresent in the working-class urban context. Brazil now sees more than 50,000 murders a year, and the violence associated with criminal markets, especially drugs, is only the sharp end of a fully marketised society. The Brazilian urban landscape sees a war of all against all play out every day. Middle-class Brazilian progressives were happy to ignore the civil war raging in the urban peripheries until the violence found a spokesperson in Bolsonaro.
B roadly, the term evangélicos refers to missionary Protestants who are not members of the historic Protestant churches in Brazil – the Presbyterians, Lutherans, Anglicans, Methodists, Adventists and Baptists who first arrived from Europe in the 19th century.
Confusingly, many historic Protestant churches carry the name ‘evangelical’ in their titles, and some have now come to adopt modes of worship evocative of charismatic or revivalist churches. But a distinction remains: historic Protestants in Brazil normally call themselves protestante or cristão , not evangélico or crente – and they tend to be middle class.
Pentecostalism arrived in Brazil in the early 20th century, taking root among the poor. Its emblematic church is the Assembleia de Deus, established by two Swedish Baptist missionaries who arrived in the Amazonian port city of Belém in 1910. The third wave, beginning in the 1950s, is marked by the arrival of the Foursquare Church (Igreja Quadrangular), and coincides with rapid industrialisation and urbanisation, with worshipers recruited over the airwaves. But even by 1970, evangelicals still accounted for only 5.2 per cent of the population, while Catholics were at 91.8 per cent.
The establishment of the IURD in 1977 marks the arrival of neo-Pentecostalism and the start of the fourth wave. Proselytising is carried out via TV and, doctrinally, a more managerial ethos is introduced. To the Pentecostals’ direct, personal and emotional experience of God is added the idea that conversion leads to financial advancement – the prosperity gospel. Macedo’s church also exemplified the movement’s growing political confidence. By the 1980s, the slogan crente não se mete na política (believers don’t get mixed up in politics) was being replaced by irmão vota em irmão (brothers vote for brothers).
Throughout, the share of Catholics in the population was falling, with an almost commensurate rise in evangelicals – by about 1 per cent per decade. But, as of 1990, this accelerates to a 1 per cent change per year . Catholics were still 83 per cent of the population in 1991 and 74 per cent in 2000, when Catholicism hit its peak in absolute terms, with 124.9 million Brazilians – making Brazil the largest Catholic country in the world, a title it still holds. But by 2010, the share of Catholics had fallen to 64.6 per cent, with evangelicals rising to 22.2 per cent. Today, evangelicals represent a third of the population, and Catholics just under half. Modellers have identified 2032 as the year of religious crossover, when each Christian camp will account for an equal share of the population: 39 per cent.
Any evangelical entrepreneur with a Bible under his arm and access to an enclosed space can set up shop
What explains this explosion? The anthropologist Gilberto Velho points to inward migration, the primary 20th-century phenomenon in Brazil. Tens of millions of poor, illiterate, rural and profoundly Catholic people from the arid northeast of Brazil migrated to big cities, especially in the industrial southeast. Spyer tells me they ‘lived through the shock of leaving the countryside for the electricity of the city – but also the shock of moving to the most vulnerable parts of the city.’ The loss of networks of support, particularly of extended family, was filled by the establishment of evangelical churches. This is why the geographer Mike Davis called Pentecostalism ‘the single most important cultural response to explosive and traumatic urbanisation’.
Sixty years ago, Brazil’s population was evenly split between town and country. Now it is 88 per cent urban, comparable with infinitely richer Sweden or Denmark, and higher than the US, the UK or Germany. The urbanisation rate is also much higher than Brazil’s fellow BRICs, China (66 per cent) or South Africa (68 per cent). Over the past decades, Brazil has also suffered ‘premature deindustrialisation’ – the loss of manufacturing jobs on the scale of the UK, for instance, but at a much lower level of income and development. Here is the recipe for what Davis called a ‘planet of slums’: urbanisation without industrialisation.
And it is in the peripheries of megalopolises like São Paulo (greater metropolitan population: 22 million) and Rio (14 million), or other large cities where informal or precarious housing and employment dominates, that nimble startup churches sprout. Unlike the slow-moving Catholic Church, which demands more established settings and that its priests undergo four years of theological study, any evangelical entrepreneur with a Bible under his arm and access to an enclosed space, no matter how rudimentary, can set up shop. To ambitious working-class men, this offers a route to a leadership position in the community, a path to self-improvement.
It was in what the sociologist Luiz Werneck Vianna called this ‘Sahara of civic life’ that Pentecostals and neo-Pentecostals built spaces of acolhimento , a word denoting both warm reception and refuge. They took root in the places abandoned by the Brazilian Left, of which the Catholic, liberation theology-inspired Comunidades Eclesiasticas de Base were a major part.
Turning up in an expensive imported car signals to co-religionaries that the prosperity gospel is working
Of course, not all evangelicals in Brazil are poor or working class. The movement has seen significant expansion into the middle class, even if the elite proper remains mostly Catholic. And there are doctrinal differences that map onto these class differences, even if incompletely.
An Assemblies of God church in the city of Cabo Frio, Brazil. Photo by Nate Cull /Flickr
The model Pentecostal will be a poor assembleiano, a member of the Assemblies of God, whose small, basic and mostly ugly structures populate the landscape, from gritty industrial suburbs to lost hamlets of a dozen inhabitants deep in the interior. In these houses of worship, eschatological themes are omnipresent and the songs are about Jesus’s second coming. On the way to or back from church, worshipers – in their Sunday best – pass each other’s houses and check in on each other, reinforcing communal ties.
At the other end of the spectrum is something like the Bola de Neve Church, founded in a surf shop by a surfing pastor in 1999. Its 560 churches across a number of countries purvey something altogether ‘lighter’. Its middle-class members arrive by car, wear casual clothing, and are treated to sermons accompanied by pop-rock and reggae. Eschatological themes are largely absent. As Alencar put it to me: ‘If Jesus returned now, he’d ruin their gig.’ Accompanying the Church’s suave and sophisticated marketing is the preaching of the theology of prosperity. Turning up in an expensive imported car signals to co-religionaries that the prosperity gospel is working.
Importantly, in Brazil, ‘everything is syncretised and miscegenated,’ explains Alencar, so although in doctrinal terms the gulf between Pentecostal and neo-Pentecostal is ‘abyssal’, in practice it is hard to draw clear lines. Moreover, Baptist, Adventist and even Catholic churches are undergoing pentecostalização , adopting charismatic or revivalist features. The prosperity gospel component cuts across many of these complicated lines, a result of the emphasis on competition, individualism and economic ascent typical of neoliberal societies.
At the Ministry of the Faith church in Brasília, Brazil, September 2018. Photo by Ueslei Marcelino/Reuters
But ultimately, for all the variety, the growth of evangelical Christianity in a society as unequal as Brazil is a phenomenon of the poor and working class. Conversion and dedication promises – and, in some cases, delivers – a better life: not just money, but also in terms of relationships, family and especially health. Belief functions as a para-medicine, be it directly through faith-healing, through the belief, determination and support to beat addiction, or simply through the provision of psychological support. In the words of Davis, it is a ‘spiritual health-delivery system’. This is the reason why evangelicals tend to be urban, young, Black or Brown women, from the least schooled strata, with the lowest salaries. It is, as Davis put it, ‘the largest self-organised movement of poor urban people in the world.’
U topian visions have attached themselves to Brazil and informed its self-conception from its European discovery through to the 20th century. Perhaps it was a coincidence, but in Thomas More’s Utopia (1516) news of a distant paradise was brought by a Portuguese sailor. Brazil was Utopia realised. As Patricia Vieira puts it in States of Grace: Utopia in Brazilian Culture (2018), it presented a ‘fantasy of easy enrichment, grounded on the perception of the region as a treasure trove of natural wealth.’
For one 17th-century Jesuit priest, the land demarcated on the east side of the Treaty of Tordesillas would be the ‘Fifth Empire’, a new kingdom of perpetual peace, where people would live in mystical communion with God, and all would have equal rights. Gradually, messianic and theologically informed visions would give way to secularised ones.
Curiously, Brazil is the only country whose demonym finishes in the -eiro suffix in Portuguese. So you have the Franc ês , the Argent ino , the Americ ano , the Israel ense … but the Brasil eiro . It suggests an occupation, like marcen eiro (carpenter), pedr eiro (bricklayer), min eiro (miner). To be Brazilian was not a state of being , but an activity, a doing . It was the Portuguese and other Europeans who went off and ‘did’ Brazil – exploited its land.
The Indigenous hero is lazy – a ‘trait that Brazilians should embrace and consciously cultivate’
So the Brasileiro is one committed to the project of Brazil, they are not a mere natural feature of the land. But this also speaks to a rapacious pattern of Brazilian development, characterised by using and discarding, rather than building and consolidating. It is a subjectivity evocative of Max Weber ’s ‘capitalistic adventurer’; a figure who would ‘collect the fruit without planting the tree,’ as Holanda put it. The utopian tangles with its opposite. Are we dealing with transformation or exploitation? Is the one who works the land subject or object?
Rejecting the exploited and exploiter dichotomy, a different utopian vision fixated on the independent, noble savage, free from work. The Índio was celebrated by Brazilian Romantics and modernists alike. In Macunaíma (1928), Mario de Andrade’s landmark novel mixing fantastical and primitivist elements, the eponymous Indigenous hero, a ‘hero without any character’, is above all lazy – a ‘trait that Brazilians should be proud of, embrace, and consciously cultivate,’ according to Vieira. But at issue is not really laziness but ócio – idleness. The Portuguese word for business is negócio , or the negation of idleness ( neg-ócio). So, Vieira argues, the ‘business‑as‑usual work mentality of the capitalist world is at odds … with the primeval ócio of Brazilian Indigenous communities…’
The modernist poet Oswald de Andrade likewise foresaw a coming Age of Leisure, enabled by technology. In this egalitarian, matriarchal disposition, Brazil could be at the forefront of nations, showing the way. Civilising work, negócio , had been done; soon the dialectic would swing back to a paradisaical ócio .
In practice, the Índio and the adventurer were locked in conflict, but they jointly stood in contrast to the avaricious European bourgeois. It is for this reason that Holanda’s Brazilian archetype of the cordial man is, as the sociologist Jessé de Souza puts it, the ‘perfect inverse of the ascetic Protestant’.
T oday’s Brazilian evangelicals are likewise not Weber’s northern European protestants. Their worship is emotional, not intellectual, filled with magic, rather than structured by reason. But pecuniary accumulation appears to unite them.
As the Left-leaning Brazilian philosopher Roberto Mangabeira Unger has noted, these are the people who ‘[go] to night school, struggle to open a business, to be an independent professional, who are building a new culture of self-help and initiative – they are in command of the national imaginary.’ A few years ago, when asked about Left-wing rejection of the entrepreneurial, evangelical sector, Unger replied that the Brazilian Left should not repeat the ‘calamitous trajectory’ of their European counterparts in demonising the petty bourgeoisie and distancing themselves ‘from the real aspirations of workers’.
This neo-Pentecostal consumer-capitalist utopia is necessarily authoritarian
The ‘neo-Pentecostal movement today flourishes in a context of dismantling of labour protections,’ argues Brazil’s leading scholar of precarity, Ruy Braga. This requires less a methodical dedication to work, and more the neoliberal self-management typical of popular entrepreneurship. We are dealing not with the Protestant work ethic, but with an evangelical speculative ethic . Quantification becomes the criteria of validation, be it for believers or churches competing in the religious marketplace. ‘Blessings are consumed, praises sold, preaching purchased,’ as Alencar puts it.
Whether this is mere capitalist survival or somehow utopian depends on whether you agree with the Catholic theologian Jung Mo Sung’s assertion that evangelicals insert a metaphysical element – perfectibility; the realisation of desire through the market for those who ‘deserve’ it – into mundane society. For a critic of the prosperity gospel like Sung, this neo-Pentecostal consumer-capitalist utopia is necessarily authoritarian. Divine blessing – manifest through the crente ’s increased purchasing power – is bestowed as a result of the believers’ spiritual war against the enemies of God: the ‘communists’ and the ‘gays’.
The ‘communists’ (who might in fact just be centrist progressives or Catholics) want to give money to the poor; these in turn may be sinners (drug users or traffickers, for instance). This goes against the way that God distributes blessings, which is to favour, economically, those who follow the prosperity gospel.
A ccording to most accounts, a unifying element in the evangelical cosmology is the confrontation between good and evil. The fiel (faithful) encounters a binary: the ‘world’ (sin, violence, addiction, suffering, evil – the Devil around every corner) vs the ‘Church’ (the negation of all that). This code is efficient in affording psychic peace to those facing a complex, rapidly changing world.
How stark is the contrast with earlier self-understandings of Brazilian culture in which ambiguity prevailed! Brazil apparently lacked a moral nexus (as the historian Caio Prado Jr saw it in the 1940s), it was a society of ‘corrosive tolerance’ (according to the literary critic Antônio Cândido in the 1970s) or represented a ‘world without guilt’ (said another literary critic, Roberto Schwarz, in the 1980s).
Outsiders, too, remarked on the absence of moral depth and pure religion. Two 19th-century American missionaries, James Fletcher and Daniel Kidder, lamented in Brazil and the Brazilians (1857) that that this natural paradise could have been a moral paradise, were it not for the fact that tropical Catholicism was superficial, pagan, and hung up on feasts and saints. North Americans of the time learned that the Brazilian was ‘amiable, refined, ceremonious’, but also that the absence of stricter moral codes led him to be ‘irresponsible, insincere and selfish’.
The emblematic Brazilian figure, another archetype, is the malandro , or trickster, slacker, scoundrel. Identified by Cândido in his reading of the 19th-century novel Memoirs of a Militia Sergeant , the malandro flits between the upper and lower classes, between order and disorder, and operates on the presumption of an absence of moral judgment, sin and guilt. He does not work full-time, but nor is he a full-time criminal, nor a slave. He gets by on his wits and adapts. For Vieira, the ‘relaxed, leisurely lifestyle of the malandro , which represents the quintessentially Brazilian way of being-in-the-world, generated a society where regulations are lax, and so can be easily bent to accommodate different customs and traditions.’
Conversions are negatively impacting samba schools, with the born-again quitting carnival
The malandro is at home in carnaval , which brackets real life, allowing for play, for freedom and fantasy. In Roberto DaMatta’s classic 1979 study, the festival is a subversive, free universe of useless activity – something that looks like madness from the perspective of capitalist work ideology.
In this light, Brazil’s great religious transition represents a cultural revolution. Evangelicals interrupt the ‘utopia’ of the idle Índio or the malandro at play in carnival. Firstly, they disdain idleness in favour of entrepreneurial activity and rigorous self-discipline. Secondly, and more directly, they scorn carnival itself. As the leading Pentecostal pastor Silas Malafaia puts it, carnival is a pagan feast ‘marked by sexual licentiousness, boozing, gluttony, group orgies and a lot of music.’ This is felt at the grassroots. Folha de S Paulo reports on how conversions are negatively impacting samba schools and other musical groups, with the born-again quitting carnival.
They say Pentecostalism and neo-Pentecostalism owe their success to their adaptability to local contexts. But, at a minimum, these doctrines’ implantation in foreign soil gives voice to deep changes in the receiving culture, and at a maximum may even serve to transform it. If toleration, moral ambiguity and easy-going malleability were central to a Catholic-inflected Brazilian identity, what will an evangelical Brazil look like?
I n The Making of the English Working Class (1963), E P Thompson comments that Methodism prevented revolution in England in the 1790s. Yet it was indirectly responsible for a growth in working people’s self-confidence and capacity for organisation. Could something similar be said for Brazilian evangelicals, whose self-starting community-building, at a minimum, could be looked at sympathetically for reconstructing associational life?
The Canadian political scientist André Corten, who taught and researched across Latin America, remarked that ‘the failure of secularised Utopias makes the persistence of theologised Utopias come to light.’ Pentecostalism, as a sect, is one such utopianism. It withdraws to an ‘elsewhere’ in social space, refuses to compromise with the social world, and is therefore ‘anti-political’. There is a popular-democratic thrust to this: no deference to a professionalised clergy, but rather a horizontal ordering of the faithful.
A comparison with revolutionary-democratic liberation theology is illuminating. Insofar as they construct the category of ‘the poor’, both liberation theology and Pentecostalism are discourses about suffering. But Pentecostalism privileges emotion in the place of cognition, glossolalia (speaking in tongues) in the place of equality of speech, and – crucially – it is a religion of the poor, not for the poor. It disdains poverty.
Evangelical churches ‘transform people who were born as subaltern – not just poor but also convinced that their social role is to be poor – and they are reborn: they come to understand themselves as equal to other people,’ argues Spyer. They seek to turn their back on poverty and change their lives so as to improve their station.
How does this relate to secular utopianism? It doesn’t. This democratic-popular component cannot be recycled by the Left, nor by conservatism; evangelicals may refuse infeudation to a category of scholars but, simultaneously, the intolerance and despotism of custom connote authoritarianism. This is a movement that is ‘at once egalitarian and authoritarian’, says Corten.
Is this not the obverse of the hegemonic culture, of progressive neoliberalism? Our societies are, prima facie, egalitarian: most forms of elitism and snobbery are ruled out, and we are tolerant of difference and accepting of minorities, because everything is relativised in a consumer society. But, in practice, there is a deep inequality of income, wealth, power and even recognition.
As evangelical Christianity ballooned, it would leave behind the anti-politics of the sect
So even if we are to conclude that the evangelical wave contains no utopian seeds, it is at the very least countercultural. Indeed, it was, as Alencar put it to me, ‘contestatory from the start: in their social behaviour, ways of greeting each other, their clothes, music, sport, life…’ But this was always a ‘force of transformation with no intentionality’, says Corten, making its logic distinct from the utopian ideologies of the Left.
In any case, as evangelical Christianity ballooned, it was always going to leave behind the anti-politics of the sect. Corten sketched out three political trajectories that might take shape.
One is assimilation : adapting to the reigning order of society. In formal politics, this is represented by evangelical political parties or cross-party benches behaving in physiological fashion – a term from Brazilian politics that means to become part of the organs of the state, with all the clientelism and corruption this entails.
The happy-clappy neo-Pentecostal churches like Bola de Neve would likewise represent a certain assimilation. Embourgeoisement, for evangelicals, represents not just certain churches becoming middle class, but questions over the professionalisation of the clergy, whether pastors should be paid a salary. These frictions are currently playing out among the faithful, with heated debate within churches – and competition between them.
A second entry point to politics is manipulation : this consists in evangelical leaders letting believers think that they continue to be ‘unacceptable’ while playing the political game. This might accord with the authoritarian thesis, whereby evangelical ‘despotism of custom’ fits seamlessly with secular authoritarian rule.
The third door leads to messianism . This would present the most obvious threat to liberal democracy, not (only) because it would be a species of authoritarian populism, but because ‘the solution to the conflict they displace outside themselves is sought in a “supernatural” outcome,’ argues Corten.
C ritical theologians join with much Left-wing opinion in denouncing the falseness and shallowness of evangelical Christianity in its guise as prosperity gospel. Forget countercultural stances, let alone utopian visions, evangelicals are fully subsumed by contemporary capitalism! Worse still, they sustain intolerant, socially conservative attitudes!
But even this may be changing. The newsweekly Veja reports that evangelicals today ‘want to participate in the institutional decisions of their faith communities, aim for more democratic and transparent environments, and are much more flexible in behavioural matters.’ And for all the community-building of proletarian Pentecostals, the number of ‘unchurched’ is growing. In tandem, the number of evangelicals who belong to ‘undetermined’ churches is growing at the same rate as evangelicals as a whole. This would be testament to an even more total victory of the forces of commodification, atomisation, reification.
In the same river swims the data on secularisation. Those professing ‘no religion’ are increasing, reaching a plurality (30 per cent) among young people in the megalopolises of São Paulo and Rio – but these people mostly do not identify as agnostic or atheist. Indeed, 89 per cent of Brazilians ‘believe in God or a higher power/spirit’, according to the latest Global Religion survey from Ipsos.
The trend, then, is for belief without belonging, toward an individualisation of faith and the adoption of eclectic, personalised beliefs used to sustain, justify or comfort the individual subject in a competitive, anomic world. The sectarianism of the closed-off world of believers awaiting the eschaton has been corroded by the fissiparousness of liquid modernity.
Others suggest that there remains a contestatory edge to evangelicals. The anthropologist Susan Harding finds a forcible strain of anti-victimhood in Pentecostal and neo-Pentecostal churches. Indeed, this is why progressives disdain evangelicals, because, unlike other groups, they don’t see themselves as victims of the system. They are financially motivated and seek to better themselves, in contrast with the exoticised or culturally relevant poor (Indigenous communities or practitioners of Afro-Brazilian religions, for instance). For the middle-class progressive, distaste for the evangelical is mere demophobia, a rejection of the urban poor, particularly when they organise themselves.
The web of evangelical churches may represent genuine social power
True as this may be, anti-victimhood tangles in complex fashion with ressentiment, a sense of being unfairly judged or treated. In turn, this is leveraged by evangelical leaders and conservative politicians. This aspect culminates in a seeming vindication of Corten’s manipulationist theory: swampy corruption and authoritarian instincts meld with apocalyptic themes. It is a confluence that was especially evident under Bolsonaro, and the only question now is whether the constellation of forces that regrouped around him will unify again.
What isn’t going away is the social presence of evangelicals as such. But as they expand towards a plurality of the population over the next decade, internal differences and divisions will grow. Neither their politics nor their politicisation is a given. Indications from the US are that evangelicals are retreating from politics, having occupied centre-stage in the 1990s and 2000s. If religion is meant to provide solace, but becomes yet another site in which antagonisms rage, either you need to quit religion or your religion needs to quit politics.
Still, the social infrastructure represented by what is ultimately a mass movement of the poor is remarkable. The web of evangelical churches may represent genuine social power. Whether it is a carrier of mainstream capitalist values of entrepreneurship and speculation, or an anti-politics of refusal, or something else entirely, remains to be seen. Capitalism’s contradictory tendencies towards individualism and collectivity play out in full here. Brazil’s religious transition is a case of both at once.
In Who Are We? (2004), the political scientist Samuel Huntington warns that Hispanic immigration would transform US culture into something more Catholic, with a consequent demotion for Anglo-Protestant work ideology. One should not see in the advance of Pentecostalism and neo-Pentecostalism in Brazil an opposite movement. We are not simply faced with a pendulum swing from leisure to work – nor, needless to say, a utopian overcoming of that division.
Instead, urbanisation without industrialisation has created a social landscape of low-key civil war. The war of all against all finds its ideological correlate not in a Protestant work ethic but in the speculative-entrepreneurial ethic of evangelicals. In a terrible duality of overwork and worklessness, a speculative leap towards prosperity looks like the only escape. And this obtains whether one follows the rigours of evangelical dedication, studying, setting up a microbusiness on credit – or turning to a life of crime. There are plenty of cases where it’s both.
Finally then, evangelical Christianity may be the form that popular ideology takes in a context of precarity, after old utopias have dried up. All that remains is a utopia in the sense that Theodor W Adorno discussed: not as a positive social vision, but as the absence of worldly suffering. Adorno, though, was mistaken: he conflated the secular notion of freedom (liberation of our finite lives) with a religious notion of salvation (liberation from finite life).
It is the former utopianism that is lacking today – that which drags us along and keeps us walking forward. We need not surrender to the grinding banality of capitalist life for the sake of ‘realism’, nor endow tawdry capitalist creeds with the name ‘utopia’. We need only note that the desire for transcendence exists – it is manifest, in both earthly and metaphysical aspects. The worldwide explosion of Pentecostalism should give us pause, and act as an injunction to invent secular transcendence once more.
Translations from Portuguese sources are the author’s own."
The Indigenous faith that reveres its own alphabet as sacred | Aeon Essays,,https://aeon.co/essays/the-indigenous-faith-that-reveres-its-own-alphabet-as-sacred,"In July 2019, I visited Siang Sawn, a small village in Chin State, western Myanmar. Sparsely populated, mountainous and underdeveloped, Chin State is one of the least accessible regions in Myanmar. In the monsoon, roads across Chin State – mostly dirt lanes – turn into pools of sludgy mud, extremely toiling to traverse. Stunning mountains shrouded in clouds dominate the horizon.
On a rain-soaked monsoon afternoon, I was in Siang Sawn to learn about Laipianism, a local religion practised in Chin State. It is one of the last surviving, well-organised Indigenous faiths that emerged in the early 20th century as a response to the spread of Christianity in colonial Southeast Asia. Siang Sawn is considered the ‘spiritual homeland’ of Laipianism, a religion that has only about 5,000 followers. In the overwhelmingly Christian Chin State, this remote village is an exclusive home to its followers. With a population of a little under half a million, the Chin people (also called Zo) are considered a taingyinthar – ‘Indigenous’ race – in Myanmar. At least 90 per cent of the Chin adhere to one or another denomination of Christianity. The rest follow Theravada Buddhism, Burmese nat cults, and Laipianism. Home to 300 people, Siang Sawn is a self-sufficient pastoral community of farmers who cultivate paddy, keep kitchen gardens and rear animals.
Walking past a few evenly spaced, bungalow-like brick houses through the main street in Siang Sawn, I came upon distinctive Laipian religious architecture: a dome-shaped mirrored building that housed an effigy and heavenly portraits of a revered man, Pau Cin Hau. The building is a place of worship, and Pau Cin Hau’s portraits also adorned the doors of each house.
A Laipian hall of worship in Myanmar. Courtesy Kam Suan Mang
A Chin healer and dreamworker, Pau Cin Hau (1859-1948) was the founder of Laipianism. The religion is believed to have originated with a series of dreams he had in 1900, in which he would see an elderly man with a radiant halo around his face. In one of the first dreams, the iridescent, saintly figure hands Pau Cin Hau a book containing certain symbols and teaches him to make certain shapes. Then, over a period of two years, he sees various symbols come afloat in recurring dreams. With these symbols, in 1902 Pau Cin Hau came up with a logographic script for the local Chin language – the first time the spoken language was rendered in writing, and a watershed moment in Chin history. Later, he simplified the script into an alphabet consisting of 57 characters. The script came to be known as lai , which means ‘reading and writing’, ‘script’ and ‘literature’ in the Chin language. The invention of the script earned Pau Cin Hau the moniker Laipianpa, meaning ‘the script creator’.
Pau Cin Hau, in longyi and donning a turban, (second from right) in an undated photo with colonial officials and Christian missionaries. Courtesy Kam Suan Mang
Pau Cin Hau said that the haloed figure he’d been seeing in his dreams was Pathian (also spelled Pasian), ‘the creator of heaven and earth, the healer of all diseases.’ And that Pathian had instructed him ‘to spread the message that the Chin people should worship only Pathian, not any other spirit.’ Soon, Pau Cin Hau started preaching monotheistic teachings to worship one God, Pathian. Locals would come to call Pau Cin Hau’s teachings Laipian Pawlpi, ‘the religion of the script creator’.
I n traditional Chin cosmology, people believed in a number of spirits of nature collectively called dawi , like the nat spirits of the Burmese . The greatest of these dawi spirits was Khuazing. However, the Chin also had a somewhat vague concept of an all-powerful higher god whom they called Pathian. A British military administrator in the region, Thomas Herbert Lewin, popularly known among the Chin as Thangliana, recorded a Chin man’s religious beliefs in his book, Wild Races of South-eastern India (1870). The Chin man told Lewin that they believed in two gods: Patyen (Pathian) and Khozing (Khuazing). Pathian ‘is the greatest: it was he who made the world’. The other god, Khuazing, ‘is the patron of our tribe, and we are specially loved by him,’ the man insisted. ‘The tiger is Khozing’s house-dog, and he will not hurt us, because we are the children of his master.’
From this description, Khuazing seems to fit the definition of what the anthropologist Marshall Sahlins calls a ‘metaperson’, found in traditional cosmologies worldwide. In The New Science of the Enchanted Universe: An Anthropology of Most of Humanity (2022), Sahlins defines metapersons as ‘other-than-human persons endowed with greater-than-human powers’ who own beings, places and resources, and determine human fate in everyday life. Khuazing is thus a metaperson of Chin cosmology who owns the forest – hence the tiger is his house-dog. The Chin made offerings to Khuazing and his subordinate dawi spirits in spots near water sources, beneath trees, at hallowed rocks, in forests and at the entrance of villages. This was an ‘immanentist ontology’, a worldview defined by the attempt to call upon a supernatural power to assist life in the here and now – to ensure wellbeing, to make the fields fertile, and the sick healthy. This power was seen to be the gift of metapersons like Khuazing.
In 1902, Pau Cin Hau started preaching that the Chin should stop making sacrifices to Khuazing and other dawi spirits – the immanent metapersons or gods – and should instead pray solely to Pathian, the transcendent God. This new theology preached by Pau Cin Hau, albeit built on traditional Chin cosmology, was a form of transcendental revolution. In the book Zo People and Their Culture (1995), the historian Sing Khaw Khai states that: ‘It was Pau Cin Hau who proclaimed that the [Pathian] were only one God. He announced that [Pathian] was the supreme being who created the universe … [Pathian] stood for God and all other living divinities were collectively referred to as Dawi .’ Pau Cin Hau preached that, by paying obeisance to God (Pathian), any threat from the dawi spirits was averted. Thus, the Pau Cin Hau movement was indeed an example of a switch to transcendentalism.
By the 1930s everyone who was not a Christian was a follower of Pau Cin Hau’s religion
The move from immanentist practices towards transcendentalism is a global phenomenon in history. Sahlins argues that the ‘enchanted universe’ of immanentist religions largely gave way to transcendentalist religions that arose in much of the world in the Axial Age of the final centuries BCE. In Unearthly Powers: Religious and Political Change in World History (2019), the historian Alan Strathern shows that the switch to a transcendental religion is often a result of conquest and contact with monotheistic cultures. In the case of the Chin Hills – conquered by the British in the early 1890s – this shift may also have been triggered by colonial contact and conquest. Arthur and Laura Carson, the first Christian missionaries to the Chin people, landed in Haka, the capital of the Chin Hills, in 1899 – a year before Pau Cin Hau claimed to have had the first of many of his dreams about Pathian, his teachings and the script.
Chin society was initially reluctant to accept new religious ideas – both Christianity and Pau Cin Hau’s teachings. While the Carsons got the first convert after five years, in 1904, Pau Cin Hau started to attract the first of his followers only around 1906. ‘I stood alone in my faith for three years during which time the members of my own family, even, reviled instead of encouraging me,’ Pau Cin Hau told J J Bennison, the superintendent of census operations in Burma, recounting the early days of the movement. ‘But gradually, as my neighbours and even people from distant villages saw me still enjoying sound health, my religion began to spread, until after six years people from all parts of the hills became my fellow worshippers.’
In spite of its initial rejection, Pau Cin Hau’s religion effectively became, in just a few decades, the traditional Chin religion, outnumbering Christians at the time. In 1967, E Pendleton Banks, a Harvard-trained anthropologist who studied Laipianism, noted: ‘The usual reply of a non-Christian to the inquiry of a government official or a census taker about his religious affiliation was “Pau Cin Hau”.’ Thus, by the 1930s everyone who was not a Christian was a follower of Pau Cin Hau’s religion.
T he first anthropologist to write on the Pau Cin Hau movement, H N C Stevenson, contended in his thesis , submitted to the University of London in 1943, that the primary appeal of Pau Cin Hau’s religion over Christianity rested on the fact that Pau Cin Hau allowed his followers to drink zu , the traditional alcoholic drink that the Christian missionaries sternly forbid.
While it began as an Indigenous reform movement, Pau Cin Hau’s movement would soon compete with Christian missionaries as proselytisation intensified in the Chin Hills. Somewhat paradoxically though, the Pau Cin Hau movement also laid the groundwork among the Chin for the spread of Christianity – the religion that the majority of Chin practise today. Two modern Zo scholars, the historian Pum Khan Pau and the theologian Philip Cope Suan Pau contend, in the former’s words , that ‘the early success of the Pau Cin Hau movement facilitated the growth of Christianity’. Once Chin society accepted Pathian as the supreme being following Pau Cin Hau’s teachings, it became easier for the missionaries to communicate the Christian gospel. Over time, ‘Pathian’ would become the name of the Christian God through a process of semantic reconfiguration of the term. Locals actively participated in the process of missionary translation through the Pau Cin Hau movement. Pau Cin Hau’s monotheistic teachings had already popularised the idea of one supreme being, Pathian, and made the community ready to accept the missionary translators’ semantic reconfiguration of the term to refer to the Christian God.
‘The Sermon on the Mount’ in Pau Cin Hau script. Photo Bikash K Bhattacharya
Christian missionaries themselves nonetheless held an ambivalent view of Pau Cin Hau’s religion. Some saw the shift to monotheism introduced by Pau Cin Hau as an awakening and a precursor to acceptance of Christianity, while others thought of it as an impediment to the spread of Christ’s gospel. The 33rd Annual Report of the British and Foreign Bible Society (Burma Agency) in 1932 stated that Pau Cin Hau’s ‘worship of one Creator God seems to be drawing near to genuine Christian ideals … with sympathetic and wise leadership this indigenous and spontaneous quest after higher things can be turned into a definite movement towards Christianity.’ On the other hand, Joseph Herbert Cope, a Baptist missionary stationed in the Chin Hills at the time, maintained that hundreds of Chin were ‘wasting their time’ on Pau Cin Hau’s script and his ideas.
There are certain structural similarities between Laipianism and Christianity
Some scholars consider Pau Cin Hau’s religion a version of local Christianity. In 1943, the anthropologist H N C Stevenson wrote that ‘the [Pau Cin Hau] cult was an indigenous variation of Christianity better suited to the local conditions.’ And in 2013, the historian Bianca Son Suantak wrote in her thesis at the School of Oriental and African Studies in London that Pau Cin Hau’s religion was a ‘carbon copy’ of Christianity.
Despite the views of these scholars, Elizabeth S Cope, who served as a missionary in the Chin Hills for 30 years and whose husband was the pioneer missionary Joseph Herbert Cope, stated that Pau Cin Hau was not a convert and indeed always opposed the missionaries. Thang Za Dal, a contemporary Chin intellectual and the author of The Chin/Zo of Bangladesh, Burma and India: An Introduction (2014), doesn’t agree that Pau Cin Hau’s religion is a version of Christianity. ‘There is really nothing about the Bible, Jesus Christ or the fullness of Christian ethical and moral teaching in Pau Cin Hau’s doctrine,’ he writes. ‘It has its own scriptures consisting of six books called Bu , all written in Pau Cin Hau script, that outlines its religious-spiritual system,’ Further, in an interview in 2020, Dal told me that researchers don’t take into account the practices of present followers of Pau Cin Hau’s religion – and thus ignore ‘ethnographic realities’ – which leads them to erroneously conclude that Laipianism is a version of Christianity.
That said, there are certain structural similarities between Laipianism and Christianity. Much like the Christian discourse of healing, Pau Cin Hau articulated the relationship between Pathian and his followers in a framework of healing: he called Pathian ‘the healer of all diseases’ and the religion he was preaching ‘a way of curing sickness’. In some contexts, this characterisation was metaphorical, but elsewhere it was just as much literal. Supplication of Pathian and living according to the teachings of Pau Cin Hau are thought to be genuinely curative. The anthropologist E Pendleton Banks speculated that Pau Cin Hau might have been exposed to the core ideas of Christianity as well as to the healing practices of a travelling missionary physician. ‘Here was a readymade conjunction between monotheism and curing,’ he wrote. Besides, just like Jesus is considered the son of God with healing powers, so is Pau Cin Hau seen as the chosen son of Pathian with power to cure diseases.
T o understand the current religious practices of Laipianism, I met with Kam Suan Mang in Siang Sawn. In his late 50s and stockily built, with a round face and black hair, he held the post of Laimang, meaning ‘script king’, the title of the religious head of the Laipian community. He lived in a two-storey brick bungalow painted in pink, with a terrace on the upper storey that offered a commanding view of the village. The walls of his house were filled with several paintings of Pau Cin Hau and framed photos of the Pau Cin Hau script. Kam Suan Mang was chosen as Laimang in 1995 in a series of dreams received by several members of the community.
‘We observe different holidays throughout the year, such as Pathian Saints’ Day [1 December], Pathian Servants’ Day [21 November], Pathian Hymns’ Day [21 February], among others,’ explained Kam Suan Mang. He looked sharp in a white long-sleeve shirt and a longyi decorated in traditional Chin patterns. ‘In Siang Sawn, these days are recognised as holidays, and you can devote yourself to worship. Worship entails singing of hymns composed by Pau Cin Hau and his early followers, and making prayers to Pathian.’ He continued: ‘If you ask me what Pau Cin Hau religion is all about, I’d say it is the faith in Pathian as the almighty creator, and the teachings of his chosen son Laipianpa Pau Cin Hau. In terms of everyday conduct, our beliefs emphasise what we call “holy behaviour” and it entails practising justice, harmony, discipline, peace and hygiene.’
The religion is not related to Christianity by any stretch of imagination although one might find some superficial similarities, he further asserted. ‘We have always been defined [by others] in terms of what we are not, and in relation to Christianity. It is often overlooked that Zo cosmology already had the concept of one God Pathian. It is Christianity that appropriated the concept once it was popularised by Pau Cin Hau.’
Chin society must have seen value in the written technology brought in by the missionaries
Perhaps what makes Laipianism truly unusual (and sets it apart from many other religions – including Christianity – in a peculiar way) is the emphasis placed on the Pau Cin Hau script both as an icon and an index: the script would not only be used to write its scriptures, but pictures or inscriptions of letters from the script would adorn the sect’s places of worship and homes of the followers, just as the Holy Cross adorns Christian churches. In Siang Sawn, as I left Kam Suan Mang’s place on that warm afternoon and walked along the pebbled street in the middle of the village, I noticed that each house had pictures of the script hanging above the door, often accompanied by a picture of Pau Cin Hau.
Writing in 1967, Banks had suggested that Pau Cin Hau’s script is a case of ‘stimulus diffusion’: a local adaptation of the missionary idea of the centrality of the text in preaching the gospel, drawn from the Protestant theological doctrine of ‘ sola scriptura ’. He tried to theorise Pau Cin Hau’s religion through Anthony F C Wallace’s concept of ‘revitalisation movements’, which are ‘deliberate, conscious, organised efforts by members of a society to create a more satisfying culture.’ Chin society must have seen value in the written technology brought in by the missionaries. Which was why, Banks argues, Pau Cin Hau and his followers adapted the idea of writing and came up with a script of their own. It is worth mentioning that mission activities were essentially centred around acts of translation: in fact, the religion scholars Brainerd Prince and Benrilo Kikon have argued that the mission is translation. Pau Cin Hau understood that – especially the centrality of writing in missionary translation activities. He knew that, if his teachings were to gain the attention of the people, he too needed to have the technology of writing like the colonial missionaries, a wonder that had stirred immense curiosity among many Chin.
There is a popular narrative among Laipian followers that explains what propelled the script to become the central symbol of their religion. It goes like this: when the British administrators and Christian missionaries rendered the Chin language into writing for the first time in the Latin script in the 1890s, Pathian was gravely displeased; and he revealed through Pau Cin Hau the ‘true script’ that can accurately render into writing the tongues of the Chin people.
The terminologies used to describe the hierarchy of the religion also suggest the importance of the script in Laipianism: the highest spiritual position in the community is called Laipianpa , ‘the script creator’, and the next role directly underneath that is Laimang , ‘the script king’. While Laipianpa is a position reserved only for the late founder of the religion, Pau Cin Hau, and no one else can take it, the person to hold the second-highest rank, Laimang, is believed to be chosen by Pathian from time to time.
I n the decades following Pau Cin Hau’s death in 1948, Laipianism gradually lost followers to Christianity, the religion that more than 90 per cent of the Chin now follow. The total number of Pau Cin Hau followers has declined from 37,500 in 1931 to around 5,000 today. The historian Pum Kham Pau argues that this shift started once the elites of Chin society started to see Christianity ‘as an alternative source of health and power’. Consequently, it paved the way for the conversion of the common people to Christianity.
The most commonly used illustration of Pau Cin Hau in Laipian religious settings. Courtesy Kam Suan Mang
Today, Chin state remains one of the poorest regions of Myanmar. In a remote mountainous area neglected by the state, Christianity provides a range of services that Laipianism can’t: English-language education, medical care and, to some extent, employment opportunities. These opportunities provided by the Christian churches of various denominations have played a very crucial role in attracting people to Christianity.
In Siang Sawn, Kam Suan Mang and the elders are trying to preserve their faith. In 2013, they published a booklet, History of Pau Cin Hau’s Siang Sawn Religious Sect , outlining the history of the Laipian community in the English language for the first time. ‘The idea behind setting up an exclusive Laipian settlement was to revive the religion and create a spiritual homeland where followers can live according to the teachings of Pau Cin Hau,’ Kam Suan Mang said. One important belief held by the sect is ‘to be in conformity with the passing of time,’ which, he explained to me, means adaptability in the face of challenges. ‘We are open to revising and adjusting our beliefs and practices when necessary to incorporate contemporary values and to be in conformity with the passing of time.’
As a silver lining, the Pau Cin Hau script has garnered wider interest among linguists in the past two decades. The script was added to the Unicode Standard in June 2014, and it is increasingly being used by the Laipian community for everyday communication. ‘Now we use Pau Cin Hau lai to write messages on mobile phones, young people use it to make posts on Facebook,’ said Salai Cin Kang, a resident in Siang Sawn. ‘The script has become one of the most visible markers of our identity on digital platforms.’ Kam Suan Mang has some reservations, though. He says this development has a different – and ‘concerning’ – side as well. As people increasingly use the script – the core symbol of the religion – for everyday communication, there is a risk of its losing sacred status, relegating it to a mundane writing system. Quoting the 13th-century Japanese Buddhist philosopher Dōgen Zenji, Kam Suan Mang said: ‘In the mundane, nothing is sacred.’"
The universal belief in witches reveals our deepest fears | Aeon Essays,,https://aeon.co/essays/the-universal-belief-in-witches-reveals-our-deepest-fears,"If asked, most people in the West would say that wicked witches who fly unaided or turn into animals don’t really exist. And, according to all available evidence, they would be right. It’s more difficult to prove that no one practises ‘witchcraft’, that is, conducts rites or utters curses in an attempt to harm others. Yet regardless of what people say about witches, or even what they believe, the idea of the witch is a universal constant looming over cultures from the islands of Indonesia to the pizza parlours of the modern United States.
Fifty years ago, as graduate students at Oxford, my wife and I were preparing to do anthropological fieldwork on the island of Sumba, in eastern Indonesia. Not long after beginning our research, an elderly ritual expert happened to mention that yet another ritualist – one of his rivals, as it later turned out – had ‘eaten’ a woman, the wife of a third man. This took me aback, in part because the woman in question was still alive. But I soon learned that the old man was accusing his rival of being a mamarung , a witch, who on Sumba is said to cause illness and death by invisibly eating people’s souls. Meeting secretly at night, Sumbanese witches also capture human souls, transform them into sacrificial animals, and then slaughter these to kill their victims and consume their bodies.
Thinking about this after returning to the United Kingdom, I realised that the same accusations of ritual killing and cannibalism were levelled during the ‘witch crazes’ of the early modern period, from the 14th to 17th centuries , in Europe, resulting in the persecution and killing of many of those accused. European witches were also said to feast on human flesh, transform themselves and others into animals, join nocturnal assemblies, and fly through the air. Far more recently, I was reminded of the universal idea of the ‘witch’ by Pizzagate – the QAnon conspiracy theory that Hillary Clinton and other members of a supposed global elite were killing and eating children in secret satanic rites, conducted while operating a paedophile ring in a pizza parlour in Washington, DC. As I read further, I realised that the Pizzagate accusations were recycled versions of allegations levelled during the 1980s and ’90s against owners and employees of US daycare centres who were identically accused of sacrificing children and eating them.
B ut what is a ‘witch’? To prove that a belief in witches really is a human universal, we obviously need a definition. We also need to be clear about what ‘universal’ means. Actually, a definition commonly used by anthropologists, historians and other academics suits well enough. A witch is a human being who, motivated by malice, wilfully harms other people not openly by any physical methods, but by unseen, mystical means. Secret acts of ritual killing and cannibalism – essentially treating people like animals – are typical expressions of the witch’s hatred of humans. For example, witches among the Navaho of the America Southwest were accused of cannibalism, just like witches in New Guinea. Charged with the same horrendous acts, those US daycare workers would simply be seen as a variety of witches. In working through Satan, these rumoured devil-worshippers resemble not only the witches of medieval and early modern Europe, but equally witches described in Africa, Asia, the southwest Pacific, and native North and South America. For not only do non-Western witches kill people and eat them; they are similarly believed to obtain their powers through local demons. To cite one of many examples, Sumbanese witches possess evil spirits called wàndi that they keep inside their bodies and send out at night to attack their victims.
It hardly needs mentioning that I’m talking about ‘wicked witches’, and not the ‘good witches’ familiar to Westerners from The Wizard of Oz , nature-loving Wiccans, or the progressive young women who populate the pages of ‘witch-lit’. Such good witches find an explanation in the history of English, specifically the derivation of ‘witch’ from an Anglo-Saxon word further applied to healers and benevolent magicians. But the important point is that, throughout history and in a great variety of cultures today, people have imagined and continue to imagine thoroughly nefarious figures corresponding to the wicked witch.
Witchcraft reveals our persistent and enduring tendency to imagine the existence of evil people
To say that witches are universal doesn’t mean belief in them has been recorded in all cultures, or that, where recorded, widespread public accusations, witch-hunts or moral panics ensue. Whereas recent accusations of satanism in the West, including Pizzagate, evidently do reveal these features, witchcraft in non-Western societies often does not. For instance, the Hopi people of Arizona never openly accused people of being witches, for fear of retribution. Instead, they believed the evildoers would be punished in the afterlife.
Anthropologists have also described a few societies as being unfamiliar with witches altogether – at least at the time they were being investigated. Or they were familiar with witches but didn’t feel particularly threatened by them, perhaps because they thought they were sufficiently protected by counter-witchcraft magic or the security of benevolent gods. An example are the Tallensi of Ghana, in whose world view the anthropologist Meyer Fortes judged witchcraft to be ‘remotely peripheral’. That is, the Tallensi do not, for the most part, believe that misfortune derives from the wickedness of other people but from the actions of just and all-powerful ancestors, so that illness and death are interpreted as rightful punishment for human wrongdoing.
What the universality of witchcraft does reveal, however, is our persistent and enduring tendency to imagine the existence of evil people, either next door or somewhere in the next valley, who constantly strive to harm us by supernatural means. Thus, the same ideas crop up time and again in places that are otherwise culturally different and geographically distant.
John Pettie Arrest for Witchcraft 1866. Courtesy the NGV, Melbourne.
S eventeenth-century Massachusetts (including Salem) provides the most popularised example of witchcraft. English immigrants, many of them Puritans, faced the challenge of a harsh existence in a new land, surrounded by sometimes hostile native peoples, and riven with religious and political divisions. They found themselves needing to account for both misfortune and the fact that some succeeded while others failed. To do so, they invoked witchcraft beliefs imported from their native England and identified particular neighbours who were reputedly in league with the devil as the cause of their ills. As a result, accusations were made, people were tried as witches, and dozens were executed.
An early illustration of witches on broomsticks from Martin Le Franc’s ‘Le Champion des Dames’ (1451). Courtesy the BNF Paris.
Historians often focus on earlier witch-hunting crazes erupting in 15th-century Europe, after a period of relative indifference. Why did the phenomenon appear so suddenly and then decline with similar rapidity in the 18th century ? Both the Enlightenment and the emergence of modern science during that century and the subsequent Industrial Revolution have been invoked to explain this change. Yet historians have recently documented beliefs in witches and malicious witchcraft, especially among rural Westerners, persisting into the 20th and 21st centuries.
José Guadalupe Posada A witch carrying a child on her broom. C1880-1910. Courtesy the Met Museum New York
Anthropologists, who have studied witchcraft from the early days of their discipline, strive to explain the phenomenon by focusing on social systems. Do accusations of witchcraft reveal social relationships that are ill-defined and likely to give rise to tension? Do they play a role in political or economic rivalry (including among co-wives in polygamous marriages)? Have they been invoked to explain why some people suffer misfortune while others do not?
The inspiration for this approach goes back to the work of E E Evans- Pritchard on the Azande of Central Africa. In his book Witchcraft, Oracles, and Magic Among the Azande (1937), E-P (as he was known to students and colleagues) treated accusations and confessions of witchcraft as essential elements of Zande cosmology and a way of maintaining an orderly social life.
Belief in witches was a way of explaining why bad things sometimes happened to good people
Zande witches were said to embody an evil substance with the supernatural power to harm a person whom the witch, consciously or unconsciously, disliked. Unlike witches in many other cultures, the Zande variants attacked by purely mental means. They simply had to harbour ill will against another, and did not, for example, need to recite spells or deliberately send invisible projectiles to injure a person as is common among witches elsewhere.
A group of abinza (witchdoctors) dancing at a seance (do avure), wearing elaborate dance costumes including rattles, headdresses and magical attachments. c1926-7. Photo by E. E. Evans-Pritchard and courtesy the Pitt Rivers Musuem, Oxford
Suffering an illness or other misfortune, especially one that afflicted only themselves and not others (eg, snakebite or another ‘accident’), a Zande man or woman (or their relatives, if the misfortune was fatal) would then suspect the work of a witch who had it in for them; with the help of a diviner, the suspect’s identity could be confirmed. Claiming they were not conscious of causing harm, the accused often confessed. The two parties were then reconciled, and the victim no longer regarded the accused as a witch. In this way, Evans-Pritchard saw belief in witches as a way of explaining why bad things sometimes happened to people, including good people. Since accusations and confessions could reveal strains in relations between people, this was a way to promote and maintain social harmony as well.
A portrait of Bagbeyo, an elder of the Zande people. Bagbeyo was apparently commonly reputed to be a nakuangua or nangbisi (a witch). Photo by E. E. Evans-Pritchard and courtesy the Pitt-Rivers Museum, Oxford
Later anthropologists followed Evans-Pritchard in interpreting witchcraft as something that served to maintain social systems in a reasonably steady state. This ‘war is peace’ approach found favour despite the social disruption, harm, unhappiness and, sometimes, injury or death of innocents that can follow from openly accusing people of being witches or simply treating them as though they were suspects. What’s more, and in line with a relativist perspective that views human cultures as essentially different from one another, cross-cultural studies came to largely consist of identifying social functions that witchcraft beliefs might perform in particular societies. Identifying someone as a witch might be useful in dissolving or reforming social relationships that are no longer supportable, for example between spouses or co-wives. Or, more generally, belief in witches might be seen as promoting good behaviour, so that people would avoid acting badly towards others for fear of either being accused of witchcraft or having witchcraft used against them.
Because the focus among historians and anthropologists was on single societies, there was little scope for generalisation. And, in any case, it soon became clear that the sorts of people who were accused or suspected of witchcraft varied considerably from group to group. Thus, while globally it is women who are mostly identified as witches, in some societies – including the North American Navaho, and several African societies – most witches are men. In many other places (including Zandeland, according to Evans-Pritchard), men and women are suspected about equally.
Similar variation is found in regard to age, social standing and wealth. Though elderly and impoverished women are suspects in a great variety of places, in some cultures, including the Tlingit and Kaska of Alaska and northwestern Canada and the Bangwa of Cameroon, many of those accused as witches were children. In a witch craze that affected the Kaska during the first two to three decades of the 20th century , dozens of children were put to death in the most horrendous ways, often by members of their own families, thereby depleting the population of this society, which apparently never numbered more than 300 or 400.
The way people make a living also does not determine the occurrence of witchcraft beliefs. Some writers have claimed that witchcraft is absent or rare among small-scale, nomadic or semi-nomadic hunter-gatherer communities with small population densities. But before their integration into modern nation-states, the aforementioned Kaska maintained a belief in witches and anti-witchcraft practices, as did many other native North and South American hunter-gatherers – including the Paiute of eastern Oregon.
R ecognising the deficiencies of earlier sociological approaches to witchcraft, my former supervisor at Oxford, Rodney Needham, proposed a better perspective. In his essay Primordial Characters (1978), Needham argued that, to properly understand the witch, we must investigate particular beliefs about witches that find expression in otherwise quite different cultures and different historical settings. Together, these add up to what Needham called ‘the image of the witch’, a complex of ideas, none of which is necessarily connected with particular forms of social organisation.
Needham calls part of the widespread image of the witch the ‘moral component’: representing witches as the absolute opposite of the moral human being. Not surprisingly, under this heading Needham first mentioned cannibalism, but he didn’t say anything else about the remarkably uniform way witches are claimed to prosecute their nefarious deeds. With few exceptions, witches the world over act invisibly; they transform victims or their souls into sacrificial animals, and attack people by piercing or stabbing them with unseen instruments and inserting harmful substances into their bodies. (Piercing is a major technique among the Kalapalo of Brazil, who call witches ‘masters of the darts’, but it also recalls QAnon’s recent claim that Bill Gates, identified as a member of the ‘global elite’, was promoting COVID-19 vaccination with the aim of injecting microchips into unsuspecting recipients.)
European witches plant crosses upside down, perform rituals backwards, dance counterclockwise
Though Needham did not specify it, an equally prominent expression of the moral component of the image is the way witches in most places act to impede normal processes of human life. Most notably, they counteract human procreation by killing both unborn babies in the womb and infants after birth, all while reputedly engaging in disapproved sexual practices that cannot lead to conception. Female witches in Ghana are supposedly able to turn a woman’s uterus upside down so she cannot conceive, while male witches can steal a pregnant woman’s fetus. In some places, including early modern Europe and among the West African Yoruba and the Mapuche Indians of Chile, witches are credited with stealing men’s penises or destroying their semen.
Other traits Needham lists are similarly widespread across cultures. People around the world describe witches as secretly meeting in groups to kill victims, as in the covens of early modern Europe. They are regularly depicted as operating at night, being able to fly (or levitate), associating or identifying with animals, participating in a wide variety of physical and other inversions, and manifesting as nocturnal lights. Though seemingly trivial, this last trait is ubiquitous. Instances are found in all parts of the world, including North and South America, Europe, Africa, Asia and the Pacific islands. Thus, the Sumbanese of eastern Indonesia often call witches ‘those who glow, shine, or flicker (in the night)’. Similarly, in Ghana, the Twi word for practising witchcraft means ‘to glow’.
‘Inversion’, where witches do things in a way opposite to what is proper or normal, is another idea found just about everywhere. European witches were supposed to plant crosses upside down, perform rituals backwards, dance counterclockwise (the inauspicious direction), and do things with the left hand that should be done with the right – just as latter-day satanists have been described as doing. Outside the West, witches are conceived as equally inverted beings. The Nagé people of the Indonesian island of Flores, among whom I conducted fieldwork between 1984 and 2018, describe witches as dancing in the ‘wrong’ direction during their nocturnal cannibalistic feasts. Nagé witches also sleep with their heads pointing the wrong way (towards the sea rather than inland). Similarly, Navaho and Western Apache witches cast harmful spells by reciting ‘good prayers’ backwards; some witches in India are credited with inverted feet; in East Africa witches walk about upside down; Burmese witches sleep on their bellies rather than their backs; ancient Roman writers described witches as capable of reversing the course of rivers – and the list goes on.
Needham argued that the components need not all occur together, and I agree. For example, if Americans recently accused of engaging in satanism are modern examples of witches, so far as I know they have yet to be credited with flying unaided, turning into animals, or walking on their heads. The absence of such attributed abilities, all of which are, of course, physically impossible according to modern physics, is readily explained by the advent of modern scientific education, which has affected the public discourse and presumably the imaginations of even the most uneducated members of Western societies. Yet, what remains in accusations of satanism are ritual homicide, cannibalism, and hindering human reproduction (by reputedly sacrificing children and promoting abortion).
Surviving accusations may thus reflect only what is empirically possible. Even so, they appear central to the image of the witch. For a start, cannibalism, eating fellow humans, is itself a kind of inversion, as well as being in most places an extreme moral outrage. Even in cultures that practise ritual cannibalism (like some in New Guinea), the cannibalism of witches is morally distinguished by its secret, excessive, uncontrolled and indiscriminate aspects. At the same time, modern Western witchcraft, or satanism, has apparently expanded on the witch’s common preference for victimising children by adding paedophilia to their roster of evil deeds. Not entirely original – since sex with children is a charge laid against witches in some traditional African cultures – this addition surely reflects features of modern child-rearing that are quite specific if not unique to the mass societies of the West.
F rom the arresting series of inversions to manifesting as nocturnal lights to flying unaided, many attributions of witches are encountered so often, in different places and different historical periods, that they cannot credibly be explained as mere coincidences, or something that originated in one culture and simply spread to near and distant others. Some beliefs may have developed independently in different places. But this only begs the question: why should this have occurred? Given the uniformity of the image of the witch worldwide, we can only conclude that it is a product of pan-human psychology.
Needham reached much the same conclusion, but he never took it any further. Drawing on Carl Jung’s concept, he simply characterised the witch as an ‘archetype’, though one that is ‘synthetic’, meaning that it consists of components that need not occur together, and that some people attribute to entities besides witches. As he noted, purely spiritual beings like gods and ghosts can also be inverted, take the form of animals, or be active mainly at night. Yet these commonalities find a ready explanation in a consistent conception of witches as beings with the same supernatural powers as spirits and, simultaneously, flawed humans.
In other words, witches habitually confuse what philosophers recognise as major ontological categories. These comprise essentially different sorts of beings and most notably human beings, nonhuman animals and spirits, which moral humans everywhere keep separate from one another, conceptually and, in some ways, physically as well. People everywhere forbid having sex with animals, while witches are often charged with just that. The Nagé of eastern Indonesia say that such transgressions would turn any individual into a witch.
Curiously, people usually identify social insiders as witches
Admittedly, these observations go only part way to explaining universal witchcraft. If the witch is an inherent tendency of pan-human thought, then its roots must lie deep in human psychology. That said, witchcraft is a complex phenomenon, so certain aspects or components require explanations different from others. Take the series of physical inversions for example. These are products of the universal proclivity to construct metaphors. Being back to front or upside down are particularly concrete, vivid, categorical and psychologically effective ways of representing moral inversion – thinking, feeling and behaving in ways completely opposite to those of ordinary humans, as exemplified by more abstract moral inversions such as sacrificing humans in place of animals, cannibalism, sexual perversion and so on. But calling physical inversions metaphorical is not to suggest they are merely figurative. Another feature of metaphors is that, over time, they can come to be, if not firmly believed, then taken for granted, rarely questioned, and simply accepted as something like fact by the majority of people who habitually use them.
Requiring a different explanatory framework is the foundational belief in the existence of inherently evil human others. Curiously, people usually identify social insiders as witches, at the very least members of the same ethnic group or speaking the same language. Often the accused are also members of the same village or family, and even spouses, parents and siblings. But, in doing so, they represent the suspects as morally alien and inhuman, individuals outside the bounds of humanity and so the exact opposite of ‘people like us’. Therefore, typically high on the list of suspected witches the world over are co-resident slaves (usually descended from war captives), other persons of low rank and, especially when they come from another village (as they often do), wives and their kin.
Linked with the witch’s essential outsiderhood, witchcraft builds on a recognition, unique among humans, of other minds just like our own combined with a countervailing conviction that other people are not exactly like us, and are indeed ‘others’.
One approach traces the hatred and fear to the universal emotional experiences of early childhood. Deriving ultimately from Sigmund Freud, this theory locates witchcraft in an infant’s growing awareness that caregivers exist separately from themselves, and can therefore frustrate as well as satisfy their needs and desires – the first step in a person’s realisation that, to quote Jean-Paul Sartre, ‘hell is other people.’ Because infants find their consequent rage too difficult to bear, the argument goes, they project it onto other people, first the primary caregiver (most likely the mother) and later onto others. The suggestion that the very idea of the witch is rooted in childhood would, in fact, help to explain, via the role that theory assigns to infant caregivers, the global prominence of women among accused witches.
Complementing this approach, cognitive scientists view witchcraft as a product of evolutionary psychology originating among the same Stone Age hunter-gatherers creating emerging religions. A belief in unseen beings is as old as our species, after all, and there’s an enduring psychological attractiveness and memorability to the minimally counterintuitive idea – one that is fantastic yet simple, and not so contrary to common sense that it’s hard to process. If witches are to possess supernatural powers comparable to spirits and engage in practices characteristic of religion, including ritual killing and communal feasting, that only makes sense.
It is no coincidence, either, that contemporary Westerners who accuse others of satanism tend to be evangelical or fundamentalist Protestants, for adherents of these Christian denominations believe in Satan as a being present and active in the world, and in evil as a real power embraced by others who, unlike themselves, are not among God’s elect. According to the evolutionary psychologists, witchcraft belief evolved as a psychological mechanism that aided survival in small-scale Palaeolithic communities by alerting people to the possible existence of internal or external human enemies behaving maliciously in unseen ways.
Like some modern evangelicals, some recent or remaining hunter-gatherers continue to believe there are witches in their midst. These beliefs may no longer serve us, but our species as a whole has survived them – just like we survived the evolution of large brains and heads, the side-effects of which are the life-threatening difficulties women can experience in childbirth. Obviously, isolating the ultimate causes of witchcraft will not immediately solve the social disruption and injustice that accusations (which, in the case of witchcraft, we may assume, are invariably false) can cause. But the more we understand the belief as a tendency inherent in the human condition, the better chance we have of controlling and counteracting its worst effects. At least we’ll have a better idea of what we’re up against."
I now think a heretical form of Christianity might be true | Aeon Essays,,https://aeon.co/essays/i-now-think-a-heretical-form-of-christianity-might-be-true,"I rejected Christianity at the age of 14, upsetting my grandmother by refusing to get confirmed in the Catholic faith of my upbringing. Partly it was intellectual issues: why would a loving and all-powerful God create a world with so much suffering? Partly it was ethical issues. It was a time when I was questioning my sexuality, and it seemed to me wrong not to allow a gay person to flourish through a loving relationship with a partner they are attracted to. But, most of all, Christianity just seemed very unspiritual. I got very little out of boring church services, and it seemed to be all about pleasing the old guy in the sky so you get to heaven. Science and philosophy seemed a more rational way to make sense of life, which ultimately led me to become a philosophy professor.
Despite rejecting religion, I always had a spiritual sense, a sense of a greater reality at the core of things, what William James called ‘The More’. But I would connect to ‘The More’ in my own way, through meditation and engagement with nature. In other words, I was a signed-up member of the ‘spiritual but not religious’ grouping.
And thus I remained for a couple of decades. I was happy in this club. There was no ‘God-shaped hole’ in my life. But, more recently, a few things have changed. The first was intellectual. Most of my fellow philosophers are persuaded either by the arguments for the very traditional idea of God, or by the case for Richard Dawkins-style atheism. I’ve come to think that both sides of this debate have something right.
I n terms of the case for atheism, I remain as convinced as ever that the suffering we find in the universe is powerful evidence against the existence of a loving and all-powerful God. But I’ve also come to think there are powerful considerations in support of something God ish. One is the fine-tuning of physics for life, the surprising discovery of recent decades that certain numbers in physics are, against incredible odds, just right for the emergence of life. The second is psycho-physical harmony, the improbable alignment between consciousness and behaviour that is presupposed in any evolutionary story of the character of our conscious experience. All this was laid out in my recent book Why? The Purpose of the Universe (2023) .
I now think the evidence points towards a hypothesis that John Stuart Mill took seriously: a good God of limited abilities. This hypothesis is able to account both for the imperfections of our universe – in terms of God’s limited abilities – and for the things about our universe that are improbably good, such as fine-tuning and psycho-physical harmony. God would have liked to make intelligent life in an instant, or by breathing into the dust as we see depicted in Genesis. But the only way God was able to create life was by bringing into existence a universe with the right physics that would eventually evolve intelligent life. God made the best universe they could.
The second change was discovering the great diversity of forms of Christianity. Wide reading and conversations with various Christian thinkers have given me a deeper sense of the mystical traditions of Christianity, as well as its radical roots. I haven’t changed my mind on the form of Christianity I rejected in my youth. However, I now think there are forms of Christianity that fit quite well with the limited God I now believe in.
The universe is in some sense inside God, and perhaps God is inside the universe
The final change was coming to see the value of a spiritual community . Being ‘spiritual but not religious’ can be a bit lonely and hard to sustain. Religion involves rituals and practices that bind people together across space and time, marking the seasons and the big moments of life – birth, marriage, coming of age, death – forming a bridge between society and the Divine. I feel happier and closer to the Divine when I can connect to it in relation to others.
The idea of God I received as a child was of something completely separate from the universe. However, there are versions of the God hypothesis that don’t see things in such binary terms. There are pantheists , who think that ‘God’ and ‘the universe’ are simply different words for the same thing. This seems like just atheism repackaged. But there are also pan- en -theists , who don’t quite identify God and the universe, but nor do they think they’re entirely separate. Panentheists believe there is an intimate connection between God and the universe; the two overlap. The universe is in some sense inside God, and perhaps God is inside the universe.
These ideas of the Divine resonate with me spiritually, in a way that the purely supernatural idea of God does not. There is a fit with the conviction of many mystics, as well as the English Romantic poets, that the Divine is present in all things. William Wordsworth spoke in the poem Tintern Abbey (1798) of ‘Something far more deeply interfused.’
Moreover, there is a close fit with the philosophical theory I have spent much of my career defending, namely panpsychism: the view that consciousness goes all the way to the fundamental building blocks of reality. For panpsychists, the particles or fields that make up our universe have their own very rudimentary form of conscious experience, and the highly complex consciousness of the human or animal brain is built up from these more basic forms of consciousness. Panentheism is more at home in a panpsychist picture of reality, as it’s easier to make sense of the Divine pervading the universe if the universe is filled with consciousness than it is if the universe is a cold, unfeeling mechanism.
Panentheism may also help us to make sense of the idea of a God that is subject to limitations. If God had to create the universe inside themselves, then it could be that the timeless and unchangeable nature of God imposed certain limitations on what could be created. Perhaps the deep simplicity and unity of God’s nature ensured that creation had to begin with a very simple starting point – the Big Bang – and could only progress to complexity over time.
I my book Why? I defended such views of our origins over both traditional atheism and traditional Western religions. What’s happened since then is that I’ve come to see that panentheism fits quite well with certain interpretations of Christianity.
M any people assume the essence of Christianity is as follows. We are all sinners and so we deserve to burn in hell for eternity. Fortunately, Jesus took the punishment we deserve and, as a result, if we accept Jesus’ sacrifice on our behalf, we’ll go to heaven to live with God when we die. Everyone who doesn’t accept Jesus’ sacrifice will burn in hell forever.
In fact, this is only one interpretation of Christianity, associated with the Protestant Reformation, although a similar view was defended by Anselm of Canterbury in the 11th century . It’s also, in my opinion, one of the most implausible theological doctrines in any of the modern global religions. I don’t think anybody deserves to burn in hell for eternity. And, even if we did, it wouldn’t achieve anything to punish an innocent man in our place. The word ‘Jesus’ is for many deeply associated with this picture, and so, in discussing an alternative, I’m going to borrow a trick from the author Francis Spufford and use the Hebrew version of Jesus instead: Yeshua.
Christianity is a little bit like quantum mechanics. In terms of the mathematics, quantum mechanics is our most successful scientific theory. The problem is nobody knows what on earth is going on in reality to make those equations work in predicting what we will observe. There are many different interpretations with no consensus on which is the correct one. Likewise, with Christianity, all Christians agree that Yeshua had some central role in the purpose of the universe. But there is no officially agreed view on the mechanics of that.
The views that are more plausible to my mind revolve around love and unity rather than sin and punishment. According to the participatory theory popular in the Eastern Orthodox Church, the Yeshua stuff was all about God becoming more similar to us so that we can become more similar to God. God wants us to share more deeply in their form of existence. But there’s a problem: the timeless, transcendent aspect of God is radically different from, say, a naturally evolved human being. Without God becoming more similar to their creation, the difference between God and creation is just too great for the two to share a common form of existence. The philosopher Robin Collins suggests this is analogous to the fact that ‘a tree branch cannot be grafted into a horse, only another tree; the horse is too alien for it.’ It is only once God, through Yeshua, shares in temporal, physical existence that the gap is bridged between God and creation, creating the potential for human beings and indeed the whole of creation to share more deeply in God’s form of existence.
If Yeshua rose as a physical body, then surely he could have revealed himself to millions
This view still doesn’t make sense to me if we’re assuming that God is all-powerful. If God can do anything, then they could have created us to share in their form of existence from the beginning, rather than subjecting us to millions of painful years of evolution. But if God is not all-powerful, then maybe they are on their way to creating a perfect universe but are only able to do this in two stages. In the first stage, they create an OK universe, one with the right kind of physics to eventually evolve intelligent life. Next, when creation has evolved enough, God begins to bring the universe to perfection by becoming more intimately involved in it, sharing in its nature that it can share in their nature. Perhaps this is a process that is still continuing – and maybe needs a bit of help from us – but which took a radical and decisive step forward in the events surrounding Yeshua.
Learning about this form of Christianity removed some of my big objections to Christianity. But the resurrection was still a big stumbling block. If Yeshua rose as a physical body that could be seen and touched, then surely he could have revealed himself to millions, making the existence of God and the truth of Christianity an indisputable historical fact.
These worries were countered only recently when I read the biblical scholar Dale Allison’s book The Resurrection of Jesus (2021), which presents a powerful defence of a slightly unorthodox view of the resurrection.
For Allison, the resurrection appearances consisted of visions, rather than literally seeing and touching a body. In other words, the resurrection appearances of the first Christians were more like the resurrection appearances of Paul on the road to Damascus. We might imagine that, soon after the crucifixion, the followers of Yeshua started being thrown to the ground and overwhelmed by intense visions: first Mary Magdalene, then Peter, later the 11 remaining disciples, 500 people at once, James the brother of Jesus, and many others, much later including Paul. Despite not involving a body that could be physically seen and touched, such novel and intense visions, occurring both to groups and individuals, could be enough to render it undeniable that reality had fundamentally altered in some radically new way.
Crucially, Allison is not denying that the resurrection was physical . He thinks that historical evidence supports the tomb of Yeshua having been found empty. But he denies the familiar narrative we find in Luke’s Gospel according to which Yeshua rose from the tomb as a body that could be seen and touched, hung around for a period of time, and then floated up to heaven – an event known as the ascension. Rather, Allison believes the first Christians identified the resurrection and the ascension.
In Paul’s letters in the Bible, he describes resurrection bodies as continuous with but radically different from ordinary bodies, as a plant is continuous with but radically different from a seed. For Allison, the tomb was empty not because Yeshua had stood up and wandered off, but because he had been transformed into a radically new form of physicality, perhaps a kind of formless energy. If we move a little beyond what Allison claims and adopt panentheism – on which the universe is part of God – then for this formless energy to be absorbed into God involves this formless energy filling the universe. In other words, Yeshua brought God closer to us not by being punished for our sin but by filling the entire universe with God’s love.
D o we have any reason to take any of this seriously? Traditional Christian apologists argue that the resurrection is the only explanation for the strange events that followed the crucifixion. We have good historical evidence that many people, including one violent opponent of the Christian movement, had experiences that persuaded them that Yeshua was in some sense alive again. They must have been incredibly powerful experiences because they motivated them to vigorously defend this conviction at great cost to themselves, including the cost of their lives in some cases.
I agree with traditional Christian apologists that there aren’t any very satisfying non-Christian explanations of the historical origins of Christianity. On the other hand, I agree with the view popularised by Carl Sagan that extraordinary events require extraordinary evidence, and I don’t think we have extraordinary evidence for the resurrection. It’s perfectly rational for an atheist to hold that Christianity was sparked by some kind of rare mass hallucination, preferring that explanation on the basis that, while improbable, mass hallucination is less improbable than a resurrection.
However, what counts as extraordinary depends on your worldview. I have tried to show how a certain form of Christianity fits quite well with a panentheist view on which God is not all-powerful, a view I believe to be well supported by current evidence. Relative to that worldview, Christianity – at least the form I have outlined – is not extraordinary; it’s one possible hypothesis as to what the purpose of the universe might be. By accepting that hypothesis, we get a more satisfying explanation of the origins of Christianity than anything available to a non-Christian (although note that the explanation I support, outlined in the previous section, is somewhat different from that of the traditional Christian). In other words, while the evidence for Christianity is not sufficient to persuade an atheist, it may be sufficient to persuade someone whose worldview is consonant with the truth of Christianity.
Pragmatic considerations can play a role when the evidence doesn’t conclusively settle matters
I hasten to add, these matters are inherently uncertain. I’ve come to think there’s a reasonable chance that a certain form of Christianity is true; but there’s also a reasonable chance it’s false. My intellectual hero William James argued that, in situations of uncertainty, when the truth is of monumental importance, it can be rational to choose to believe. He gives the analogy of being stuck in the mountains with the only way of escaping being to leap across an enormous chasm between two precipices. Intellectually speaking, it is uncertain whether or not you can make it. But if you choose to believe you will make it, you raise the chances that you will succeed.
The analogy is not perfect, as nobody is suggesting that a religion is more likely to be true if we believe it. But James’s example shows how pragmatic considerations can play a role when the evidence doesn’t conclusively settle matters. To take a contemporary analogy, it’s highly uncertain whether human beings will deal with the climate crisis. But it can be rational to believe we will, if that belief can provide meaning and motivation.
Faith is not about certainty. It is fundamentally a decision to trust your spiritual experiences, and to trust a certain framework for interpreting and acting upon those experiences. Hindus interpret their spiritual experiences as awareness of Ultimate Reality at the core of one’s being, and respond by meditating to realise their identity with Ultimate Reality. Christians interpret their spiritual experiences as awareness of a loving creator, and pray to deepen their relationship with God. These decisions to trust certain experiences influence how you see the world, how you respond to other people, and how you engage with nature. For a person of faith, each moment of daily life is permeated with meaning and significance.
This openness to uncertainty allows for pluralism. If faith requires certainty, then people of faith must be certain that their religion is right, and hence certain that other religions are wrong. But for trust to be rational, it’s only required that we’re not putting our trust in something wildly improbable. If there’s a 30 per cent chance that my loved one will make it, then it’s rational to have faith that they’ll pull through. But if the doctors tell me the chances of survival are sadly less than 1 per cent, then my loved one and I should enjoy our last moments and prepare to say goodbye.
This doesn’t mean faith gets a free pass. If Dawkins is right, there’s less than a 1 per cent chance that God exists, in which case it’s irrational to trust in the tenets of a theistic religion. However, if Dawkins is wrong, it might turn out that more than one religion is probable enough to have faith in. I have come to think that Christianity, in a certain form, is a credible possibility. But I have no problem with the idea that other religions may also be probable enough for it to be rational to have faith in them. If it is highly uncertain which religion is true, it may be rational to bring in pragmatic considerations, such as which religion you feel culturally comfortable in, to select a faith to follow.
Finally, I want to bring in one crucial element I haven’t mentioned so far: the extraordinary teaching of Yeshua. His focus on the poor and the weak, his talk of loving your enemies and turning the other cheek, his attacks on those who overvalue tradition or social status, were light years ahead of their time, and have played a crucial role in shaping the modern ethical ideals that we still struggle to live up to. This in itself proves nothing. But, for me, it’s a vital element in the mix, giving credibility to the possibility that the events depicted in the New Testament describe some profound moment in the evolution of reality.
Life is short and much is uncertain. We all have to take our leap of faith, whether that’s for secular humanism, one of the religions, or simply a vague conviction that there is some greater reality. In deciding, it’s important to reflect on what’s likely to be true but also what’s likely to bring happiness and fulfilment. For my own part, I have found a faith that is certain to bring me happiness, and which is, in my judgment, probable enough to be worth taking a bet on."
The life of Mishaqa and the birth of Arab-Ottoman modernity | Aeon Essays,,https://aeon.co/essays/the-life-of-mishaqa-and-the-birth-of-arab-ottoman-modernity,"The 19th-century Middle East presents a paradox. On the one hand, this was a time when the idea of a modern, secular society became possible. On the other, it saw the rise of newly divisive religious identities that stood opposed to that idea. Across the Arab provinces of the Ottoman Empire, intellectuals were creating a modern public sphere: a vibrant periodical press, a series of cultural associations and new-style schools. Adherents of different religions – Muslims, Christians, Jews – rubbed shoulders in these institutions, debating society, science and culture. A few – like the ‘materialist’ Shibli Shumayyil – began to recommend modern science as an alternative to religion. The Ottoman state, meanwhile, proclaimed the formal equality of its subjects of different religions, in 1856. Many intellectuals took it at its word, appealing to their fellows to set aside religious differences in the name of a ‘homeland’ or ‘nation’ for people of all faiths .
At the same time, these Arab provinces – notably Syria and Lebanon – were racked with major conflicts between religious communities. In Lebanon, the 1840s and ’50s saw bitter fighting between armed groups from the Druze Abrahamic and Maronite Christian communities. Tensions rose across Syria, fuelled by the intervention of European states and fears of their growing power within the Ottoman lands. In 1860, these pressures came to a head: in the provincial capital of Damascus itself, a crowd of armed Muslims massacred several thousand Christians. Meanwhile, religious leaders and revivalists were seeking to create more homogeneous religious communities, clearly distinct from one another. Muslim activists of the early Salafi movement, like Rashid Rida, advocated a return to the pure example of the Prophet Muhammad’s time. Catholic clerics like Patriarch Maximus Mazlum aimed to mark off their communities unambiguously from other Christian groups as well as Muslims. These projects were, in their way, as modern as those of secularity or scientific rationalism. They set out to eradicate an older set of religious practices – like the common worship of saints and shrines, or shared processions and holy days – that had at times blurred the boundaries between faiths.
The 19th century in the Arab-Ottoman world has thus left a contradictory legacy. It saw the emergence of two kinds of project that would go on to divide the Middle East, until today: ones aiming at a secular society, science and rationalism on the one hand, and ones insisting on religious identity and the separateness of religious communities on the other. How did these tendencies – so contradictory on the surface – come into being in the same moment, side by side?
Mikha’il Mishaqa in Damascus, Syria in 1859. Courtesy Wikipedia
One way of answering this question is through a microhistory: by looking at the story of one individual who played a part in the emergence of both kinds of project. Such a person was Mikha’il Mishaqa, a man who helped to shape both scientific rationalism and religious revival, as he followed his own eccentric path through the Arab 19th century. The story of his unusual journey through doubt to faith illuminates a shift taking place in the place of religion in Arab-Ottoman society: one that helps us see how the roots of rational secularism and of divisive religious identity were intertwined.
M ishaqa arrived in the Egyptian port of Damietta in 1817, aged 17. In the bustling city, the main hub for trade between Syria and Egypt, he found a world very different from the Lebanese mountain town, Dayr al-Qamar, where he had grown up. Along the Nile frontage, barges unloaded coffee, rice or linens directly into the waterfront entrances of houses and caravanserais. Passing through the port came peasants from Palestine and imams from Istanbul; Jews from Rhodes, and Egyptian Coptic Christians on pilgrimage to Jerusalem. At the heart of the city’s trade was the small but wealthy community of Christian merchants from Syria: the young Mikha’il took his place among them, lodging with his uncle and elder brother who were already established in Damietta, and set out to learn the business and make money.
But there was a fly in the ointment. Each spring, the plague appeared in town. At its height, up to 100 funeral processions could be seen leaving the city. And the disease regularly continued infecting and killing Damietta’s inhabitants for several months, into June or July. It presented a problem for everyone: but there was no consensus on how to deal with it. Many, both Muslims and Christians, resorted to prayer; some to magical means, like drawing squares or diagrams on the outside of rooms and houses as protection. Others (or the same people) looked to medical means – but doctors disagreed on the plague’s true causes and treatment. Some thought it spread by touch, others by breathing foul air; some thought tobacco smoke or laudanum useful protections, others dismissed these remedies. Damietta’s wealthy Christians sought to preserve themselves by a form of ‘lockdown’, shutting themselves in their houses or apartments for several months, and washing all goods that entered in water or vinegar.
These different reactions could cause disputes: adopting elaborate medical precautions could seem to deny God’s power, while relying on magic or prayer alone could seem dangerously careless. In the mixed society of the busy Egyptian port, different belief-systems rubbed up against each other as all confronted the shared challenge of plague. Soon after Mikha’il Mishaqa arrived in Damietta, his elder brother Andrawus caught the plague, but recovered. Above the door of his room, though, Mikha’il found papers bearing a religious slogan, placed there by the local Catholic priest. These, he was informed, were supposed to ‘stop the plague entering the place’: but, as Mikha’il remarked, they had clearly failed, because Andrawus had caught the plague anyway. ‘Don’t be of little religion and sow doubts,’ he was told. Yet the doubt persisted: these contradictions in dealing with the plague were one factor leading Mishaqa to doubt religion as such.
‘I came to reckon everything I read and heard in the books of the sects as falsehoods and delusions of utter futility’
The other major factor also had its roots in the unusually varied social mix of Damietta. For more than a decade, the richest Christian merchant of the city, Basili Fakhr – in touch with Greek clerics and sailors, and with Western European travellers and scholars – had been sponsoring the translation into Arabic of works of the European Enlightenment. This was the first time that post-Newtonian science, or the sceptical thought of French deists, was available in Arabic. And Mishaqa had already read some of these books at his home in Mount Lebanon, thanks to an uncle who had brought them from Damietta: now he read more. Enlightenment science offered him a system of ‘natural laws’ to explain and predict the phenomena of nature – like the movements of the planets and stars – with mathematical precision. By contrast, the ‘laws’ laid down by religion came to seem doubtful and irrational. The seal was set on Mishaqa’s rejection of religion when he read another of Fakhr’s Arabic translations. This was the Ruins of Empire by the French philosophe Constantin-François de Volney, who had himself travelled through Egypt and Syria in the 1780s.
Volney argued that human affairs – like the natural world – were governed by ‘natural laws, regular in their course, consistent in their effects’. Religions, however, were mystifications of these laws, derived from misunderstandings of the cosmos, and perpetuated by priestly elites who sought to maintain their own power. As Mishaqa would later put it: ‘I [came] to think that all religions were lies, and that the religious laws had been created by the wise, as a bridle for the ignorant.’ Like Volney, Mishaqa did not reject the notion of a divine being, who had created the wonders of nature, but resolved to act ‘according to the guidance of the natural light’ that God had ‘planted within us’. Though remaining outwardly a Catholic, so as not to cause scandal to his family and coreligionists, he ‘came to reckon everything I read and heard in the books of the sects as falsehoods and delusions of utter futility’, unacceptable to ‘sound reason’.
While Mishaqa – and, around him, a small circle of young Syrian Catholic men, like his brother Andrawus – was drawn to a deist position based on ‘natural law’ and reason, others in Damietta set out to counter them. The learned Catholic priest Saba Katib wrote a set of essays aimed to refute the ‘heresies’ of materialists both ancient (like Democritus and Epicurus) and modern (like Voltaire and Hobbes). Against them, he made the well-known argument from design: the wondrous universe revealed by science must surely have a divine Creator. And, significantly, Katib adopted a style of argument that was then unusual in Christian apologetics: since his adversaries ‘do not believe in any Scripture, nor in the sending of a Prophet,’ he wrote, ‘I have made the basic issue in the controversy the proof of reason alone.’ Like his deist or atheist adversaries, that is, Katib adopted ‘reason’ – not Biblical texts or ecclesiastical traditions – as his criterion.
Map of the Emirate of Mount Lebanon, 1752. Courtesy Wikipedia
M ishaqa returned to Mount Lebanon in 1820 – fed up of the plague and lockdowns that ‘imprisoned [me] in [my] house for around five months’ of each year. Building on his family’s role as merchants and bankers to the emir of Mount Lebanon Bashir al-Shihabi, Mishaqa soon found himself near the centres of local political power. Still in his 20s, he was appointed chancellor to the emirs of Hasbaya, a town in the Anti-Lebanon mountains, and granted the rents from extensive lands. He continued his self-education, studying medicine after a brief illness, and writing an innovative treatise on Arab music. He maintained his scepticism about religion: he records in his memoirs several incidents where he poked fun at the pretensions of clerics, Christian and Muslim. This, too, was his attitude when he first encountered, in 1823, a new kind of religious figure: Jonas King, one of the American Protestant missionaries who had recently arrived in Syria. Hearing the ‘handsome’ young New Englander dispute with Catholics in Dayr al-Qamar, Mishaqa ‘laughed secretly at both sides’.
The Evangelical missionaries brought to Ottoman Syria a heavy dose of Western condescension: they regarded its inhabitants as generally ‘uncivilised’ and ‘ignorant’, in need of their own brand of religious enlightenment. But they also brought an unusual interest in the individual beliefs of people they encountered. As they proceeded around the eastern Mediterranean, they reported meeting – in addition to believing Muslims, Jews and local Christians – what they described as ‘infidels’, individuals apparently unconvinced of any of the faiths on offer. They included a Maltese architect, who cited Volney as authority for his belief that ‘the Bible is an imposture’; the deist Dr Marpurgo, a leading Jewish physician of Alexandria; and the Armenian monk Jacob Gregory Wortabet, whom they succeeded in weaning away from his ‘infidel and deistical opinions’ and converting to Protestantism. As the American missionaries embedded themselves in the towns of Syria from the 1830s to the 1850s, they found similar groups, often of young Christian men, dissatisfied with their local churches and rapidly becoming ‘entirely sceptical on the subject of religion’.
To convince these groups of the merits of religion and of Protestant Christianity, the Protestant missionaries drew on rationalistic themes in their own heritage. As the American Pliny Fisk put it in 1823, true Christianity – that is, Evangelical Protestantism – could be seen as a ‘golden medium’ between ‘the two extremes of superstition and infidelity’: between the beliefs of local Christians, Muslims and others, and deism or scepticism. To convince local Christians – particularly Catholics – they often took aim in their preaching and writing at what they saw as irrational aspects of their beliefs: the worship of saints and images, the transubstantiation of bread and wine into Christ’s flesh and blood. And to convince sceptical ‘infidels’, they stressed what they saw as the ‘evidences’ for the truth of Christianity. Much like the Catholic Saba Katib in the 1810s, they based their appeal to such people not on Scripture or tradition, but on ‘reason’.
‘Reason’ continued to be his watchword: yet he no longer believed in reason unaided and alone
It was in the 1840s that Mishaqa felt the force of this appeal. By now, he was living in Damascus, married and prospering as a moneylender, doctor and merchant. He had witnessed dramatic changes in Syria: its occupation by the army of the powerful governor of Egypt, Mehmed Ali Pasha, and the first stirrings of sectarian violence that followed this army’s withdrawal, in 1841. Already swayed by a range of social and intellectual factors to look again for religious truth, in 1842 or 1843 Mishaqa came across a book translated into Arabic by the Protestant missionaries in Malta, called Evidence of Prophecy . In this Evangelical bestseller, the Scottish Presbyterian minister Alexander Keith had set out to prove that the prophecies made in the Bible had in fact come true – thus proving that the text must be divinely inspired. To this end, he compared the Scripture with modern European travellers to the Holy Land, piling up ‘evidence’ of the annihilation of cities that God had said would be destroyed. He even cited the travel narrative of the deist Volney in support of his claims. And to drive home his point, he accompanied the book with engravings – and later, photographs – giving empirical proof of the ruins of Petra, Nimrod or Babylon.
Mishaqa, after some thought and hesitation, found Keith’s arguments convincing, thanks especially to his rationalistic style, so different to that of ‘the doctors of my Church’, with their talk of ‘matters which … sound reasons reject’. A few years later, after further hesitations, and after growing closer to the American Protestant missionaries in Beirut, to other Syrians who had themselves become Protestants, and to the British consul in Damascus, Mishaqa took the irrevocable step of public conversion. In November 1848, he announced his belief ‘in the Christian faith according to the Holy Scriptures’ and was soon embroiled in a bitter controversy with the local head of the religion he was leaving, Maximus Mazlum, patriarch of the Greek Catholic Church. In his arguments against Catholicism – which the Protestant missionaries printed for him in Beirut – Mishaqa denounced the irrational ‘superstitions’ and ‘priestly inventions’ of Catholic doctrine, in similar terms to those he had used of religion in general in his deist youth. ‘Reason’ continued to be his watchword: yet he no longer believed in reason unaided and alone. He now accepted divine revelation as a surer guide, pointing out that rational judgments are often changeable and uncertain.
Ruins of a Greek church in the Christian quarter of Damascus, Syria, 30 April 1862. Photo by Francis Bedford, courtesy the Royal Collection, London
This rational Christianity sustained Mishaqa for the remainder of his life, through further upheavals. In 1860, along with other Christians of Damascus, he was attacked by a Muslim crowd in a bloody episode of sectarian violence. He fled through the streets with his two young children: badly beaten, he was lucky to escape with his life. He was able, though, to re-establish himself in Damascus as a prosperous man of business and vice-consul for the United States, positions he passed on to his sons. He kept up a reputation for learning across a broad range, from mathematics to music, and wrote a shrewd and vivid memoir of his life and times, before his death in 1888.
M ishaqa had certainly contributed to creating a modern public sphere in Arabic, and to the cause of scientific rationality. He was an early member of scientific and cultural associations in Ottoman Syria, and wrote for their publications and the growing periodical press. His religious writings, addressed to a public of many faiths, pioneered the use of printed pamphlets in public controversy. And his rationalism could have a sharp edge, as when he denounced popular beliefs and customs, as well as religious practices, as ‘superstitions’.
Yet Mishaqa was also a religious activist. Alongside the American missionaries, he sought to create an Arab form of Evangelical Christianity. His anti-Catholic polemics did much to shape the small but influential Syrian Protestant community, presenting Evangelical doctrines in a form suitable to an Arabic-reading public: the missionaries would reprint them well into the 20th century. They entered a tradition of inter-religious controversy, being picked up by Muslim as well as Catholic apologists. As a result of this, digital copies of these 19th-century texts can now be found on websites devoted to propagating Islam: eg, Quran For All and the Comprehensive Muslim e-Library.
One theme runs through these apparently disparate aspects of Mishaqa’s work and legacy: his insistence on ‘reason’. For him, this was the standard by which any belief should be justified – whether a scientific theory that could be tested by experiment, or a faith in divine revelation that surpassed human understanding. And this had apparently contradictory consequences. Mishaqa could entertain the possibility of being without religion entirely – he had, after all, spent 25 years as a deist – and judge different beliefs from outside, by the light of reason. But then, having opted for a particular faith, he had to insist on its rational credentials, dissociating it sharply from irrational ‘superstition’ and drawing its boundaries more tightly.
The poles of secularism and religious revivalism continue to animate cultural discourse in the Arab world today
And Mishaqa was not alone, in 19th-century Syria, in adopting the post-Enlightenment view of religion as something to be justified by reason, or of the true faith as a ‘golden medium’ between ‘superstition’ and unbelief. Even the Catholic reformer Maximus Mazlum had written a defence of Church doctrine against a Muslim scholar of Al-Azhar University in Cairo, in which he appealed not to Scripture or tradition but to ‘rational, philosophical proofs’. In the decades following Mishaqa’s death in 1888, this way of arguing about religion became increasingly common in the growing Arabic public sphere. Christians, Muslims and scientific ‘materialists’ alike now debated the relationship between faith and reason: and all were coming to assume, like Mishaqa, that they had to place themselves somewhere on a spectrum between unthinking faith and godless reason.
But these same reformers were also emphasising the boundaries between religious communities and practices. Muslim and Christian reformists – the early Salafi movement, Protestant missionaries, and reforming Catholics like Mazlum – set about denouncing the ‘superstitions’ of the common people. Some of these practices blurred the lines between religious communities, as Muslims or Druze visited the shrine of a Christian saint, or vice versa. Others blended religion and magic, or, like popular Sufi gatherings, offended the hardening standards of public morality and elitist ‘good taste’. In order to present their faiths as consistent creeds that could be embraced by rational individuals, reformists of all stripes had to strip away these heterodox, vernacular, collective and often syncretic practices. Yet, as they did so, they unpicked the fabric of a shared religious culture: a hierarchical but multifaith order in which Muslims, Christians, Jews and others had found a place. In its stead, they helped to create the contradictory possibilities of modernity: on the one hand, the image of a secular society and public sphere, of coexistence and equality between faiths; on the other, projects of ‘rationalised’, identitarian revival, of exclusive and homogeneous religious communities.
The eccentric figure of Mikha’il Mishaqa reminds us that these contradictory possibilities came out of a shared transition: towards the justification of religious faith in terms of reason, and the stress on individual belief as opposed to collective practice. The two poles of secularism and religious revivalism continue to animate much cultural discourse in the Arab world today. Mishaqa’s story recalls that they are both aspects of the same, modern, reality: that they emerged together, locked into a quarrel that was also a dialogue."
How Scots freethinkers managed to loosen Christianity’s grip | Aeon Essays,,https://aeon.co/essays/how-scots-freethinkers-managed-to-loosen-christianitys-grip,"On the morning of Saturday 3 June 1843, the Edinburgh police made their way past Calton Hill to the tenements of Haddington Place. Their target was the residence of Thomas Finlay, a former cabinetmaker who was suspected of running an ‘infidel’ library on the premises. Described by his sympathisers as a ‘respectable and venerable old man’, he had never previously attracted unwanted attention from the authorities. Yet Finlay had fallen foul of Scots blasphemy law, which banned the publication, sale or circulation of any work that denied or ridiculed Christianity or the divine inspiration of the scriptures. From the 1820s, the authorities had sought to clamp down on such materials and, for the first time since the 17th century, several Scots faced charges of blasphemy. The raid on Finlay’s home, which he later recalled had left the ‘innermost corner of my dwelling ransacked, and the very locks of hair of my departed father and mother strewed about’, duly uncovered numerous blasphemous books and pamphlets. These materials were seized in evidence and Finlay spent a grim night in Calton Jail before being released on bail, paid by his friends, to await trial.
Finlay’s arrest was just one of several dramatic episodes in a fractious battle over belief that unfurled in the early decades of 19th-century Scotland. At the heart of the struggle lay fundamental questions about freedom of expression. Should individuals have the right to share publicly controversial views on religion? Or were there ethical grounds to suppress the public dissemination of such ideas for public safety? The debate had been sparked by the striking emergence of minority groups of self-professed ‘conscientious unbelievers’ or freethinkers during the 1820s, one of which Finlay had joined at its inception.
The most vocal members of these communities were either materialist atheists who denied God’s existence or deists who believed in a creator deity but rejected the divinely inspired status of the Bible. Despite theological differences, freethinkers shared the view that Christian theology, institutions and clergy had harmed the welfare of individuals and society. Many were sympathetic to political reform and argued that the erroneous belief in providence encouraged acceptance of the sociopolitical status quo as part of the divine plan. Others pointed to the distress caused by belief in the Calvinist doctrine of double predestination, which asserted that the elect were foreordained for salvation while the damned were destined to eternal damnation. Above all, it was agreed that social and individual flourishing depended on the right to unrestricted intellectual enquiry and freedom of expression on all subjects, including religion.
Groups who held such controversial views were unprecedented in Scottish cultural history. Scotland was overwhelmingly Christian and Protestant. Calvinist Presbyterianism represented the dominant strain of Scottish religiosity. It underpinned the Church of Scotland, established by law to protect the nation’s moral and spiritual welfare, and forged a distinctive religious culture that set Scotland apart from its southern Anglican neighbour. So pivotal was Calvinist Presbyterianism to the national identity of most Scots that, as Colin Kidd argued in Unions and Unionisms (2010): ‘Until the 1920s religion was unquestionably the central issue of division between Scots and English within the Union.’
Calvinism was also shared by many other denominations outwith the ecclesiastical establishment. From May 1843, this included the Free Church of Scotland, which was born of the ‘Great Disruption’ of the Church of Scotland, when more than a third of the latter’s ministry left in protest at state encroachments on its spiritual independence. Smaller Christian groups, including Episcopalians and Catholics (whose numbers would expand significantly with Irish immigration in the latter half of the century), did not share the dominant Calvinism, and there were also minority Jewish communities. Yet the emergence of communities of freethinkers, who felt that they could not ‘conscientiously’ subscribe to any existing religious group, were very much a novel feature in the landscape of belief.
This is not to say that disbelief in the Judeo-Christian God had been entirely unheard of in Scottish society. Faith, then as now, was rarely static throughout a person’s life, and biographies, letters, spiritual diaries, conversion narratives and poems reveal that Scots across the centuries have experienced periods of doubt or loss of faith. Nor were more radical forms of disbelief entirely new. During the previous century, the age of the Scottish Enlightenment, the celebrated philosopher and historian David Hume (1711-76) had acquired a notorious reputation for ‘infidelism’. His philosophical scepticism denied the possibility of any certain knowledge of God’s existence and he had published highly provocative critiques of miracles, the soul’s immortality and the distinctiveness of Christianity in global religious history. Scottish freethinkers themselves were conscious of Hume’s legacy. Speaking at his trial in December 1843, Finlay reflected on the irony of his imprisonment in Calton Jail, just a few minutes’ walk from Hume’s grand mausoleum: ‘I thought it strange to be confined for my infidelity, in a prison close by the walls of which stands a splendid monument in memory of the celebrated Infidel … David Hume.’
Yet there were radical differences between the modern unbelief of figures such as Finlay and the Humean scepticism that had raised eyebrows, but not the arm of the law, during the Scottish Enlightenment. What set the radical ‘conscientious unbelievers’ of the early 19th century apart from their 18th-century predecessors? How far did they succeed in changing opinions over the right to freedom of expression on religious matters? And what became of Finlay and his fellow freethinkers?
F irst of all, the new type of unbeliever tended to come from the middling or lower ranks of Scottish society. Many were skilled artisans, apprentices or owners of small shops, and some belonged to less lucrative professions, including weavers and factory workers. By contrast, notorious freethinkers of the previous century, such as Hume, had belonged to the intellectual and scholarly elite. For leading figures in Church and State, it was another matter altogether for radical freethinking ideas to be circulating more widely among the people. ‘Popular unbelief’ summoned the terrifying spectre of the French Revolution of 1789 and the radical de-Christianisation campaign of the Reign of Terror. To members of the civic and ecclesiastic establishment, popular unbelief was inseparable from the dangerous threat of republicanism, violence and radical social upheaval.
Secondly, 19th-century freethinking groups tended to eschew the philosophical scepticism of Hume. Rather than stressing the limits of humankind’s ability to definitively determine anything about the nature and existence of God, they favoured deism or atheism and were thus more attracted to the bold writings of French Enlightenment philosophes such as the materialist Baron d’Holbach. These new unbelievers were also forging increasingly visible and organised communities. Three crucial developments were pivotal to this process: radical newspapers, the emergence of substantial freethinking societies, and freethinking bookshops.
Portrait of Richard Carlile (1790-1843) by unknown artist. Courtesy the National Portrait Gallery, London
The Republican newspaper played a central role in bringing Scotland’s freethinkers together. Launched in London in August 1819, and dedicated to the destruction of ‘kingcraft’ and ‘priestcraft’, its pages forged a sense of community for freethinkers across Britain. Many were moved by the plight of the Republican ’s bold editor, Richard Carlile. A former tinplate worker, Carlile faced charges of blasphemy and sedition in October 1819. He was sentenced to three years’ imprisonment in Dorchester Gaol and hit with an eye-watering £1,500 fine. Unable to pay such a large sum himself, he was released only in 1825. Year on year, many sympathisers had contributed towards the payment of his fine, with lists of subscribers printed in The Republican , which Carlile continued to edit from prison. The names included men and women from Edinburgh, Glasgow, Dundee, Paisley, Aberdeen, Kirkcaldy and Falkirk. A handful of these Scots identified themselves as ‘liberal Christians’ who opposed Carlile’s prosecution and supported freedom of expression. Yet the majority were fellow unbelievers who agreed with the editor’s critique of Christianity. Some listed themselves by name, but others remained anonymous, using tags such as ‘A Sceptic’, ‘A Deist’, ‘A Materialist’ or ‘An Enemy to Devil Manufacturers’.
‘Infidel’ parties were arranged by local weavers where controversial theological works were read aloud
Subscribers to Carlile’s fines were among those who founded the freethinking ‘Zetetic’ societies in Edinburgh and Glasgow in the 1820s. Taking their name from the Greek verb zētein , meaning to ‘to seek after’ or ‘enquire’, they provided new hubs for conscientious unbelievers. The Zetetics met weekly on Sundays to debate controversial theological and philosophical topics and provided their members with a library. In 1820, the Edinburgh Zetetic Society, which Finlay joined, rented premises in Potterrow, at the heart of the city’s Old Town. By 1822, they were regularly attracting audiences of 300 to 400, including a small number of women and children. Despite having to restrict their meetings after a police raid in November that year, they remained active until at least 1826.
By the late 1830s, another ‘Society of Freethinkers’, in all likelihood a descendant of the Zetetics, was active in nearby Blackfriars Street. Glasgow’s Zetetic Society, established in 1824, also attracted several hundred members. By the 1830s, the group was renting a hall in Nelson Street, south of the Clyde, where it continued to meet until at least the early 1840s. Although Scotland’s rapidly expanding cities were home to the most organised and substantial societies of freethinkers, more informal groups existed elsewhere. As Norman Macleod, a minister in the rural parish of Newmilns in Ayrshire was to discover in 1839, ‘infidel’ parties were being arranged by local weavers where controversial theological works were read aloud.
A sense of community among unbelievers was also fostered by a handful of individuals who set up freethinking bookshops, reading rooms or lending libraries in Edinburgh and Glasgow. James Affleck, a former grocer and founding member of Edinburgh’s Zetetic Society, opened the nation’s first freethinking bookshop in 1823 in the commercial district of Adam Square, now Chambers Street. Affleck declared that a central motivation for doing so was to ‘have a better opportunity of coming in contact with liberal-minded men’. Affleck’s bookshop proved short-lived. Despite proceeding with caution and selling radical literature only to known freethinkers, he was undone by a police spy who posed as a fellow unbeliever for several months. Duped into selling the police agent a copy of the highly controversial Theological Works of Thomas Paine (1824), Affleck found himself facing trial for selling blasphemous works. Three other radical booksellers had already faced blasphemy charges between 1819 and 1820 in the first of such cases since the 1690s. Rather than stand trial, they had fled the courts and were outlawed in absentia. Affleck chose to face the charges and was found guilty. His books were confiscated and in 1824 he was sentenced to three months’ imprisonment in Calton Jail and ordered to pay a £100 fine.
Others, however, emerged to take Affleck’s place. Among the most notable was Finlay’s son-in-law, Henry Robinson, a radical publisher and bookseller in Brunswick Place in Glasgow’s South Side during the 1830s. By the 1840s, he had relocated to Edinburgh’s Greenside Street, close to Calton Hill. The same day that Finlay was targeted, Robinson’s shop was also raided by the police and led to his arrest in Calton Jail. Their cases prompted English freethinkers to rally to Edinburgh, including Thomas Paterson, who provocatively established a ‘Blasphemy Depot’ near Robinson’s shop, and Matilda Roalfe, who set up another radical bookshop on Nicholson Street, close to the original site of the Zetetic Society. Both booksellers were also arrested for selling blasphemous books.
S trikingly, however, these were the last trials for blasphemy in Scottish history. This turning point in the cultural landscape is especially significant when we consider that several individuals continued to be prosecuted for blasphemy in England between the 1850s and 1920s. Indeed, the last blasphemy trial in England took place in 1977, when the Gay News case of Whitehouse v Lemon resulted in a conviction. How, then, did Scotland’s 19th-century freethinking communities succeed in turning the tide of opinion against prosecution for circulating ‘blasphemous’ ideas?
A Street in Old Edinburgh, West Bow(?) (1857) by Henry Duguid. Courtesy National Galleries of Scotland, Edinburgh
In part through sheer persistence. Despite the risks involved, many continued to participate in freethinking societies and debates, to run radical bookshops, and to pen or purchase controversial publications. Booksellers were particularly vulnerable to blasphemy charges, which concentrated on the circulation of books that denied or ridiculed Christianity or the scriptures. Yet all known unbelievers faced some degree of risk. Some were ostracised by friends or family, others suffered economic consequences as the loss of respectability depleted their customers, and several faced periodic waves of suppression by the civic authorities. Emotions could run high and strong language was not infrequently used by concerned Christians, clerics or civic authorities who wished to put a stop to unbelief. On rare occasions, such rhetoric encouraged violent action. Emma Martin, an English freethinker who visited Edinburgh for a lecture tour, discovered this first-hand in 1845. Martin’s controversial response to a sermon in Newington’s Free Church led her to be stoned by an angry mob, forcing her and her daughter to take shelter at Roalfe’s nearby bookshop. By continuing to promote freedom of expression despite the potentially negative consequences, Scotland’s freethinkers ensured that the debate over freedom of expression could not be ignored.
Sheer persistence, however, cannot fully explain why blasphemy prosecutions fell into disuse after 1843. More significantly, many within the Christian mainstream were increasingly sympathetic to unbelievers’ arguments in favour of freedom of expression. We’ve already seen that several Christian Scots had anonymously contributed to Carlile’s large fine for selling blasphemous books and had expressed their distaste at the persecution of opinion. Other Christian individuals had made such arguments much earlier. In 1808, the nonagenarian John Goldie ( 1717-1811), a friend of the late poet Robert Burns (1759-96) and a man known for his heterodox theological views, opposed clergymen who sought to stamp out religious debate. His Conclusive Evidences Against Atheism (1808) made an ardent appeal for allowing ‘impartial investigation’ into religious subjects and opposed the persecution of unbelievers for their opinions alone. Goldie stressed that many had exemplary morals, and he asserted that good conduct was far more important than belief.
Correct opinions were impossible to acquire without ‘unlimited enquiry, free discussion, and liberty to publish’
As the periodic attempts to suppress unbelief through official or unofficial persecution make clear, this view was by no means universally shared by Scottish Christians in the first half of the 19th century. In response, freethinkers mounted a campaign to persuade their contemporaries of the virtues of freedom of expression. During the 1820s, the Zetetic Societies of Edinburgh and Glasgow sent petitions to the government to make their case. The Edinburgh group declared that the right to free discussion was essential to advancing human knowledge, otherwise ‘men have no way of detecting error, and arriving at the truth of any subject; and the boasted freedom we are said to enjoy is only an empty name.’ What’s more, they argued, restricting free debate put morality at risk by forcing individuals to hide their true opinions and take refuge in hypocritical conformity. A change in law was needed so that, on all subjects, human beings would be ‘convinced by reasoning, and not be forced by law, as at present, to be hypocrites’.
In connecting freedom of expression with the pursuit of honest virtue, freethinkers were subtly disputing the common argument that state protection of Protestant Christianity was essential for the moral welfare of society. The same theme was picked up in an 1824 petition from the Glasgow Zetetic Society that attracted 420 signatures. The group stressed that good conduct was dependent on correct opinions, which were impossible to acquire without ‘unlimited enquiry, free discussion, and liberty to publish whatever may be deemed of consequence in the elucidation of truth.’ Any opinions, they argued, that were not ‘embodied into actions detrimental to the peace and welfare of the community’ ought to be free from prosecution. Cannily appealing to the views of their audience, they added that it was precisely because religion was so important that they should not be ‘restrained in their researches and the expression of their thoughts on this momentous subject’. What’s more, attempts to use the arm of the law to protect Christianity were fundamentally flawed and implied that it was ‘incapable of withstanding opposition without the assistance of human laws’.
Similar arguments were echoed by many Scottish freethinkers in the coming decades. An important refrain was that freedom of expression was fundamental to liberty of conscience. Hitting the Protestant establishment where it hurt, many suggested that, by fostering disingenuousness and hypocrisy among unbelievers, the Church of Scotland and the civic authorities were no better than inquisitorial Catholics. This claim was made with relish by the Scottish Anti-Persecution Union, established by a group of freethinkers in 1843 in the wake of the arrests of Robinson and Finlay. Blasphemy legislation, they declared, contradicted the ‘great principle of Protestantism, which gives to all the right of individual judgment, and professes, therefore to interfere with the conscience of no man.’
B y the 1840s, there were important signs that increasing numbers within the Scottish Christian mainstream were sympathetic to such arguments. Among those to express support was the liberal Tait’s Edinburgh Magazine , which objected in 1840 to a government crackdown on radical publications by asserting that such policies blasphemed the deity by robbing his creatures ‘in God’s name, of their most sacred and inalienable right, the right of free thought and free speech’. The unfolding of the last blasphemy trials also indicated that change was in the air. Lord Justice-Clerk John Hope (1794-1858), a stern judge who was eager to see both individuals convicted, was appointed to oversee the trials of Robinson and Paterson. Significantly, however, Hope felt compelled to remind the jury that they were not ‘engaged in a theological discussion’ over the propriety of blasphemy prosecutions. Rather, they were simply to determine whether the accused were innocent or guilty of the charges laid before them. He also attempted to add nuance to the terms of the indictments laid against Robinson and Paterson. Traditionally, blasphemy charges had applied to the circulation of any book that either denied or ridiculed Christianity or the scriptures. Yet Hope informed the jury that the mocking tone of the confiscated books was crucial. Had the works under discussion offered ‘a fair and serious discussion of the truth and authority of the Holy Scriptures’, he claimed, the booksellers would not have been prosecuted.
Hope passed a severe judgment on both Robinson and Paterson, sentencing them to 12 and 15 months’ imprisonment. Yet it is striking that even a figure so supportive of their conviction felt obliged to justify the nature of the legislation. To be sure, some Christians agreed with Hope and felt blasphemy laws protected society at large. As the Free Church newspaper The Witness declared in 1843, there was ‘both wisdom and humanity in the law’ that had consigned Robinson and Paterson to prison. Yet the wider public response to the trials suggests that the tide was turning. Two weeks later, a crowded public meeting arranged by the Anti-Persecution Union attracted more than 1,000 freethinkers and Christians. Unconvinced by Hope’s suggestion that the tone of the works was significant, the majority agreed to petition the government for the abolition of blasphemy laws and the prisoners’ release. Among the Christian thinkers present, one appealed to Jesus’ injunction to turn the other cheek, declaring that Christian persecutors did greater harm to the faith than unbelievers. For another Christian attendee, prosecution was antithetical to the ‘spirit of Christianity’.
Christians increasingly felt that open debate was preferable to the legal suppression of freethinkers
The petition was ultimately unsuccessful. Yet the mid-19th century nevertheless marked a decisive turning point in debates over freedom of expression. No further blasphemy prosecutions were heard in Scotland’s High Court. When Finlay was eventually called to stand trial in late December 1843, he was tried only in the Sheriff Court, which could impose a maximum sentence of 60 days or a £10 fine. Sheriff George Tait was obliged to follow the precedent of the High Court and impose the maximum penalty, yet even he took the opportunity to praise Finlay’s ‘very excellent and judicious defence’.
Minority groups of unbelievers remained a presence in Scotland, later leading to the establishment of communities such as the Edinburgh Secular Society in the 1860s and proto-humanist ethical societies in Edinburgh and Glasgow in the 1890s. Yet attitudes towards the toleration of these communities had begun to change by the latter half of the century. Christians within the mainstream increasingly felt that open debate was preferable to the legal suppression of freethinkers. A shared commitment to liberty of conscience and freedom of expression on religious subjects had created an unexpected area of common ground between freethinkers and many Christians. The debate over blasphemy, in which emotions ran high, convictions were tested and boundaries pushed, had therefore contributed to the emergence of a more openly pluralistic landscape of belief.
At his trial in 1844, Finlay claimed that his arrest was ironic since a commitment to freedom of conscience ostensibly lay at the root of evangelical Christianity. ‘I thought it very unseemly,’ he explained, ‘to find myself in prison, and treated as a felon by Christians, for merely doing what Christians themselves glory in, and think it their duty to do, that is inviting the consideration of their fellow-creatures to those opinions which they conscientiously believe to be well-founded, and conducive to the welfare of human society.’ Finlay even went so far as to argue that liberty of conscience and freedom of discussion on all topics was fundamental to civilisation itself. Echoing Hume, he asserted that the progress of civilised society was utterly dependent on the right to put ‘all … modes of thought to the test of comparison with other modes of thought’ and to use our ‘two ears for the purpose for which they are adapted, to hear on both sides.’ By the latter half of the 19th century, most Scots had come to agree.
The research for this article was funded by the Leverhulme Trust."
How Savarkar invented Hindu supremacy and its cult of violence | Aeon Essays,,https://aeon.co/essays/how-savarkar-invented-hindu-supremacy-and-its-cult-of-violence,"To understand Narendra Modi’s India, it is instructive to grasp the ideas of the Hindu Right’s greatest ideologue, the world of British colonial India in which they emerged, and the historical feebleness of the present regime.
Vinayak Damodar Savarkar was a polymath who read law in London, enjoyed Shakespeare, admired the Bible, wrote important historical works, and became an accomplished poet and playwright. His lifelong obsession was politics.
Savarkar took up political activity in his teens and became a cherished anti-British revolutionary. While serving a long prison sentence for inciting violence against the British, he transformed into a Hindu supremacist bent on dominating Indian Muslims. His pamphlet Essentials of Hindutva (1923), written secretively in jail, remains the most influential work of Hindu nationalism. In this and subsequent works, he called for Hindus, hopelessly divided by caste, to come together as one homogeneous community and reclaim their ancient homeland from those he considered outsiders, primarily the Muslims. Savarkar advocated violence against Muslims as the principal means to bind antagonistic lower and upper castes, writing:
Savarkar has proven prescient if not prescriptive. Over the past four decades, the Hindu Right’s violence against Muslims has indeed helped Modi’s Bharatiya Janata Party (BJP) to cement a degree of Hindu political unity long considered unattainable.
Some of Savarkar’s views on Hindus and their religion embarrass the Right. An agnostic, Savarkar declared that Hindutva – his construction of Hindu nationalism – was bigger than Hinduism, the actual religion of the Hindus. Later in life, he railed against Hindus and urged them to become more like Muslims (or his perception of them). Writing about Muslims in the medieval period allegedly raping and converting Hindu women any chance they got, Savarkar characterised it as ‘an effective method of increasing the Muslim population’ unlike the ‘suicidal Hindu idea of chivalry’ of treating the enemy’s women with respect. He wrote disparagingly about cow worship and other Hindu practices, and refused to discharge the funeral rites for his devout Hindu wife. Although Savarkar’s Hindutva helped inspire the launch of the BJP’s parent organisation, Rashtriya Swayamsevak Sangh (RSS), a century ago, he was disdainful of its decision to avoid direct political participation. ‘The epitaph for the RSS volunteer will be that he was born, he joined the RSS and he died without accomplishing anything,’ he reportedly said.
Until Modi became prime minister in 2014, Savarkar was known to few Indians, and those few knew him as a minor freedom-fighter. Since then, the BJP-RSS have placed Savarkar at the centre of their efforts to rewrite Indian history from a Hindu supremacist perspective. Today’s BJP positions Savarkar as a nationalist icon on a par with Jawaharlal Nehru and Mahatma Gandhi, if not greater. If Savarkar’s ‘repeated warnings against the Congress’s appeasement politics’ had been heeded, India could have avoided Partition, the separation of Pakistan from India, writes Mohan Bhagwat, the RSS chief.
In fact, this invocation of Savarkar disguises a much more complicated history that the Right is desperate to suppress.
S avarkar was born in 1883 to a Brahmin family near Nashik, a city in western India. In the first part of Vikram Sampath’s extensive, hagiographical biography of 2019, Savarkar is presented as a child prodigy who loved reading and lapped up Hindu epics, books, newspapers and political journals in Marathi – his mother tongue – and English. A newspaper ran one of his Marathi poems when he was 12; another published an article of his on Hindu culture.
The Savarkar brothers; from left: Narayan, Ganesh and Vinayak. Courtesy Wikipedia
The second of four siblings, Savarkar lost his mother to cholera when he was nine, and his father to the plague seven years later. Still in his teens, he formed a secret society of young revolutionaries against the British. According to Sampath, he found the constitutional methods of the Indian National Congress – an organisation gently pushing local interests – unappealing, and instead drew inspiration from the few revolutionaries who assassinated British officials. Savarkar would give speeches on historic nationalist movements to his secret society and extol the 19th-century European nationalist revolutionaries Giuseppe Garibaldi and Giuseppe Mazzini, who exercised considerable influence on his thought. After his marriage to a Brahmin girl was arranged by his uncle, Savarkar enrolled in college in 1902 for a major in the arts. He studied widely, reading Sanskrit and Greek classics, English poetry, international history and biographies of revolutionaries.
After graduation, Savarkar moved to London to read law but also to continue his political activity in the enemy’s bastion. He stayed at a boarding house for Indian students, where he met many co-conspirators, not a few of whom he helped to radicalise. Abhinav Bharat, Savarkar’s secret organisation, would smuggle arms and bomb-manuals to India; in 1909, the group assassinated William Hutt Curzon Wyllie, an aide to the Secretary of State for India, in London. Savarkar had already worried the British enough that, by the time he arrived in London in 1906, they had put him under surveillance. In 1910, he was arrested and deported to India to be tried. By this time, India had endured British colonial rule for more than a century. Colonial narratives greatly influenced the worldviews of Savarkar and other Indian nationalists.
How could a vast nation like India be conquered by a distant island a fraction of its size and population?
Over a 70-year period starting in the 1750s, the British East India Company defeated both European and local rivals and turned the Mughal dynasty that had ruled India for more than 200 years into its puppet. Britain’s barbaric traders carried out their conquest through loot and rapacity, while its scribes, missionaries and historians provided the moral justifications by portraying India as a degenerate civilisation that British rule might redeem. Some European thinkers, Orientalists and Romantics valorised ancient Hindu India as the cradle of civilisation, but they too lamented its decay.
Under British colonialism, elite Hindus often accepted the British narratives for colonial rule. They were especially tortured by the question: how could a vast nation like India be conquered by a distant island a fraction of its size and population? Such musings about Indian or Hindu history furthered the development of Indian nationalism. By assuming that a ‘national’ Hindu-Indian identity had existed since time immemorial (it hadn’t), elite Hindus felt driven to recover their Hindu-Indian identity in the present. In fact, until British rule, people in the subcontinent hadn’t seen themselves as Hindu (or Muslim) in the modern sense. They balanced various identities, including those of place, caste and family lineage; religion merely provided one among several, as the political theorist Sudipta Kaviraj and others have written . However, in the 19th century, some upper-caste Hindus, awed by the power of Britain’s military and industrial superiority, launched vigorous movements to ‘purify’ their religion and make it more like Christianity. They moved to cast off what they saw as the appendages dragging down Hinduism – the inegalitarian caste system, the large diversity of gods, sects and practices – believing this reformation would make India great again.
British historical narratives portrayed Hindu-Muslim enmity as a fundamental, self-evident feature of Indian history. In reality, religious pluralism and toleration – not fanatical religious hatred – had been the norm among people of various religions in South Asia. In The Loss of Hindustan (2020), the historian Manan Asif Ahmed writes that, before British rule, many elite Hindus and Muslims had thought of Hindustan as a homeland not only of the Hindus, but of the ‘diverse communities of believers’ including Muslims and Christians. British colonialism constructed a different narrative, one in which Hindus had been subjugated in their home for 1,000 years by Muslim invaders. This distorted the South Asian experience of Hindustan into claims of immutable enmity between Hindus and Muslims.
The British census aggregated Hindus and Muslims across India into homogeneous groups and facilitated the creation of solidarity – and belligerence – among them. Towards the end of the 19th century, colonial influences combined with what the historian Christopher Bayly in 1998 called ‘old patriotisms’ to contribute to the invention of a pan-Indian Hindu nationality, and a more inchoate Muslim nationality.
W orking in this legacy, Savarkar made his first lasting contribution to Indian politics in 1909, with the publication of a historical work, The Indian War of Independence of 1857 . In 1857, large numbers of Indian soldiers and gentry in northern and western India had risen under the banner of the fading Mughal dynasty in the largest armed uprising against the British Empire by a ruled people. British historians had played down this war as a ‘sepoy mutiny’, restricted to disgruntled soldiers rather than a polity – a view Savarkar set out to correct. In Hindutva and Violence (2021), an authoritative work on Savarkar, the historian Vinayak Chaturvedi shows that Savarkar was a master at reclaiming Indian history from the British by reading colonial records and works of scholarship ‘against the grain’. Drawing inspiration from the French and American revolutions as well as the ultranationalism of Mazzini, Savarkar reconstructed 1857 as the ‘first war’ for Indian independence. To this day, 1857 is understood as such in India. His passionate, romantic account glorified Indian war heroes with the intent of inspiring a revolution against the British.
In the book, Savarkar introduced the central motif in his historical works: violence as mystical unifier. He held that Hindus and Muslims had become united for the first time ever during the war through the means of violence. The literal ‘shedding of [British] blood’ together had forged the Hindu-Muslim bond, as the political theorist Shruti Kapila characterises Savarkar’s idea in Violent Fraternity (2021). Savarkar’s conception of Hindu-Muslim history had been partly shaped by the long tradition of religiopolitical enmity against the Mughals in his homeland of Maharashtra, as the historian Prachi Deshpande shows in Creative Pasts (2007). But Savarkar, always the innovative thinker, borrowed only what suited his purposes. He wrote that, since Hindu kings had avenged centuries of Muslim oppression by defeating the Mughals in the 18th century, the ‘blot of slavery’ had been ‘wiped off’. Having re-established their ‘sovereignty’ at home, they could now fraternise with Muslims. And finally, such was the power of the violence in 1857 that India now became ‘the united nation of the adherents of Islam as well as Hinduism’. Indian War and its author were admired across the political spectrum.
Nearly driven to suicide, he filed mercy petitions, abjured revolution, and promised to serve the empire
The book was the high point of Savarkar’s youth. Soon he lost his infant son to smallpox, and his elder brother was arrested for treason. In 1910, Savarkar himself was sentenced to life imprisonment at the Andamans, a brutal penal colony in the Bay of Bengal. He had become notorious on account of the violent activities of his secret society. But more than this, it was his ‘seditious’ writings with their potential to sow widespread disaffection that had threatened the British, the historian Janaki Bakhle wrote in 2010.
Prison broke Savarkar. In his autobiography, Savarkar writes about frequently suffering from dysentery, lung disease and malaria. He was put in solitary confinement for months, and for eight years was denied permission to see his wife. The Irish jailor was sadistic, and Muslim warders were cruel to Hindus. Nearly driven to suicide, he filed mercy petitions, abjured revolution, and promised to serve the empire (the issue most debated about Savarkar today). The petitions were rejected but in the early 1920s Savarkar was moved to a less harsh prison in western India.
By then, Gandhi’s leadership of the Indian National Congress had revolutionised Indian politics. His religiosity and asceticism attracted the masses to the independence movement, which had been limited to a tiny section of educated Indians. But, unusually, Gandhi emphasised nonviolence, ethical conduct, social reform and Hindu-Muslim unity as much as political independence. He also often upset fellow nationalists. After the collapse of the Ottoman Empire, some Indian Muslims launched a movement to compel the British to preserve the institution of the Islamic Caliphate, a symbol of international Muslim solidarity. Gandhi encouraged Hindus to join in, even though they had no stake in the cause.
Savarkar had met Gandhi, and had disdain for the man and his politics, which seemed to him anachronistic and effeminate. The Caliphate movement also triggered Savarkar’s fears about India being invaded again by Muslims. This wasn’t simply Islamophobia. Many elite Muslims resisted the slow democratisation unfolding through the colonial period, for fear of losing out to Hindus. They saw themselves as India’s historical rulers whose say in its affairs ‘could not be merely proportionate to their numbers’, as the political scientist Christophe Jaffrelot writes in The Pakistan Paradox (2015), a history of Pakistan. Some Muslim leaders used the rhetoric of pan-Islamism and threats of violence to push their claims with the British. After the Caliphate movement, Savarkar felt that Indian War ’s paean to a composite nationalism had been rejected by Indian Muslims because of their ‘divided love’ (the other interest being Muslims outside India); he reacted like a ‘spurned lover’, writes Bakhle in 2010.
I n Hindutva , Savarkar applied the European framework of nationalism – that a nation needed a homogeneous community, a common culture, a long history – to the subcontinent. In western European nations and the United States, Christianity, race and language had offered the basis for a common history and identity (or so their nationalists claimed). But what could work for India? Hinduism, the religion of the majority, seemed unfit since it lacked a unifying mechanism of one book or church. India’s resident Muslims, Christians, Sikhs, Jains, Buddhists and others also bitterly resented attempts to hitch an Indian nationality to Hinduism. Hinduism thus posed ‘the main obstacle’ in Savarkar’s quest for a big-tent Indian identity, as Kapila notes. To resolve this conundrum, unlike religious nationalists, Savarkar strove to secularise Hindus – instead of Hindu scriptures, he chose as the foundation of his ideology the discipline of history, the paradigmatic secular form of the enlightened political thinker.
By turning to history, Savarkar wanted to show that followers of all religions born in India – Hinduism, Sikhism, Buddhism, Jainism – owed allegiance to a common genealogy: Hindutva, or Hindu-ness. ‘Hindutva is not a word but a history,’ Savarkar wrote in his pamphlet. He also seized the chance to redefine who is a Hindu. Essentially anyone whose ‘fatherland’ and ‘holy land’ resided within the subcontinent qualified as Hindu, he concluded. Not only followers of Hinduism, but Sikhs, Jains and Buddhists counted as Hindus – a novel interpretation. Muslims and Christians, however, were outsiders as their holy lands lay beyond India, he emphasised. The influence of social evolutionism was clear. Hindus must remember that ‘great combinations are the order of the day,’ Savarkar wrote. ‘The League of Nations, the alliances of powers Pan-Islamism, Pan-Slavism, Pan-Ethiopism, all little beings are seeking to get themselves incorporated into greater wholes, so as to be better-fitted for the struggle for existence and power.’
Savarkar ridiculed nonviolence, which, along with Muslim hatred, became his lifelong obsession
Savarkar believed that Hindu India needed to assimilate audaciously as well as exclude ruthlessly to recover its lost glory.
He theorised that Hindu identity had been formed chiefly through violence, Chaturvedi notes, whether it was in the Islamic period that lasted more than a millennium starting in the 8th century or even earlier. In the long war with the Muslims, ‘our people became intensely conscious of ourselves as Hindus and were welded into a nation to an extent unknown in our history,’ Savarkar wrote in Hindutva . He ridiculed nonviolence – to negate Gandhi’s ideas – which, along with Muslim hatred, became his lifelong obsession.
Eloquently written with a clear sense of urgency, Hindutva became The Communist Manifesto of the Hindu Right. Soon after its publication, K B Hedgewar, a former Congress member from Savarkar’s homeland, founded the Rashtriya Swayamsevak Sangh (RSS) in 1925. He conceived it as a sociocultural organisation that would transform the character of Hindus through indoctrination and paramilitary training, and make them masculine in order to defeat ‘outsiders’. Hedgewar thought RSS would stay away from direct politics. It would operate in the shadows to avoid backlash from the British, and build Hindu unity from the ground up to realise a Hindu nation in the future.
I n 1924, Savarkar was released from prison after 13 years inside. Still banned from political activity and put under house arrest, he launched social-reform initiatives and became a prolific writer of plays, poetry, articles and historical works. Despite opposition from orthodox Hindus, he campaigned aggressively against untouchability and in favour of intercaste dining and marriage. ‘A national foolishness’ that created ‘eternal conflict’ among Hindus, the caste system deserved ‘to be thrown in the dustbins of history,’ he wrote. His aim was to dissolve barriers enough for Hindus to realise political unity; caste discrimination, not caste itself, was his target. Despite Gandhi’s emergence, Savarkar still burned to become the leader of the Hindus. In his autobiographical works, blissfully free of modesty, Savarkar presented himself as a great Hindu in an ancient line of civilisational warriors. After his death, it emerged that one of his adulatory ‘biographies’ may have been authored by Savarkar himself.
In 1937, after he was allowed to re-enter politics at the age of 54, Savarkar assumed the presidency of the Hindu Mahasabha, a former wing of the Indian National Congress that broke out as a militant Hindu party. Anxious to stay away from prison, he greatly tempered his anti-British stance. Instead, he took aim at his two obsessions: Gandhi and the Muslims. But Savarkar, whose strengths lay in literary writing and polemics, lacked the energy and vision to mount a serious challenge against the Congress. His health had never fully recovered from the prison ordeal, and help from the RSS was inconsistent. Even though its members sometimes participated in Congress-led campaigns against the British, the RSS as an institution largely stayed out of the independence movement. RSS leaders and Savarkar were ambiguous about the Congress-led struggle partly because of their hatred of Gandhi’s politics of nonviolence and his pursuit of Hindu-Muslim unity.
In 1948, Nathuram Godse, one of Savarkar’s acolytes, assassinated Gandhi
Flailing around on the periphery of power, Savarkar could only lash out at Gandhi’s ‘appeasement’ of Muslims. When in the 1930s the Muslim League began to demand a separate nation carved out of India for Muslims, he was appalled (as were other Hindu politicians including Gandhi and Nehru, although for different reasons). Desperate to avoid conceding land to Muslims, Savarkar called for one secular state with equal rights for everyone, where minorities would be free to practise their religion. But he revealed his hand by accusing Muslims of anti-Indian activities; meanwhile, on the ground, his party stoked communal polarisation and organised violence against Muslims. Unlike Gandhi, Savarkar agreed with Muhammad Ali Jinnah, the leader of the Muslim League, that Hindus and Muslims constituted ‘two nations’; but, obsessed with establishing Hindu supremacy, he opposed the creation of Pakistan.
Savarkar and other Hindu extremists blamed Gandhi for the bloody Partition of 1947, the division of India into Muslim-majority Pakistan and Hindu-majority India overseen by the British. They were incensed by the fast the old man undertook to compel India to give money owed to Pakistan. In 1948, Nathuram Godse, one of Savarkar’s acolytes, assassinated Gandhi. Savarkar’s reputation was irredeemably stained. He was put on trial for allegedly conspiring to murder Gandhi. His fear of returning to prison was so intense that in court he distanced himself from Godse, who was hurt by his mentor’s ‘calculated, demonstrative non-association’. After his acquittal, Savarkar withdrew from politics and spent the rest of his life in anonymity.
I n the first three decades after independence, the Indian National Congress dominated Indian politics. Drawing on the legacy of the freedom struggle, Nehru and his successors attempted to cultivate a secular democratic culture. In this period, the Hindu Right struggled politically even as the RSS multiplied its presence across India. Godse had been an RSS member and the organisation was widely seen as culpable in the murder. Banned for 18 months after the assassination and fighting for its survival, the RSS was compelled to enter politics directly. It decided to people a new Hindu nationalist party, the Bharatiya Jana Sangh, with its members. The safeguard turned into a permanent feature, as the allure of political power proved to be too seductive.
In 1963, Savarkar – hobbled by old age and ailments – published his final historical treatise, Six Glorious Epochs of Indian History . The ‘glorious epochs’ referred to those eras when civilisational warriors freed the Hindu nation ‘from the shackles of foreign domination’. In this ambitious work, Savarkar excavates a triumphant Hindu will to power in history so as to furnish a guide to establishing a Hindu nation. He spends a majority of the book on the Hindu-Muslim encounter, which he characterises as an ‘epic war’ that lasted more than a millennium.
Savarkar essentially prescribed ‘permanent’ war for Hindus within their homeland
Six Glorious Epochs is striking for its vicious polemic – against Hinduism, Buddhism and, most of all, against Hindus. Reminiscent of Friedrich Nietzsche’s hatred of Christianity and lay people, Savarkar rants at the ‘perverted sense of virtues’ of the Hindus, like nonviolence, religious tolerance and ethical conduct in war. Hindus, according to Savarkar, had been corrupted by Buddhism and its nonviolent creed (like Christianity-corrupted Roman culture in Nietzsche’s telling). He writes that nonviolence ‘emasculates human beings’ and that it ‘should at times be killed by cruel violence!’ Savarkar castigates past Hindu rulers for their ‘suicidal’ practices; he moans that they did not massacre Muslims en masse after winning battles, avoided raping Muslim women, refrained from enacting forcible conversions, and did not destroy mosques. According to him, this is precisely what Muslims did to Hindus, an attitude he praises as ‘highly pious and thoroughly sound’ in war. But their ‘perverted sense of virtues’ had made Hindus ‘slovenly and imbecile, and insensible to all sorts of shameful humiliation’.
The Hindu will to power was manifest only in a few ‘heroic men and women warriors’; the rest suffered from the Savarkarist version of false consciousness. He was clear that, in order to realise their latent Hindu-ness, Hindus would have to relinquish the values they held dear. Savarkar essentially prescribed ‘permanent’ war for Hindus within their homeland, as Kapila and Chaturvedi both note .
Written in the aftermath of Partition, Gandhi’s martyrdom, the unrelenting dominance of the Congress and Savarkar’s own disgrace, his bitterness in Six Glorious Epochs is a giveaway: the lover first spurned by the Muslims had been rejected by his Hindus too. In 1966, the ailing Savarkar died by suicide, aged 82.
I n 1975, the prime minister Indira Gandhi, Nehru’s daughter, suspended democracy and imposed authoritarian rule, which later drew great public anger. Within two years, the Indian National Congress was voted out of power for the first time and a makeshift grouping of parties that included the Hindu nationalist Bharatiya Jana Sangh formed the union government. The Congress soon bounced back but its dominance had ended.
In the 1980s, the erstwhile Jana Sangh, now reinvented as the BJP, spearheaded the Rama Temple movement that permanently changed Indian politics. Riding an old myth, the BJP and its allies claimed that a mosque in the northern city of Ayodhya had been built by 16th-century Islamic invaders over the ruins of a Rama temple at the deity’s alleged birthplace. The desecration of his birthplace was a living symbol of Hindu India’s historical oppression by Muslims, the BJP thundered, as it feverishly mobilised the masses to restore the temple. Worshipped devoutly by hundreds of millions of Hindus, Rama proved to be irresistible: in 1992, the mosque fell to a Hindu mob. The BJP went from winning just two seats out of more than 500 in 1984, to the head of the ruling coalition by 1998. Since 2014, Modi, who played a minor role in the Rama temple campaign, has dominated Indian politics.
The Rama temple evangelism was manufactured by an insurgent BJP primed to knock over the decrepit ancien régime of the Congress. It is the same former insurgent – now a dominant but deeply insecure incumbent, haunted by its discreditable past – that orchestrates the Savarkar propaganda. Both campaigns share a common feature: the Right’s felt need to locate its legitimacy in history. The BJP has carried on Savarkar’s legacy of turning to history instead of Hindu religious texts for validation. It’s not the Vedas or the Bhagavad Gita , the greatest Hindu scriptures, that ordained the BJP’s rule, but the civilisational history of the Hindus that did. Positing an unbroken chain stretching back thousands of years, the BJP-RSS present themselves as the guardians of the great Hindu civilisation, successors to iconic kings like Chandragupta Maurya (reign c 322-298 BCE), Prithviraj Chauhan ( c 1178-92) and Shivaji (1674-80).
The Hindutva antipathy for Gandhi and his methods is hard to hide, indeed central to their formation and history
The significance of their success in appropriating Indian history cannot be overstated. The appropriation allows for the exclusionary politics of the BJP-RSS to subsume, even replace, religious belief. For example, the inauguration of the Rama temple by Modi this January, one of the biggest events in modern Indian history, incited a national frenzy among Hindus. But the spectacle wasn’t mainly a celebration of Rama bhakti (religious devotion). It was about a politically united Hindu community declaring its pre-eminence in its homeland.
If the BJP-RSS have worked very hard to make history – admittedly, partly a colonial one – their strength, it is also their weakness. The RSS is hypersensitive to its shaming non-participation in India’s freedom movement. (This is what Congress party members meant when they called Right-wing leaders ‘anti-national’, which, now, unsurprisingly, is one of the Right’s favourite labels for its critics.) There is no escaping the fact that Indian independence came under Gandhi using Gandhian methods, and the Hindutva antipathy for Gandhi and his methods is hard to hide, indeed central to their formation and history. The Right cannot fundamentally alter public perception of these facts all at once. Savarkar is the one figure who cannot be claimed by the Congress and who has genuine links with the anti-British struggle. His revolutionary past and later marginalisation yield a counterfactual interpretation that can cover somewhat for the Right’s embarrassing absence. In the Right’s telling, Savarkar was sidelined by Gandhi and Nehru while the Hindu polity foolishly rejected Hindutva – Partition was the calamitous outcome of these two decisions. If Hindus had chosen Savarkar’s (and the RSS’s) macho Hindutva over Gandhi’s ‘Muslim appeasement’, they would have reigned supreme in undivided India, it is implied.
The icon of Savarkar thus reminds Hindus: without Hindutva, India’s national security is perennially under threat. Only by heeding ‘the man who could have prevented Partition’ can you secure Hindu India, especially when Islamic terrorism is perceived as a threat, and Muslims constitute 14 per cent of India’s population. Muslims oppressed Hindus for centuries and won a nation for themselves by expropriating Hindu territory – why shouldn’t Hindus become masters in whatever was left of their own ancient homeland? Gandhi had dedicated his life to fighting such realpolitik, a struggle carried on by Nehru after independence.
Hindutva now, however, enjoys wide legitimacy among Hindus of all castes. The BJP won about 37 per cent of the votes cast in the last national election of 2019, but that number greatly understates the public’s approval of Hindutva. Rival parties can criticise the BJP, but they dare not oppose Hindutva. The self-professed secular Congress party, for instance, tends to respond to the BJP’s Savarkar propaganda by questioning his lack of machismo for filing mercy petitions with the British, instead of contesting his Hindu supremacism lest it be seen as anti-Hindu.
As BJP and RSS leaders have brought Savarkar to prominence in Indian politics and thought, a cult of Gandhi’s assassin Godse has flourished among party loyalists. In recent years, statues and even temples dedicated to Godse have cropped up, while Gandhi memorials are defaced.
As resurrected Hindutva icons, they stand in death as they did in life: Savarkar, the guru, behind the pulpit; Godse, the disciple, on the streets. Savarkar would have thought that India’s Hindus today are finally being cured of what he hated as their perverted virtues of nonviolence, tolerance and respect for adversaries."
"Why the son of God story is built on mythology, not history | Aeon Essays",,https://aeon.co/essays/why-the-son-of-god-story-is-built-on-mythology-not-history,"Most New Testament scholars agree that some 2,000 years ago a peripatetic Jewish preacher from Galilee was executed by the Romans, after a year or more of telling his followers about this world and the world to come. Most scholars – though not all.
But let’s stick with the mainstream for now: the Bible historians who harbour no doubt that the sandals of Yeshua ben Yosef really did leave imprints between Nazareth and Jerusalem early in the common era. They divide loosely into three groups, the largest of which includes Christian theologians who conflate the Jesus of faith with the historical figure, which usually means they accept the virgin birth, the miracles and the resurrection; although a few, such as Simon Gathercole, a professor at the University of Cambridge and a conservative evangelical, grapple seriously with the historical evidence.
Next are the liberal Christians who separate faith from history, and are prepared to go wherever the evidence leads, even if it contradicts traditional belief. Their most vocal representative is John Barton, an Anglican clergyman and Oxford scholar, who accepts that most Bible books were written by multiple authors, often over centuries, and that they diverge from history.
A third group, with views not far from Barton’s, are secular scholars who dismiss the miracle-rich parts of the New Testament while accepting that Jesus was, nonetheless, a figure rooted in history: the gospels, they contend, offer evidence of the main thrusts of his preaching life. A number of this group, including their most prolific member, Bart Ehrman, a Biblical historian at the University of North Carolina, are atheists who emerged from evangelical Christianity. In the spirit of full declaration, I should add that my own vantage point is similar to Ehrman’s: I was raised in an evangelical Christian family, the son of a ‘born-again’, tongues-talking, Jewish-born, Anglican bishop; but, from the age of 17, I came to doubt all that I once believed. Though I remained fascinated by the Abrahamic religions, my interest in them was not enough to prevent my drifting, via agnosticism, into atheism.
There is also a smaller, fourth group who threaten the largely peaceable disagreements between atheists, deists and more orthodox Christians by insisting that evidence for a historical Jesus is so flimsy as to cast doubt on his earthly existence altogether. This group – which includes its share of lapsed Christians – suggests that Jesus may have been a mythological figure who, like Romulus, of Roman legend, was later historicised.
But what is the evidence for Jesus’ existence? And how robust is it by the standards historians might deploy – which is to say: how much of the gospel story can be relied upon as truth? The answers have enormous implications, not just for the Catholic Church and for faith-obsessed countries like the United States, but for billions of individuals who grew up with the comforting picture of a loving Jesus in their hearts. Even for people like me, who dispensed with the God-soul-heaven-hell bits, the idea that this figure of childhood devotion might not have existed or, if he did, that we might know very little indeed about him, takes some swallowing. It involves a traumatic loss – which perhaps explains why the debate is so fraught, even among secular scholars.
Secondo Pia’s photograph of the Shroud of Turin (May 1898), digital print from the Musée de l’Élysée, Lausanne. Courtesy Wikipedia
When I’ve discussed this essay with people raised as atheists or in other faiths, the question invariably asked goes something like this: why is it so important for Christians that Jesus lived on earth? What is at stake here is the unique aspect of their faith – the thing that sets it apart. For more than 1,900 years, Christianity has maintained the conviction that God sent his son to earth to suffer a hideous crucifixion to save us from our sins and give us everlasting life. Jesus’ earthbound birth, life and particularly his death, which ushered in redemption, are the very foundation of their faith. These views are so deeply entrenched that, even for those who have loosened the grip of belief, the idea that he might not have been ‘real’ is hard to stomach.
Y ou’d think that a cult leader who drew crowds, inspired devoted followers and was executed on the order of a Roman governor would leave some indentation in contemporary records. The emperors Vespasian and Titus and the historians Seneca the Elder and the Younger wrote a good deal about 1st-century Judea without ever mentioning Jesus. That could mean simply that he was less significant an actor than the Bible would have us think. But, despite the volume of records that survive from that time, there is also no death reference (as there was, say, for the 6,000 slaves loyal to Spartacus who were crucified along the Appian Way in 71 BCE), and no mention in any surviving official report, private letter, poetry or play.
Compare this with Socrates, for example. Though none of the thoughts attributed to him survive in written form, still we know that he lived (470- 399 BCE) because several of his pupils and contemporary critics wrote books and plays about him. But with Jesus there is silence from those who might have seen him in the flesh – which is awkward for historicists like Ehrman; ‘odd as it may seem,’ he wrote in 1999, ‘[i]n none of this vast array of surviving writings is Jesus’ name ever so much as mentioned.’ In fact, there are just three sources of putative proof of life – all of them posthumous: the gospels, the letters of Paul, and historical evidence from beyond the Bible.
Christian historians base their claims for a historical Jesus on the thinnest mentions of early Christians by the Roman politicians Pliny the Younger and Tacitus (who write of Christians they interviewed early in the 2nd century – in Pliny’s case, a tortured female deacon – all followers of ‘The Way’ who talked about Jesus) and by Flavius Josephus, a Romanised Jewish historian. Josephus’s 20-volume Antiquities of the Jews , written around 94 CE, during the reign of Domitian, contains two references to Jesus, including one claiming that he was the Messiah crucified by Pontius Pilate. This would carry some weight if Josephus actually wrote it; but the experts, including evangelicals like Gathercole, agree this reference was likely forged by the 4th-century Christian polemicist Eusebius. The other reference is to ‘the brother of Jesus, who was called Christ, whose name was James’. Some scholars say the ‘called Christ’ bit was a later addition, but it hardly matters when Josephus was drawing from stories told by Christians more than six decades after Jesus’ assumed crucifixion.
If the crucifixion was prophesied, then how can it have been embarrassing?
The earliest evidence testifying to a historical figure comes not from contemporary records, but from the letters of Paul, which date broadly from 50 to 58 CE (of the 14 letters originally attributed to Paul, only half are now thought to be mainly his writing, with the rest thought to be written sometime in the 2nd century) . The problem with Paul for proof-seekers is how little he says about Jesus. If Jesus lived and died in Paul’s lifetime, you might expect he’d refer to Jesus’ ministry on earth – to his parables, sermons and prayers – and that his readers would want this crucial life story. But Paul offers nothing on the living Jesus, such as the stories or sayings that later appear in the gospels, and he provides no information from human sources, referring only to visionary communication with Jesus and to messianic Old Testament quotes.
Which brings us to the gospels, written later, and not by those whose names they bear (these were added in the 2nd and 3rd centuries). The gospel of Mark, which borrows from Paul, came first and set the template for the gospels that followed (Matthew draws from 600 of Mark’s 661 verses, while 65 per cent of Luke is drawn from Mark and Matthew . ) The first version of Mark is dated between 53 CE and around 70 CE, when the Second Temple was destroyed, an event it mentions. The last gospel, John, which has a different theology and stories that contradict those of the three ‘synoptic’ gospels, is dated at around 100 CE. All four gospels include sections written in the 2nd century (among them, two different virgin birth narratives in Matthew and Luke), and some scholars place the final 12 verses of Mark in the 3rd century. Several historians assume that Matthew and Luke had an earlier source they call Q. However, Q has never been found and there are no references to it elsewhere. Barton suggests that a belief in Q may serve a ‘conservative religious agenda’ because to say these gospels drew from an earlier source ‘is an implicit denial that they made any of it up themselves’.
Taken together, what can the gospels tell us about the historical Jesus? Secular scholars agree that much of their content is fictional, and note, as Ehrman puts it, that ‘these voices are often at odds with one another, contradicting one another in minute details and in major issues’. And yet Ehrman is convinced that Jesus existed; he contends that the gospel writers heard reports about Jesus and ‘decided to write their own versions’. A few basic facts, like the dates of Jesus’ birth and death (gleaned from their mention of various rulers), are widely accepted, and several of Jesus’ sayings are said to be close to his real words. To separate the factual wheat from the fictional chaff, they employ ‘criteria of authenticity’ – stories and words that ring true. The three main criteria are: embarrassment (are those details out of step with 1st-century Judaism and, if so, why would the gospel writers invent things that would cause problems?); multiple attestation (the more sources, the better); and coherence (are details consistent with what we know?)
However, there is good reason to interrogate this approach. With regard to the criteria of multiple attestation and coherence, we know the gospel writers borrowed from each other, so we’d expect them to include the same stuff. The gospel of Luke, for instance, borrowed Matthew’s ‘consider the lilies of the field’ speech, but if Matthew’s tale is fabricated, Luke’s repetition hardly adds credibility. In addition, the ‘embarrassment criterion’ relies on our knowing what went against the grain. But the Church was diverse when the gospels were written and we can’t be sure what might have embarrassed their authors . It’s often argued, for example, that the gospel writers went to such great lengths to show that the crucifixion was predicted in the Hebrew scriptures in order to make it palatable to an audience convinced that no true messiah could be thus humiliated. But this argument can be turned on its head if we accept that the crucifixion tale was included because the gospel writers – pace Paul – believed it was required to fulfil prophesy. If the crucifixion was prophesied, then how can it have been embarrassing?
On the subject of the crucifixion, it’s worth noting that, while the four accepted gospels have Jesus sentenced to death by Pontius Pilate, in the non-canonical gospel of Peter it is Herod Antipas who does the deed. The gospel of Thomas, meanwhile, makes no mention of Jesus’ death, resurrection or divinity at all. According to the 4th-century theologian Epiphanius, the Torah-observant Nazorean Christians (thought to have descended from the first group of believers), held that Jesus lived and died during the reign of King Alexander Jannaeus (10- 76 BCE) – a century before Pontius Pilate. And the Babylonian Talmud agrees, claiming that Jesus was executed by stoning and ‘hanging’ in the town of Lydda (not Jerusalem) for ‘immorality, sorcery and worshipping idols’. So, even when the ‘criteria of authenticity’ are met, historical consensus is hard to establish.
T he most concerted effort to separate fact from fiction started in 1985 when a group of mainly secular scholars were drawn together by the lapsed Catholic theologian Bob Funk. Funk’s ‘Jesus Seminar’ met twice a year for 20 years to ‘search for the historical Jesus’. At its launch, Funk said the group would enquire ‘simply, rigorously after the voice of Jesus, after what he really said.’ These scholars (eventually numbering more than 200) used the ‘criteria of authenticity’ to assess the deeds and words of Jesus as reported in the gospels. Many seminars later, following much debate, they concluded that Jesus was an iconoclastic Hellenistic Jewish preacher who told stories in parables and spoke out against injustice; that he had two earthly parents; and that he did not perform miracles, die for people’s sins or rise from the dead. The veracity of his sayings and deeds was decided by a group vote. Scholars were invited to place plastic beads in a box: red (three points) if Jesus said it; pink (two points) if he probably said it; grey (one point) if he didn’t, but it reflected his ideas; black (zero) if invented. When tallied, there were black or grey beads for 82 per cent of Jesus’ Biblical sayings, and 84 per cent of his deeds.
Such methods are regarded as quaint, at best, by scholars researching non-Biblical historical figures. One of those I canvassed was Catharine Edwards, professor of classics and ancient history at Birkbeck, University of London, who said that some historians of the ancient world tend towards scepticism – ‘for example, we can’t really know anything about the earliest stage of Roman history beyond what is gleaned from archaeological evidence’ – while others tend towards ‘extreme credibility’. But, even among those, ‘criteria of authenticity’ are not a familiar tool. She added that the coloured-beads approach ‘sounds naive and on the credulous end of the spectrum where scholars make assumptions about the character of a particular ancient individual and on that basis decide what they think he (invariably) may or may not have said.’
Hugh Bowden, professor of ancient history at King’s College London, said that there was more evidence for the existence of Socrates and Pericles than for Jesus, but ‘much less hangs on it’. The focus on the historicity of Jesus has ‘no real equivalent in other fields, because it is rooted in confessional preconceptions (early Christianity matters because modern Christianity matters) even when scholars claim to be unaffected by personal religious views. Historians in other fields would not find the question very important.’
The sceptics believe that Jesus was a mythical figure who was subsequently historicised
If we remove those preconceptions, it seems commonsensical to apply caution to the historicity of the gospels and let doubt lead our interrogations. The first gospel, Mark, was begun nearly half a century after Jesus’ ministry (and its final verses much later). Jesus’ Aramaic-speaking followers were probably illiterate, and there were no reporters taking notes. The likelihood of Jesus’ words being accurately reproduced by writers who’d never met him, and were elaborating on increasingly fanciful tales passed down through the decades, seems remote.
One scholar who was part of the Jesus Seminar and yet harboured such doubts, is Robert Price, a respected New Testament professor with a PhD in ‘Systematic Theology’, and a former Baptist pastor turned atheist. Price came to query the methodology used to establish historicity, prompting him to doubt whether Jesus ever lived. ‘If there ever was a historical Jesus there isn’t one anymore,’ he said, later writing: ‘There may have been a real figure there, but there is simply no longer any way of being sure.’
Price became the heavyweight figure for a fringe group of ‘Christ myth’ sceptics – historians who propose that early Christians, including Paul, believed in a celestial messiah and that he was placed in history by the gospel writers in the next generation. So, while most of the 200 believe Jesus was a historical figure mythologised by the gospel writers, the sceptics believe the opposite: he was a mythical figure who was subsequently historicised.
Such ideas have been around for centuries. Thomas Paine was an early adopter but it was the 19th-century German philosopher Bruno Bauer who advanced the theory most assiduously. Bauer, an atheist, recognised the gospel themes as literary rather than historical, arguing that Christianity had pagan roots and that Jesus was a mythical creation.
I n recent decades, it has become widely accepted by secular scholars that the Hebrew Bible (Old Testament) is more myth than history. In particular, the Israeli archaeologist Israel Finkelstein and his American colleague Neil Asher Silberman have written in The Bible Unearthed (2002) that none of the patriarchs, from Moses and Joshua backwards, existed as historical figures; that there was no record of Jews having been enslaved in Egypt (instead, they descended from the Canaanites); that David and Solomon were warlords rather than kings; and that the first temple was built three centuries after Solomon. But the view that the Christian Bible is similarly lacking in veracity has, until recently, been drowned out by those arguing for a flesh-and-blood Jesus. One reason for the consensual chorus may relate to the fact that tenured positions in departments dealing with Bible history tend not to be offered to those who doubt that Jesus was real. So the revival of the ‘doubters’ camp’ owes much to the internet, as well as to the missionary zeal of its key proponents.
Momentum began to gather in the 1990s with a series of books by Earl Doherty, a Canadian writer who became interested in scripture while studying ancient history and classical languages. Doherty claimed that Paul and other early Christian writers did not believe in Jesus as an earthly figure, but instead as a celestial being crucified by demons in the lower realms of heaven and then resurrected by God. His views (ironically, on the face of it, the most ostensibly religious, in being so thoroughly spiritualised) were rejected by historical Jesus scholars who claimed that Doherty lacked the academic nous to understand ancient texts. But the next wave, which included Price, was more firmly rooted in academia.
Price believes that early Christianity was influenced by Middle Eastern myths about dying and rising deities that survived into the Greek and Roman periods. One was a Sumerian legend, ‘The Descent of Inanna’, which tells of the queen of heaven who attends an underworld funeral only to get killed by demons and hung from a hook like a piece of meat. Three days later, however, she’s rescued, rises from the dead, and returns to the land of the living.
For ‘Christ myth’ scholars, the Jesus story fits the outlines of the mythic hero archetype
Another is the Egyptian myth of the murdered god-king Osiris. His wife, Isis, finds his body, restores it to life and, via a flash of lightning in one version, conceives his son, Horus, who succeeds him. Osiris goes on to rule over the dead. In Plutarch’s Greek version, Osiris is tricked to lie in a coffin, which floats out to sea before washing up at the city of Byblos. There, Isis removes Osiris’ body from a tree and brings it back to life.
Several Jewish texts in circulation at the time reinforced the messianic aspects of these narratives. For instance, 1 Enoch (a book written mainly in the 2nd century BCE, and particularly revered within the Essene community, thought to be responsible for the Dead Sea Scrolls) refers to the ‘Son of Man’ (a phrase used for Jesus in the gospels) whose name and identity will be kept secret to prevent evildoers from knowing of him until the appointed time.
The favourite ‘Christ myth’ source is the Ascension of Isaiah , written in bits and pieces in the 1st and 2nd centuries. It includes a section dealing with a journey through the seven heavens by a non-human Jesus who is crucified in a lower heaven by Satan and his demonic ‘archons’ who are the rulers of that realm and yet do not know who he is. Again, the story ends with Jesus rising from the dead.
‘Christ myth’ scholars believe that ancient tales of death and resurrection influenced the gospel writers, who also borrowed from Homer, Euripides and the Hebrew Bible. For them, the Jesus story fits the outlines of the mythic hero archetype of the time – a spiritual saviour killed by ‘archons’ before rising triumphant. They contend that later Christians rewrote Jesus as a historical figure who suffered at the hands of earthly rulers.
The rock star of scepticism is Richard Carrier, a Bible scholar with a very modern aptitude for using social media (some of his lengthy YouTube videos have attracted more than a million viewers). He enters into fervent debates with rivals, lectures, and writes acerbic, clinical and fact-laden books. With his PhD in ancient history from the University of Columbia and his record of publishing in academic journals, his credentials are less easily dismissed than Doherty’s. Ehrman, for instance, acknowledges Carrier and Price are serious New Testament scholars.
At one time, Carrier accepted the historicity of Jesus but he became contemptuous of the mainstream position because of what he saw as the parlous state of scholarship supporting it. He and the Australian Bible historian Raphael Lataster use Bayes’ theorem, which considers historical probabilities based on reasonable expectations (weighing up the evidence and attaching mathematical odds to it), to conclude that it is ‘probable’ that Jesus never existed as a historical person, although it is ‘plausible’ that he did.
T he ‘Jesus myth’ advocates get plenty of airplay, but the fringe label has stuck, and not just because religious studies departments freeze them out. Their own methodology has been criticised, not least their use of Bayesian methods. Bizarrely, Carrier offered odds to his readers, concluding that the likelihood of a real-life Jesus was no better than 33 per cent (and perhaps as low as 0.0008 per cent) depending on the estimates used for the computation, which illustrates the wooliness of this use of Bayes’ theorem.
Carrier and his comrades do a fine job poking holes in the methods of historicists but what they offer in exchange seems flimsy. In particular, they have found no clear evidence from the decades before the gospels to show that anyone believed Jesus was not human. Each reference in the epistles can be explained away as referring to a celestial saviour, but it all feels like a bit of a stretch. Paul frequently refers to the crucifixion and says Jesus was ‘born of a woman’ and ‘made from the sperm of David, according to the flesh’. He also refers to James, ‘the brother of Christ’. Using these examples, Ehrman says there’s ‘good evidence that Paul understood Jesus to be a historical figure’. Which was certainly the view of the writer/s of Mark, a gospel begun less than two decades after Paul’s letters were written.
Like the grain of sand that begat Robin Hood, the Jesus story developed fresh layers over time
If we accept this conclusion, but also accept that the gospels are unreliable biographies, then what we are left with is a dimly discernible historical husk. If Jesus did live at the time generally accepted (from 7- 3 BCE to 26- 30 CE) rather than a century earlier as some of the earliest Christians seemed to believe, then we might assume that he started life in Galilee, attracted a following as a preacher and was executed. Everything else is invention or uncertain. In other words, if Jesus did exist, we know next to nothing about him.
One way of looking at it is to think of a pearl, which starts as a grain of sand around which calcium carbonate layers form as an immune response to the irritant until the pearl no longer resembles the speck that started it. Many legends have developed in this way, from the tale of the blind bard Homer onwards.
The outlaw and thief Robert Hod was fined for failing to appear in court in York in 1225 and a year later he reappeared in the court record, still at large. This could be the grain of sand that begat Robin Hood, whom many people assumed to have been a historical figure whose legend grew over the centuries. Robin started as a forest yeoman but morphed into a nobleman. He was later inserted into 12th-century history with King Richard the Lionheart and Prince John (earlier versions had Edward I), along with his ever-expanding band of outlaws. By the 16th century, he and his Merry Men had mutated from lovable rascals to rebels with a cause who ‘tooke from rich to give the poor’.
The Jesus story likewise developed fresh layers over time. At the start of the common era, there may well have been several iconoclastic Jewish preachers, and one of them got up the noses of the Romans, who killed him. Soon his legend grew. New attributes and views were ascribed to him until, eventually, he became the heroic figure of the Messiah and son of God with his band of 12 not-so-merry men. The original grain of sand is less significant than most assume. The interesting bit is how it grew."
What explains the perpetual need for political enemies? | Aeon Essays,,https://aeon.co/essays/what-explains-the-perpetual-need-for-political-enemies,"Listen to this essay
Some people seem driven more by what they oppose, reject and hate than by what they promote, affirm and revere. Their political commitments, personal identities and emotional lives appear to be structured more by opposition, resentment and hostility than by a positive set of ideals or aspirations.
Tucker Carlson, a prominent Right-wing television host and former Fox News anchor, has no shortage of enemies. On his shows, he has condemned gender-neutral pronouns, immigrants, the removal of Confederate statues, mainstream media, the FBI and CIA, globalism, paper straws, big tech, foreign aid, school curricula, feminism, gingerbread people, modern art – and the list goes on. Each item is presented as an existential threat or a sign of cultural decay. Even when conservatives controlled the White House and the US Senate, he presented those like him as under siege. Victories never brought relief, only more enemies, more outrage, more reasons to stay aggrieved.
In April 2025, Donald Trump took the stage to mark the 100th day of his second term as US president. You might have expected a moment of triumph. He had reclaimed the presidency, consolidated power within the Republican Party, and issued a vast range of executive orders. But the mood wasn’t celebratory. It was combative. Trump spent most of his time attacking his predecessor Joe Biden, repeating false claims about the 2020 election, denouncing the press, and warning of threats posed by immigrants, ‘radical Left lunatics’ and corrupt elites. The tone was familiar: angry, aggrieved, unrelenting. Even in victory, the focus was on enemies and retribution.
This dynamic isn’t unique to the United States. Leaders like Narendra Modi in India, Viktor Orbán in Hungary and Jair Bolsonaro in Brazil have built movements that thrive on perpetual grievance. Even after consolidating power, they continue to cast their nations as under siege – from immigrants, intellectuals, journalists or cultural elites. The rhetoric remains combative, the mood aggrieved.
Figures like Carlson and Trump don’t pivot from grievance to resolution. Victory doesn’t bring peace, grace or reconciliation. Instead, they remain locked in opposition. Their energy, their meaning, even their identity, seem to depend on having an endless list of enemies to fight.
So there’s an interesting dynamic: certain individuals and movements seem geared toward perpetual opposition. When one grievance is corrected, another is found. When one enemy is defeated, another is sought. What explains this perpetual need for enemies?
Some people adopt this stance tactically: they recognise that opposition and condemnation can attract a large following, so they produce outrage or encourage grievance as a way of generating attention. Perhaps it’s all an act: what they really want, what they really care about, is maximising the number of social media followers, building brands or getting elected. But this can’t be a full explanation. Even if certain people adopt this tactical stance, their followers don’t: they appear genuinely gripped by anger and condemnation. And not all leaders appear to be calculating and strategic: Trump’s outrage is genuine.
T his pattern of endless denunciation and grievance has been noticed by many scholars. As a recent study puts it, ‘grievance politics revolves around the fuelling, funnelling, and flaming of negative emotions such as fear or anger.’ But what makes this oppositional stance appealing? If it’s not just strategic posturing, what explains it? We can begin answering that question by distinguishing two ways that movements or orientations can be oppositional.
Sometimes, movements face a vast set of obstacles and opponents. Take the protests against the Vietnam War in the 1960s and ’70s. This movement had a clear goal: ending US involvement in Vietnam. It lasted for more than a decade and unfolded across multiple fronts, which ranged from marches to acts of civil disobedience to teach-ins to draft resistance. Participants faced real costs: jail time, government surveillance, public backlash, even violence. The targets of opposition shifted over time – from the Lyndon B Johnson administration, to Richard Nixon, to Gerald Ford. The tactics evolved: from letter-writing campaigns to draft-card burnings, mass marches, lobbying from wounded veterans, and testimony from grieving families. Nonetheless, this was a movement that aimed at a concrete goal. Opposition was necessary, but it was a means to an end. The focus remained on the goal, rather than on sustaining conflict for its own sake.
The anti-apartheid movement offers another example. For decades, activists fought to dismantle a specific political system in South Africa. The struggle demanded great sacrifice and long-term opposition, but these efforts were tethered to a definite goal. Once that goal was achieved, the movement largely dissolved. Its antagonism had a purpose and, when that purpose was fulfilled, the opposition faded.
What’s essential is the continuous expression of hostility, rather than the attainment of any particular goal
The Vietnam War protests and the anti-apartheid movements both involved forms of opposition and grievance, but their antagonism was in the service of positive goals. The movements discussed above – those led by Carlson, Trump, Orbán and so on – are very different. Their energy, coherence and sense of identity seem to depend on opposition itself. Grievance animates their followers; hostility to enemies becomes central to how they think, feel and see themselves. Without enemies, the movement would unravel.
These examples indicate that hostility, anger and opposition don’t necessarily make a movement problematic. On the contrary, they can be signs of moral concern, legitimate reactions to the fact that something precious is being threatened. As Martha Nussbaum has argued, anger can play an essential role in democratic life by expressing moral concern and galvanising collective action. Iris Marion Young has made similar points, showing how opposition can affirm shared values. And in 1968 Martin Luther King Jr claimed that ‘the supreme task is to organise and unite people so that their anger becomes a transforming force.’ But there’s a difference between opposition that aims to realise a shared good, and opposition that is pursued for its own sake. Some movements use opposition as a means to build something they value. Others make opposition itself the point. That’s the distinction I want to highlight: between what I call contingently negative and constitutively negative orientations. Contingently negative movements treat opposition as a means to a positive end, a way of building something better. Constitutively negative movements are different: what’s essential is the continuous expression of hostility, rather than the attainment of any particular goal.
Grievance politics involves a constitutively negative orientation. That’s what sets it apart from liberatory movements, struggles to realise ideals, or efforts to defend cherished values. If you value something, you’ll be disturbed by threats to it. You’ll be sensitive to people who might undermine it, and you may be moved to defend it. That’s normal. That’s part of what it is to value anything at all. But constitutively negative orientations are different. Values are just pretexts for expressing hostility. If one value is secured, we just need a new outlet and a new justification for hostility. The driving need is not to protect or preserve, but to oppose.
But why would anyone be drawn to a constitutively negative orientation? Why are these orientations so gripping? The answer is simple: they deliver powerful psychological and existential rewards. Psychologically, they transform inward pain to outward hostility, offer a feeling of elevated worth, and transform powerlessness into righteousness. Existentially, they provide a sense of identity, community and purpose.
T o see how this works, we need to distinguish between emotions and emotional mechanisms. Emotions like anger, hatred, sadness, love and fear are familiar. But emotional mechanisms are subtler and often go unnoticed. They are not individual emotions; they’re psychological processes that transform one emotional state into another. They take one set of emotions as input and produce a different set of emotions as output.
Here’s a familiar example: it’s hard to keep wanting something that you know you can’t have. If you desperately want something and can’t get it, you will experience frustration, unease, perhaps envy; you may even feel like a failure. In light of this, there’s psychological pressure to transform frustration and envy into dismissal and rejection. The teenager who can’t make it onto the soccer team convinces himself that athletes are just dumb jocks. Or, you’re filled with envy when you scroll through photos of exotic vacations and gleaming houses, but you reassure yourself that only superficial people want these things – your humble home is all that you really want.
Friedrich Nietzsche photographed in 1882 by Gustav Adolf Schultze. Courtesy Wikipedia
There’s a similar mechanism that transforms humiliation and low self-worth into a form of spiteful hatred. Philosophers call this ressentiment – a French word for resentment, but with a twist. It names not just a passing feeling, but a deeper emotional mechanism, one that transmutes pain, powerlessness and humiliation into condemnation. In the late 19th century, Friedrich Nietzsche argued that ressentiment is the emotional mechanism behind many of our values. Modern ‘morality begins’, Nietzsche wrote in On the Genealogy of Morality (1887), ‘when ressentiment itself becomes creative and gives birth to values.’ Since then, a range of thinkers have traced the way that ressentiment shapes social and political life. As Wendy Brown describes it, ressentiment is a ‘triple achievement: it produces an affect (rage, righteousness) that overwhelms the hurt, it produces a culprit responsible for the hurt, and it produces a site of revenge to displace the hurt …’ Put simply, ressentiment is an emotional mechanism that transforms feelings of worthlessness or humiliation into vindictive feelings of superiority, rancour and blame.
His sadness transforms into anger. He has enemies to rail against and grievances to voice
We can see how this plays out in individual lives. Imagine someone who grows up in a declining rural town. She dreams of escape, fantasising about the vibrant lives she sees portrayed in cities, lives full of culture, opportunity, wealth and success. As the years go on, the dream seems unattainable. Jobs are scarce, advancement elusive, and nothing in her life resembles what she once imagined. Frustrated and unhappy, she feels like a failure in life. But then she encounters grievance-filled populist rhetoric. The people she once admired and envied – the people she now identifies as the urban elite – are cast as the cause of her suffering. They are selfish, out of touch, morally corrupt, and hostile to her way of life. What once seemed like an image of the good life now appears as injustice. And, rather than focusing on specific policy proposals for correcting structural economic injustices, she becomes energised by condemnation and hostility.
Or picture another person, a lonely man who watches others form friendships, build relationships, and move easily through social spaces, while he remains on the margins. He feels isolated, sad, alone. One day he stumbles into a corner of the internet that offers an explanation: the problem isn’t him, it’s the world. Reading incel websites, he comes to believe that feminism, social norms and cultural hypocrisy have made genuine connection impossible for someone like him. In time, he internalises this story. His disappointment becomes a source of pride, a mark of insight. His sadness transforms into anger. He has enemies to rail against and grievances to voice.
These cases differ in an interesting way: the economic case involves a real form of structural injustice, whereas the incel case involves an ideology that invents a grievance. But notice that beyond this difference, there’s a similar emotional arc. A person starts out with a positive vision of the good. But their life is full of hardship, disappointment and despair. Initially, they might blame themselves. And that’s painful. It’s hard to sit in one’s own pain, feeling responsible for it, feeling like a failure. It’s especially hard when you see other people enjoying the life that you wish you had.
In time, these people encounter a narrative that redirects the blame. Their unhappiness isn’t their own fault, it’s the fault of someone else. They are being treated unfairly, unjustly; they are being attacked, oppressed or undermined. This kind of story is seductive. It offers release from feelings of diminished self-worth. It offers a way to deflect pain, assign blame and recast oneself as a victim. It also offers a community of like-minded peers who reinforce this story. What emerges is a kind of negative solidarity: bound together through animosity, they attack or disparage an outgroup. The individual now belongs to a group of people who share outrage and recognise the same enemies. The chaos and turmoil of life is organised into a clear narrative of righteousness: in opposing the enemy, we become good.
A s the 20th-century thinkers René Girard and Mircea Eliade remind us, opposition can do more than divide – it can bind. Girard saw how communities forge unity through a common enemy, channelling their fears and frustrations onto scapegoats. This shared act of condemnation offers not just relief, but belonging. Eliade, approaching these points from a different angle, examined our yearning to fold personal suffering into a larger, morally charged drama. Grievance politics draws on both patterns. It doesn’t just vent rage; it weaves pain into a story. It offers a script in which hardship becomes injustice, and outrage becomes identity.
These patterns aren’t just speculative. Scholars have traced how pain, disappointment and a sense of failure can be transfigured into grievance – how personal frustration becomes political identity. In her work on rural Wisconsin, Katherine Cramer shows how economic stagnation can give rise to resentment toward urban elites. Arlie Russell Hochschild, drawing on interviews in Louisiana, describes a ‘deep story’ in which people feel passed over, displaced, left behind. Kate Manne and Amia Srinivasan examine how narratives in incel communities convert rejection and loneliness into a sense of moral entitlement. And a wide range of research in psychology, sociology and philosophy explores how diminished self-worth can be redirected outward: into anger, blame and opposition.
The most effective narratives are superficially plausible. But they tend to be exaggerated and simplistic
When movements are formed and sustained in this way, they are no longer organised around a shared vision of the good. Instead, they are structured by shared animosity. Opposition isn’t incidental. It becomes the structure through which meaning, coherence and solidarity are generated.
Often, these narratives begin with real problems and legitimate grievances – the economic case certainly does. The most effective narratives are superficially plausible. But they tend to be exaggerated and simplistic. It may be true that economic opportunities are scarce and financial security is precarious. But the ressentiment narrative turns this into a story of blame and hostility, painting a simple picture of who is responsible and what can be done about it. It transforms genuine frustration into wholesale animosity.
And that’s why these movements need enemies. They define themselves through rejection. Unlike contingently negative orientations – which are built around the pursuit of some good, some value worth realising – constitutively negative orientations draw their energy from resistance, antagonism and negation. Their integrity depends on the persistence of something to oppose. The result is a kind of political metabolism that requires enemies to function. If the enemy disappears, the orientation loses its shape.
This is not simply a matter of having enemies, which is common to many political movements. Nor is it a criticism of all forms of opposition; many just causes require resistance and a focus on enemies. The key point is structural: in constitutively negative orientations, opposition is pursued for its own sake. Opposition is no longer a means to an end; it is the end itself. Resolution becomes a threat rather than a goal, for resolution would rob the movement of the very antagonism that gives it purpose.
V iewed in this light, the Carlson monologues and Trump rallies aren’t simply strategic or performative. They’re sustaining a structure of belonging built around the rhetoric of attack. What the movements share is an inability to rest, to consolidate, to affirm. They live through negation.
With all of that in mind, we can now see the structure of grievance politics more clearly. In the traditional picture, grievance begins with ideals. We have definite ideas about what the world should be like. We look around the world and see that it fails to meet these values, that it contains certain injustices. From there, we identify people responsible for these injustices, and blame them.
But grievance politics operates differently. It begins not with ideals, but with unease, with feelings of powerlessness, failure, humiliation or inadequacy. Political and ethical rhetoric is offered that transforms these self-directed negative emotions into hostility, rage and blame. Negative emotions that would otherwise remain internal find a new outlet, latching on to ever-new enemies and grievances. The vision that redirects these emotions will cite particular values and goals, but the content of those values and goals doesn’t matter all that much. What’s most important is that the values and goals justify the hostility. If the world changes, the values and ideals can shift. But the emotional need remains constant: to find someone or something to oppose.
That’s why traditional modes of engagement with grievance politics will backfire. People often ask: why not just give them some of what they want? Why not compromise, appease or meet them halfway? Surely, if you satisfy the grievance, the hostility will subside?
Devotion is capable of bringing deep, serene fulfilment without requiring an enemy
But it doesn’t. The moment one demand is met, another appears. The particular goals and demands are not the point. They are just vehicles for expressing opposition. What’s really being sustained is the emotional orientation: the need for enemies. Understanding grievance politics as a constitutively negative orientation – as a stance that draws its energy and coherence from opposition itself – changes how we respond. It explains why fact-checking, appeasement and policy concessions fail: they treat symptoms, rather than the cause. If opposition itself is the source of emotional resolution and identity, then resolution feels like a loss rather than a gain. It drains the movement’s animating force. That’s why each appeasement is followed by a new complaint, a new enemy, a new cause for outrage. The point is not to win; the point is to keep fighting and condemning.
Seeing the dynamic in this way also clarifies what real resistance would require. The aim isn’t just to rebut false claims, to condemn hostility or to attempt appeasement. The solution is to redirect the energies that grievance politics mobilises. To do so, we need alternative forms of meaning, identity and belonging, which satisfy those needs in a way that doesn’t depend on hostile antagonism. We need an orientation that is grounded not in grievance, but in affirmation. One that draws strength not from hostility, but from commitment to something worth loving, revering or cherishing.
What we need, then, are narratives that can sustain devotion. Devotion is a form of attachment that combines love or reverence with commitment and a willingness to endure. It orients a person toward something they regard as intrinsically worthwhile – something that gives shape to a life, even in the face of difficulty or doubt. Like constitutively negative orientations, devotion can provide identity, purpose and belonging. But it does so without requiring an enemy. Its energy comes not from opposition, but from fidelity to a value that’s seen as worthy of ongoing care.
In my own work, I’ve argued that devotion can supply a stable sense of meaning, identity and purpose, without lapsing into antagonism and dogmatism. This picture resonates with Josiah Royce’s claim that loyalty – which he understands as a form of devotion – provides ‘a personal solution of the hardest of human practical problems, the problem: “For what do I live? Why am I here? For what am I good? Why am I needed?”’ It aligns with Harry Frankfurt’s claim that a person’s life is meaningful only if it is devoted to goods that the person cares about for their own sake, and with Thomas Aquinas’s observation that ‘the direct and principal effect of devotion is the spiritual joy of the mind …’ Devotion, so understood, is a steadfast responsiveness to what we cherish, capable of bringing deep, serene fulfilment without requiring an enemy.
Of course, offering devotion as an alternative to grievance politics does not mean dismissing all grievances. Many forms of suffering and injustice – economic inequality, systemic racism, political exclusion – warrant deep frustration and sustained protest. To feel aggrieved in the face of real harm is not pathological; it is often morally appropriate. Anger, complaint and critique are vital political tools. What makes grievance politics problematic is not the presence of complaint, but the constitutively negative orientation. Grievance politics is not rooted in a desire to repair or transform, but in a need to oppose. The problem isn’t the grievance itself – the problem is when perpetual grievance becomes the whole point, and opposition displaces aspiration.
Grievance politics offers coherence, energy and a sense of belonging. But it does so by centring life around perpetual opposition. Its psychological and existential satisfactions are real, but profoundly damaging. When identity is built through antagonism, it becomes dependent on conflict. And that means it can’t stop; it can’t rest. The deeper challenge, then, isn’t just to rebut its claims or counter its policies. It’s to offer orientations that can sustain identity, meaning and solidarity without requiring an endless sea of enemies. That’s a harder task – but it’s the only hope for combatting the politics of grievance."
Our political moment is ripe for David Cronenberg’s body horror | Aeon Essays,,https://aeon.co/essays/our-political-moment-is-ripe-for-david-cronenbergs-body-horror,"Listen to this essay
What does government govern? What, in other words, is government the government of ? The answer – at least in the West – has shifted over time. In the age of religion, kings and queens ruled over souls, preparing them for the divine beyond. After the Enlightenment, the soul gave way to the mind as the focus of governance. By the late 18th century, the target had shifted again. In the first volume of The History of Sexuality (1976), the philosopher Michel Foucault observed that, as the 19th century approached, government was turning its gaze to the body itself. Biological life was no longer incidental to politics: life and death, sickness and health became objects of management, control and regulation. Foucault called this new regime ‘biopolitics’ or ‘biopower’.
Across the centuries since, the government of bodies has grown only more visible – and more contested. So much of our politics now revolves around managing them. We see it in battles over trans participation in sports and access to abortion, the furore over Elon Musk’s Neuralink brain implants, and the push to regulate red dye in food. Even an issue as seemingly distant as school funding is, at bottom, a struggle over the body; that makes sense, since research shows how profoundly the environment impacts brain development and neuroplasticity from childhood all the way through early adulthood. When we talk about school budgets, we’re talking about the kinds of brains (and bodies) we want to produce. When we worry about AI in classrooms, we’re worrying that students may never form the pathways of critical thought and focus they need.
The reigning biopolitical disputes hinge on deceptively simple questions: what is the body for ? What should we do with it? Are there levels of biology where a difference of degree becomes a difference of kind? If a human body changes (or is changed) beyond a certain point, does it stop being an altered human body and become, instead, something else?
There’s no shortage of modern artists and thinkers wrestling with these questions. The choreographer Meg Stuart pushes bodies to extremes of movement; the multimedia artist ORLAN and the transgender artist Cassils use performance to test the boundaries of flesh and identity; another performance artist, Stelarc, stages the body as machine, grafted with prosthetics. Patricia Piccinini’s sculptures imagine hybrid anatomies, while the films of Julia Ducournau – director of the body horror Titane (2021) – and Claire Denis probe bodily desire and transformation. In scholarship, Yuval Noah Harari tracks the future of the human species, Kate Crawford critiques the bodily costs of AI, and Paul B Preciado theorises on gender transition and pharmacopolitics.
And yet, few voices have been as persistent – or as transformative – as David Cronenberg’s. Since the 1970s, he has been cinema’s great anatomist, staging dramas of growth, decay and mutation. Over the decades, his vision has shifted: from a romantic belief that altered bodies deserve celebration, to a more careful insistence that people should be free to alter themselves only if they choose. The arc feels natural, but it is also urgent right now. At a moment when the fight over bodies threads through disputes on everything from vaccines to elder care, Cronenberg offers a framework we need: a way to affirm bodily autonomy without stoking the panic that casts every transformed body as a threat. His cinema points toward a politics of protection – one that secures the vulnerable while refusing to weaponise their difference, and that shows how the defence of bodies can be a form of solidarity rather than a spark for fear.
‘J ust because you’re paranoid doesn’t mean they aren’t after you.’ That’s a quote often, though wrongly, attributed to Joseph Heller’s novel Catch-22 (1961). But it might as well be the theme of Cronenberg’s cult classic Videodrome (1983). The film’s ‘they’ are the menacing minds at Spectacular Optical – a supposed global corporate citizen whose public face is the production of reading glasses for the developing world, while its true business is weapons technology. The company is run by Barry Convex, who tells the hapless protagonist Max Renn (James Woods) not about missiles, but about another product altogether: Videodrome.
The name refers to a top-secret video stream picked up by pirate TV stations like the one Max runs in Toronto – Civic-TV. Part-snuff, part-hardcore porn and entirely unburdened by sentiment, Videodrome delivers in brutal closeup the sadomasochistic torture and murder of its ‘contestants’. Unbothered by the violence and desperate to attract more audience to his flagging station, Max resolves to license Videodrome for Civic-TV . Max is hardly a prude. Reviewing clips of erotica that his production assistant Harlan recommends, Max responds only that ‘There’s something too soft about it.’ He wants something ‘tough’ enough to ‘break through’ and finds it in Videodrome. The world may be a ‘shithole’, says Max, but at least TV still thrills.
Like the film’s audience, Max is surprised to discover such unhinged, Dionysian material produced by a weapons manufacturer. But ultimately the logic becomes clear: Convex reveals himself to be a scolding emblem of the emergent Moral Majority. He can’t understand why anyone would watch a ‘scum show’ like Videodrome. So, he discloses to Max, he produces it precisely to rid the earth of such people. Videodrome optically triggers the growth of a malignant brain tumour in people who watch it repeatedly. (Hence the use of ‘drome’ in the game’s name, from the Greek dromos , meaning ‘track’ – later folded into the medical term prodromal , for the early symptoms of a disease.)
Sex and violence, to his mind, are crucial emblems of liberal society, and essential elements of democracy itself
Echoing both the recently elected Ronald Reagan and his allies, Convex bemoans that ‘North America’s getting soft, patrón , and the rest of the world is getting tough. Very, very tough … we’re going to have to be pure and direct and strong, if we’re going to survive them.’ Purveyors of smut like Max, Convex complains, are ‘rotting us away from the inside’ with their morally bankrupt entertainments. A foot soldier on behalf of a larger biopolitical system in the Foucauldian sense, Convex thus deploys Videodrome to destroy the weak and create a standing army of able bodies ready to serve political aims.
Convex’s murderous speculations horrify Max. Yet, for audiences in 1983, it wasn’t hard to see the dark convergence between Convex’s ‘strong’ moral virtue and the genocidal politics of the day, targeting the vulnerable like gay men, heroin addicts and others who just needed help. The wages of ‘sin’ for both Convex and his real-world analogues in the evangelical, fundamentalist movements riding high when the film was made were paid in death; the government of bodies as well as souls was on the rise, on the screen, in the streets, all around.
Max’s discovery of this repellent politics radicalises him, pushing him beyond the ambivalence of the film’s first half. If at first Max found purely economic value in the lurid broadcasts, he now becomes evangelical about their ethical and political value. Sex and violence, to his mind, are crucial emblems of liberal society, consummations of the personal imperative and essential elements of democracy itself.
He goes so far as to reconceptualise the tumour not as a malignancy, but as a new form of bodily potential. ‘Long live the new flesh!’ becomes his mantra, a cry echoing in the real world far beyond the film. By the 1990s, after all, gay men were forming ‘bug chasing’ subcultures in the wake of antiretroviral therapies that rendered HIV a manageable, largely asymptomatic condition. These viral experimenters reimagined HIV as a means of inviting a shrine to the dead within their very cells. In this way, HIV functioned as the serological analogue of a votive candle or in memoriam tattoo. Max’s heroic reconfiguration of the tumour still calls across generations, not just to gay men, but to trans communities and even disabled people now tapping implants and other cyborg tech.
But if the resonance stretched outward, Cronenberg kept grounding it in the visceral images of his film. By the end of Videodrome , Max’s TV screen ripples into a living membrane, a hand straining through and clawing violently toward him. His abdomen yawns open like a wound, revealing a cavity where a pistol waits in the dark.
From Videodrome (1983)
From Videodrome (1983)
With dark humour, the scene makes its point: video doesn’t just influence minds, it cuts into flesh. Videodrome is an aggressive, even phallic force – but Max’s vaginal wound turns him, and the resistance he embodies, into a strange feminist figure. Cronenberg riffs on the vagina dentata – the myth of the toothed vagina – aligning pornographic liberation with the woman who fights back. By the finale, when Max’s stomach bites off the hand of his betrayer, Harlan, Videodrome leaves no doubt about its allegiances. Convex and Spectacular Optical embody a reactionary politics: militarism, puritanism, the fantasy of bodies hardened for war. Against them stands Max, whose radical – and, through his wounds, feminist – politics insist that bodies can be more than weapons. They can mutate, sprout new organs, expand toward pleasure. Max is the film’s hero, but so is the new flesh he champions. Bodies, the film insists, should change, should open, should invite the world in.
The clarity of Cronenberg’s stance in Videodrome is striking when set against the two films he made just before it: The Brood (1979) and Scanners (1981). In The Brood , the villain is a psychotherapist who urges patients to expel their traumas through a process he calls ‘psychoplasmics’, producing growths and deformities as a kind of physical exorcism. But unlike in Videodrome , mutation here brings only pain and regression. The new flesh is no liberation – only pathology.
Two years later, in Scanners , Cronenberg pushed further into the politics of bodily change. The film is set against ConSec, a shadowy military-industrial conglomerate attempting to harness humans who have telepathic and telekinetic powers. ConSec wants to weaponise them for surveillance and war. At first, the story seems to pit one such psychic or ‘scanner’, the rebel Darryl Revok, against ConSec, but his aims prove no less authoritarian. The real moral centre is Cameron Vale, another scanner, who ultimately decides that the mutations are too dangerous to persist. By the end, Vale gets his wish: the scanners must be stopped.
Compared with The Brood , Scanners complicates the politics of mutation: gone is the clear-cut villain of the mad scientist, and Vale himself embodies the power he condemns. Yet, despite this nuance, the film still portrays mutation as affliction, not possibility. Only with Videodrome does Cronenberg’s position crystallise, offering a categorical break from his earlier ambivalence. Here, at last, mutation becomes emancipation.
R eleased four decades later, Crimes of the Future (2022) finds Cronenberg returning to nearly the same themes and questions. Set in, well, the future, it centres on Saul Tenser (Viggo Mortensen), a man whose body spontaneously grows novel organs. Saul earns a living by staging public events in which his assistant Caprice (Léa Seydoux) surgically removes these organs in front of a live audience. Saul and Caprice are pursued by two rival groups. On one side are representatives of the National Organ Registry, whose goal is to maintain an active catalogue and archive of these growths for research. On the other side is a rogue cell of outlaws – radical evolutionists – who are actively trying to accelerate changes to the human body outside governmental control. Their chief aim is to induce a new kind of digestive system that can process synthetic chemicals and plastic-looking bars made of toxic waste. They have succeeded in at least one case: a boy named Brecken, son of the group’s leader, Lang, can eat the plastic bars – until he’s murdered by his mother, who believes her child (perhaps rightly) to be a monster.
The evolutionists want Saul and Caprice to leverage their wide audience and perform a public autopsy on Brecken’s body so the world can see that their evolutionary goals have been realised. Saul and Caprice agree. But upon beginning the autopsy, they discover that the boy’s body has been altered: the mutant organs are gone, and a traditional digestive system has been implanted into his corpse. For Saul, the implication is clear: someone from the government has intervened to sabotage the evidence of evolutionary change, undercutting the evolutionists’ claims. Disenchanted with what he sees as reactionary governmental overreach, Saul seems to throw in his lot with the rogue evolutionists. The film’s final scene shows Saul fitted into his ‘Breakfaster’ – a cybernetic chair he has used throughout the film.
From Crimes of the Future (2022). Courtesy Serendipity Point Films
The chair jars and shakes his upper body to help him digest food. In this last sequence, Saul asks Caprice to feed him one of the toxic bars. As he takes a bite, a closeup shows a tear falling from his eye. The chair falls still, and the film cuts to credits.
Saul has grown new organs in political sympathy, and now digests the toxic bar comfortably
So, what does this enigmatic conclusion mean for the film – and what does it say about Cronenberg’s latest views on the politics of bodily modification?
The first question is easier. Like Videodrome , Crimes casts a jaundiced eye on the view that bodies should be constrained, that change should be reversed. While the bureaucrats present themselves performing an academic task – cataloguing new organs – they are clearly allied with secret police willing to use espionage and infiltration to mutilate a dead child’s corpse.
What’s less clear is whether Saul’s choice to eat the plastic means he has joined the radicals who want to accelerate human evolution. One reading says yes: the stillness of the Breakfaster suggests that Saul has grown new organs in political sympathy, and now digests the toxic bar comfortably.
But another reading resists that optimism. Saul Tenser’s first name evokes one of Western mythology’s most famous conversions: the Hebrew jurist Saul struck by revelation on the road to Damascus, who became Paul, disciple of Christ. Yet Cronenberg’s character is not Paul. He remains Saul – forever unconverted. His last name underscores the point: Tenser, a man still taut with strain. If the chair’s quieting seems to suggest relaxation, his Saul-ness and ongoing digestive tension undermine the idea that he has embraced the evolutionists. Just as likely, Saul’s decision to eat the bar is a kind of suicide. His tear marks not conversion but surrender: a choice to opt out entirely.
The 40 years between these films seem to have tempered Cronenberg’s bullishness on the politics of the new flesh. He remains repulsed by the reactionary forces that would rein in or reverse bodily change; he’s still hostile to biopolitical fantasies about the kinds of bodies governments might cultivate for their imperial, domestic or utopian ends. But that distrust no longer drives him to embrace the opposite camp – the revolutionaries. Put differently: opposing the enemies of the new flesh no longer means championing the new flesh itself. Crimes finds a Cronenberg who gestures toward solidarity between flesh both old and new.
I t’s possible, of course, that this evolution reflects only the personal journey of one Canadian filmmaker. More interesting, though, is viewing Cronenberg’s body horror as a response to the cultural and political moment it emerged from. I make that wager – despite the uncertainties – because questions about bodies loom larger in politics today than they did in 1983. Not because the stakes are higher now; the HIV/AIDS crisis had already made those stakes brutally clear back then. But the arguments are more explicit now. Battles over abortion, trans rights, AI and neural implants dominate our political discourse. Cronenberg’s films offer a way to think about these struggles differently. His shifting vision gives us tools to reframe today’s fights over the body in more productive ways.
More precisely, Cronenberg now seems to model a view of bodily autonomy that values dignity and sovereignty over confrontation. Instead of celebrating the altered body in defiance, his later work suggests a quieter politics – less about pushing change, more about protecting space for it. Sceptical of how readily people accept difference, he invests not in promoting transformation outright but in securing zones of privacy where it can unfold. In this sense, his outlook is less progressive than liberal, in the broadest sense of those terms. Solidarity with the ‘new flesh’ no longer comes from demanding its arrival, but from portraying bodily change as natural, inevitable and quietly human.
Cronenberg’s cinema has always been about human bodies in transition
It would be entirely possible to depict Cronenberg’s transition between the early 1980s and today as yet another instance of a youthful radical becoming conservative with age – not so different, perhaps, from the Romantic trajectory of William Wordsworth. But I think that’s a facile read. Instead, the movement between Videodrome and Crimes documents a progressive becoming a liberal – and finding in that latter tradition a more effective way to derive freedom and dignity for progressivism’s traditional beneficiaries: those whose atypical bodies bear and bare, to borrow one of Cronenberg’s own titles, a history of violence. To up the ante, I speculate that Cronenberg has discovered the price of out-and-proud progressive advocacy. He has discovered how much people hate being told what to do, and how they will begrudge those on whose behalf their change (or is it ‘growth’?) is so feverishly induced. It could be that the most vulnerable among us find their greatest freedom and dignity when left to the civic protections of noninterference. What Cronenberg’s movies disclose is less a selling-out than the education of an idealist.
Of course, the example of HIV/AIDS proves how fragile and ineffective liberal politics can be. In the case of that plague, mere noninterference was not enough to save a generation of gay men from catastrophe. So, a charitable reading of Cronenberg would insist that it was that historical moment which generated the more confrontational politics assumed in Videodrome . But today’s moment demands something different. This is where the ongoing salience of trans rights comes into focus. In a way, Cronenberg’s cinema has always been about human bodies in transition. And Crimes , I think, stands as his letter to the court of public discourse – the accumulated wisdom of someone whose art has examined the altered body for nearly half a century now.
What’s not clear is if we’re prepared to listen."
The rise and now fall of the Maoist movement in India | Aeon Essays,,https://aeon.co/essays/the-rise-and-now-fall-of-the-maoist-movement-in-india,"On 6 April 2010, a company of India’s central paramilitary soldiers came under attack from Maoist guerrillas in the central-eastern state of Chhattisgarh. The Maoists, who had turned this region into their stronghold, had laid a trap. With little training and scant knowledge of the Amazon-like jungle, the Indian soldiers found themselves ambushed. They fought back, but they could not escape the ambush. Seventy-five soldiers and a state policeman accompanying them were killed.
Never before had the Indian forces suffered so many casualties in a single incident, not even in Kashmir , where, for more than 20 years, they had been fighting a protracted battle against Islamist extremists. As the body bags of the soldiers reached their native places in different parts of India, a deep sense of anger generated among people who till recently had only a vague idea about who these Maoists were, and even less about the hinterland that the Maoists had turned into a guerrilla zone.
Since the mid-2000s, the Maoists had grown in strength, launching audacious attacks against government forces and looting police armouries and declaring certain areas as ‘liberated zones’. Their operations ran in a contiguous arc of land, from Nepal’s border in the east to the Deccan Plateau in the south – an area the Maoists called Dandakaranya or DK, using the name in its historical sense. This is a region where India’s indigenous people, the Adivasis, lived; it also holds valuable minerals and other natural resources in abundance. The Indian state wanted control over the natural resource wealth, but the Maoists were proving to be an obstacle. Then, in 2009, the then prime minister Manmohan Singh called them India’s ‘greatest internal security threat’.
O n the morning of the attack in 2010, I’d landed in a part of DK, in a city a three-hour car ride from the edge of a town beyond which was a forest ruled by Maoists. From there, if one started walking, one would, without getting on to a motorable road, reach the spot of the Maoist ambush. There would be only a sprinkling of tiny hamlets, inhabited by Adivasis, who thought of Maoists as the ‘government’. Beyond that, they had very little idea of life outside, least of all the blitzkrieg of ‘India Shining’, a political campaign by a previous government (the conservative BJP), which had in spirit continued to exist as a cursor to economic optimism.
The scrawny man – I’ll call him ‘A’ – who came to pick me up from the railway station on his battered motorcycle was a former Maoist. He was also a Dalit, the so-called ‘lower-caste’ people at the bottom of the Hindu caste system , who, like the Adivasis, have been historically maltreated in India. He lived in a slum and had been recruited in the 1980s, along with several others in the city, by a woman Maoist. After a few years, ‘A’ had quit the party to raise a family. Life outside had been harder than inside the forest. For people like ‘A’, it was difficult to come out of the poverty and bitterness that came with their ascriptive status in the caste system. He struggled with odd jobs, and in the night drank heavily and sang resistance songs by the yard to temporarily rid himself of the bile.
People had begun to talk of a ‘trillion-dollar economy’, while in some areas the poor would still die of hunger
I’d spend a day or two in that city and then travel towards the edge of the town, from where a Maoist sympathiser would pick me up from a stipulated spot. After a bike or jumpy tractor ride, followed by a walk of several hours, contact with a Maoist squad would be established. From there, I would travel with them, sometimes for weeks, from one hamlet to another, crossing rivers and hills, evading bears and venomous snakes, hoping that, once I returned, I wouldn’t be gripped by a fever, which could indicate malaria, endemic in these areas.
Twelve years earlier, my own history had prompted my interest in Kashmir and the Maoists. My family belonged to the small Hindu minority in Kashmir – the only Muslim-majority state in an otherwise Hindu-majority India. In 1990, we were forced to leave, as Islamist extremists began targeting the Hindu minority. In a few months, the entire community of roughly 350,000 people was forced into exile. For journalists, though, that expulsion was not very newsworthy. As the Indian forces began conducting operations against militants, resulting in brutal clampdowns and sometimes excesses against civilians, Kashmir became a dangerous place. But, for journalists, it turned into a harbinger of awards, of grants and fellowships.
I decided to go away from Kashmir, not at first by design, but by a chance trip to the guerrilla zone. I had barely travelled beyond Delhi, just a few hundred miles south of Kashmir. But, as I began exploring the mainland further, I ended up in hinterland areas where India’s poorest of the poor lived. In the new India, where people had begun to randomly drop into conversations terms such as ‘trillion-dollar economy’, these areas still remained where the poor would die of hunger. What I saw in my journeys into rural India came as a revelation; in contrast, my own exile, of leaving a modest but comfortable home and instead facing the humiliation of living in a run-down room in exile in Jammu city, seemed bearable. Maoists weren’t yet receiving a lot of attention. The prime minister Singh’s pronouncement about Maoists was more than 10 years away, so it was difficult to convince editors to cover them, but I persisted, mostly because I felt that I was receiving a real education, one that a journalism school would never offer. In these years, the lack of government interest in the Maoists was an advantage – one could travel easily to DK without invoking the suspicion of the security apparatus.
W hen I said goodbye to ‘A’ and headed into the forest from where DK began, I knew I’d have to be more cautious. I had avoided spending more than a few hours in the city where we’d met; a hotel reservation could give me away, and the police would put me under surveillance. By the afternoon of the next day, I was in a Maoist camp, under a tarpaulin sheet, meeting, among others, Gajarla Ashok, a Maoist commander, and another senior leader, a woman called Narmada Akka. Like most of the Maoist leadership, they both came from urban areas. These leaders had been teachers, engineers, social scientists and college dropouts, moved by the idea of revolution. They had come to DK with the same dream of establishing an Indian Yan’an (the birthplace of the Chinese communist revolution). But the core of Maoist recruitment came from the Adivasis, and before that from the working class and peasantry among Dalits and other ‘backward’ communities.
The Maoists had decided to enter Chhattisgarh and its adjoining areas (which comprised DK) in the early 1980s. That was their second effort to bring about a revolution. An earlier attempt had been made in a village called Naxalbari in West Bengal, in the late 1960s. Peasants , who tilled the fields of landlords and received only a minuscule proportion of the harvest, rose against the iniquity of their small share. The rebellion was inspired by members of the mainstream Communist Party of India, who had begun to grow disillusioned with their organisation. This questioning had also taken place in other parts of the world. In France, for example, during the May 1968 Leftist student protests, the postwar Left came to be seen as an obstacle to real social transformation. What do we win by replacing ‘the employers’ arbitrary will with a bureaucratic arbitrary will?’ asked the Marxist thinker André Gorz. A similar sentiment had been expressed almost 4 0 years earlier by an Indian revolutionary, Bhagat Singh, whom the British then hanged in 1931 at the age of 23. In a letter to young political workers a month before his hanging, Singh warned that the mere transfer of power from the British to the Indians would not suffice, and that there was a need to transform the whole society:
Singh’s prescription proved to be right. Even as the prime minister Jawaharlal Nehru, whose commitment to social justice could not be doubted, took over from the British in 1947, the poor and the marginalised communities like the Dalits and Adivasis continued to remain outside the welfare circle of his five-year plans. Feudalism did not go away. Land reforms to break up the feudal concentrations of wealth and power were initiated, but the rich and the powerful also found means to circumvent the law. The rich quickly joined politics, and the police acted as their private militia. As recently as 2019, a survey by the Indian government revealed that 83.5 per cent of rural households owned less than one hectare of land. The government’s Planning Commission figures (1997-2002) put the landless among Dalits at 77 per cent, while among the Adivasis it was 90 per cent. The government’s National Sample Survey in 2013 revealed that about 7 per cent of landowners owned almost half of the total land share.
Small guerrilla squads began to indulge in ‘class annihilation’, killing hundreds of landlords
In the 1960s, these disillusioned communists felt that the Communist Party of India had grown complacent and corrupt, and that its leaders were ‘conscious traitors to the revolutionary cause’. They made their case in long papers and articles full of communist jargon in publications like Deshabrati , People’s March and Liberation . The essence of their indictment was that the poor and the working class had been let down by the parliamentary Left. In 1969, these breakaway communists formed their own party, the Communist Party of India (Marxist-Leninist), which announced its aim to unite the working class with the peasantry and seize power through armed struggle. They sought help from China, which was quick to offer it, calling the uprising ‘a peal of spring thunder’. Some of the men inspired by Mao travelled to China through Nepal and Tibet, receiving political training from Mao’s associates.
The Maoist message spread from Naxalbari to other parts of India, like Bihar, Andhra Pradesh, Punjab and Kerala. Inspired by its main leader, Charu Majumdar, small guerrilla squads began to indulge in ‘class annihilation’, killing hundreds of landlords and their henchmen, policemen and other state representatives. ‘To allow the murderers to live on means death to us,’ Majumdar declared. Liberation , the party’s mouthpiece between the years 1967-72, is full of reports of killings of landlords, and how land and other property they owned had been ‘confiscated’ by peasant guerrillas. In practice, however, ‘class annihilation’ proved counterproductive. On the streets of Calcutta (today’s Kolkata), for example, naive men from elite colleges would roam around with crude bombs and even razors with which they attacked lone policemen.
N onetheless, in the late 1960s, the Naxalbari movement inspired thousands of bright men and women from elite families studying at prestigious schools. They said goodbye to lucrative careers and made the feudal areas, where the poor faced the utmost oppression, their workplace. Beginning in July 1971, a brutal government response killed hundreds of Indian Maoists, probably including their leader Majumdar; he died in police custody in 1972.
Kondapalli Seetharamaiah, popularly known as KS, was one of those dissatisfied with the shape that the parliamentary Left had taken in India. He was a school teacher in Andhra Pradesh, which had a long history of feudalism and communist struggles. In the state’s North Telangana area, bordering Chhattisgarh, for example, feudal customs of slavery like vetti were still being practised decades after India became free. KS, a former member of the Communist Party of India, had not lost all hope, and decided to join hands with Majumdar’s line. But before he could restart, he decided the Maoists needed a rear base, just like Mao had urged, for the guerrillas to hide in the forest. The other amendment to Majumdar’s line was with regards to the formation of overground organisations to further the cause of revolution, something that Majumdar had strongly opposed. In 1969, KS sent a young medical student to a forest area in North Telangana to explore the possibility of creating the rear base. But in the absence of any support, the lone man could not achieve anything and had to return. In the mid-1970s, KS sent yet another man, this time a little further inside, into Chhattisgarh. Spending a few months inside, the man, who had acquired basic medical training, started treating the poor tribals. But, again, how much could one man or two do? So, he returned as well.
So KS made another change in strategy – he took the Maoists out of the shadows and founded a few organisations that, on the surface, were civic associations, but were meant to further the Maoist ideology. Prominent among these was the Radical Students Union (RSU), launched in October 1974. Along with a cultural troupe, Jana Natya Mandali, young RSU members began a ‘Go to Village’ campaign on KS’s instructions. In this campaign, the young student radicals and ardent believers in the armed struggle would try to make villagers politically ‘conscious’. The ‘Go to Village’ campaign enjoyed some initial success, attracting students and other young people from working-class backgrounds. Hundreds of young people in universities and other prestigious institutions in Andhra Pradesh left their studies and vowed to fight for the poor. Fourteen students from Osmania University in what was then Andhra’s capital, Hyderabad, joined; 40 from other parts of the state joined the Maoist RSU.
The Maoists’ ‘Go to Village’ campaign found fertile ground in the town of Jagtial, in the state’s Karimnagar district. There, as across Andhra, people celebrate the festival Bathukamma, which includes theatre performances in villages that were home to landlords from the dominant castes. The caste segregation of the villages was complete: the landlords lived in the village centre, while the Dalits lived on its periphery. But now in Jagtial, the Dalit labourer Lakshmi Rajam took the performance to the Dalit quarters. Another Dalit man, Poshetty, occupied a piece of government-owned wasteland, which would usually be in the landlords’ control. These acts enraged the landlords, who killed both these Dalit activists.
As the Maoists pushed on, the state retreated, and the Adivasis began to exert their rights over the forest
On 7 September 1978, under the influence of the Maoists, tens of thousands of agricultural labourers from 150 villages marched through the centre of Jagtial. The march was led by two people, one of them Mupalla Laxmana Rao, alias Ganapathi. He came from Karimnagar itself and would become KS’s closest confidante, later taking over from him to become the Maoist chief. The other was Mallojula Venkateshwara Rao, alias Kishenji, a science graduate, who would prove to be an efficient leader and military commander.
The Jagtial march rattled some landlords so much that they fled to cities. The poor also decided to boycott the landlords who would not agree to any land reforms. Services that the poor provided – washer men, barbers, cattle feeding – were denied to the landlords. This strike led to further backlash from landlords, as reported by the respected Indian civil rights activist K Balagopal.
From these village campaigns, KS decided to move ahead and try to create a guerrilla zone where armed squads would mobilise peasants and contest state power. In June 1980, seven squads of five to seven members entered the hinterland – four of them in North Telangana, two in Bastar in Chhattisgarh, and one in Gadchiroli in Maharashtra, an area where the Adivasis lived. They were mostly food-gatherers, and their life had remained unchanged for hundreds of years. Abundant mineral wealth lay in the land under where the Adivasis lived, but they lacked even basic modern services like education and healthcare. Petty government representatives like forest guards would harass the Adivasis for using resources like wood, citing archaic forest laws. At first, the Adivasis did not welcome the presence of the Maoists. However, before long, a kind of alliance between them developed, where the common enemy now was the state. As the Maoists pushed on, the state retreated, and the Adivasis began to exert their rights over the forest. In many areas, the feudal landlords were served ‘justice’ like Mao had dictated.
In 1980, the Swedish writer Jan Myrdal visited the Maoists, and one of the comrades told him of an incident from North Telangana, which Myrdal recounts in his book India Waits (1986). A notorious rowdy there had instilled fear among the people on behalf of his master, a landowner. He raped a washer-girl. In shame, she jumped into a well and drowned herself. When the Maoists came to know of it, four of them, till recently students, called him out in the bazaar. When he arrived, the rebels caught him with a lasso, cut off his hands and nailed them to a wall inside a shop.
The rough vigilante justice inspired more young people to join the Maoists: men like Nambala Keshava Rao, a graduate of the much-respected Warangal engineering college, and Patel Sudhakar Reddy, who held a master’s degree from Osmania. It also brought in young women like Maddela Swarnalata and Borlam Swarupa. Swarnalata came from a poor Dalit family and was recruited through the Radical Students Union. In the early 1980s, she’d taken part in clashes against Right-wing student groups, especially the Akhil Bharatiya Vidyarthi Parishad. The police would follow her and pressurise her into revealing details of her comrades who had already gone underground. Soon it became impossible to avoid arrest, so she too went underground, joining a Maoist squad, before dying in an encounter with the police in April 1987. Meanwhile, Swarupa had become active through campaigns with farmers’ groups for a better price for their crops. The Maoist leadership placed her as a labourer in a biscuit factory in Hyderabad, in order to recruit among workers there. Once she’d been exposed, Swarupa was asked to shift to the guerrilla zone, where she became the first woman commander, leading a squad in North Telangana, until she was killed in an encounter in February 1992. One of the prominent features of the Maoist movement is the way it attracted women to its fold. For women from the working class, who led difficult lives under a patriarchal mindset, joining the Maoists felt like a liberation.
Recruits to the Maoists often attracted their friends, siblings and other family members to join too. Doug McAdam, professor of sociology at Stanford University in California, has written about this ‘strong-tie’ phenomenon, in which personal connections draw people into ‘high-risk activism’ of violence. In Bastar and elsewhere, the Maoist guerrillas targeted people and agencies they considered exploiters. For example, they started to negotiate better rates for the collection of tendu leaves, used in the manufacture of local cigarettes, which was a lucrative business. But along with that, they also started to take cuts from businessmen for running their organisations. The Norwegian anthropologist Bert Suykens, who has studied the tendu leaf business, called it a joint extraction regime. The Maoists also began to extort a levy from corporate houses involved in mining in these areas, as well as from government contractors. In the process, they deviated from their promise – of returning the forest to the Adivasis, and of helping the poor. They spent most of their time running their organisation and launching attacks against government forces. In her research in central Bihar in 1995-96, the Indian sociologist Bela Bhatia concluded that the Maoist leaders ‘have taken little interest in enhancing the quality of life in the villages.’ In fact, these leaders regarded development ‘as antagonistic to revolutionary consciousness,’ she wrote in 2005.
I n the meantime, the Indian state was growing impatient with the Maoists. In 2010, a London-based securities house report predicted that making the Maoists go away could unlock $80 billion of investment in eastern and central India. New Delhi began preparations for a large-scale operation to get rid of them. But, before that, the extraordinary arrest in 2009 of the Maoist ideologue Kobad Ghandy in Delhi heightened political interest in the insurgents.
Special police agents from Andhra Pradesh had managed to locate Ghandy, who had been living in a slum using fake identification. He came from an elite Parsee family in Mumbai; his father was the finance director of Glaxo; he had studied with India’s political dynasts at the elite Doon School, and had then gone to London to pursue further education as an accountant. In the UK, he was introduced to radical politics, and returned to Mumbai in the mid-1970s, where he met Anuradha Shanbag, a young woman from a family of notable Indian communists and a student of Elphinstone College in Mumbai. Shanbag and Ghandy were both drawn to Maoism, fell in love and married. Soon afterwards, in 1981, they met KS in Andhra Pradesh and shifted to a slum area in a city where Shanbag recruited my friend ‘A’ and others. In 2007, Shanbag was promoted to the Maoist Central Committee, a rare accomplishment for a woman. A year later, however, she died from complications due to malaria she had contracted in a guerilla zone.
After Ghandy’s arrest in 2009, rumours arose that he had been sent to work among the labourers as part of the Maoists’ urban agenda. His arrest became a hot topic in Delhi circles: for the first time, it sparked interest in the Maoist movement among people who did not bother to read a newspaper beyond its Fashion section. Ghandy’s abandonment of his elite background to fight for the poor created a wave of empathy for the Maoist movement.
Around the time of his arrest, I got a rare opportunity to meet the Maoist chief, Ganapathi. The meeting happened by chance. Through some overground sympathisers, he had learnt that I was in a city close to the guerrilla zone in which he was then hiding. By this time, state surveillance was at its peak, and the Maoist leadership was extremely cautious of any contact with outsiders. Ganapathi in particular barely met anyone except his commanders. After days of travel through the guerrilla zone, I was allowed to record our conversation on a digital device provided by his men. After Ganapathi left the area, I transcribed the interview, but even that I was not allowed to carry with me. A month later, I received the transcript through one of his overground workers in Delhi.
As part of its anti-Maoist operation, the government began to push infrastructure
A few months later, in 2010, while I spent time with the Maoist leaders Gajarla Ashok and Narmada Akka in their camp, I sent a questionnaire to Ganapathi. His reply came a few weeks later, in which he made mention of the importance of work in urban areas: ‘If Giridih [a small town in the east] is liberated first, then based on its strength and on the struggles of the working class in Gurgaon [now Gurugram, a satellite city close to Delhi where most multinational corporations have their offices], Gurgaon will be liberated later. This means one is first and the other is later.’
It was a tall order. There were innumerable problems in cities, including poverty. But with the liberalisation of the 1990s, middle-class insularity had made most people oblivious of the suffering of others. The Maoists wanted to make inroads through slums and labour unions, but did not find enough reception.
The curiosity and empathy the Maoists generated among ordinary people in cities soon dissipated. The conservative BJP, which was rising to national power, relentlessly used Kashmir to rouse Hindu sentiment in mainland India. In the first decade of 2000, Islamist radicals targeted mainland India, creating friction with the Muslim minority. The Indian Parliament had come under attack in 2001; Mumbai city faced a terrorist attack in 2008. Between these, many Indian cities like Delhi, Hyderabad, Varanasi and Jaipur were targeted with bomb blasts, killing scores of people. At the same time, the overground sympathisers of the Maoist movement began hobnobbing with separatist elements from Kashmir and India’s Northeast, which had a long history of secessionism, and these potential alliances stirred controversy. This resulted in a backlash against Maoist sympathisers, and a new term was coined for them: ‘urban Naxal’. Hindu nationalism was on the rise in India and, in the coming years, this term would become a ruse for the government to suppress all activism, resulting in the incarceration of civil rights activists like the human rights lawyer Sudha Bharadwaj. What also did not help is the number of body bags – of forces killed in Maoist ambushes – going to different parts of the country.
As part of its anti-Maoist operation, the government began to push infrastructure – primarily roads and mobile/cellphone towers – in the Maoist-affected areas. It led to further entrenchment of state forces, which also weakened the Maoists. Their leaders who were in hiding in cities began to be hunted down.
The new roads and phone towers were welcomed by rural people. The Maoists began killing Adivasis on suspicion of being police informers. This violence alienated Adivasis, and others too. Earlier, the Maoists would visit a village in the night and slip away. Even if their presence was reported, it was of no use to security forces because the information would reach them quite late. But now, with cellphone networks, the people could call immediately, leading to encounters between the Maoists and state security forces.
Since about 2020, the decline of India’s Maoist movement has been rapid. The Maoist commander Ashok – whom I had met in the forest in 2010 – surrendered in 2015. One of his brothers had already died in an encounter. Meanwhile, Akka was arrested in 2019 in Hyderabad where she was seeking treatment for cancer; she died in a hospice three years later.
The government raised a special battalion of Adivasis, which included surrendered Maoists, to hunt down the Maoists. It started getting big results. In May this year, Nambala Keshava Rao, who had taken over as the Maoist chief from Ganapathi in 2018, was killed in a police encounter. A few weeks later, another of Ashok’s brothers, a senior commander, was also killed by police. The entire Maoist leadership, barring a few, has been wiped out. Ashok has, of late, joined the Indian National Congress Party. ‘A’ has not been in touch in the last few years, ever since some of his friends were arrested as ‘urban Naxals’. A friend of his told me the other day that he has stopped interacting with people. A month ago, a friend in Gurugram told me of an incident where he lives. His local Resident Welfare Association had put a cage in their park, with a banana inside it to lure marauding monkeys in the vicinity. A few hours later, they found that the banana had been consumed by someone and the peel left outside the cage. It made me imagine how hungry that person would have been, most likely a poor worker. The friend sent me a screenshot of the residents association’s WhatsApp group. ‘Check the CCTV,’ someone had written.
The Maoists have completely surrendered now, asking the government to accept a ceasefire. A statement released this September, purportedly by part of the Maoist leadership, apologises to people, saying that, in the process of revolution, the leadership made several tactical mistakes, and that the ceasefire was now important to stop the bloodshed. What those mistakes are, the letter wouldn’t say. As anti-Maoist operations go on with even more rigour, a handful of those still inside the forest will ultimately surrender or be killed. How history remembers them is too early to say; but it is a fact that, had it not been for them, the much-needed focus on the hinterland of DK would not have been there. However, to the man in Gurugram who stole the banana, and to the man in Giridih, who doesn’t even have a banana in sight, it means nothing."
The sovereign individual and the paradox of the digital age | Aeon Essays,,https://aeon.co/essays/the-sovereign-individual-and-the-paradox-of-the-digital-age,"In the mid-1950s, IBM approached Jacques Perret, a Classics professor at the Sorbonne, with a question. They were about to sell a new kind of computer in France, the Model 650 . What, they asked, should it be called? Not the model itself, but rather the whole class of device it represented. An obvious option was calculateur , the literal French translation of ‘computer’. But IBM wanted something that conveyed more than arithmetic. ‘Dear Sir,’ Perret replied,
Besides, Perret added, the implicitly feminine connotation already present in IBM’s marketing materials could carry over to the new term:
The female reference was not entirely inappropriate. Up until the mid-20th century, the term ‘computer’ meant an office clerk, usually a woman, performing calculations by hand, or with the help of a mechanical device. IBM’s new machine, however, was intended for general information-processing. The masculine and godlike version prevailed. The term soon entered common language. Every computer in France became known as an ordinateur .
IBM 650
Courtesy Wikipedia
T he IBM Model 650 was sold from 1953 to ’62. Its Console Unit ran on about 2,000 vacuum tubes in conjunction with a spinning magnetic drum capable of storing about 20,000 digits. It was the size of a couple of large refrigerators. The accompanying power supply was about the same size again. The first one was installed in the controller’s office of the Old John Hancock Building in Boston, headquarters of the John Hancock Mutual Life Insurance Company, in 1954. Its job was to calculate sales commissions for the company’s 7,000 or so insurance agents.
The Model 650 was a big success. IBM initially thought they might make 50 of them; in the end they sold more than 2,000. Not terribly impressive numbers in comparison with iPhone sales, to be sure, but not bad for a device whose price started at about $1.5 million in current US dollars. What drove sales, to IBM’s initial surprise, was the variety of its commercial applications. It was put to work to run payrolls, manage inventory, analyse sales, and control costs. Its hardware was quickly augmented by the first commercial disk drive, the RAMAC, which could store about 3.75 megabytes of data. Unlike systems where storage was based fully on punch cards, it could access that data right away. This enabled a novel form of processing that happened in real time. Air traffic control was an early application. But it also allowed companies to look up client records immediately, and – for example – decide on the spot whether to extend a policy or a credit line. From then on, the breadth and depth of ‘computerised’ data endlessly expanded. The paper records and handbooks of the late 19th century became the relational databases of the late 20th.
The phone you are probably reading this on is a spiritual descendant of that machine. We are used to the idea that, over the past 70 years, computers have gotten faster and faster, smaller and smaller. Their real power, though, lies in how they link the ordinary choices we make into systems that constantly adjust and modulate themselves. Think, for example, of people deciding which restaurant to go to and how to get there. They choose with the assistance of Apple or Google Maps. The map shows their position, and many options for their destination. The locations all have descriptions and ratings attached, together with information on how busy the place is likely to be. Perhaps they will be offered a coupon or some other deal. Once a choice is made, the phone helps find the most effective route, monitoring the position of their car, receiving information about the general flow of traffic, the state of the weather, the presence of accidents or speed traps, and so on as they make their way.
Data about the flow of choices is used to update and enhance a system’s global view of its own state
In navigating this flow, drivers also constitute it. As transportation planners like to say, these commuters are not in traffic, they are traffic. Their phones track them individually while also aggregating information about the global state of things using data from thousands of beacons just like theirs. Some information from the resulting network’s-eye view is fed back to the user. This aids individual drivers, helping them choose the right route. But this information also modulates the overall system by prompting drivers as they make their individual decisions. Would you like to accept a faster route, or stay on your present course? A speed trap is reported ahead. The next off-ramp is temporarily closed. Sometimes this mode of control takes the form of reassurance: there is a 20-minute delay, but you are still on the fastest route. (Please do not do anything rash, like believing you know a shortcut.) The individual user relies on the information flowing through their phone to make their choices, and their decisions then feed back into the overall system.
Photo by Nurphoto/Getty Images
What is true of people in their role as drivers also applies in their role as diners, and a thousand other social activities. Data about the flow of choices is used to update and enhance a system’s global view of its own state. Once the meal is done, the guests might decide to rate the restaurant, leave a review, or share a photograph of their dessert. If they left their car at home and took an Uber instead, they will have rated and been rated by their drivers. On the way home, they may check to see if the selfie they took at dinner has gotten any likes.
What is happening here is more than an abstract flow of information. It is more than a means of surveillance. It is more than a price mechanism. Rather, it’s as if the air traffic control and insurance commission functions of the IBM 650 have been fused, shrunk, and wholly generalised. This is the real computing revolution. Much of what we do is immediately authenticated as we do it, stored as data, classified or scored on some sort of scale, and deployed in real time to modulate some outcome of interest – usually, the behaviour of a person, or a machine, or an organisation.
P erret’s instinct to name the device for a being ‘who brings order to the world’ proved prescient. It is through their ability to observe, judge and manage people across social domains that les ordinateurs exert their most significant powers in society. Everywhere, the bureaucratic logic of organisations merges with the calculative logic of machines, feeding on the data emitted by ever-smaller and more powerful devices that ended up first in the homes, then on the laps, and then in the hands of billions of individuals. From this mass of information, ordinateurs spit out scores that create difference, define priorities, organise queues, and provide a tremendously useful and powerful basis for action. They create order by categorising people, things and ideas, and then by matching them to each other, to social positions, to goods, services and prices.
The resulting patterns are what we think of as social structure – a sort of ordinal society , where computer-generated outputs become guideposts for choices. In the economic sphere, for example, these methods help set wages and work schedules. They calculate rents, price insurance, and determine eligibility for social services. They facilitate new forms of rent-seeking, and accelerate the development of new asset classes that can be sold on financial markets. They have also changed the relationship between individuals and the groups they form and belong to. They organise the flow of information, the distribution of social influence, and the means of political mobilisation.
Our ability to form meaningful social bonds and to act together has been fundamentally altered
Because of this transformation, our sense of who we are is assembled in a strange and tangled fashion. The machinery of ordinalisation attends carefully to individuals rather than coarse classes or groups. By doing so, it appears to liberate people from the constraints of social affiliations and to judge them for their distinctive qualities and contributions. It promises incorporation for the excluded, recognition for the creative, and just rewards for the entrepreneurial. And yet this emancipatory promise is delivered through systems that classify, sort and, above all, rank people with ever-greater precision and on a previously unimaginable scale. The resulting social order is a sort of paradox, characterised by constant tensions between personal freedom and social control, between the subjective elan of inner authenticity and the objective forces of external authentication. It gives rise to a certain way of being, a new kind of self, whose experiences are defined by the push for personal autonomy and the pull of platform dependency.
Friedrich Hayek’s book The Road to Serfdom (1944) warned that government control of the economy would destroy individual freedom and inevitably lead to tyranny. Today’s predicament is different. The tyranny may come, instead, from digital platforms that enhance individualism and interpersonal competition to such a degree that our ability to form meaningful social bonds and to act together has been fundamentally altered. We are now travelling down a road to selfdom, where we must cultivate and attend to distinctive digital identities, develop our own understanding of the world, and hope to harness technology to carve out spaces of personal sovereignty and domination.
I n the early days of the internet, being online brought certain freedoms. Not only was online anonymity or pseudonymity common, it was celebrated as a kind of liberation. Users embraced the opportunity to experiment with different versions of themselves. This multiplication of identities was a feature, not a bug. It also reflected the technical architecture of a less integrated internet, which gave participants what we might call ‘interstitial liberty’. This is the liberty granted us by the gaps between systems that will not or cannot efficiently talk to one another. It is a kind of negative freedom. If your gaming profile cannot easily be linked to your professional email or your forum discussions, you enjoy a form of privacy that depends less on explicit legal protections and more on the technical limitations of systems that are connected in principle but not integrated in practice. At other times, the preservation of these gaps is more of a choice. Until recently, in the United States, undocumented immigrants were safely able to work and contribute to the tax system. This deliberate administrative separation allowed US businesses and governments to benefit from immigrant labour while also creating a functional sanctuary by which millions could fulfil their tax obligations (using individual taxpayer identification numbers) without fearing it would trigger deportation proceedings. The Department of Government Efficiency (DOGE) has decided to change all that.
This is also the liberty that shrinks further when private companies are suddenly pressured to share their own data with public agencies, as happened in China around financial credit scoring, or when US government agencies decide to scrutinise the social media profiles of visa and citizenship applicants. Closing these technical gaps and fusing data from market and state institutions not only makes surveillance much more pervasive, it makes it more powerful. Tools that recognise patterns, predict behaviours and detect anomalies can now work across previously separate domains. Today, staying anonymous requires elaborate countermeasures, whether through legal instruments like the ‘right to be forgotten’ or technical solutions like private digital networks. However, in a world where digital presence is expected, protecting your privacy can make it look like you have something to hide. And perhaps you do. There are all sorts of potential embarrassments or vulnerabilities in the data about you. Proving one’s blamelessness is a near-impossible task.
Social connections that were sources of pride and support suddenly become potential liabilities
Beyond identifiability, the more insistent question is one of authentic identity: who are you, really? The ordinateurs want to know. To help us unlock this information, they have transformed it into a matter of public record, to be shared proudly and widely. Social media companies skilfully exploit our thirst for sociability and our romantic ideals of self-realisation. They relentlessly encourage individuals (and organisations, too) to publicly express their core commitments and enrol allies to validate them.
The compulsion to authenticity frequently backfires. Being exposed as inauthentic can be devastating to reputations and livelihoods. The sociologist Angèle Christin has described savage online battles between vegan influencers who push the envelope of vegan purity or expose their rivals as secret meat-eaters. Other authenticity traps are more ominous, as when organisations use social media feeds as public proof of who we truly are – an agitator, a gangster, a covert terrorist. In his book Ballad of the Bullet (2020), the ethnographer Forrest Stuart found big gaps between the performances that drill musicians put up for social media consumption and the more banal reality of their lives. Young people making themselves look tough to sell music on YouTube may learn the hard way that law enforcement officers and judges tend to interpret these signs literally, rather than seeing them as the status games and identity play that they most likely are. Similarly, the Trump administration’s reliance on tattoos as one easily harvested, measurable piece of evidence of gang membership takes an often superficial marker and turns it into a datapoint in a deportation scoring system. And in a country where the government has taken it upon itself to use people’s professed views against it in immigration proceedings, the effect is chilling. Self-disclosures and social connections that until recently were sources of pride and support suddenly become potential liabilities.
Authenticity traps multiply in other ways, too. Generative AI increasingly blurs the boundaries between real and synthetic texts, images and sounds. Traditional concerns about inauthentic or misinterpreted performances have given way to more fundamental questions about truth. Hopeful startups raise millions of dollars to develop ‘cheat on everything’ AI tools, and jobseekers can artificially generate their application materials and even fake their job interviews. All of this has the effect of shifting emphasis from authenticity to authentication, from demonstrating the truth of one’s identity to proving the reality of one’s testimony. The question is no longer whether an identity is genuine (‘Is that really you ?’) or even authentic (‘Who is the real you ?’) but whether each element of your digital presence is unmediated by artificial intelligence (‘Is it really you?’) This emergent regime of authentication transforms interactions from a set of performances to be judged into a series of actions to be verified by machines at every step.
Being a legitimate self now requires one to be publicly identifiable, authentic and, increasingly, fully authenticated. What began as a celebration of individual uniqueness that avidly encouraged the production of digital evidence is evolving into an elaborate system of verification that will treat any trace as a potentially suspect record. As fake versions of ourselves start to circulate, we may soon find ourselves caught in endless cycles of proving and defending the reality of our own existence, submitting ourselves more and more to a machinery of institutionalised scepticism that would have repulsed the early internet’s champions of identity play and experimentation.
T he political and technical crises of authentication extend well beyond the individual self. Knowledge itself has massively expanded and diversified with the rise of the internet. But it has also become more bespoke and more parochial in the process, as people interact with the web in ways that build upon, and further elaborate, their own personal convictions and representation efforts. The advent of generative AI possibly worsens the epistemic challenge: when everything must be authenticated, but fakes get more sophisticated all the time, how do we know anything?
We live in an era of disintermediated knowledge. With the click of a button, we can look up original legal documents, download rich datasets, have an AI assistant help write code to analyse them, and quickly write up the results. We should not lose sight of how stunning and remarkably empowering a transformation this has been in many ways. For all its problems, if you asked any scholar whether they would go back to a fully pre-digital, pre-networked world of knowledge-sharing, academic communication and data availability, the answer would overwhelmingly be ‘absolutely not’. But this transformation in everyday work lives has been accompanied by something else. The disposition to search has quietly become second nature. Today, ‘doing one’s own research’ is more than a habit of academic professionals. It is a moral imperative, a civic command, and – a little like being a good driver – a skill everyone thinks they have.
Individuals equipped with the capacity to search the network and query large language model (LLM) oracles, and in possession of the self-confidence and the means to broadcast their findings, tend to become an authoritative source of opinion. At least that is how it feels to them. We can also understand why knowledge produced in this manner is often so emotionally charged. The more people invest in researching and developing their own understanding, the more their pursuit of knowledge transforms into a form of personal revelation, where everyone is both seeker and interpreter of their own truth. What began as an exercise in independent reasoning becomes a matter of belief , belief defended all the more passionately because it seems to have been self-discovered rather than externally given.
The very idea that we could arrive at a broadly accepted consensus on facts has grown more remote
In consequence, traditional hierarchies of knowledge and sources of expertise find themselves bypassed in favour of self-piloted algorithmic searches that generate a precisely ‘relevant’ answer to a query or a prompt, whether as a page of links or a summary paragraph of text. At its best, the ability to do this kind of work tends toward a kind of Deweyan democratic ideal , revitalising knowledge production as a participatory enterprise, making it work in a democratic spirit of open enquiry and collective truth-seeking. This is what many hoped for in the early days of the world wide web, up to and including the first waves of social media from blogging to Twitter. At its worst, though, the distribution of knowledge ends up channelled through platforms that mass-personalise results and promote engagement with extreme or misleading content because shock value and pandering are what produce advertising dollars. When the Canadian government in 2023 required internet companies to compensate media outlets for links to news published on their platforms, Meta simply blocked those links on Facebook and Instagram. The resulting information vacuum was quickly filled by unverified and Right-wing content, which helped prop up the local Trumpian candidate.
By now we are drowning in examples of tech platforms wielding market power to bend information ecosystems to their business needs, regardless of societal consequences. Google revolutionised search by, in effect, treating web pages as a huge reputational network. The relative authority of sites was set by a wider world of independent decisions to link or not to link to them. But the desire to tailor results to individual preferences increasingly became guided by the effectiveness of clickbait or ad placement. This has led to a fragmentation of the knowledge that people access, now produced to facilitate market manipulation by catering to pre-existing beliefs. While the sense of searching online as a form of active, critical thinking has persisted, for some, finding good information can be difficult. This makes the work of integrating a sense of shared reality much harder. Even the very idea that we could arrive at a broadly accepted consensus on facts – regardless of their content – has grown more remote. It is down these knowledge sinkholes that the sense of ‘selfdom’ gets further elaborated, sometimes in tragic ways, through the belief that the self is the only true source of its own enlightenment.
The combination of epistemological self-centredness and hyperconnectivity makes people susceptible to diffuse forms of ‘supersense’-making (to borrow a term from Hannah Arendt ). Seeking some meaningful truth, people search for significant clues scattered across the internet, using commercial algorithms and recommender systems to connect the disparate pieces of information they venture upon into some sort of coherent worldview. What may begin as a playful existential quest can easily crystallise into reality-bending beliefs that thrive on and foster new social types and politically potent associations. At its peak, QAnon exemplified the interactions between the searching disposition, digital mediations and for-profit targeting. Its members saw themselves as critical thinkers uniquely equipped to discover hidden truths and interpret byzantine clues. They ferociously denied being part of a cult, since, as one of them put it to the researcher Peter Forberg, ‘no cult tells you to think for yourself.’
One might think that the advent of LLMs will counter these tendencies. Perhaps if properly integrated with a search engine, an LLM might distil vast amounts of information into coherent responses that do not pander. It can certainly provide seemingly authoritative summaries that look like answers, though its inner workings remain essentially opaque. But it is not clear whether such systems – even if they work as advertised – can solve the problem of reliable knowledge in a balkanised public sphere. Just as commercial incentives led to fake content and filter-bubbles, LLMs likely face the same pressures in a world of sharply diminishing returns. Because the firms training them desperately need to make money, the familiar business logics of personalisation and tiered benefits are likely to reassert themselves, with customised epistemic universes now served up by models catering to publics with particular tastes and different abilities to pay.
W hat happens when authenticated, epistemically egocentric selves enter the world of politics? If you are an authentic, self-directed individual, your greatest cultural fear is of being swallowed up by mass society, just as your greatest political fear is of surveillance by an authoritarian state. These fears are still very much with us. But in a world chock-full of socially recognised categories and authenticated identities, new dilemmas present themselves. On the individual side, everything – public behaviours, statements, metrics – can potentially become a source of difference, and thus of identity. On the organisational side, the data that users generate will lump or split them in increasingly specific, fleeting and often incomprehensible ways. The more precise social classifications are on either side or both, the more opportunities arise for moral distinctions and judgments.
The main casualty is the possibility of broad-based, stable political alliances. The more citizens are treated, individually, as objects of market intervention, the more disaggregated politics becomes. Traditional voter-targeting began with a political message and sought out individuals receptive to it. The rise of big data reverts this logic, starting from the cultural dispositions of electorates and building resonant messages from the ground up. Before Cambridge Analytica, Italy’s Five Star Movement (M5S) arguably pioneered this data-driven approach to politics. The essayist and novelist Giuliano da Empoli describes it in his book Les ingénieurs du chaos (‘The Engineers of Chaos’, 2019). The whole thing started in 2005, when a digital marketing specialist with a taste for direct democracy, Gianroberto Casaleggio, recruited a popular comedian and satirist, Beppe Grillo, to launch an eponymous blog to share his political disillusion and outrage with the public. The blog encouraged public participation, allowing Casaleggio – succeeded after his death in 2016 by his son Davide – to track resonant grievances and proposals through likes, comments and user feedback, and to test, tailor and refine Grillo’s political messaging. The result was the birth of the first ‘algorithmic party’, whose chaotic ideology was cobbled together from insights supplied by the data. Soon, ‘the people of the blog’ would be invited to migrate to the streets, supported by the social media infrastructure of another digital tool, the Meetup app. In 2018, the M5S became the largest party in Italy, helping form a short-lived coalition government.
A small cadre of ultra-wealthy men have reclaimed control over the state through a direct appeal to the masses
Modern political campaigns have evolved this approach into something more sophisticated and arguably more manipulative. Social media-generated data about cultural practices, emotions and dispositions on a wide range of subjects helps craft new narratives and aesthetics, reshape people’s informational environment and social connections, and activate their votes at strategic times. The desired political goal is typically achieved through an elaborate ‘persuasion architecture’ of personalised messaging and repeated exposure. For instance, advertising algorithms find patterns of successful actions (donations, likes, purchases, shares) and target similar users who can be induced to repeat these behaviours. Each iteration uses real-time response data to create an ever-more granular map of manipulable targets. Political mobilisation is, in effect, governed cybernetically through algorithms. Its operational logic emerges from constellations of variables that are hard to grasp all at once, giving the resulting political formations an emergent, ad hoc quality, somewhat independent from traditional mediating bodies like political parties and social movements.
The weakening of these conventional structures and the ability to individualise political messaging also produces highly personalised forms of social domination. Populist leaders thrive on perceptions that they have a direct connection to the public – even though this connection is often attended to by an entire ecosystem, a carefully constructed ‘propaganda feedback loop’. Owners of social media can even force this connection onto users via self-serving algorithmic manipulation, as Elon Musk and Donald Trump have both reportedly done on their respective platforms. Intoxicated by the ideal figure of the ‘sovereign individual’ unconstrained by national borders, social norms or the law, a small cadre of ultra-wealthy men have been able to reclaim control over the state and traditional elites through a direct appeal to the masses and to market freedom. Pushing the logic of sovereign individuality to its logical extreme, some are busy trying to carve out independent territories for themselves, complete with their own rules and possibly their own currencies. Others invest in revolutionising existing institutions from within. Until he withdrew from his government role, Musk hoped to oversee a radical remaking of the state as a centralised and largely automated computing infrastructure. Trump made himself into a digital token, offering his own status and reputation as an investment opportunity to enthusiastic followers and anyone striving for access, while his sons leveraged their father’s position to build (or attempt to build) a cryptocurrency empire.
Most people have to work harder. The digital economy is full of rags-to-riches stories built on the back of technical systems, encouraging every teenager to compete for and capitalise on quick fame. But the rules of these games are twisted. Money, time and social capital play a big role in propping up some individuals over others. This economy rides on payments for algorithmic boosters, ancillary supports for production and advertising, and connections to others, including bots, who can relay the message, both within and outside online spaces. The rise of self-branding is in part a mark of desperation, an ideological smokescreen masking the much bleaker social reality on the ground. As the deployment of digital technologies continues to generate ever-more stratospheric concentrations of wealth, the masses sink deeper into the void left by the evisceration of social solidarity and the rise of automation. The often-missed point about sovereign individuals is that not everyone gets to be one. But everyone should aspire to be one, and in the meantime follow one, as they walk down the road to selfdom.
W hen he wrote to IBM France in 1955, Jacques Perret had one slight reservation about his chosen name for the new machine:
Professor Perret was more correct than he knew. In the 70 years since he baptised it, the descendants of the Model 650 have indeed taken on quasi-religious functions in modern society. Computers authenticate our souls and find our innermost truths. They shape our search for meaning in a disorienting and fragmented world. They foster new forms of political communion and sectarian schism. Above it all, stands the sovereign individual – the embodiment of modern selfdom, served by the ordinateur ’s ruthless logic and its power, while it lasts, to manufacture gold out of bits."
We need a planetary system of diplomacy for the 21st century | Aeon Essays,,https://aeon.co/essays/we-need-a-planetary-system-of-diplomacy-for-the-21st-century,"On 31 July 2024, an intriguing ceremony took place on Pheasant Island, a tiny sliver of land in the river Bidasoa, marking the border between France and Spain in the Basque Pyrenees. Under a lush canopy of trees, a handful of people disembarked from rubber dinghies and walked towards a monument, the only man-made structure on the island. Most of them were wearing the pristine white uniforms of the French and Spanish navies. The walk was a short one, as the island is only 200 metres long and 40 metres wide.
Near the monument, there were speeches. Floral wreaths were laid down, trumpets, cornets and bugles resounded, and a number of gun salutes were fired. On the flagpole, the Spanish bandera was lowered and the French tricolore hoisted. The island’s anthem – yes, it has one, despite being uninhabited – was played. The atmosphere was a unique blend of solemn military protocol and gleeful exuberance, just like it was the previous year and the years before. Every year on 31 July , France reassumes sovereignty over Pheasant Island, six months after it has been transferred to Spain.
Courtesy hendaye.fr
Courtesy hendaye.fr
The island, with an area smaller than a soccer field, changes nationality twice a year. Pheasant Island is the only example in the world of a temporal condominium, a political territory shared by multiple powers with alternating sovereignty. Governance is, in turns, entrusted to the French and the Spanish naval commanders stationed at Bayonne and San Sebastián, who carry the honorific title of ‘viceroy’ – a curious title, especially in France, where royalty has ended in exile or decapitation.
In 2022, for the first time, a vicereine was appointed, Pauline Potier, a naval commander and deputy director in the French civil administration. Upon assuming her functions, she said that the strange fate of the island was more than just amusing folklore: ‘It is a symbol of the success of diplomacy over war.’
Plan of Pheasant Island (Île des Faisans) c 1660, at the time of the Treaty of the Pyrenees between France and Spain. Courtesy Gallica/Bnf
Pheasant Island ( Île des Faisans in French, Isla de los Faisanes in Spanish, Konpantzia in Basque) has been undivided since November 1659. It was here that the Treaty of the Pyrenees was negotiated and signed, bringing an end to decades of war between France and Spain. Top diplomats like Cardinal Mazarin and Don Luis Méndez de Haro sat together for months in a temporary building to discuss the terms of peace, including a new border between the two kingdoms, the one that still runs through the Pyrenees today.
Portrait of Jules Mazarin (1658) by Pierre Mignard
The successful peace talks were sealed by a royal marriage six months later when 21-year-old Louis XIV of France, the future Sun King, set foot on the tiny island to receive the Infanta Maria Theresa, daughter of the Habsburg king Philip IV of Spain, as his bride. She came walking through the Spanish side of the sumptuous pavilion that was decorated by none other than Diego Velázquez, the most brilliant painter of his time.
The Treaty of the Pyrenees was a triumph of modern diplomacy. It served as the capstone to the Peace of Westphalia, the continent-wide settlement that put an end to a century of devastating wars in Europe. The preceding Thirty Years’ War (1618-48) had been the most brutal phase, killing approximately 8 million people. Europe had been ravaged from Sweden to Spain, a third of Germany’s population was gone, it was the bloodiest conflict on the continent before the First World War. But diplomacy had brought it to a close and the deal on Pheasant Island completed it.
W hat happened in Westphalia still shapes the way we conduct international relations today, now on a global scale. Planetary politics is still very much in its infancy, but the format it takes is invariably diplomatic: the 2015 Paris Agreement, for instance, was negotiated by national diplomatic delegations. And if the history of diplomacy teaches us one thing, it is that institutions, when faced with existential challenges, can change and reinvent themselves accordingly. By looking back, we may not only find a few insights but also some hope.
Modern diplomacy goes back to early 17th-century Europe from where it spread across the globe. Of course, there is nothing intrinsically ‘Western’ or ‘European’ about diplomacy. For millennia, countries and civilisations have been engaged in formal talks with other countries and civilisations. Mesopotamian city-states concluded peace treaties with each other around 2500 BCE. Egyptian pharaohs had envoys negotiating peace with the Hittites as early as 1259 BCE. The Greek city-states of the 1st millennium BCE had heralds and honorary consuls for short- and long-term representation abroad.
Around the same time, a sophisticated system of diplomacy developed in ancient China between the feuding kingdoms of the Warring States period (475-221 BCE). In India, Emperor Ashoka used diplomacy to spread Buddhism throughout the subcontinent in the 3rd century BCE. The Maya and Inca kingdoms, too, relied on envoys to represent their interests in the wider region, as did the Romans, the Vikings, the Arabs, and the Vatican.
Yet something peculiar happened in early 17th-century Europe. Diplomatic envoys no longer just represented their king, emperor, sultan or pharaoh, but something new and infinitely more abstract: the state. And it was this vision of diplomacy that would become dominant in modern times.
Diplomacy was distrust clad in good manners
When Geoffrey Chaucer was sent as an emissary to Italy between 1372 and 1378, he was basically on personal business trips for Edward III. The king sought to get a loan from the Florentines, a harbour from the Genoese, and a daughter-in-law from the Milanese.
However, when in the 1620s Cardinal Richelieu, France’s principal minister under Louis XIII, established the first modern foreign ministry, its foundational principle was not royal benefit, but raison d’état , the national interest. The concept had been championed by Niccolò Machiavelli a few decades earlier in Florence, and Richelieu applied it to the Thirty Years’ War. As a Catholic, he chose to support the Protestant Swedes against the Catholic Spaniards – a cynical move that made France dominant in Europe.
For Richelieu, the modern state was a political organisation with a strong centralised power holding exclusive sovereignty over a clearly defined territory. In order to scrupulously observe the balance of power with other states, highly skilled diplomats had to be organised in a permanent professional corps, with ambassadors settling abroad for years, not months. Only in this way could they gather all relevant intelligence, report back to the homeland, and engage in what Richelieu called la négociation continuelle.
His vision laid the foundation for the system of sovereign states that was formalised in 1648 with the Peace of Westphalia. During this first act of diplomatic history (1600-1800), negotiations took place mainly on a bilateral basis: France with Spain, Sweden with Russia, Poland with the Holy Roman Empire. Diplomacy was about defining borders, maintaining balances of power, and defending national interests. It was distrust clad in good manners.
The Ratification of the Treaty of Münster (1648) by Gerard ter Borch II. The treaty was part of the Peace of Westphalia. Courtesy Rijksmusuem
Of course, this new type of diplomacy would not stop all wars, but it became increasingly favoured as an alternative to armed conflict. A good ruler, wrote the influential French diplomat François de Callières in 1716, must not ‘employ arms to support or vindicate his rights until he has employed and exhausted the way of reason and of persuasion.’ Like many others in the Enlightenment, he hoped for a world order built on reason and dialogue rather than religion and war.
When France emerged as the dominant political power on the continent, Richelieu-style diplomacy spread across Europe, and French became the primary language of international affairs, giving us terms like corps diplomatique, chargé d’affaires, aide-mémoire, attaché, communiqué, entente, détente, accord, protocol and passport.
As a side-effect, French etiquette and gastronomy gained worldwide prominence. Whether Richelieu can be fully credited for having introduced the blunt table knife at formal dinners remains uncertain (he is reputed to have abhorred the custom of using knives for picking teeth – or fights), but la nouvelle cuisine française of the 18th century did put entirely new food items like oysters, lobsters, truffles, foie gras and champagne on the menu of the aristocracy from Russia to America. Life seemed good under the ancien régime, and nothing appeared poised to change it.
A new chapter started with a bang. In 1814, the Austrian foreign minister Prince Klemens von Metternich was convinced that diplomacy had to make a fresh start. After the French Revolution and the Napoleonic wars, the old order was clearly over. Leisurely bilateral chats by wig-wearing, white-powdered aristocrats sipping coffee or tea in rococo salons while gently discussing some border issue would no longer do. Napoleon’s conquest of continental Europe had shattered the old balance of power, and a radically new brand of diplomacy was needed, one that was built on the consensus of the European governments.
Metternich became to diplomacy’s second act what Richelieu had been to the first: its main architect. As a political conservative deeply preoccupied by stability, he favoured monarchism over all sorts of revolutionary adventures. He certainly did not go as far as Immanuel Kant who had suggested that long-lasting peace could be realised by bringing different countries together in a federation of free states, but he did side with the idea that international diplomatic cooperation was now key to political stability in Europe.
The Congress of Vienna (1815) by Jean-Baptiste Isabey. Courtesy the Royal Collection
Act II saw the emergence of multilateralism as a foundation of modern diplomacy, and would remain dominant for the next two centuries. In 1814 and 1815, the Congress of Vienna brought together delegates from five major powers and 12 other nations to deal with the aftermath of Napoleon. Together, they drew a new map of Europe and crafted a delicate balance of power that was to be overseen by the so-called ‘Concert of Europe’, a general agreement on multilateral consultation that had never existed before and lasted until the outbreak of the First World War. If the world changed, diplomacy had to change too.
La balance politique (1815), formerly attributed to Eugène Delacroix. Courtesy BnF, Paris
The model set up by Metternich in Vienna was soon repeated elsewhere for more topical discussions. The Berlin Conference of 1884-85 brought together 14 European nations with imperial ambitions to discuss the rules for colonising Africa. The Hague conventions of 1899 and 1907 had dozens of countries negotiating the rules of warfare. Multilateralism did not necessarily equate to internationalism. At this stage, it could be best understood as a form of ‘international nationalism’. The concept of raison d’état remained paramount, but if that ideal could be achieved by multilateral dialogue, all the better.
The rise of multilateralism did not spell the end of bilateral diplomacy
This period of increased international exchange also gave rise to the World’s Fairs (first in London, 1851) and the modern Olympic Games (Athens, 1896). It was multilateralism for the millions: competitive entertainment where European countries came together to challenge each other.
The First World War ended the Concert of Europe, but not multilateral diplomacy. The Treaty of Versailles (1919) deepened Metternich’s model. Multilateralism became the norm and took on a permanent form with the League of Nations, a first attempt at institutionalising international dialogue, which proved rather toothless in practice. After the Second World War, multilateralism was approached even more thoroughly, with the United Nations as its most important outcome. The UN was meant to succeed where the League of Nations had failed: maintaining world peace. In the 1950s, just after completing his PhD, Henry Kissinger wrote that, in an age of nuclear threat, it was ‘only natural’ to look at the Congress of Vienna, ‘the last great successful effort to settle international disputes by means of a diplomatic conference’.
After the end of the Second World War, multilateral institutions flourished: the International Atomic Energy Agency, the World Health Organization, the UN’s Food and Agriculture Organization, the World Bank and the International Monetary Fund. On a regional level, the European Union came about, as did the African Union, the Association of Southeast Asian Nations (ASEAN), Mercosur (the Southern Common Market), the Pacific Alliance, the Non-Aligned Movement, and many others.
The rise of multilateralism did not spell the end of bilateral diplomacy, as countries continued to engage in mutual negotiations. In practice, there was considerable overlap between the two approaches. Major international conferences would focus on multilateral discussions during plenary sessions while leaving room for bilateral talks during coffee breaks, breakfast meetings and dinner parties.
Classical bilateral diplomacy also spread globally, as countries decolonised and the Cold War ended. What had been a Western style of diplomacy served as a template for many new African, Asian and east European regimes. The UN membership expanded from 51 members in 1945 to 193 in 2024, adding a new layer of diplomatic dialogue.
Somehow it worked.
For all its shortcomings – multilateral organisations are notoriously unwieldy and bureaucratic – Act II diplomacy has contributed to a safer world. There has been less warfare between countries in recent decades, and fewer people have died annually from armed conflict in the past 30 years than in the previous century, despite the recent wars in Ukraine, Ethiopia, South Sudan and the Near East. The result is far from being perfect but, as the former UN secretary-general Dag Hammarskjöld once said, multilateral bodies like the UN were ‘not created in order to bring us to heaven, but in order to save us from hell.’ That minimal programme has been achieved, somehow. That the postwar world has remained free from nuclear warfare is a success story for which multilateral diplomacy deserves more credit than it usually gets.
It was no coincidence that the classic model of multilateral negotiation was chosen when, starting in the 1970s and ’80s, an entirely new threat to world peace emerged: global warming. How was this hell to be averted? In 1988, the Intergovernmental Panel on Climate Change (IPCC) was established – the UN climate panel – followed by the UN Framework Convention on Climate Change in 1992, which was signed by 166 countries and now counts 198 parties. Its highest decision-making body is the annual Conference of the Parties, or COP, which led, among other things, to the Kyoto Protocol in 1997 and the Paris Agreement in 2015.
International climate policy is thus the direct heir to four centuries of diplomatic history. From the 17th and 18th centuries (the first act), it inherited the notion of sovereign states; from the 19th and 20th centuries (the second act), the willingness to engage in multilateral dialogue. But raison d ’ état – the enlightened self-interest of anthropocentric world politics – was thereby anchored at the heart of the fledgling planetary geopolitics. That could not remain without consequences.
‘I t is an enhanced, balanced, … historic package to accelerate climate action,’ said Sultan Ahmed Al Jaber during his closing remarks as president of COP28 in Dubai in December 2023. ‘We have language on fossil fuel in our final agreement … We have helped restore faith and trust in multilateralism. And we have shown that humanity can come together to help humanity.’
The unexpected climate accord was hailed as ‘a diplomatic victory’ by The New York Times , ‘a landmark deal’ by Le Monde , and ‘a historic consensus … of milestone significance’ by the Chinese press agency Xinhua. The main reason for this almost universal praise was a line in the agreement that called on countries to start ‘transitioning away from fossil fuels’. Such explicit language was a first in 28 years of COP.
There was something peculiar about the enthusiasm. Why did it take nearly 30 years – and almost 200 diplomatic delegations – for an annual international climate conference to formally acknowledge what scientists had long shown? Negotiators had known for decades that climate change is anthropogenic, that fossil fuels cause more than 75 per cent of emissions, and that even modest warming brings serious consequences. They also knew 2023 was the hottest year on record, at that point. So why did they merely ‘call on’ countries to ‘transition away’ from fossil fuels ‘by 2050’ in an ‘orderly’ fashion, without binding commitments?
The answer is straightforward: faced with diverging national interests and relentless industrial lobbying, traditional multilateralism has proven tragically inadequate for addressing long-term planetary crises. Despite the stability and cooperation it once delivered, modern diplomacy is falling short in the face of fundamentally new threats. We have outlived Act II, but not yet entered Act III. Since the millennium, we’ve been stuck in an extended intermission, with diplomacy paused while Earth’s drama intensifies.
We are unprepared for the storms ahead and unwilling to redesign the vessel
And climate change is only one of several critical challenges. Scientists have identified nine planetary boundaries; six have already been crossed. Besides climate, these include changes in land and freshwater use, biodiversity collapse, disruptions to nutrient cycles, and the spread of novel entities like PFAS (‘forever chemicals’), GMOs and microplastics. Ocean acidification is now reaching a tipping point. These threats are scientifically clear, yet none has been met with adequate international action.
In truth, the Earth system is entering uncharted waters, but diplomacy still behaves as if we’re in familiar territory. We are unprepared for the storms ahead and unwilling to redesign the vessel. It is 1814 again – but without Metternich’s imagination. Today’s diplomats remain bound to institutional tradition, while even the most conservative figures of the past showed greater adaptability.
If an attempt is made to rethink international relations, it usually boils down to another fruitless debate on the much-needed yet never-realised reform of the UN Security Council. When the gaze is institutional, the solution is institutional. Multilateralism today is hampered by its inability to renew itself. Meanwhile, the postwar world order is visibly falling apart.
Just as humanity should be uniting to confront its greatest challenge yet – safeguarding the planet’s life-support systems – we are more divided and less resourceful than ever. Regional wars destabilise old power structures, geopolitical shifts create new fault lines, and international agreements unravel.
With each passing year, the UN seems to resemble the world of 1945 more than the world as it might look in 2045 – soon, the organisation may follow the same path as the League of Nations. And each year, COP grows larger, but also more toothless – the power of the fossil fuel lobby is increasing steadily. At the 2023 climate summit in Dubai, no fewer than 2,456 fossil fuel lobbyists were granted official access passes, four times more than the year before. They outnumbered all official delegations from scientific institutions, Indigenous communities, and vulnerable countries combined. Even the presidency was held by one of the fossil sector’s top CEOs, the previously mentioned Sultan Ahmed Al Jaber. ‘We’ve just decided to not even disguise it anymore,’ Al Gore fumed. He called for reforming these international institutions ‘so that the people of this world, and including the young people of this world, can say: “We are now in charge of our own destiny. We’re going to stop using the sky as an open sewer. We’re going to save the future and give people hope.” We can do it!’
Yet, in the meantime, nothing happens. While Metternich swiftly reimagined diplomacy after Napoleon, we appear remarkably sluggish in responding to the urgent demands of life on a burning planet .
The UN was founded to manage conflicts between countries, not between humanity and the planet
The reason why tools from the past won’t suffice is that the task involved has become dramatically different. The planetary polycrisis we are facing is not a regular war, nor even a world war or a global nuclear threat. We are talking about an entirely new form of complexity here, well beyond classical intrahuman conflict. The polycrisis is anthropogenic in its origin, but it cannot be anthropocentric in its solution. It has become a physical reality of its own, with its own ever-accelerating dynamics, its own centrifugal forces catapulting the more-than-human consequences away from its human causes.
And here lies the heart of the problem: the Earth system is in deep crisis , but we confront it with the usual solutions of the human world. No wonder that the existing concepts – national sovereignty, raison d’état , multilateral diplomacy, and so-called stakeholder engagement (a polite term for consultations with lobbyists) – fall so painfully short. The UN was founded to manage conflicts between countries, not to resolve the conflict between humanity and the planet. A flat organisation cannot solve a vertical problem.
Where did we go wrong? Somewhere along the road of postwar politics, we have begun to assume that ‘international institutions’ were synonymous with ‘global governance’ and that that was enough. We forgot that the word international meant just that: inter-national, literally between different nations. That was the logic underlying the conferences in Vienna, Berlin or The Hague. But the planet is more than a sum of countries. Clinging to the multilateral paradigm is like trying to run a country with just a convention of mayors. No wonder parochial imperatives continue to prevail over planetary needs.
How can national sovereignty remain the bedrock of international relations when we are faced with colossal planetary challenges? What can be ‘foreign’ about ‘policy’ when on the most existential of issues the world is more deeply interconnected than ever? The whole notion of ‘foreign policy’ feels increasingly meaningless in the age of planetarity. The clearcut distinction between foreign and domestic affairs comes from a time when the geophysical fiction of borders largely shaped historical societies. But extreme weather patterns, biosphere integrity, ocean acidification, sea level rise, freshwater change, mass migration, global pandemics and runaway machine intelligence laugh at the political boundaries between nation-states. This does not mean that we have to do away with borders altogether – they still structure part of our lives – only that we have to start thinking about levels of diplomacy that are not sovereignty-driven. Beyond the logic of raison d’état , we urgently need to develop the principle of raison de Terre – an encompassing approach that prioritises the interests of the Earth system above all national considerations.
P leading for the relativisation of national sovereignty undoubtedly sounds, at first glance, like heresy. It concerns nothing less than the granite foundation of four centuries of modern diplomacy, which still forms the basis of the UN today. Doesn’t the UN Charter itself state that relations between countries must be ‘based on the principle of the sovereign equality of all its Members’? A noble principle, certainly – but the result is that we are no longer capable of viewing the world in any other way than as a colourful puzzle of countries.
This, however, is a very recent reality. The idea that Earth was neatly divided into a patchwork of nation-states, all guarding their sovereignty and engaging in diplomacy with one another, had not been true for very long. In Children of a Modest Star (2024), the political scientists Jonathan Blake and Nils Gilman argue that, in 1945, half the world’s population did not live in a nation-state, but in a mandate territory, colony, protectorate or overseas possession. Only from around 1965 onwards have nearly all people on Earth lived in modern states. This, of course, is thanks to the wave of decolonisation. Colonies had to become countries, foreign domination had to give way to autonomy, and all these new nations had to be treated as equals. Wonderful ideals – but they also led to an absolutisation of the principle of sovereignty. What was in reality a relatively recent and arbitrary development – the world as a jigsaw puzzle of autonomous states – was etched in stone and presented as timeless.
Today, the EU proves that it is possible to add a layer of decision-making that transcends the individual nation-state without denying national dynamics. Why shouldn’t this be possible on a global scale? Europe is far from perfect, but it has given its member states a degree of clout they did not have on their own. Why should a global governance structure that is slightly less voluntary than the current UN be considered unthinkable by definition? Why do we stubbornly cling to an outdated horizontal model that, with each passing day, proves itself unfit for planetary problems?
The discrepancy between what people expect and what diplomacy delivers is quite staggering
Contrary to what social media and political rhetoric often suggest, humanity is much less divided in terms of planetary politics than we tend to think. A massive poll conducted in 2024 by the UN Development Programme and the University of Oxford reveals that 80 per cent of the world’s inhabitants want their country to do more about climate change. With more than 73,000 people surveyed in 77 countries, who together represent 87 per cent of the world’s population, it counts as the largest public opinion survey on climate. The results are eye-opening: a majority of people in 80 per cent of the countries surveyed were more concerned about climate change than they were the previous year, 79 per cent think that richer countries should support poorer ones in their fight against climate change, and 86 per cent believe that countries should transcend their differences and work together to address climate change.
If a very large majority of humanity desires these outcomes, why do a large majority of diplomats fail to enact them? The discrepancy between what people expect and what diplomacy delivers is quite staggering. The world already embraces the raison de Terre , but where is the public sphere where the inhabitants of the world can speak as inhabitants of the earth? Where can they express themselves, apart from in an occasional poll? The answer is sobering: nowhere. Old-school multilateral diplomacy has hijacked the global conversation on the climate. Partisan groups with vested interests such as financial and industrial lobbies and major civil society organisations have easier access to the COP negotiators than billions of everyday people whose future it is all about.
Citizen participants in the 2021 Global Assembly. Courtesy the Global Assembly on the Climate Crisis
It was precisely for this reason that, in October 2021, the very first Global Assembly was held, a bottom-up initiative without formal mandate that drew the attention of the UN secretary-general António Guterres and the COP26 president Alok Sharma. With the help of a NASA database on human population density, the team behind the project generated a random sample of 100 dots that were plotted on a map of the world. At each of these dots, they sought a local partner to select four to six everyday citizens through conversations in the street or door-to-door recruitment. In order to achieve a balance in terms of age, gender, geographical distribution, educational attainment and attitude toward climate change, a final group of 100 participants was drafted by lot from the pool of 675 candidates. The organisers readily acknowledged that the number of participants was too small to be representative of the global population, but this was just a pilot and, with a budget just under $1 million, they did what they could.
The eventual group looked like a pretty good snapshot of the world. It contained 18 members from India, 18 from China, five from the US, four from Indonesia, three from Brazil, Pakistan and Nigeria, two from Russia, Bangladesh, the Philippines and the Democratic Republic of Congo, and one from 38 other countries. During the Assembly, 42 different languages were being used, with English, Chinese and Hindi being the most common. Participants came from all corners of the world. In line with global statistics, more than half of them were younger than 35, two-thirds lived on less than $10 a day, more than a third had never used a computer in their life, a third had never attended school, and 10 per cent could neither read nor write. Sixteen members belonged to an Indigenous community, and six were refugees.
In 12 weeks, they had achieved more than COP had in 30 years
Over a period of 11 weeks, the participants spent 68 hours together online, both in plenary and breakout sessions. One of the members was Li Shimao , a student from Wuhan, studying international trade; climate change was never a major worry for him. During the sessions, he met Mohamed Salem , an elderly goat farmer from the Yemenite island of Socotra who had to travel 60 km to get online. Mohamed told Li and the other assembly members how his goats were suffering from repeated droughts, as the landscape was getting increasingly barren. There was also Madeleine Kiendrebeogo , a young female domestic worker from Côte d’Ivoire, who exchanged with someone like Chom Chaiyabut , a villager from the forests of southern Thailand.
This random sample of everyday people from the planet had access to information, translation and facilitation in order to articulate and amplify their community voices. They took their job very seriously and felt incredibly empowered by the process. ‘Previously, I felt like I was under a big tree,’ Chaiyabut said at the end. ‘Now, it’s like I am on top of the tree.’
Most importantly, together they created the People’s Declaration for the Sustainable Future of Planet Earth, a call ‘to deliver a flourishing Earth for all humans and other species, for all future generations.’ The Declaration advocated for upholding the Paris Climate Agreement – it was therefore not in opposition to classical multilateralism, but built upon it, displaying more ambition and creativity than we usually see at COP conferences. The Global Assembly, for example, called for a fair distribution of responsibilities based on historical emissions and capacity, inclusive climate action so that vulnerable countries can participate in decision-making, the inclusion of environmental rights in the Universal Declaration of Human Rights, legal protection of nature against ecocide, comprehensive climate education for all, a just energy transition with support for less affluent countries, and a shared responsibility between citizens, governments and companies to ensure a sustainable future. In 12 weeks, they had achieved more than COP had in 30 years.
At the end, Chom Chaiyabut from Thailand said: ‘The ideals of the Global Assembly make me hopeful for humanity in successfully tackling the climate change crisis around the world. I am sure we can do it, as I believe we all have mutual love towards our planet.’ He was not alone in praising what had been achieved. Even the UN secretary-general Guterres applauded the initiative as ‘a practical way of showing how we can accelerate action through solidarity and people power’.
Suppose such a global citizens’ assembly were to become an integral part of the COP meetings. After a pre-conference online phase, which could involve several million participants, a random sample of 1,000 of them, doing justice to the diversity and demography of the world, would participate. And suppose they were allowed to deliberate, not just in the Green Zone where visitors and activists stroll, but in the Blue Zone, the heart of the congress, where the official proceedings take place. And suppose this assembly had access to the best available science on climate change and its causes. They would also hear from national politicians, civil society organisations, private sector actors, religious leaders and Indigenous communities. At the end, they would deliver their recommendations to the leaders of the world. Would they need almost 30 years to state the obvious, namely that we have to get out of this fossil nightmare as soon as we can? Most probably not. They would take planetary custodianship to an entirely different level, well beyond the pathetic bargaining of national and industrial interests at the annual COP conference. They would show that, above bilateral and multilateral diplomacy, another level is possible: planetary diplomacy.
T he proposal to include a global citizens’ assembly at the heart of international climate diplomacy is less utopian than it might seem at first glance. Ideas in this direction are gaining international traction. That, at least, was the conclusion I got from the conference in Oxford in July 2024: ‘A Permanent Global Citizens’ Assembly: Adding Humankind’s Voices to World Politics’.
A few months later, an essay by Laurence Tubiana and Ana Toni was published, entitled ‘The Case for a Global Climate Assembly’. The authors were no lightweights: Tubiana was one of the lead negotiators of the Paris Agreement, and Toni is currently Brazil’s secretary of climate change and the main person responsible for COP30, which will be held in the Brazilian city of Belém in November 2025.
They, too, recognised that our existing diplomacy was falling short, and suggested involving ordinary people as well. They pointed to national citizens’ assemblies with randomly selected citizens in Ireland and France, and participatory processes in Brazil. It was shown that randomly selected citizens deliberate more freely and are less hindered by party interests than political elites, resulting in more ambitious and coherent recommendations.
We need spaces where the world can speak as the world on the problems of the world
Tubiana and Toni felt it was high time to bring this approach to the global level, just as Richelieu had brought the raison d’état from the local to the national level:
They wrote that the time is ripe for this. In G20 countries , 62 per cent of citizens support the idea of citizens’ assemblies, while in countries such as Brazil, India, Indonesia, Mexico and South Africa, the figure rises to over 70 per cent, and in Kenya, more than 80 per cent of the population is in favour.
Courtesy Ipsos, Earth4All and the Global Commons Alliance Global Report, June 2024
Their conclusion was as obvious as it was bold:
It is expected that this first attempt to integrate a global citizens’ assembly at an international climate summit will face considerable challenges. While Brazil has a long tradition of participatory processes and social dialogue, it is also an oil-exporting country that in February 2025 joined OPEC+, the forum for oil producers.
This time around, the core assembly will be preceded by a large number of community assemblies worldwide. Organisations from across the globe have been involved in shaping the governance and designing the process. Particular attention is given to non-Western forms of knowledge.
In diplomacy’s third act, we need spaces where the world can speak as the world on the problems of the world. Global climate governance involves deep moral choices about the future of the planet that cannot be left in the hands of national negotiators alone. For instance, how are we going to distribute the remaining carbon budget? Can rich countries continue as before because their economies are so carbon-intensive, or should the last gigatons be given to the poorer countries who need them for their basic development?
Even more important will be the debate on geoengineering . As the planet approaches irreversible tipping points and faces the risk of a runaway climate for centuries to come, should we buy some time by spraying sulphate particles into the stratosphere to reflect the Sun’s rays? This type of solar radiation management could create an artificial volcanic winter, giving humanity a few extra years to get its act together. Is it too dangerous to attempt? Or is the greatest danger that governments might cease all other efforts once they can cool Earth by simply sprinkling dust? These are such fundamental choices for the world that a large, representative sample of its inhabitants should at least be given the opportunity to weigh in on the appropriateness of such a far-reaching intervention.
There is no shortage of major questions. Just to name a few: should humanity have a say in matters such as PFAS and microplastics, or can these issues continue to be settled behind closed doors by political and economic elites? Should the Moon be opened up for the exploitation of its minerals and solar energy, and, if so, under what conditions? And how about Mars and the growing use of interplanetary space? Who determines whether and when spatial exploration may become spatial exploitation?
T he question is easy: how on earth are we going to save Earth? Do we content ourselves with quietly continuing to watch the painful spectacle of the past decades, believing that this protocol is the only one possible? Or do we draw hope and inspiration from global opinion polls and fascinating experiments that show that everyday people want so much more action and can play a crucial role themselves?
We are at a crossroads. If diplomacy is to have any relevant role in the age of planetarity, it has to update its current multilateral paradigm. Just like the bilateralism of the ancien régime was fundamentally transformed by the cataclysmic events of the Napoleonic era, the multilateral model must be structurally renovated given the catastrophic conditions of our time. Every 200 years, diplomacy needs an update. Why wouldn’t it be able to renew itself today?
The big question, then, is not whether diplomacy will change, but how. For 400 years, it served the nation-state; in the future, it will have to serve in the name of Earth. To begin moving in that direction, diplomacy must, as soon as possible, bring the voice of the planet’s inhabitants into the heart of its crucial deliberations – not to replace current negotiations, but to complement them, just as Metternich’s multilateralism once enriched rather than replaced Richelieu’s bilateralism.
At this juncture, it may be more instructive to revisit the older traditions of non-Western diplomacy
At COP or the UN General Assembly, these assemblies may become essential for contemplating the common good and the long term, offering a moral framework for future action. Ideally, their recommendations would have legal status and become an integral part of the global decision-making process. To be effective, a follow-up mechanism will need to be integrated. Inspiration can be taken from the first institutionalised citizens’ assemblies, like the one in the German-speaking community of Belgium, the first place on Earth where the elected parliament is flanked by a permanent assembly drafted by lot. Each time everyday citizens make recommendations, elected politicians are obliged to respond.
Whatever its precise form, Act III diplomacy will require a loosening of the raison d’état in favour of the raison de Terre. This may sound as visionary as Kant’s ‘federation of free states’ did in 1795, but two centuries later his idea has materialised, notably in the European Union. Moreover, the EU exemplifies how the voices of both countries and citizens can be combined in international decision-making, with its delicate balance of power between the national governments and the transnational parliament.
However, diplomacy’s third act should not overly fixate on European examples. Sufficient inspiration has been drawn from the West over the past 400 years. At this juncture, it may be more instructive to revisit the older traditions of non-Western diplomacy, largely overshadowed by the Westphalian system of sovereign states.
Classical Chinese diplomacy, for instance, centred on the notion of tianxia , ‘all under Heaven’, encompassing the entire physical world of lands, seas and mortals. Confucian values like ren (benevolence), yi (righteousness) and xin (trustworthiness) continue to inspire Chinese diplomats and may prove relevant when sketching the outline of a planetary democracy. Similarly, the Indian concept of Vasudhaiva Kutumbakam , a Sanskrit phrase meaning ‘the world is one family’, could help us – it goes back to one of the Upanishads written between 800 and 500 BCE and was used as the theme of India’s G20 presidency in 2022-23. Indonesia has inscribed the traditional practice of musyawarah‐mufakat , village-based deliberation and consensus-making, in the foundational philosophy of the country’s democracy. The African philosophy of ubuntu – ‘I am because we are’ – remains a potent reminder of human interconnectedness and the universal bond between all living things. In post-apartheid South Africa, it has notably inspired new forms of justice that prioritised collective healing over individual punishment. Ideas and moral values matter, as they shape the institutions with which we work. Act III diplomacy will, therefore, need the best ideas the world has on offer, in order to succeed.
There is also an obvious need for more demographic justice. China, India and Indonesia are three of the four biggest countries in the world – together, they present more than 38 per cent of the world population. And in 2050, a quarter of the world will be African, as its population will rise to 2.5 billion. It seems about time that some of their most central philosophical and spiritual ideas help shape global politics of the future.
A number of years ago, I walked the Pyrenean High Route, an 800 km- long trail from the Atlantic to the Mediterranean, following a high course in the mountains that stays as close as possible to the French-Spanish border, the one established so long ago on Pheasant Island. As I was climbing up and down the granite and limestone border ridge, I found myself cursing more than once at the negotiators of Pheasant Island who had gladly drafted a line on the map without actually getting up there themselves. But the hike was absolutely magnificent, and gave me ample time to reflect on the beauty and the fragility of the world.
At a given point during my hike, the reality of climate change kicked in with such violence I will never forget. I was camping in front of the brutally beautiful north face of the Vignemale summit, the highest peak on the French side. At dusk, the silence of the spectacular setting was interrupted when a section of the mountain’s easternmost glacier broke loose and came tumbling down its scree slopes in a cloud of dust and stones. The sound – a raw, thunderous roar – was unearthly and deeply unsettling. That night in my tent, I struggled to find words to write down in my diary. The following days, I kept on pondering over the event during my climbs. Even if it was too early to find satisfying answers, I began to sense that there was something fundamentally wrong with a system of world politics just based on national sovereignty. If the broken Vignemale glacier had made one thing clear, it was that private vices did not lead to planetary virtues.
We have already moved from the Enlightenment to the age of ‘Entanglement’
Later at home, I came to realise that there is a certain Cartesian quality to the Western-style diplomacy that has dominated the world. Richelieu was reshaping France’s foreign policy at the same time that Descartes wrote his Discours de la méthode (published in 1637). Both placed the self at the heart of their logic. What the cogito was to Descartes was the raison d’état to Richelieu: a vantage point in the middle from which all the rest was to be deducted.
The timing was perhaps not a coincidence. Just a few years earlier, Galileo Galilei had demonstrated that, not Earth, but the Sun stood in the middle of the planetary dance. Descartes and Richelieu came up with the metaphysics and politics of self-centredness, as if something had to be compensated – from geocentric to egocentric, so to speak. Right after Earth was dethroned from the centre of the solar system, a self-centred perspective became deeply ingrained in the core of Western philosophy and diplomacy, and it has remained there until now. It continues to shape the way we deal with the planet today, from Pheasant Island to the COPs in Sharm el-Sheikh, Dubai or Baku.
Today, it is time to develop a new geocentric model – not in the astronomical sense, of course, but philosophically: a fundamental awareness that places the Earth system at the centre of our thinking and actions, and that considers the raison de Terre as the keystone of global governance. Drawing on a wide range of philosophical and spiritual traditions, this geocentric awareness can even look beyond the interests of present generations and strictly human concerns to take into account the distant future and the more-than-human life. Such a perspective was already pioneered by the Global Assembly when its members unanimously called for ‘a flourishing Earth for all humans and other species, for all future generations’. More than we might realise, we have already moved from the Enlightenment to the age of ‘Entanglement’. It is time to design a diplomacy that befits this new reality. It is time for planetary governance."
Why my campaign to protest kidnapping in Nigeria failed | Aeon Essays,,https://aeon.co/essays/why-my-campaign-to-protest-kidnapping-in-nigeria-failed,"For students of Greenfield University in northwest Nigeria, 20 April 2021 was like any other day. Muslim students observing Ramadan prepared to break their fast with iftar in the company of friends, many walking in the warm evening from the dormitories or the library to the cafeteria. Then, at about 8:15 pm , gunshots rang out. For some, the sound was not all that perturbing since the school, situated on a hill, was close to a major highway where armed robbers frequently attacked motorists. But this time was different.
Armed men, some in camouflage so that students would mistake them for military personnel, rounded up 23 people – 18 students and five staff – took their phones and herded them into the bushes. Some female students were half-dressed. Many had no shoes on. They trekked for hours before arriving at waiting motorbikes. Driven in twos or threes, they were brought to a camp where other kidnapped Nigerians were held.
Three days later, the kidnappers told parents that they had a message for them: the remains of Dorathy Yohanna, Precious Nwakacha and Sadiq Yusuf Sanga had been left on the side of a road.
Yohanna, a third-year student of political science, turned 23 a few days before she was kidnapped. The night before her death, she cried and prayed, fretting over her mother’s reluctance to let her go back to Greenfield due to security concerns. Nineteen-year-old Nwakacha, in her third year of accounting, was outspoken and articulate, which was why her classmates thought she was a good choice when she was purportedly selected by the kidnappers to take a message to the parents – they knew she would be a good representative. Sanga, who was in his third year of a cyber security course, woke the morning he was killed to share his dream: three people were going home, and he was one of them.
I n Nigeria, there are two popular aphorisms related to sleep: one is ‘When you wake up is your morning’, the other is ‘You cannot wake a person pretending to be asleep.’ Though I did not know the details when I heard the news that three of the students kidnapped from Greenfield had been killed, their deaths were a morning of sorts for me.
Greenfield was not an anomaly. Schools are soft targets for terrorists and mercenaries, and this kidnapping was not more horrific than the murder of 59 students at the Federal Government College of Buni Yadi in 2014. My distress came from the peculiarly exploitative nature of the Greenfield kidnappings. The attack on Buni Yadi was ideological terror, not a negotiation for money: the kidnappers, in this case, demanded the families of the students pay a ransom of 800 million naira (around US $2 million ). The average income of Nigerians is $190-$355 a month.
Channelling the desperation that I imagined the families were feeling, I reached out to two feminist activists, Olabukunola Williams and Chioma Agwuegbo, to find out what could be done. It was unfathomable that Nigerians, especially Nigerian women, would not be mobilising in response to this. It was time for us to wake up.
Williams, Agwuegbo and I developed a plan to respond to the Greenfield kidnappings: mobilise women, as influencers in their homes and communities, in order to organise Nigerians to demand that the federal government deliver on its responsibility to ‘Secure Our Lives’ – which became the name of our campaign. Our goal was to force the government to acknowledge the toll kidnapping was taking on Nigerians and to bring an end to what had become a terrifyingly profitable industry.
We were left with one enduring question: why did the campaign fail?
Six weeks after we started, the campaign received donor funding. By then, there were reports that some of the kidnapped students were gravely ill, and two more had been murdered: Feso Osunlalu, a third-year student of international relations, and Shaheed Abdulrahman, a first-year student who had been kidnapped the very day he started at Greenfield. But, on 29 May 2021, all the survivors of the Greenfield kidnapping were released after a ransom of 150 million naira (around $375,000) was paid, together with the delivery of eight new, fuelled motorbikes. Some parents, still in debt today, had to sell properties, crowdsource and borrow to make up their contributions to the ransom. Neither the Kaduna state government nor the federal government were involved in the negotiations or tried in any way to alleviate the financial burden on parents.
From the Secure our Lives campaign
Though relieved by their release, we carried on with the campaign because kidnappings at schools continued to occur. We began to put together a coalition of feminist, civil society, media and faith-based organisations that were involved in security, gender equity, human rights protection, and access to education. We brainstormed on how best to mobilise the public and capture the attention of the government. How could we get people to wake up? In addition to a day of mass action on which Nigerians would make noise at a set time of the day, reminiscent of banging pots during the COVID-19 pandemic, we planned to incorporate poetry and public installations to raise awareness about the human costs of kidnapping. (One poem began: ‘How much blood will this land drink before it says enough?’) We also published a ‘red list’ of Nigerians who had been killed or were unaccounted for from kidnapping and disappearances.
The day of action, 28 July , arrived, but the impact was not even close to what we had hoped to achieve. We continued advocating by hosting live audio conversations on X/Twitter Spaces, and anchoring our engagement to global recognition days such as International Day of the Girl Child and 16 Days of Activism against Gender-Based Violence. By the end of 2021, the campaign had petered out. We had not succeeded in waking up any significant number of Nigerians, and there were funds to spare. We were left with one enduring question: why did the campaign fail?
M y reflections about this campaign over the past few years have been shaped by observing other attempts at organising, as well as by reading works on the nonprofit world, also known as the ‘third sector’. Anthea Lawson’s The Entangled Activist: Learning to Recognise the Master’s Tools (2021) and Issa Shivji’s Silences in NGO Discourse: The Role and Future of NGOs in Africa (2007) both explore and critique, in different ways, individual and organisational activism. Shivji, a leading expert on law and development, identifies NGOs (non-governmental organisations) as handmaidens of neoliberalism who replace a degraded state that is no longer capable of providing social goods and welfare. NGOs pretend to be ‘pro-poor’ while maintaining the capitalist, globalised hegemony that creates so much poverty.
Adopting some of Shivji’s criticisms of NGOs as a framework for assessing the Secure Our Lives campaign has been useful for processing my disquiet about NGOs and the state of organising in Nigeria today. One critique is that activists try to change the world without understanding it: ‘how can you make poverty history without understanding the “history of poverty”?’ Shivji asks. Indeed, how could we successfully demand that the government secure our lives without understanding the nature of the state and the sources of insecurity? Investigating the roots of insecurity in Nigeria aligns with Shivji’s observation about the NGO space that geopolitical realities are all too often ignored. For our campaign, the geopolitical realities were highly complex, with the politics of West Africa and the Sahel largely stemming from the Cold War, foreign military bases and their implications for sovereignty, the proliferation of illegal arms, a decades-long conflict with jihadists in northeast Nigeria, and the impact of global warming on traditional economic activity. All of this needed to be taken into account but, in 2021, we were reacting to an immediate emergency. We did not have time to build the necessary theoretical knowledge about the extractive, neoliberal nature of the Nigerian state and the influence of geopolitics.
In a neoliberal, extractive state, where the majority earn a daily wage, participating in civic engagement is a luxury
On the other hand, it must be noted that my fellow organisers and I live these geopolitical realities. And, as university graduates and activists working on gender equality, governance and social justice, we are already grounded in the relevant theory and prepared to build our organising framework on our knowledge. But that is not always the case for many organisers. Courses on critical thinking were stripped from university curriculums in Nigeria in 1986 at the height of the president Ibrahim Babangida’s struggle with student unions, who were protesting the conditions of the ruinous Structural Adjustment Programmes (a number of neoliberal economic reforms forced, by the Bretton Woods institutions, down the throats of developing countries in the Global South as the magic tonic for development). This reality is arguably mitigated by the fact that philanthropies are increasingly led by people with doctorate degrees and the NGO world has a healthy supply of intellectuals. The questions remain: is Shivji right to claim that intellectual curiosity and theoretical frameworks are missing from activism? Whose theorising is required to demand a better society?
These questions are connected to the silence of the people in the communities that our campaign intended to help. Shivji’s argument is that the voices of the majority are neglected in elite approaches to development, advocacy and organising. Local wisdom and knowledge are too often sidelined. Although it might have been a novel position in the 2000s when Shivji made this observation, it is conventional now that movement-building and social justice advocacy need to be bottom-up and organic. In other words, those versed in ‘foundationese’, as the social commentator Dwight MacDonald termed it in 1956, are today implored to get out of the way. The legitimacy of campaigns requires grassroots representatives to be part of the conversation. Inclusion strengthens planning. However, the reality is that in a neoliberal, extractive state, where the majority earn a daily wage, participating in civic engagement is a luxury. This is why civil society often pays people to participate and protest. Maybe the challenge is not so much that the elite are doing much of the organising but that they are trying to organise without clear ideological values.
On the politics of aid, Shivji points out that funding and aid create dependencies within civil society and make it harder for people and institutions to collaborate. This was our experience. Once it was known that the campaign had funding, there was an expectation that we would distribute funds to mobilise women in the typical fashion: pay to protest. But that was precisely what we did not want. This caused a few organisations to withdraw from the campaign.
The recent suspension of aid from the United States Agency for International Development (USAID) puts arguments about dependencies in focus. As one of Africa’s top recipients of US foreign aid, estimated at $1.02 billion in 2023 alone, the suspension has sparked major debates in Nigeria about the politics of aid. Some blame international aid for dulling and dissipating civic dissent and lowering citizens’ expectations of government to provide public goods. Others argue that aid is a form of repayment of the debt owed from colonialism, neocolonialism and neoliberalism. Still others insist on acknowledging the positive impact of aid, particularly on public health services, while a small fraction of people point out that the US is the biggest beneficiary of its aid because it builds global soft power and supports US businesses and organisations. Of USAID’s total contract spending in 2021, almost $5.4 billion out of $5.5 billion of went to US bidders.
We experienced subterfuge, competition for visibility and recognition, co-option and infiltration
Because, in the civil society space, people always follow the money, our plans for Secure Our Lives became politically fraught by virtue of funding and focus. As the feminist activist Almut Rochowanski writes: ‘Money is never “just money” … it is power incarnate.’ By deciding not to spend on mobilising citizens to join campaign activities with transport or lunch allowances, we partly sealed the failure of the campaign. But we were not going to follow industry norms and organise people by paying them. Instead, we wanted an organic campaign – like Bring Back Our Girls, launched in response to the kidnapping of 276 schoolgirls in Chibok in 2014, where people went to rallies and sit-outs (the movement’s term for sit-ins) because of shared humanity and experience, not because they had been promised the standard 1,000 naira ($2.50).
Strategy sessions at Secure Our Lives became battles about representation and identity. The pathologies of organising in a traumatised country, where power is exercised through bribery, spying and the weaponisation of identity – no matter if you’re running the government or a civil society organisation – soon became evident. We experienced subterfuge, competition for visibility and recognition, co-option and infiltration. In this, Shivji is right that NGOs too often disregard the existing power structures around which societies are organised. In hindsight, our attempt at drawing in the Catholic Women Organization (CWO) and the Federation of Muslim Women Association of Nigeria (FOMWAN) was misplaced and mishandled. Misplaced because religious organisations in Nigeria tend to defend the status quo and eschew anything radical. I might have botched our chances of signing up CWO by leading that engagement. As a Muslim from Middle-Belt Nigeria, I was probably not the best person to build an alliance with Catholic women. Then again, I was not successful with FOMWAN, so being of the same religion did not help either.
Our campaign barely made a dent in the consciousness of the working-class women we claimed as our primary target. Is the daily wage earner, the one civil society pays to attend meetings and protests, the same person that Shivji expects to lead organically? The women outside the formal NGO sector who took part in the campaign were largely affiliated with civil society organisations working in urban and rural areas. Were we preaching to the converted or were our messages unwittingly designed for them? Whatever the case, we could hardly say that educated, elite women supported the campaign in larger numbers than working-class women.
D espite the failure of the campaign, I’m left with two final questions: why was that fateful day in April, when the kidnappings took place, my morning? And why couldn’t we wake up more people?
The answers might be linked. I have had several mornings where I’ve been shaken awake by events and a sense that if we do not force government to act in the way that it is supposed to, it never will. I realise now that, for what might be a majority of Nigerians, the government does appear to act exactly in the way it is supposed to. So complete is the government’s abdication of its responsibilities that many have no concept that a government is supposed to provide public goods.
My orientation towards liberal, representative democracy – one based on a social contract in which I, as a taxpayer and voter, can demand accountability from my elected representative – is not the reality for many Nigerians. As a primarily resource-based economy reliant on extraction and trade of raw mineral resources, our government revenue does not depend on the wellbeing of Nigerians, and because the tax system is informal, for the majority there is no sense of a social contract.
I had been unable to explain the inertia of citizens, uncorrupted by donor funds, who presumably had aspirations for a better society. But this was not inertia. Those who want better security are not demanding anything from a government of which they have no expectations. Instead, they are hiring vigilantes and terrorist groups like Lakurawa to protect them.
As the economic situation in Nigeria has worsened, kidnapping has become big business
Within the first three months of 2025, at least 537 Nigerians were kidnapped. Despite generous defence budgets ( $4.47 billion in 2021, up to $5.13 billion in 2024) and global support to tackle insecurity, Nigeria has remained among the top 10 countries most impacted by terrorism since 2011. In 2015, when Muhammadu Buhari was sworn in as president, it was ranked second globally for impact of terrorism. By 2021, Nigeria’s rating had improved by three places but, while terrorism declined, kidnapping rose.
Kidnapping is only one cause of insecurity in Nigeria. Cattle rustling, armed robbery, terrorism, gang violence, illegal gold mining, secessionist movements, police predation, crude oil bunkering and drug smuggling all exist across the country, feeding into each other. It is possible, as security analysts report , that kidnapping funds transnational organised crime, and there is anecdotal evidence that, as the economic situation in Nigeria has worsened, kidnapping has become big business. The recent rise in the kidnapping of medical personnel is an indication that the kidnapping industry is evolving, requiring the services of doctors and nurses for kidnap camps. Insecurity is a result of decades of steady, near-complete abdication of governance by bureaucrats and politicians. It is also about industrial capitalism, identity politics and extraction. It is about Nigeria, Africa’s place in the world economy, what we can and cannot produce, who governs us and why. It is about our elections, politics and lack of public accountability. It is impossible to address insecurity as a single issue.
Nigeria has an enormous number of NGOs – around 200,000 – but society is less organised. Typical donor funding for programmatic activities is unlikely to provide the critical requirement for successful organising: time. Some might say there is a dissonance in expecting grants to fund organising. If many Nigerians have given up on the idea that government owes them a duty of care, information alone cannot wake them up. Without an anchoring ideology or philosophy that provides values around which people identify and organise, appeals to enlightened self-interest and an assumption that ‘we all want to be secure’ will never be enough.
However, mobilising a critical mass of citizens is still one of the few ways to exert pressure to reform systems and structures – especially with the global regression in democratic values and institutions. As civic spaces constrict and social media algorithms deepen polarisation, reaching into the history of organising in Nigeria and investing in new models are more necessary than ever.
In 2011, the political scientists Erica Chenoweth and Maria Stephan, reviewing resistance campaigns from 1900 to 2006, put forward a magic number: 3.5 per cent. According to their research , that’s the critical mass required to ‘shift’ an issue in any given society. In Nigeria, that means 8 million people. That’s a lot of people to wake up at the same time."
How did Kerala go from poor to prosperous among India’s states? | Aeon Essays,,https://aeon.co/essays/how-did-kerala-go-from-poor-to-prosperous-among-indias-states,"India is a union of 28 states (provinces). The population in some of these states is bigger than that of the largest European countries. For example, Uttar Pradesh is home to more than 240 million people, almost three times the population of Germany. Although a part of a federal union, every state has a unique history, shaped by its environment and natural resources, princely or British colonial heritage, language and culture. Since the end of British rule in the region in 1947, their economic trajectories have diverged, too.
With roughly 35 million people, Kerala, which sits along India’s southwestern tip on the Indian Ocean, is among the smaller Indian states, though it is densely populated. In the 1970s, Kerala’s average income was about two-thirds of the Indian average, making it among the poorest states in India. This difference persisted through the 1980s. In the coming decades, a miracle occurred. Kerala, one of the poorest regions in India, became one of the richest. In 2022, Kerala’s per-capita income was 50-60 per cent higher than the national average. What happened?
Even when it was poor, Kerala was different. Though income-poor, Kerala enjoyed the highest average literacy levels, health conditions and life expectancy – components of human development – in all of India. Among economists in the 1970s and ’80s and among locals, ‘Kerala is different’ became a catchphrase. But why, and different from whom? One big difference Kerala presented was with North India, which had an abysmal record of education and healthcare. While the population grew at more than 2 per cent per year in the rest of India, Kerala’s population growth rates remained significantly lower in the 1970s. High literacy and healthcare levels contributed to this transition.
Kerala’s unusual mix of high levels of human development and low incomes drew wide attention, including from leading scholars. Among the most influential writers, K N Raj played a big part in projecting Kerala as a model for other states. Anthropologists like Polly Hill and Robin Jeffrey drew attention to some of the unique features of the society that led to these achievements. In a series of influential works , the Nobel-laureate Amartya Sen and his co-author the economist Jean Drèze praised Kerala’s development model for prioritising health and education, even with limited resources, and claimed that this pathway led to significant improvements in quality of life. Kerala vindicated the intuition that Sen and others held that health and education improved wellbeing and shaped economic change by enhancing choices and capabilities.
Why do Kerala’s differences matter? What lessons did the economists draw from the state’s unique record? Around 1975, India’s economic growth had faltered, and a debate started over whether the country should give up its socialist economic policy in favour of a more market-oriented one, in which the government would take a backseat. Kerala suggested three lessons for those engaged in the debate: (a) income growth rate was a weak measure of standards of living; (b) what mattered was quality of life, including education, good health and longer lives; and (c) the government was necessary to ensure investment in schools and hospitals. The three lessons would coalesce into the Kerala Model, an alternative recipe for development to the neoliberal model then being pushed by Right-wing lobbies.
But Kerala was about to grow even more different, confounding orthodoxies in political science and economics. In the 2000s, average income in the state forged ahead of the Indian average. Compared with Indian averages, the post-1990 growth record was less impressive regarding human development, as India caught up with Kerala (see graph below). The forging-ahead in income was offbeat and is still poorly understood. This question remains unanswered because, so far, the attention of economists has been elsewhere – welfare policies – whereas the income turnaround suggests an emerging pattern of private investment that strides in basic health and literacy alone cannot explain.
Before we tackle that question, it will be useful to discuss the huge presence of the state in development studies. Where does it come from? Why does the state fascinate so many social scientists?
F rom a historical perspective, Kerala has at least four distinct qualities that most states in India do not share. First, it has a centuries-long history of trade and migration, particularly with West Asia and Europe. Second, Kerala is rich in natural resources, which have been commercially exploited. Third, Kerala boasts a highly literate, skilled and mobile workforce. Finally, the state has a strong Left political movement. Any story we tell about its advances in health and education or its recent income growth must refer to some of these longstanding variables.
Why was Kerala different? In the minds of many economists, the state’s heritage of Leftist trade unions (more on this later) and successive rule by Leftist political parties helped provide the foundation for strong human development. Socialism was not just a popular ideology but had a real chance to deliver in this state. Others stressed geography, princely heritage and social reform movements. For example, the British anthropologist Polly Hill noted that Kerala differed due to its coastal position, semi-equatorial climate, maritime tradition, mixed-faith society and princely rule. The combined share of the population following Islam and Christianity in Kerala is about 45 per cent; for India as a whole, it is 16.5 per cent. The state is home to one of the oldest branches of Christianity. Further, the strategic location along the Arabian Sea facilitated interactions with traders worldwide, including Arabs, Europeans and others. The local rulers were generally tolerant of diverse religious practices.
Many economists in Kerala who noted the difference did not think there was much reason to celebrate. Some said that the record on healthcare and education hid a profound inequality from view. Others said the low and stagnant income pushed the state’s fiscals into bankruptcy, making the model unsustainable without active markets driving investment and income growth. By the 1990s, the model’s limitations became apparent as the state struggled with low economic growth and financial strains.
If the situation did not lead to a severe crisis, this was due to inward remittances. The state had a long history of labour migration, with significant numbers of people moving to the rest of India and the Persian Gulf states for work. This migration led to substantial remittances, which sustained private consumption, income and investment. By 2010, the excitement over the Kerala Model was dead, and incomes started forging ahead.
The Left changed their focus from land and educational reforms to private investment and decentralisation
The economists (above) who joined the developmental debate took Kerala’s income poverty for granted. They neither saw the income growth coming nor were prepared to explain it. Some Left-leaning economists attributed the resurgence in per-capita income to education and healthcare. But this is not persuasive. A surge in economic growth everywhere and at all times implies rising investment in productive capital, and basic education and healthcare would not deliver that.
The Indian economy in the 2000s saw robust investment and economic growth. But Kerala was not a major destination for mobile private capital. The forging ahead owed to more specific factors, some more peculiar and powerful than those driving India’s transformation.
Here, we must return to Kerala’s historical engagement with the world economy, its natural resources, its literate workforce and its distinctive political landscape. In different ways, all these reinforced private investment. Deep connections with the global economy were pivotal to the recent history of labour migration. While migration created a flow of remittances into consumption, another significant flow went into investment, especially in service sector enterprises in healthcare, education, hospitality and tourism. The state’s temperate semi-equatorial climate, mountainous topography and abundant water resources supported plantations and natural-resource extraction and processing industries for centuries. Some declined in the mid-20th century, but investment in these activities revived later.
The communist movement in Kerala began in the 1930s with the formation of the Congress Socialist Party, driven by peasant and labour movements and anticolonial struggles. The movement joined electoral politics after the formation of the state in 1956, and since then, Left-ruled governments have formed from time to time, almost always with coalition partners. The Leftist political movement in Kerala helped shape the state’s economic policies. In recent years, the Left also changed their focus from land and educational reforms to private investment and decentralisation. Capable local self-government institutions strengthened democratic governance.
In short ways, four forces of change – Kerala’s reintegration with the global economy, remittances from the Persian Gulf, strong welfare policies from a legacy of Leftist government, and private investment from individuals and businesses who shared the remittance flows – have combined to form the structure of Kerala’s miracle of human wellbeing with economic growth.
A round 1900, Kerala was a region composed of three political units: the princely states of Travancore and Cochin, and the British Indian district of Malabar. There were a few other smaller princely states as well. There was a broad similarity in the geography across the three units. India’s climatic-ecological map will show that all of Kerala is a semi-equatorial zone with exceptionally heavy monsoon rains, whereas most of India is arid or semi-arid tropical. The region has plentiful water and almost no history of famines, unlike the rest of India.
Geologically, too, Kerala was distinct. The mighty Western Ghats mountain range runs along its eastern borders throughout. Although the southwestern coast offered little scope for agriculture because good land occurred in a narrow strip between the sea and the mountains, the uplands produced goods like black pepper, cardamom, cloves, cinnamon and ginger, which had a ready demand in the world market. Plentiful coconut trees offered scope for coir rope manufacture. The climate was suitable for rubber and tea plantations. The sailing ship construction industry on the western coast obtained timber from the Malabar forests. In the present day, plywood is a major industry.
In the interwar period, poorer and deprived people circulated more
Around 1900, the authorities in all three regions helped foreign capital, which produced or traded in plantation crops like coffee, tea and pepper, and forest-based industries including timber, rayon, coir and rubber. Some of these products were traded globally. These businesses relied heavily on local partners and suppliers, which led to the accumulation of wealth in the hands of groups like the Syrian Christians.
Some of this wealth was invested in small-scale plantations and urban businesses, which encouraged the local migration of agricultural labourers. In the interwar period, poorer and deprived people circulated more. They sought work outside traditional channels like agricultural labour where they had been at the beck and call of upper castes or caste Hindus. At the same time, protestant missions, social reformers and Leftist political movements became active in ameliorating their conditions. These forces led to a significant focus on mass education. The princely states stepped into mass education late but with greater resources on average than a British Indian district. Their investment reinforced the great strides in health and education that made Kerala different.
N ine years after India gained independence, Malabar merged with Cochin and Travancore to form the Kerala state. At that time, the livelihoods in the region, like the rest of the country, were based on agriculture. However, a much larger proportion (half or more) of the domestic product was urban and non-agricultural, compared with India as a whole. Nearly 40 per cent of the workforce was employed in industry, trade, commerce and finance, compared with 20-35 per cent in the larger states in India.
One reason for this was the scarcity of farmlands. The state’s mountainous geography made good land extremely scarce. The exceptionally high population density in the areas of intensive paddy cultivation ensured a level of available land per head (0. 6 acres) that was a fraction of the Indian average (3 .1 acres ) around 1970, and low by any benchmark. Paddy yield was high in these areas. Still, with the low size of landholding, most farmers were families of small resources.
Urban businesses processing abundant natural resources were another story. Some of these businesses were small, non-mechanised factories processing commercial products like coir in Alappuzha (Alleppey) and cashew in Kollam (Quilon). Some areas, such as Aluva (Alwaye), had larger, mechanised factories producing textiles, fertilisers, aluminium, glass and rayon. The region also had tea estates in the hills, and rubber and spice plantations east of Kottayam. Kerala today is a leading region in Indian financial entrepreneurship. Businesses from the region established banks, deposit companies and companies supplying gold-backed loans, which have a presence throughout India. Several of these companies emerged in the interwar period to finance trading and the production of exportable crops.
Thrissur (Trichur) and Kottayam were service-based cities with a concentration of banks, colleges and wealthy churches. Most local businesses were small-scale, semi-rural and household enterprises. Foreign multinationals owned tea estates and export trading firms at the apex of the spectrum of firms. Nearly everything else – from banks to small plantations, trading firms, agencies, transport and most small-scale industries – were Indian-owned family businesses.
Before statehood began in 1956, a powerful communist movement had emerged
From this base, the two decades after 1956 saw a retreat of private investment from industry and agriculture. Partly because of adverse political pressure, the foreign firms left the businesses, and plantations changed ownership. A militant trade union movement rose in the coir- and cashew-processing industries, and most firms, being relatively small, could not withstand the pressure to raise wages. Some shifted operations across the border with Tamil Nadu, where the state did not protect trade unions and labour costs were cheaper. With the central government’s heavy repression of private financial firms and the retreat of private banks, the synergy between industry, banking and commerce was broken. Private capital retreated from industrial production and trading. Following the socialist trend present in India in the 1960s, Kerala state invested in government-owned industries, which were inefficiently managed and ran heavy losses, usually resulting in negative economic contributions.
Private investment in agriculture declined, too. The Left political movement, which was concentrated in agriculture, was again partly responsible. Before statehood began in 1956, a powerful communist movement had emerged. The movement’s leaders understood that inequality in this part of India was not based on class alone. The agricultural countryside was characterised by inequality between the landholders and landless workers, which was only partly based on landownership but also drew strength from oppression and deprivation of lower castes by upper castes.
A narrow strip of highly fertile rice-growing plains in the central part of the state was the original home of Leftist politics. From the 1940s, it was a political battleground. The Leftist political parties organised the poorest tenants and workers into unions. Class-based movements to get higher wages, better employment terms or more land merged with movements to achieve equal social status. The agricultural labourers came from the depressed castes so they were interested in both class and caste politics .
When in power for a second time (from 1967), the communists ruling in coalition delivered on a promise made long ago: radical land reform. The policy involved taking over private land above a ceiling, redistributing it to landless workers, and bringing them under trade unions. The policy was successful in the extent of land redistributed (compared with most states that followed a similar policy) and in sharply raising wages. However, it did have a damaging effect on investment.
Many employers migrated to the Persian Gulf, leaving their land unattended
From the 1970s, private investment withdrew from agriculture. The cultivation of tree crops held steady, if on a low key. But cultivation of seasonal field crops, especially paddy for which the lowlands and the river basins were especially suitable, fell throughout the 1980s. By 1990, traditional agriculture was reduced to an insignificant employer and earner, and for most people still engaged in it, the land provided no more than a subsidiary income. A relative retreat from traditional agriculture is not unique to Kerala, it happened all over India. But in Kerala, the fall was spectacular.
In this densely populated area, the average landholding was small. Most landholders were middle-class people and not particularly rich. The policy squeezed their resources. Investment and acreage cropped fell. Those who remained tied to land did so because they had nowhere to go or worked the land mainly with family labour. The first Green Revolution unfolded in the rest of India, including Tamil Nadu, and had little impact on the state. Many employers migrated to the Persian Gulf in the late-1970s or ’80s, leaving their homesteads and the land unattended. What made all this anomalous was the high unemployment rate in the countryside, possibly the highest in the country. How were high wages and the retreat of a significant livelihood possible in this condition?
The answer is Gulf remittance. Hundreds of thousands of people migrated to the Persian Gulf states like Saudi Arabia, Kuwait, the United Arab Emirates, Bahrain and Qatar to work in construction, retail and services, sectors that saw a massive investment boom following the two oil shocks of 1973 and 1979. As they did, the money from the Gulf flowed into construction, retail trade, transport, cinema halls, restaurants and shops in Kerala. An emerging service sector labour market absorbed the effort of those who had been made redundant in agriculture or did not want to work there anymore.
What drove emigration to the Gulf? And why did Kerala lead the emigration of Indians to the Gulf? One answer is that the region had for centuries deeper ties with West Asia than any other part of India. Also, high unemployment pushed skilled individuals to seek work outside the state. Kerala, for at least three decades (1975-2005), supplied a significant share of the workers who moved to these labour markets. The demand for skilled workers increased as the Gulf economies diversified from oil-based jobs to finance and business services. While offering jobs in the millions, the migration also had a series of broad effects back home on occupational diversification, skill accumulation, changing gender roles, consumption, economic and social mobility, and demographic transitions.
I n the 1990s, the Indian economy liberalised, reducing protectionist tariffs and restrictions on foreign and domestic private investment. In the following decades, increased private investment led to generally elevated economic growth rates. At the same time, the political culture shifted away from emphasis on socialist ideas, becoming more market-friendly than before. Kerala was not untouched by these tendencies, but its specificities – natural resource abundance, Leftist legacy, migration history – joined the pan-Indian trend distinctly. There were three prominent elements in the story.
First, a demographic transition completed by 1990, when population growth decreased substantially. The fall in population growth rate was not unique to the state but aligned with broader Indian trends. However, the levels differed. Of all states in India, Kerala was ageing much faster than the rest and from earlier times.
Second, politics changed. Again, the legacy of Left rule was an important factor behind the shift. A communist alliance won the first state assembly elections in 1957, lost in 1960, returned to power and ruled the state in 1967-70 (with breaks), 1970-77, 1978-79, 1980-82, 1987-91, 1996-2001, 2006-11, and since 2016. The composition of the Left coalition changed multiple times, never consisting only of ideologically Left parties. It included, for example, the Muslim League and some Christian factions allied with the communists. However, until 1964, the main constituent of the coalition was the Communist Party of India (CPI), called CPI (Marxist), or CPI (M), after 1964. In no other state in India, except West Bengal (and later Tripura), did the CPI/CPI (M) command a popular support base large enough to win elections.
The Left turned friendly towards private capital and shed the rhetoric of class struggle
The Left Democratic Front, which had ruled Kerala in different years, returned to power in 2016 and has been in power since then. In the 2000s, the Leftists quietly reinvented themselves. They needed to because the older agenda was almost dead. In elections in the 1960s and ’70s, agricultural labourers in this land-poor state formed the main support base for communist victories based on the promise of land reforms. Caste-equality social reform movements coalesced around the Leftist movement. After the Leftists delivered land reforms, there was not much of an agenda.
From 2000, the Left turned friendly towards private capital and shed the rhetoric of class struggle. In practical terms, the state retreated from regulating private capital and strengthening trade unions, and focused on infrastructure investment to strengthen small businesses. The reinvention was a success and delivered election victories. As the private sector took charge of investment in education and healthcare, the state could afford to focus on decentralised governance, corruption-free administration, improved public services and urban infrastructure. The class-based politics of the 1960s and ’70s died. With private investment rising, the state had more capacity to fund welfare schemes and public administration. Tourism promotion is an excellent example of a new form of synergy: the state builds roads, private capital builds hotels, and lakes and mountains supply the landscape.
Third, investment in Kerala revived. Over the past three decades, the private sector has increasingly driven education and healthcare. Since 1990, many new types of small-scale businesses have flourished in the state. There is no single story of where the money came from and what these enterprises add to employment potential. We know much of it happened on the back of natural-resource processing. In all fields, value was added by accessing niche export markets, using new technologies, and forming many micro, medium and small enterprises. The state has one of the highest concentrations of startups. Natural resource extraction does not mean any more plantations packaging harvested spices but the extraction of nutraceuticals. Jewellery manufacture involves invention and experimentation with designs. Rubber products diversified from automotive tyres to surgical accessories.
Although foreign investment inflow, which supported business development in the princely areas, was revived via the Gulf route, most of the business development is concentrated in non-corporate family firms. Few raise significant equity capital or are publicly held. Most service sector enterprises in tourism, trade, transport, banking and real estate are relatively small. Family business remains a strong organisational model. Little research exists on the externalities that these businesses generate. The one large exception to this rule is investment in IT clusters near the big cities.
L et us start with a restatement of the main points of the story. Not long ago, Kerala was celebrated for its exceptional human development indices in education and healthcare, with many scholars attributing this to an enlightened political ideology and communist influence. These advances also resulted from factors like the princely states’ higher fiscal capacity, favourable environmental conditions, and a globally connected capitalism. During the 1970s and ’80s, government interventions weakened market activity and growth, making human development look even more striking than otherwise. Since previous commitments to social infrastructure were maintained, the state was heading toward a fiscal crisis.
In the 2000s, an economic revival came through mass migration and remittances, initially supporting consumption and construction. At the same time, a wealthier and technically skilled diaspora invested in the state, in services and manufacturing. New sectors like tourism, hotels, spice extracts, ayurvedic products, rubber products and information technology drove this revival. Remittances also flowed into new forms of consumption. The urban landscape transformed, with towns developing shopping malls, restaurants and modern businesses. While earlier regimes discouraged private investment, now there is a symbiosis between the private sector and the state, as market activity supports public welfare commitments.
The New Left, unlike the Old Left, is open to private capital and acknowledges the importance of the market, including the global market. Without compromising welfare expenditure, the state has expanded the hitherto neglected infrastructure projects, crowding in private investments. This is the second turnaround in the development trajectories of the state. The first turnaround happened during the early 1980s fuelled by remittance money. The second turnaround happened in the 2010s, when social growth, always Kerala’s strength, joined unprecedented levels of capital expenditure. If both the Left and non-Left political parties could take credit for the first turnaround, the credit for the second one should rest with the New Left.
Recent climate change and overdevelopment have increased disaster risks
Looking forward, the pathway of recent economic change has both strengths and challenges. The strengths include the generally high quality of life in small towns, improved youth aspirations often marked by an increased flow to foreign universities, better worker safety, the ability to attract skilled and unskilled migrants, unique natural-resource advantages and a degree of sociability in relations between castes and religions. The challenges are poor higher education quality, environmental threats from new forms of tourism infrastructure and climate change, a rapidly ageing population, and the possibility of a fiscal crisis.
Some of these challenges are enormous, and are already straining the budget and state capacity. Land reforms brought some equality, but the absence of follow-up actions prevented productivity improvements. Kerala produces less than 15 per cent of its food requirements, and relies heavily on central supplies and neighbouring states. To respond to this problem, the government has strengthened its public distribution system. That, along with the care of the elderly and scaling up of public services, particularly education and health, will place enormous burdens on the state’s public finances in the near future.
Historically, the state’s unique climate with abundant rainfall provided natural advantages, supporting high life expectancy and diverse agricultural opportunities. However, recent climate change and overdevelopment have increased disaster risks. The environmental transformation has been primarily driven by private construction, especially Gulf-funded developments in dwellings, hotels and service sectors. Land has become the single most speculative asset of the real-estate lobbyists. Extensive economic activities in ecologically sensitive regions, possibly accompanying tourism development with its tagline of ‘God’s own country’, allegedly led to landslides, soil erosion and environmental vulnerabilities. In recent years, an accent on ‘responsible tourism’ has tried to reduce the potential risks.
There is more. Human-wildlife conflicts and soil erosion have increased, and declining rainfall poses significant challenges. The devastating floods in 2018 and the near-disaster in 2019 highlighted the consequences of excessive construction and poor environmental management. The state now has one of India’s highest levels of consumption inequality. The quality of higher and technical education remains poor, contributing to educated unemployment.
The state’s future success will depend on balancing economic growth with environmental sustainability, improving the quality of education, improving the employability of graduates, and social equity. It is a complicated task precisely because so much of the recent growth owes to exploiting the environment. There is a real prospect of worsening inequality along caste, class, gender and age lines if the current pattern of growth slows. On the other hand, recent advancements in the digital and knowledge economy, combined with sustainable infrastructure, open fresh spaces for egalitarian development. Still, the future is hard to predict because the regional economy is deeply dependent on integration with the world economy and the ever-changing ideological alliances."
Why does the world have such an intense stake in Israel/Palestine? | Aeon Essays,,https://aeon.co/essays/why-does-the-world-have-such-an-intense-stake-in-israel-palestine,"In 2013, the German geographer Benjamin Hennig produced a series of maps depicting the size of countries in terms of the number of articles about them in The Guardian in any particular year. The maps depicted a swollen, obese United States, an emaciated Latin America, massively enlarged European countries, and shockingly small African and Asian ones. The Arab Middle East, however, loomed large and, despite the uprisings and civil wars throughout the region, Israel was among the largest.
Courtesy Benjamin Hennig/ Worldmapper
Hennig has not produced more recent iterations of this map, but it’s safe to say that Israel would feature at least as prominently, if not more so, if the map were released today. Why does Israel attract so much attention, and, even more important, why does it elicit such passion? Even now, when Israeli forces have killed scores of thousands of people in Gaza and thousands in Lebanon, the answer is not obvious. At no time in history have Americans, especially those on elite university campuses, been so exercised about a conflict in which American youth were not being sent to die. The last major wave of campus protests – against South Africa’s apartheid regime during the 1970s and ’80s – pales in comparison with the current moment.
The question I am asking – why Israel? – is valid and important, but only if approached with honesty and an open mind. All too often this question is employed by Israel’s supporters as a rhetorical tool known as tu quoque , or, in simple English, ‘what about?’ What about millions of deaths in Ethiopia, Sudan and the Democratic Republic of the Congo, one might ask. Or mass killing and destruction of cities in Syria? Or widespread Russian atrocities in Ukraine? What about mass incarcerations in China’s Xinjiang Province, and the ongoing Sinicisation of Tibet? Why didn’t students disrupt classes, occupy and vandalise buildings, and set up encampments when US forces killed hundreds of thousands of civilians during the Persian Gulf War?
These are fair questions but, when employed as a form of advocacy, ‘what about’ is in fact a hostile riposte, a means of suppressing criticism. When raised in connection with Israel, it commonly invokes antisemitism as the ultimate explanation for global ire against the Jewish state.
Antisemitism has always played a role in accounting for animus towards Israel. But it is not the only factor. Ever since Israel’s creation and the Palestinian Nakba in 1948, Palestine has served as an affective magnet that attracts and generates emotions such as religious fervour, anticolonial anger, and Holocaust guilt and shame. Other global conflicts evoke high levels of emotional intensity, but mainly within the region where they are waged and among the ethnonational communities that wage them. Russia is obsessed with Ukraine, the People’s Republic of China with Taiwan, and ethnic strife in sub-Saharan Africa has killed millions, but few Americans who are not of Russian, Chinese or African origin are as emotionally invested in these struggles as they are in Israel/Palestine.
The prominence of Israel/Palestine in public opinion cannot be explained only in terms of the number of deaths, nor in the extent of Gaza’s or Lebanon’s ruination, nor in the Palestinians’ status as a dispossessed and oppressed people. (There are others, such as Kurds, Masalit, Rohingya, Sahrawis, Uyghurs, and Yazidis.) All national conflicts generate waves of emotion, but Israel/Palestine generates tsunamis. Or, to switch metaphors, Israel/Palestine is a superconductor of emotional current. There’s nothing else like it.
T he source of Palestine’s uniqueness lies in the past. During the 1800s, the ancient religious significance of Palestine for Christians, Jews and Muslims became politicised. First, Palestine fell into Europe’s colonial orbit, and then it became both the object of the Zionist movement’s yearnings for a Jewish homeland and a site of early Arab nationalism. Palestine’s significance as a subject and object of political emotion expanded during the interwar period, when Britain held a League of Nations mandate to govern Palestine. Zionists found British rule to be at first exhilarating but then infuriating, as early expectations of Jewish sovereignty gave way to accusations of betrayal when Britain attempted to balance its commitments to Zionism with its geopolitical interests in the Arab world. The British Mandate, in turn, exasperated and enraged Arabs and Muslims, whose strivings for independence made progress throughout Asia but were blocked in Palestine. These politically framed emotions enhanced, and were in turn amplified by, religious sentiment. In 1929, rumours that Jews planned to purchase the Temple Mount (the Al-Aqsa mosque compound) in the Old City of Jerusalem provoked calls among Palestinians that ‘Al-Aqsa is in danger’ – a phrase that has remained in use to this day.
Mohammed Amin al-Husseini, with attendants, leaving the offices of the Palestine Royal Commission in 1937. Courtesy Library of Congress
In 1931, Mohammed Amin al-Husseini – the mufti of Jerusalem – and the Indian Muslim leader Shawkat Ali convened a World Islamic Conference in Jerusalem that strengthened perceptions of Palestine as not only a pan-Arab concern but also a pan-Islamic one. After the Second World War, just as the world’s Jews, reeling from the genocide of European Jewry, united around the call for a Jewish state, Palestine grew into a symbol of anticolonial aspirations. In the Western world, which perpetrated the Holocaust, abetted it or stood by while it took place, the fate of the Jews was a sensitive and fraught subject. Despite professions of sympathy for the survivors, few countries in Europe or the Americas wanted to repatriate them or accept them en masse as refugees. For many countries, shunting the survivors to Palestine was an attractive solution – an option advocated by the vast majority of Jews and opposed just as strenuously by the Arabs of Palestine and throughout the Muslim and Arab worlds. During the immediate postwar years, the world was rife with crises – civil war in China and Greece, anticolonial uprisings in Indonesia and Indochina, and the division of Europe between East and West. Even then, what was widely known as the ‘Palestine Question’ had the greatest global reach.
Solidarity with a colonised people helped produce strong contemporary support for the Palestinian cause
In 1947 and ’48 – the years of the United Nations’ debates about partitioning Palestine, the 1948 war, Israel’s establishment, and the Palestinian Nakba – global discourse about Palestine expanded still further and grew even more intense than during the interwar era. The United States’ Middle East policies in 1948, and US responses to events in Palestine, have received a great deal of scholarly attention, but ‘the Palestine Question’ was even more prominent and controversial in Asia from the Arab Middle East to the Subcontinent, in western Europe, and in Latin America. It commanded global attention.
The Arab Muslim heartland was both an observer of and a participant in the struggle for Palestine. Many Arab Muslims responded to the international community’s endorsement of partition with incredulity, indignation and profound fear that a Jewish state would dominate the region. On the Subcontinent, solidarity with a colonised people (and, for India’s Muslims, co-religionists) helped produce strong contemporary support for the Palestinian cause. That support was also more measured and less inclined to draw upon antisemitic motifs than in the Middle East. In Europe, the Holocaust generated sympathy for Zionism rooted in compassion, shame and guilt , but antisemitism and Cold War political calculation caused considerable scepticism about the establishment of Israel. In Latin America, the presence of substantial Jewish and Arab communities, the legacy of the continent’s own anticolonial struggles against Spain and Portugal, and the influence of Roman Catholicism all fostered competing and contradictory sympathies for the antagonists in Palestine. The affective responses on these three continents were evident in force from the beginning of the UN debates about partition and formed a complex tapestry, much of which remains recognisable to this day.
W hen Britain handed the matter of Palestine to the UN in February 1947, the representatives of the states of the Arab League spoke of Palestine as an integral part of the Arab world. They demanded that it be established as an Arab state. These Arab leaders were incredulous and indignant that the UN, an organisation dominated by the colonial powers, would assume the authority to intervene in Palestine’s natural path towards independence. The Arab Higher Committee (AHC), the umbrella group of Palestinian political parties, boycotted the UN’s enquiries.
Nonetheless, in July 1947, members of the UN Special Committee on Palestine (UNSCOP) met in Geneva with the Palestinian leader Musa Alami and in Beirut with political and diplomatic figures from Syria, Lebanon, Egypt, Saudi Arabia and Iraq. All of them demanded statehood for what they called the Arabs of Palestine, whom they also referred to as Palestinian Arabs or, more rarely, Palestinians. They justified the creation of a Palestinian state via appeals to universal human rights, the principle of popular sovereignty, and the Arabs’ historic presence in Palestine. In turn, they depicted Zionism as the handmaid of British and, more recently, US colonialism. (‘Colonialism’ as understood in this sense referred to Great Power desire for political and economic control over the Middle East. The displacement of Palestinians that lies at the heart of the currently popular notion of settler colonialism had barely begun.) Following a long line of anti-Zionists, Jewish and non-Jewish alike, the Arab diplomats also denied that Jews constituted a nation and so they had no right to self-determination. The Arab diplomats insisted that Jews were a religious community, whose freedom of religious practice would be guaranteed in a Palestinian Arab state.
Whether one agreed with these arguments or not, they had historical and ethical foundations. The resentment and anger underlying these claims, however, led to an embrace of overtly antisemitic views. Faris Bey el-Khouri, an eminent Syrian politician and diplomat, told the UN General Assembly that Jews had no claim on Palestine because 90 per cent of eastern European Jews were descended from Slavs, Germans, Franks and Khazars, without a connection to the ancient Middle East. (The ‘Khazar hypothesis’, attributing the origins of Ashkenazic Jewry to a dubious mass conversion of the medieval Khazar kingdom in the Caucasus, was commonly invoked in Arab critiques of Zionism at this time, and remains popular.) El-Khouri added that ‘the choice of Palestine to satisfy Zionist aspirations was based not on humanitarian sympathy but on the intention of the Zionists in the United States to launch an economic invasion of the whole eastern world and to achieve that end by creating a bridgehead in Palestine to be the headquarters of their activities.’ Such a state would rely on the US and on the Zionists’ ‘coreligionists’ in the US, who had thus far flooded Palestine’s Jewish community with dollars to give it what el-Khouri called the ‘symptoms’ of prosperity.
The Arab Higher Committee accused Zionist forces of having crucified Arab prisoners
In late 1947, when fighting between Jews and Arabs in Palestine broke out, Arab public rhetoric see-sawed between realistic and fantastic assessments of the Zionists’ war aims and methods. In January 1948, the Austrian ambassador in Ankara spoke of moderate Arabs who were willing to accept partition but feared – accurately, as it turned out – that Jews would infiltrate territory allocated by the UN to the Arab state. In May, however, the Jordanian ambassador to Ankara said that Palestine’s Jews planned to invade the entire Levant and that Arab Legion troops had found maps of Palestine and Iraq proving Zionist intentions to establish a Jewish empire.
In 1948, the AHC submitted pamphlets about the partition to the UN that toggled between accuracy and baleful myth. The pamphlets accused Zionists of barbaric and intentional cruelty towards the Arabs of Palestine. Zionist actions, such as the notorious massacre in April 1948 of some 100 Arab civilians in the village of Deir Yassin, on the western outskirts of Jerusalem, provided some justification for such an accusation. The Egyptian embassy in Vienna, for example, responded to the massacre by contending that the ‘true purpose of Zionism is the complete annihilation of the Arab element in these territories.’ The AHC pamphlets, however, went even further, claiming that Jews were eternally and preternaturally violent. Their ‘cold-blooded and well-calculated Zionist plan for the massacre of populations’ was ‘a renewal of the technique adopted by them when they first entered Palestine about the year 1400 BC.’ Zionist savagery, according to the AHC, reflected their longstanding status as ‘international intriguers’, controllers of ‘the channels of propaganda in important countries’. Jewish global power, embodied in the Jewish Agency, had enabled the construction of a ‘totalitarian’, ‘fanatic’ and ‘terrorist’ community in Palestine, employing means of violence ‘equalled only by the Nazis’. The AHC accused Zionist forces of having crucified Arab prisoners.
The Ad Hoc Committee on the Palestinian Question of the United Nations General Assembly in Lake Success, New York, November 1947. Courtesy UN Photo
This language reflects not only hatred but also deep-seated fear – fear stoked by the UN’s approval of the partition of Palestine in November 1947. On the day after the passing of the partition resolution, the Syrian education minister, Munir al-Ajlani, spoke of the ‘liberation of Palestine and the conservation of Arabism in spite of the great world conspiracy against us.’ Al-Ajlani feared an impending ‘catastrophe’ ( nakba ) for Syria and the entire Arab world. Shortly thereafter, Hassan al-Banna, founder of the Muslim Brotherhood, described Palestine as the Muslim world’s ‘first line of defence’ of Islam, and in 1949 he described Israel’s creation as the product of a Jewish conspiracy against the Arab world and Islam alike.
Al-Ajlani’s application of the term nakba to the Arab world, rather than the Palestinians in particular, is telling. During and immediately after the Second World War, what became known in later decades as the onset of the Palestinian Nakba – the dispossession of their homes and homeland – had a more popular meaning. In his booklet The Meaning of Catastrophe (1948), the Syrian intellectual Constantin Zureik used the term nakba to depict the disunity, backwardness and weakness of the Arab world, as opposed to the alleged towering strength and fortitude of its Zionist enemy. A zealous proponent of political pan-Arabism, Zureik saw Israel as the greatest threat to the creation of a powerful, vibrant and healthy united Arab polity. He fretted that, if under the current circumstances such a state were created:
Even more worrying, Zureik said in a radio broadcast on 31 May 1948:
In Arab responses to the war, the line between manipulation and sincere belief was blurry. Shortly after the Deir Yassin massacre, Hussein Fakhri al-Khalidi, secretary of the AHC, directed the Arabic service of the Palestine Broadcasting Corporation to depict the atrocities in the most gruesome terms, including graphic allegations of rape. Al-Khalidi hoped that lurid narratives about Jewish atrocities would bolster the Palestinians’ determination to fight against the Zionist enemy and dissuade them from fleeing the country. (The opposite was the case: both Arab and Zionist propaganda intensified Palestinian fears for their lives and accelerated Palestinian flight.) Even if Arabs at times intentionally exaggerated the Jews’ malevolence, the malevolence was taken for granted. In 1947 and ’48, Arab fear and hatred of Zionists in Palestine was entangled with fear and hatred of Jews everywhere and throughout history.
I n South Asia, Muslims had a different approach. They had a long history of sympathy for the Palestinian Arab cause, but in 1947 and ’48, the partition of India and its aftermath overwhelmed their concerns for Palestine. (Between August 1947 and January 1948, at least 10 million Hindus and Muslims fled for India and Pakistan respectively, and a million died in intercommunal violence.) Perhaps for these reasons, or maybe because of physical and cultural distance from the Arab Muslim heartland, the rhetoric of Indian Muslims was relatively free from antisemitism. Muhammad Zafarullah Khan, who after partition became Pakistan’s first foreign minister, was a fierce critic of Zionism, as was the Muslim League, the political party that, since the early 20th century, had championed the interests of India’s Muslims. In his memoirs, however, Khan chided his Arab counterparts in the UN for resorting to antisemitism, which he saw as unnecessary to their cause. Khan thought that Arab Muslims weakened ‘solid arguments’ with speech that was largely ‘emotional’:
In 1940, Khan proposed a federal scheme for Palestine with neither partition nor population transfer, claiming that
Muhammad Zafarullah Khan in 1939. Courtesy the NPG London
In 1947, Khan repeated these arguments at the UN, while the Muslim League fiercely objected to any justification for the partition of Palestine via references to the ongoing partition of India. Indian Muslims who accepted the creation of Pakistan were parties to a partition with at least some measure of mutual agreement, whereas the partition of Palestine was being forced on Arab Muslims. Indian Muslim leaders pointed out that the partition of India was based on demographic majorities within provinces, whereas in Palestine Jews formed a majority only in the district of Tel Aviv and its environs.
The Indian press strongly supported an independent Arab Palestine
Mohammad Abdur Rahman, a distinguished jurist in Lahore and the Indian delegate to UNSCOP, opposed the partition of Palestine in principle, and as the result of personal experience. While he was working for the committee, his country was being torn apart by partition. Abdur Rahman’s family was forced to flee its home and their possessions in Delhi, barely making it to Lahore. Under pressure from the prime minister Jawaharlal Nehru, Abdur Rahman proposed to UNSCOP a Jewish-Arab federation that resembled Khan’s idea, but he preferred a unitary Palestinian state with guarantees of religious freedom for Jews. Dismayed by the partition vote, Abdur Rahman predicted that the partition of Palestine would turn out badly, writing that ‘there is no other land in the whole world which arouses so much religious sentiment and feeling.’ A year after the vote, Abdur Rahman wrote to another committee member, the Canadian jurist Ivan Rand, who had forcefully supported partition, ‘please do not call me presumptuous as I say so with all humility, but I still feel that federation was and probably would be the best solution of Palestine as it would have been of [my] country also.’
The predominantly Hindu Indian National Congress shared the Indian Muslim opposition to the partition of Palestine, and for similar reasons. In October 1947, Vijaya Lakshmi Pandit, Nehru’s sister and the Indian ambassador to the UN, cabled her brother that ‘the Arab demand is based on the same principle of right of self-determination and freedom, which [the Indian National] Congress … has always fought for.’ During debates, the Indian press strongly supported an independent Arab Palestine. Referring to the British tradition of using religion as a divide-and-rule strategy in India as well as recent ‘tragic conditions’ in the country, The Leader , The Bombay Chronicle and the National Herald , all of which had ties to the Indian National Congress, urged that, when considering Palestine’s future, UNSCOP should not take religious differences into account. Throughout 1947 and the first half of 1948, The Times of India , whose views were also close to the government’s, condemned partition, championed the proposal for a Palestinian state that Nehru had devised, and characterised both Jews and Arabs as ‘inspired by and victims of narrow, essentially selfish and sectional interests.’ Yet in September 1948, the newspaper acknowledged that ‘actual and effective partition had been achieved by Jewish arms.’ For entirely different reasons, such pragmatism and non-partisanship was harder to come by in the Arab world, or, for that matter, in Europe.
E uropeans who had committed, abetted or benefitted from the Nazi genocide of the Jews had a vested interest in endorsing Zionism as a means of expiating guilt. Or, to the extent that they were still antisemites, Europeans saw Zionism as a means of getting the surviving Jews out of Europe. Social-democratic and Soviet bloc governments with longstanding ties to the Zionist labour movement, which was the dominant political force in Palestine’s Jewish community, saw in a socialist Jewish state a kindred spirit and a blow to British imperialism in the Middle East.
In France, although the foreign and intelligence services cautioned against support for Zionism lest it harm relations with the Arab world, the government and public opinion favoured Israel’s creation. (The only exception was the Catholic Church, which at that time was not politically influential.) An intricate combination of sympathy for Jewish victims of the Holocaust, solidarity with Jews who had played a prominent role in the French resistance, and unacknowledged guilt for wartime indifference towards Jews (or outright collaboration with the Germans) led to frequent comparisons between the Jewish fighters in Palestine and the partisans in the French countryside, the maquis. When, in late 1947, three Jewish men were tried for arms smuggling on behalf of the Zionist paramilitary organisation the Irgun, the coverage in the Leftist and Gaullist press was sympathetic to the defendants. So was the judge, who said as much when he acquitted one of the men and handed out light sentences to the other two. The prestigious newspaper Le Monde wrote of Israeli smuggling of aircraft to Palestine with thinly concealed excitement and admiration of a ‘brilliant and massive improvisation’.
Joseph Kessel was France’s most prominent journalist of the time, a Jew of Russo-Lithuanian origin who attained literary fame as a novelist (he was the author of Belle de Jour ), and in 1962 was elected to the Académie française. In his wartime dispatches from Palestine, which were published in major French newspapers, Kessel extolled the Zionists in language that reflected common Jewish sensibilities of the time: eg, intense admiration for the Israelis’ tanned bodies, calm demeanour, dedication and sensitivity. He also connected with his French audience via specific references to ‘the traditions of the maquis, the partisan fighters, and colonial engagements’. The Jewish state, he wrote, was ‘a vast maquis’, and its actions in war were ‘maquisard expeditions’. Like the maquisards , they had no uniforms, possessed only light arms, and kept their names and faces secret. Kessel described the ‘extraordinary Jewish people’ he encountered as constituting ‘a sort of Foreign Legion that had gathered on the soil of its ancestors.’ Kessel made frequent and hostile references to Britain, with whom the French had a long history of rivalry despite their wartime alliance, as the Jews’ true enemy.
The reportage had a Christian perspective, concerned with whether Jews and Arabs alike were threats to holy sites
Kessel wrote as a Zionist Jew, but his work was published in France’s major newspapers and aimed at a mostly non-Jewish readership. His writing was not an apologetics for Zionism in the face of Gentile criticism so much as a triumphant account of the establishment in Palestine of a maquis-state, composed of Jews from the Middle East as well as Europe, both dark- and pale-complexioned, and endowed with a mission civilisatrice towards what he depicted as the backward Arabs of Palestine.
French philo-Zionism had many sources, ranging from guilt to compassion, and saw no contradiction in conceiving of Zionism as both settler-colonial, on the one hand, and rooted in collective resistance and liberation, on the other. Ambivalence towards Zionism alongside unconcealed antisemitism nonetheless survived in Europe, including in the country that perpetrated the Holocaust, Germany. Until 1949, Germany was under Allied occupation and had no independent government, so it is difficult to access the real-time views of senior diplomats and political leaders on the Palestine Question. But despite Allied control and censorship, there was a relatively free press, whose representations of Palestine are surprising. Between the partition vote at the end of November 1947 and the first truce in mid-June the following year, Palestine was on the front page of the major German newspapers on a regular basis – and, for some periods, for weeks at a time. In 1948, the popular newsmagazine Der Spiegel covered Israel in 29 issues, as opposed to 20 for China, 12 for Greece, seven for India, and five for Indonesia. Most of the newspapers’ stories came from wire services and contained no analysis or commentary, but their prominence is revealing. During these immediate postwar years, German newspapers were slender, at most eight pages, and they carried a page or two of foreign news. News from trouble spots like Palestine, China or India competed for precious space with the brewing Cold War in Europe, the Marshall Plan, the currency reform in the western zones of occupied Germany, and, from June 1948, the Berlin blockade. Nonetheless, the war in Palestine, even if reported in the most neutral manner, was of paramount concern. Much of the reportage concerned Jerusalem and reflected a Christian perspective, concerned with whether Jews and Arabs alike were threats to holy sites. The little commentary tended to bemoan the war in general terms and expressed little sympathy for the Zionist cause.
The weekly newspaper Die Zeit , based in Hamburg, took a more hostile position towards Israel. The future doyenne of German journalism, Marion Dönhoff, began her career as the newspaper’s Middle East expert. Hailing from an old Prussian aristocratic family, she opposed the Nazis but was no republican and not free from antisemitism. For Dönhoff, Israel was a ‘foreign body’ that stood in the way of a territorial contiguous Arab realm with capacity for ‘an independent creative Arab politics’ nourished by Saudi oil revenues. Israel, Dönhoff, wrote, could not possibly solve the ‘Jewish problem’ as it was too small, and Arabs would never accept it unless it prohibited additional Jewish migration. Dönhoff described Israel simultaneously as a staging ground for Soviet communism and a continuation of Nazism in the form of a ‘noxious’, ‘extreme’ and chauvinistic nationalism.
On the other hand, the German weekly Der Spiegel covered the Zionist leader David Ben-Gurion as a hero. Der Spiegel also condemned Amin al-Husseini as a Nazi ideologue and collaborator, and it criticised the US and the UN for retreating from support for partition in the late winter of 1948. Der Spiegel featured several cover-page photographs of Israeli fighters, and it sympathetically documented the travails of displaced persons striving to get to Israel. Antisemitism, however, showed up in the magazine as well. In a story about female passengers on the SS Exodus who had, after a gruelling and protracted ordeal, finally arrived in the Jewish state, the magazine wrote that ‘many of the …women today wear gold rings, are manicured, made-up, and banknotes pile up in their husband’s briefcases.’ Der Spiegel described Tel Aviv as containing the ‘concentrated financial power of world Jewry’. Despite their interest in the travails of displaced Jews from Europe, Der Spiegel never mentioned why they were displaced. The magazine made no connection between the Nazi genocide and the displaced persons, it juxtaposed Christianity as a religion of peace versus the allegedly fanatical, hateful and enraged behaviour of Jews such as the refugees on the SS Exodus. Like Die Zeit, Der Spiegel took pains to condemn the Irgun and Stern Gang as ‘Jewish fascists of the first order’.
In 1948, during the war over partition, Germany had neither a government nor a foreign policy, but Austria did. Diplomatic correspondence from Austria’s ambassadors in the Middle East to its foreign ministry in Vienna show antisemitism shaping Austrian views. Israel, wrote the Austrian ambassador in Ankara in 1948, is divided between moderates and communists, or to be more specific, ‘western, democratically oriented Jews’ and ‘outcast eastern Jews who know no limits’. Notions of Jewish financial and political power in the US were omnipresent in the correspondence. So was the belief that Jews in Palestine enjoyed considerable wealth, some of it earned by German immigrants who moved to Palestine in the 1930s and manufactured knockoffs of patented German goods. Again, as in the German newspapers, the circumstances behind these Jews’ sudden departure from Europe passed unmentioned. In December 1948, a letter from the Austrian ambassador to the Holy See referred to ‘atrocities during and after the Second World War’. It did not mention the atrocities’ nature, scope, victims or perpetrators.
I n Latin America, sympathy for Jewish victims of the Holocaust was largely free from the guilt felt by its European perpetrators and bystanders. Physical distance from both the killing fields of Europe and the battlefields of Palestine made for a more abstract approach to the Palestine Question. Enrique Fabregat of Uruguay and Jorge García-Granados of Guatemala, for example, were liberal, pro-Zionist members of UNSCOP. Fabregat was a Christian humanitarian and hostile to fascism; both of these commitments originated in the struggle against Francoist Spain. García-Granados shared these political views as well as anti-imperial animosity towards Britain, the master of neighbouring British Honduras (later, Belize). Zionist lobbying groups promoted Fabregat’s and García-Granados’s affective blend of attachment, admiration and solidarity against a common enemy throughout Latin America. The Jewish Agency, the Zionist proto-governmental body based in London and Jerusalem, established these groups following the Second World War in virtually every Latin American state. They were called ‘Pro-Palestine committees’, a reminder that before 1948 the word ‘Palestine’ was employed by both Zionists and Palestinian Arabs to refer to the same territory.
Across Latin America, the Pro-Palestine Committees, which had hundreds of members in larger Latin American states and dozens in smaller ones, mobilised the elite of their countries’ societies – politicians, judges, professors, attorneys – to lobby state governments to support the Zionist cause. There were similar organisations in the US, but in Latin America there was one major difference – the presence of significant communities of Arab immigrants, often Christians from Syria, Lebanon and Palestine. The Arab Christians in Latin America were often behind the Palestinian cause. In Argentina, the Zionist Pro-Palestine Committee’s nemeses included the Prensa , described as:
In the US, this kind of philanthropic organising and emotional messaging was far more widespread among Jewish organisations than Arab ones. In Latin America, Jews and Arabs fought on a more level playing field.
Then, as now in the US, people projected domestic political scenarios onto Middle Eastern terrain
As a result of these duelling pressures from Arab and Jewish groups alike, a half dozen Latin American governments abstained in the UN General Assembly vote on 29 November 1947 on the partition of Palestine. In doing so, Argentina and Mexico and other Latin American countries signalled their independence from the US, which had urged them to support partition. But it is also important not to underestimate the domestic political forces at work. Uruguay and Guatemala expressed support for partition before the US did. The Chilean president, Gabriel Gonzáles Videla, was pro-Zionist and was involved in his country’s Pro-Palestine Committee. Nonetheless, lobbying from the Chilean Arab community led Videla’s government to abstain from supporting partition. In Argentina, president Juan Perón’s decision to abstain on the partition vote was an acknowledgment of Argentinian Arab support for his regime. Despite US pressure, Cuba voted against partition. Ernesto Dihigo, who gave an impassioned speech at the UN supporting the Palestinian Arab right to self-determination, cast Cuba’s negative vote. The decision to vote against the resolution, however, came directly from the Cuban president, Ramón Grau San Martín, due to lobbying by Cuban Arabs and pique at the US over sugar trade policy.
The competition between Jewish and Arab communities in mid 20th-century Latin America reminds us of our own day, where Muslim and Arab voices in the US are growing more prominent. Then, as now in the US, people projected domestic political scenarios onto Middle Eastern terrain. As Mexico’s foreign minister in the late 1940s, Jaime Torres Bodet, wrote in his memoirs:
In November 1947, Mexico’s newspaper of record, Excélsior , also related the Palestinian cause to Mexico’s loss of territory to the US a century earlier:
Mexico itself was a settler-colonial state that had gained independence from Spain less than three decades before its defeat by the US. In 1948, as today, the interpretation of events in Palestine through the subject position of the observer was common.
T he partition of Palestine and the 1948 war riveted the world. The emotional matrix framing Israel/Palestine at the time of the 1948 war – anticolonial anger and frustration, as well as a mixture of shame, guilt and compassion in response to the Holocaust – has proven highly durable. The antisemitism exemplified by German and Austrian coldness to Israel, and German, Austrian and Arab fantasies of Jewish aspirations for regional domination have also endured. The blend of sympathy for the Palestinians and antisemitism was not limited to the Arab world. On the day that the Mexican newspaper Excélsior published its editorial comparing Palestine with Mexico, it published another editorial accusing Jews of having ‘been repeatedly converted from persecuted to persecutors and vice-versa throughout their entire history.’ Moreover, ‘wherever they are, [Jews] isolate themselves, without mixing themselves or even less merging themselves with who they live with.’ Due to their ‘cohesion and solidarity … they are able to rapidly triumph in all means, finally becoming hated and provoking the most extraordinary explosions of violence against them.’ In other words, according to the editorial, Jews deserve to be hated and have invited their own persecution – perhaps not for killing Christ, but for their invariable success.
On the eve of Israel’s creation and the Nakba, people throughout the world viewed Palestine with powerful and varied feelings. Then, as now, Christian and Muslim fervour, anti- and philo-semitism, anticolonialism, altruistic humanitarianism and political self-interest all shaped how people around the world saw the region. The extraordinary symbolic power of the Holy Land to Muslims and Christians everywhere; a fascination, even obsession, with Jews that has both positive and negative valences; and anti-imperial principle and anger at the US for reasons beyond its support of Israel have all fuelled acute and enduring passions about Israel/Palestine. Palestine has become a palimpsest onto which people around the world project problems within their own countries, adding another layer of commitment – and, often, distortion. The magnitude and intensity of feelings about Palestine in the immediate postwar period, feelings created by an interaction of multiple, overlapping historical forces of singular strength and intensity, created an affective black hole possessed of enormous power, which remains with us to this day."
Pundits and historians declare neoliberalism over: Mexico begs to differ | Aeon Essays,,https://aeon.co/essays/pundits-and-historians-declare-neoliberalism-over-mexico-begs-to-differ,"The history of neoliberalism was born from the aftermath of the Great Recession. With the political economy in crisis, historians across the Anglosphere set out to trace the genealogies and movement of ideas associated with neoliberalism. Historians of Europe led the way, delving into the archives of the Vienna Chamber of Commerce, the League of Nations in Geneva and the Mont Pelerin Society in the Swiss Alps. Long before North Atlantic reading publics deemed themselves neoliberal, this research made clear, the first decades of the 20th century had seen the forging of neoliberal ideology.
Quinn Slobodian’s Globalists: The End of Empire and the Birth of Neoliberalism (2018) offered a sweeping intellectual history of this process: as 20th-century empires eroded and democratic movements gained force, European elites were convinced that private property had to be salvaged, somehow, against budding nation-states and their empowered masses. Along the way, influential economists, notably Ludwig von Mises and Friedrich Hayek, became globalists, conceiving the supranational as a space where international institutions could be built and empowered to protect markets, economic freedom and wealth from more democratic nation-states.
Other historians have followed Slobodian’s trails, finding neoliberalism’s origins in the deeper corners of modern European thought. With few if important exceptions, these intellectual histories of Europe have most frankly posed the question ‘How did we, the neoliberals, get here?’, sometimes glossing over a more existential and harder to fathom one: ‘Who are we?’ If societies and their governments became neoliberal at some point in the previous century, what that point was and how it arrived is important to understanding contemporary political economy and the type of collectives and people it has produced.
Following European history, American political history joined the project of historicising neoliberalism while redefining its goals. If scholars of the history of Europe sought the more distant intellectual origins of the present, historians of US politics now seek to periodise neoliberalism as a hegemonic political project. The historian Gary Gerstle’s book The Rise and Fall of the Neoliberal Order: America and the World in the Free Market Era (2022) makes the case for a beginning and an end to neoliberalism through an analysis of Anglo-American high politics, political discourse and public sentiment. While the intellectual historians of Europe saw a nascent neoliberal project to protect markets and wealth from society in the tumultuous interwar, historians of US politics date the beginning of neoliberal statecraft to a more familiar point of origin in the 1970s and ’80s.
Here, amid stagnant economic growth, high inflation and public discontent in the 1970s, the postwar, Keynesian state lost legitimacy, conditioning a neoliberal takeover of government. The Ronald Reagan administration in the United States, and Margaret Thatcher’s in the United Kingdom, sought to free markets from the shackles of government controls. Helping to globalise this mission were international institutions like the General Agreement on Tariffs and Trade (GATT), the International Monetary Fund (IMF), and the World Trade Organization (WTO), which contained national sovereignty, much like von Mises and Hayek once imagined. For historians of US politics, however, the era of neoliberalism that began in the 1970s is now over at home and abroad, with these scholars also speaking for the world created by US postwar hegemony and the fall of the Soviet Union. A consensus seems to be emerging that we, the people of the world, no longer live in neoliberal times.
One striking feature of this latest turn in the history of neoliberalism is Latin America’s absence. If the intellectual historians of Europe found neoliberalism’s origins in the cafés of Vienna and the mountains of Switzerland, the political historians of the US have found its end on the steps of the Capitol Building on 6 January 2021, as an ostensible public and political consensus around a neoliberal project eroded. Indeed, Gerstle’s book has no index entry for Augusto Pinochet or Chile, and Mexico appears only as the sender of unwanted migrants or the receiver of a disenchanted populace’s racist scorn. This excising of Latin America and much of the Third World produces a curious result: a distinct nostalgia for a neoliberal past. If these are no longer neoliberal times, and if a post-neoliberal present is defined through an erosion of consensus, then it follows that populations once wholeheartedly consented to being ruled by neoliberalism. Was it part of the ‘norms’ for which many in the US now yearn?
N ow, if you’re losing patience with this topic, I do not blame you, for neoliberalism can be a tiresome subject. Almost two decades ago, in 2007, I arrived at the University of Cambridge to study an undergraduate degree in social anthropology. I chose anthropology because I wanted to understand the world, and I could barely contain my excitement at the opportunity to learn how to do this under the tutelage of its clearest-eyed professors and alongside its brightest pupils. My relocation from Mexico to Britain to receive a world-class education was the latest iteration of countless imperial itineraries before me. I had moved to that metropole, to its finest of ivory towers, Trinity College, thanks to the economic sacrifice of my parents, who took on extra jobs to afford their hardworking daughter’s ambitions and deemed Britain’s Oxbridge the best education money could buy.
My bedsit overlooked the cobblestoned streets of King’s Parade, and was sparsely furnished with the contents of a suitcase, a frayed poster of the Strokes, a coffee pot and mug, two kilos of Mexican coffee, and a carton of cigarettes. The rest, I believed, would come from my education. I rushed to orientation and picked up my book lists and syllabi, eager to be schooled by the masters. But when going through the course reading lists, disappointment came over me. It was neoliberalism smeared on neoliberalism. Articles on what made the privatisation of Bolivian water ‘neoliberal’. Books on social movements across Latin America that analysed these movements as responses to ‘neoliberalism’. Pieces that played with the rise of religious orthodoxy and the rekindling of Indigenous identities as phenomena that were simultaneously ‘neoliberal’ and ‘anti-neoliberal’. Analysis that began and ended with the ‘Washington Consensus’, insisting that since then we had come to live in ‘neoliberalism’ because we were now required to be self-managing individuals.
My British classmates were enthused, but I had not crossed an ocean to be educated on neoliberalism. After all, I had been reckoning with the thing, experiencing, analysing and making sense of it alongside my parents, teachers, peers and fellow Mexican citizens, for as long as I could remember. A simple n-gram search reveals that the word ‘neoliberalism’ has been in Latin American vocabularies far longer than in Anglo- American ones .
As a Mexican born in the 1980s, I grew up in a fever of neoliberalism. I went to a private school in the bourgeois core of Guadalajara founded by a posh, Lefty, Cuban-Mexican woman claiming to follow the vision of four men: Darwin, Marx, Piaget and Freud. The school did not like it when students went barefoot, and we were allowed to smoke weed only in the school’s garage ‘out of respect for the neighbours’. There were no exams. All our schooling was self-managed, and we graded ourselves. We learned based on monthly study guides on which we worked ‘at our own pace’, meeting targets alone, and we graded ourselves based on our own assessment of ‘effort’. We all belonged to ‘co-ops’ that we were told were collectively owned, and each student could in fact get a little money from the profits of these enterprises at the end of the month. So we worked for them to be successful (or to get them out of the red, which was the case for my parent-subsidised and rotund failure of a cinematheque venture). To teach discipline and responsibility, but also because the school ran on a shoestring budget, students were the ones who cleaned the school after class.
Neoliberalism made an increasingly unequal society open for global investment
Back then, in between reading books and drafting essays on my own, failing to sell cinema tickets or making salsas and marmalades for market, and scrubbing toilets and mopping floors, I got a sort of education. The education was that neoliberalism existed and was bad. That it hurt Indigenous peoples. That it corrupted government and nation. That it was the new, and the heir of the old, imperialism. When the Ejército Zapatista para la Liberación Nacional (EZLN, or the ‘Zapatistas’) came to town, they always stopped by my school. On these occasions, the children of the professional middle classes would ritually join forces with Indigenous paramilitaries to denounce neoliberalism. Still, at least some of us were clearly neoliberals. We were as neoliberal as the school and the country.
Throughout my lifetime, the term ‘neoliberalism’ has been unavoidable for Mexican reformers and resistors alike, a cacophonic echo in the tumultuous decades that remade my country. After the end of the Mexican Revolution in the 1920s, Mexico had been governed by the Partido Revolucionario Institucional (PRI), an authoritarian political party that effectively incorporated the revolutionary factions, from capitalists to peasants, and created political stability and economic growth through the continuous co-option of national interest groups and planned economic development. However, as the last two presidents of the PRI’s ancien régime – Carlos Salinas de Gortari (1988-94) and Ernesto Zedillo Ponce de León (1994-2000) – coped with the fallout of the debt and currency crises that ravaged Mexico in the 1980s and ’90s after the Volcker Shock, they became textbook neoliberals.
The debt crisis had halved Mexicans’ purchasing power, but the government would now privatise public assets and streamline the economy towards export markets even if it meant deindustrialising and further impoverishing much of the nation. Fiscal austerity became enshrined in the law. Continuously depressing wages was now a necessary condition for competing in global markets. Neoliberalism made an increasingly unequal society open for global investment and structurally driven by poverty to the benefit of few. To see through these difficult reforms, Salinas (PhD Harvard) and Zedillo (PhD Yale) appointed administrations that increasingly saw themselves as technocrats; the same ones who would later be charged with making politics procedural, distant and guarded from the demos of the democracy they touted. When Zedillo rendered Mexico’s democratic transition possible by putting a ‘healthy distance’ (‘ la sana distancia ’) between the presidency and his party, loosening the PRI’s 70-year grip on power, the opposition’s Vicente Fox Quesada became president of Mexico, and the world approved. Fox was the cheerful scion of a wealthy family who had risen to become CEO of Coca-Cola Mexico without a BA, let alone a PhD. His quasi-provincial confidence, and his sincere love of markets and consumer culture, brought Mexican neoliberalism to new highs.
Fox’s bravado and good humour delighted most Mexicans. He promised homeownership and upward mobility for all, and Mexicans took him up on that promise. His administration doubled down on fiscal austerity and deregulated the lending industry, flooding the country with consumer credit to grow the middle class and the economy. His approval rates were robust throughout his six-year tenure, almost always remaining above 50 per cent. And yet, masses of Mexicans did not like neoliberalism; a fact that was on display when millions took to the streets in support of the EZLN and against the North American Free Trade Agreement (NAFTA) during Fox’s first year in office.
After college, as I figured out what I would do with my life amid a Great Recession that made highly competitive unpaid internships the new normal, I decided to stick with anthropology and the problem of neoliberalism. If these were sunk costs, they had become mine to bear. Since 2013, I have spent more than a decade researching and thinking about Mexican neoliberalism. Choosing a PhD topic is a fraught business, but my questions were clear to me all along. I wanted to understand how, why and to what end my country had changed through its democratic transition and disillusion. More specifically, I wanted to understand the paradoxical project of maintaining austerity and creating growth, and what consequences ensued when Fox tried to resolve this paradox with a flood of mortgage finance for Mexico’s impoverished masses. All of this is just a particular and substantive way of approaching the historical origins, fluctuating qualities and real consequences of Mexican neoliberalism. So, for years now, I have been reading the latest wave of neoliberalism literature with interest, empathy and concern.
Neoliberalism is what Neil Brenner called a ‘rascal concept’. ‘Promiscuously pervasive, yet inconsistently defined, empirically imprecise and frequently contested,’ the term lends itself to the whims and aims of its user. The concept also signals a sticky phenomenon that societies and populations seem to repeatedly fail to transcend. Declarations of the imminent end or final crisis of neoliberalism have been so far met by relentless, structural continuity. But we are increasingly told the contrary by the eminent among us, as essays and books assert that one can pinpoint neoliberalism well enough to assert its end with conviction. In the Anglophone academy, it has been the historians at the most elite of universities who have touted the end of neoliberalism the loudest, producing award-winning volumes documenting the rise and fall of a social order, the end of an era in world history or, more modestly, the messy culmination of a global but rooted historical period. People are understandably excited by the proclaimed ending of neoliberalism. Polished pieces in the Financial Times and The New Yorker , alongside more mundane newspaper fodder, have pushed the bipartisan line that neoliberalism is over.
However, the arguments supporting the new consensus that neoliberalism is over are quick and thin, parochial or just naive: Donald Trump’s electoral triumph in 2016 had to mean the end of something, and there’s also Brexit; COVID-19 killed global trade, and also the populists; Joe Biden’s CHIPS and Science Act evinced the return of the state. Inarguably, the past decade has been eventful, and these writers have been courageous to take on such a rascal concept on the edge of history. Perhaps, if ‘actually existing neoliberalism’ (to quote Loïc Wacquant) began in the late 1970s or early ’80s, we are lingering at the threshold of the historical craft’s tacit norm to cut the past into 40-year tranches. But, beyond this, a generous but shrewd reader of these accounts is left with little if not punditry. The argument of neoliberalism’s end is so profoundly unsatisfactory that I am left asking whether we must divide history into periods at all. I rummage through the stacks. Jacques Le Goff argued no but concluded yes, and Marc Bloch warned about the idol of origins. What is our answer?
I f the breaking point of neoliberalism came around Trump and Brexit, then it would appear that its ascendancy was a time of relative consensus, peace (in Europe) and optimism. In fact, conflict, war, violence, pain, pessimism, frustration and illiberalism abounded in the past four decades, but the reigning rhetoric is otherwise. For scholars and natives of Latin America, the narrow, triumphalist if mournful viewpoint of Anglo-American elites avoids the discomfort of addressing the Latin American experience of, and abundant scholarship on, neoliberalism. We should not be surprised. Including Latin America in these nominally global histories would make it impossible to suggest that neoliberalism is tantamount to consensus; that it was peacefully developed, not violently imposed; that it is defined by a sense of optimism; that it is not imperial. Without Latin America, we forget about the intimidating rise of Third World power before the 1980s, epitomised by movements for a New International Economic Order. We forget about the weaponisation of the Volcker Shock and the destruction wrought by its ensuing debt crises. We forget the reassertion of First World domination through enforced ‘structural adjustment’. More importantly, we are able to ignore that neoliberalism is structurally ongoing.
Mexican perspectives have a special timeliness in the face of epochalism, or the tendency to define change as a paradigmatic rupture between eras. The preoccupation with carving eras out of our immediate past has not been limited to the public-seeking, higher echelons of the Anglo-American academy. Epochalism has also taken over politics, and nowhere has this been more glaring than in Mexico, where the previous president, Andrés Manuel López Obrador (AMLO), declared the end of neoliberalism with his inauguration in December 2018. The Left-wing populist regime that currently governs Mexico, and which its new president Claudia Sheinbaum has inherited, is so epochalist that it understands itself as the ‘Fourth Transformation’ (‘ la Cuarta Transformación ’, or ‘ la 4T ’ for short) of Mexican history. The First was independence from Spain; the Second was Liberal Republicanism, or the separation of Church and state; the Third was the Mexican revolution; and then there was AMLO. It is easy to mock the audacious naivety of the ruling party Morena’s periodisation, but there is more to its historical idealism. For the party’s ideologues, these transformations do not constitute mere chapters of national history. Rather, they are also iterations of that which came before them. In each, a break for transformative change, or justice, is asserted only to deteriorate through the decades, as anti-democratic powers regain control. The transformations of la 4T are sublimations, rendering neoliberalism the affirmation and negation of everything that came before it. Just as neoliberalism was the last four decades of a post-revolutionary Mexico that had gradually lost the revolution it once won, it is the ground for a new break.
So then, when the former president López Obrador spoke of the ‘neoliberal period’ as something that had been overcome, he engaged in a kind of historical mysticism. The message is that neoliberalism had retreated into the past but is also still with us, and this message resonated well enough. López Obrador’s approval ratings leaving office reached 77 per cent. Sheinbaum campaigned on nothing but continuing López Obrador’s Fourth Transformation and finished 33 percentage points ahead of her challenger, with a legislative supermajority to boot. Morena now controls three of every four governorships in the country and will soon control the Supreme Court. In other words, the promise to continue overcoming the phantom of the past radically increased Morena’s mandate.
Under López Obrador, Mexico has remained one of the most regressive fiscal states in the world
Today, Mexico is a case study in why declaring neoliberalism dead conceals much more than it reveals. The urge to pronounce its end speaks to a grave discomfort with being stuck, suspended, with how it is not as easy to overcome as we want it to be. López Obrador’s administration increased the minimum wage significantly above inflation for the first time in 35 years of stagnant real wages, but, by any serious measure, Mexico remains deeply neoliberal. Take, for instance, basic economic growth. As Thomas Piketty and others have pointed out, the decades since the 1980s have been characterised by slow growth in most countries. In Mexico, the average GDP per capita growth rate between 1982 and 2018, the year that López Obrador came into power, was below half a per cent. Yet, throughout López Obrador’s term from 2018 to 2023, Mexico continued to grow slowly, again below a half per cent. These rates contrast with those preceding the 1982 debt crisis, from 1961 to 1981, in which Mexico grew at an average of 3.6 per cent per capita, faster than the US.
Most other significant trends have stayed on course since the 1980s. This is the case of austerity, which López Obrador rebranded ‘republican austerity’ (‘ austeridad republicana ’) in homage to the great Mexican liberal, Benito Juárez, but which translated into a continuation of low public spending. Austerity is the tendency to cut or maintain low public spending in the name of economic solvency and private industry, uniting most common tropes of contemporary politics, from budget cuts to employment deregulation. López Obrador’s administration spent heavily in its final year, largely as an electoral strategy, but total public spending during the first five years of his government remained virtually unchanged from the preceding administrations. In fact, the administration decreased the budgets of 30 of 48 federal branches of government and of nine of 17 ministries to fund an oil refinery, a train for tourists, an airport, the armed forces and new social programmes. The Fourth Transformation’s social programmes, which include small but universal transfers to the elderly, have been celebrated by Leftists. However, these programmes are more regressive, or less redistributive, than those of previous administrations. In the lowest deciles, D1 and D2, poverty deepened under López Obrador, and the general population’s ability to access health services and social security decreased while the education gap increased. The ‘rheumatic elephant’ of Mexican bureaucracy, to use the president’s own epithet valorising austere government, overall suffered from diminished capacities due to personnel cuts, decreased salaries, and lower budgets.
Neoliberal or ‘republican’, Mexican austerity is rendered a necessity by the country’s tax code, which overwhelmingly benefits the rich, the very rich and large corporations against state capacity and the general interest. Mexico is an extreme case, but the rise of regressive taxes was not a particularly Mexican phenomenon. Indeed, with or without the ideology of trickle-down economics, the previous four decades saw the widespread erosion of progressive taxation. Under López Obrador, Mexico has remained one of the most regressive fiscal states in the world and has continued to register the weakest fiscal capacity in the OECD. The country’s fiscal revenue trails far behind the OECD and Latin America averages, and is even lower than that of the poorest countries in the region. Yet, to this day, López Obrador’s Fourth Transformation has repeatedly refused to take on fiscal reform, promising increased tributary efficiency instead of changing the tax code.
Along with protecting monopolies, a regressive fiscal state continues to enable the disproportionate enrichment that Mexico’s rich and very rich have enjoyed under Morena. In the first two years of López Obrador’s administration, for instance, Mexicans with more than 500 million MXN (roughly $24 million) saw their income grow 117 times faster than the Mexican economy but contributed only 0.03 per cent of total tax revenue. Mexican oligarchs have doubled their wealth during the Fourth Transformation, and the wealth of the richest amongst them grew twice as fast as that of their Latin American counterparts. Directly and indirectly, neoliberalism overwhelmingly helps the wealthy, and that is still the case in Mexico. Against all expectations, the Fourth Transformation has been resolutely pro big business. After all, why did López Obrador and Trump, despite their discourse and the desires and needs of their bases, ratify NAFTA?
The point is not that López Obrador is neoliberal. It is rather that, beneath a proliferating discourse proclaiming otherwise, most of the great structural tendencies that began in the 1980s remain on course, often exacerbating – in Mexico, but one could easily argue the same for most societies, including the US and the UK. The structural pull that we are experiencing should not surprise us, for it is processual, following from that which came before it.
T he progressive impoverishment of most and the enrichment of few tends towards its reproduction. It also tends to generate political responses that are discursively antagonistic to the social structure but in practice reaffirm it. My research into the housing and mortgage markets in Mexico shows this is exactly what happened in 2001. At that time, Fox’s administration deregulated the lending industry to double down on austerity, spike the economy, and give a leg-up to poor Mexicans. Their plan to turn Mexico’s poor into homeowners through mortgage finance seemed to promise a win-win for all. On the one hand, the Mexican masses would benefit from the sudden opportunity to formally acquire property and build assets. Financial inclusion would in fact make history, and a new middle class, for most Mexicans had historically lacked the right or ability to access consumer credit. On the other hand, the extension of mortgage loans to the Mexican masses would benefit the owners of capital and global investors, triggering positive feedback cycles across the Mexican economy and feeding financial markets with new securities and stocks. The Fox administration’s decision to deregulate lending while subsidising private lenders created a pipeline of prequalified homebuyers for the construction industry; a housing boom that moved 20 million people, or a fifth of the country, from old informal settlements and shanties into freshly built suburbs.
In Tlajomulco de Zúñiga alone, a once sleepy municipality to the south of Guadalajara, 200 macro housing complexes were built in a few years. Today, just one of these complexes, Hacienda Santa Fe, is home to more than 100,000 people. Over years of ethnographic fieldwork, I witnessed how the beneficiaries of Mexico’s housing boom continuously fell back on mortgage payments and found themselves further impoverished by homeownership. Yet, they were not evicted, remaining instead captive to further extraction of fees and interest while failing to contribute to equity. The owners of the construction and lending industries, and the many investors who had capitalised on Mexico’s democratising feat, grew only richer, offering more and more debt-based products that Mexico’s poor bought to try to make ends meet.
As the people of Hacienda Santa Fe and neighbouring complexes grew desperate, holding on to their homes by remaining on the giving end of rentierism, a new political party emerged to gain their sympathy and votes. Movimiento Ciudadano, or Citizens’ Movement, was an early exponent of the political discourse that Morena would later join: they berated the neoliberals, the elites and their technocratic bodies, for they had destroyed the country. They also promised a return to politics and the arrival of true democracy. And they told the beneficiaries of Mexico’s homeownership venture that, through them, they would keep their houses and regain control of their lives.
They built a pyramidal bureaucracy that runs on free labour in the name of civic virtue
In 2010, thanks to the overwhelming support of the hundreds of thousands who had arrived with the promise of homeownership and upward mobility the decade prior, the municipality of Tlajomulco de Zúñiga gave Movimiento Ciudadano its first electoral triumph. The party inherited a political nightmare. The municipality’s population had grown fourfold with Fox’s housing boom, but its coffers were empty, and its mounting municipal debt had been downgraded by ratings agencies. Much like its constituents, the municipal government could not afford to service its own debt. Nor could the government’s running costs be covered, since the new homeowners who now called this place home were too destitute to pay property taxes. The level of disorder brought on by the housing boom was so severe that, nine years into the flood of mortgage finance, a government survey revealed that 72 per cent of inhabitants did not know that they lived in a place called Tlajomulco de Zúñiga. So how does a broke government, with no fiscal base and hundreds of thousands of equally broke constituents in need of infrastructure and services, actually govern? The anti-neoliberal Movimiento Ciudadano came to find neoliberal solutions.
Movimiento Ciudadano began to crowdfund government, building a pyramidal bureaucracy that runs on free labour in the name of civic virtue. This structure is organised into territorial confederations under the supervision of a handful of paid party officials, but daily governance is the responsibility of citizens acting as coordinators of councils entirely made up of unpaid inhabitants. This pyramidal network enables the municipality to distribute and allocate austere resources to its citizens at low cost. More importantly, though, it recruits citizens into performing administrative and collaborative functions on behalf of government, for their own wellbeing. Tlajomulco de Zúñiga’s civil councils oversee and administer their localities, organising neighbours into putting in time, labour and resources of their own to patch up streets and sidewalks; paint crosswalks; cover up graffiti; clean and sweep public spaces; guard the entrance of schools; organise disaster relief and public health campaigns, and many other actions that were the responsibility of government. In other words, Movimiento Ciudadano meets its liquidity problem by turning citizens into self-managing, unpaid civil servants. For a moment, I am reminded of cleaning my Lefty school and of the unpaid internships I did not take.
This neoliberalism from below was developed as a response to the same from above. Structurally, if decades of neoliberalism bankrupted people and state, what was at hand but austerity? In places defined by this double bankruptcy, the only alternative to neoliberalism tends to become a neoliberal government touting an anti-neoliberal discourse. The extreme case of this corner of the global periphery should remind us of the power of structure in the face of epochalism.
Social change is rarely epochal, and epochs are bigger than presidential administrations. Decades of political policy create social structures that engender tendencies that are hard to break, even when we want to. Future events are deeply rooted in past conditions. For example, López Obrador’s social programmes greatly increased the wealth of Ricardo Salinas Pliego, the main owner of Mexico’s usurious lending industry for the poor. But this was because he owns the infrastructure necessary to quickly start distributing government transfers to all Mexicans.
In times of moral panic and ontological fear, it is only human to want to take control of time, to declare the end or beginning of something. Which is to say that, when our present feels disjointed, we try to break history at its joints. The anthropologist Clifford Geertz coined the term epochalism to name the anxious discourses of postcolonial states that had attained legal freedom but remained captive to the economic and political power of their former rulers. Much like the old imperialism, the neoliberal past will not be so easily transcended if the structures that it built over decades remain. Careful longitudinal research often reaches conclusions that are as truthful as they are tedious to publics demanding radical change. Even when we are trying to break free, we are always in the middle of history.
In memory of my mother, Mercedes González de la Rocha."
Why Brazil fell for Pentecostalism but not liberation theology | Aeon Essays,,https://aeon.co/essays/why-brazil-fell-for-pentecostalism-but-not-liberation-theology,"In 1962, the Colombian priest and sociology professor Camilo Torres Restrepo travelled to the north of the country to investigate a dispute between powerful landlords and subsistence farmers. Spending days wielding machetes with Black cane cutters and nights drinking rum with them, he learned of their 40-year struggle against their wretched conditions at the mercy of a local landowner and senator. Competing for land with his grazing cattle, they had been ‘whipped, fired upon, beaten back to the riverbanks and their humble dwellings burned to the ground.’
Camilo Torres Restrepo with Fabio Vázquez and Victor Medina Morón, in the journal OCLAE , no 2, 1967. Courtesy Histoire Engagée
Described by his biographer Walter J Broderick as the ‘Che Guevara of the Christian world’, Torres began working across spiritual lines with communist, student and trade union groups as he dedicated himself to improving the conditions of the downtrodden. ‘More concerned with the action itself than with the theory behind it,’ Torres was relieved of his orders in 1965, after which he took up arms with the National Liberation Army, a Marxist-Leninist guerilla group. ‘The common people,’ he wrote to a friend, are ‘the only hope for change.’
Insisting on being given no special treatment and fighting as a regular soldier, the following year Torres wrote a powerful homily to the nation on his decision to take up arms. ‘For many years the poor of our country have waited to hear the call to arms which would launch them on the final stage of their long battle against the oligarchy,’ he wrote. Years of corrupt elections and failed coups meant that ‘the moment for battle has arrived.’ Adamant that he had ‘not betrayed’ the people, Torres added: ‘I have not ceased to insist that we give ourselves to this cause unto death.’
It was a prophetic letter. Torres died shortly after writing it, killed in his first battle, by the first bullet fired at him. Long viewed by the military as a threat, Torres was taken, his body secretly buried and its location kept a state secret for years so he could not become celebrated as a martyr. One man’s heroism is another’s terrorism, and Torres’s story was often cited as proof that Left-wing social justice movements, supported by a new breed of clergymen, wore a crown of daisy-chains to conceal a violent and sinister mission.
Even for the many who disapproved of Torres’s decision to trade his vestments for khakis, the mood for change within the Catholic Church was already taking powerful shape.
A t the Second Vatican Council (Vatican II) between 1962 and 1965, the Church agreed a series of reforms to meet the modern world’s challenges. Even Pope John XXIII had, a month before the Council’s opening, noted that in ‘underdeveloped countries, the Church is, and wants to be, the Church of all and especially the Church of the poor.’ The decrees that were eventually issued by Vatican II sought to make the Church more democratic and modern, but they did not go far enough for many in its progressive wing.
Lay people wanted change, too. Groups that became known as Christian Base Communities were both a response to Vatican II’s call for ordinary believers to take a more active role in the Church, and a sheer necessity. Largley growing from the most desperate regions of the Brazilian Amazon, they began as Bible study and prayer groups formed in poor and remote parts of the continent that were starved of pastoral care. In 1970, some 40 per cent of Brazilian priests were foreigners. By the 1980s, there was only one priest for every 9,367 people, and the ratio continued climbing in the 1990s.
There simply weren’t enough priests to effectively look after their flock, another strain amid growing disconnection from the Church in Europe. As the sociologist Cecília Loreto Mariz puts it in Coping with Poverty (1994), the largest Catholic nation in the world was ‘still a mission outpost’. Demographic needs were one thing, but a cultural shift was underway too. At a time when global economic forces were accelerating ideas of self-reliance, spiritual nourishment was not immune – something that would become telling two decades later, when a more vibrant gospel of personal betterment would sweep all before it.
Liberationists believed that they were bringing the Church back to the teachings of Jesus
Real and necessary as they were, liberation theologists emphasised Base Communities beyond their influence. Impoverished and often illiterate people coming together to practise their faith by any means necessary was a romantic idea, blown out of proportion. Initial studies seriously overestimated the scale of Base Communities by as much as a factor of 10. In fact, no more than 4 per cent of Brazilians – in 1970, that meant about 3.8 million people out of 96 million – are believed to have ever belonged to one. What they did, however, was give voice to ordinary people, and empower the progressive wing of the Church, which was calling for a more democratic approach to faith and wider society.
The 1968 General Conference of the Bishops of Latin America and the Caribbean (CELAM) held in Medellin, Colombia saw the nascent movement that would become known as liberation theology begin to take its first steps. The bishops issued a document that understood three meanings of poverty: an evil God does not want; spiritual poverty, or a readiness to do God’s will; and, finally, solidarity with the poor. The conference also consciously introduced the term ‘liberation’ to the cause for spiritually led societal change.
Joseph Ratzinger, the future Pope Benedict XVI, in 1965. Public domain
With this new direction came the first rumblings of criticism. It was 1968 after all, and the febrile geopolitical climate saw great possibility, but a strong pushback against change. Until that year, a young German theologian named Joseph Ratzinger – the future Pope Benedict XVI – had been politically liberal, but he began developing an increasingly conservative outlook that divorced faith from secular politics. In promising to forefront the poor, and go beyond charity to seek broad and lasting solutions to their poverty, liberationists believed that they were bringing the Church back to the teachings of Jesus. Ratzinger, and many of his European colleagues, saw things very differently.
F ive years after Torres’s battlefield death, his former schoolmate and friend from the Catholic University of Louvain, the Peruvian theologian Gustavo Gutiérrez, put the ideas of the fledgling Latin American movement into print with his book A Theology of Liberation (1971). He understood the movement as a ‘commitment to abolish injustice and to build a new society’, where freedom from exploitation, and ‘the possibility of a more human and dignified life’ was built on four principles. First and foremost is the prioritisation known as the ‘preferential option for the poor’, which he later said encompassed 90 per cent of the movement. He also believed that an unequal society was a sin, and valued the importance of unchurched lay gatherings of the faithful, such as Base Communities, and the commitment to taking a ‘see, judge, act’ approach to injustice.
God makes his sympathies clear from Cain and Abel, Gutiérrez argued, the parable mirroring ‘God’s predilection for the weak and abused of human history’. Jesus makes the will for material justice even more explicit, best surmised in the Bible verse: ‘I have come that they may have life, and that they may have it more abundantly.’ This wasn’t a God of death and afterlife, but of the here and now. On a deeply Christian continent that had (and still has) some of the greatest wealth and resource disparity on Earth, abundance was out of reach for most. ‘In liberation theology, faith and life are inseparable,’ Gutiérrez later wrote in a reflection on his original work. ‘These concrete, real-life movements are what give this theology its distinctive character.’ The poor deserve to be elevated, Gutiérrez argued, ‘not because they are morally or religiously better than others,’ but as it is expressed in the gospel of Matthew, that ‘the last will be first, and the first last.’
Spiritual life could not be divorced from the material world, and it was an existential issue for the Church. From the colonial era through to the brutal far-Right and military dictatorships that came to dominate Latin America in the 20th century, the Catholic Church was replicating the hierarchies that harmed God’s people. For too long, God’s emissaries in Rome had served earthly power. Too often, the almighty dollar had been a servant of poverty, and a master of the poor.
It wasn’t only the Vatican that was on the offensive against liberation theology
One year after Gutiérrez’s book, the Brazilian philosopher-priest Leonardo Boff published Jesus Christ Liberator . Unabashedly shaped by Marx and intent on ‘decentring’ the Church to welcome disparate cultures, Boff was a champion of Base Communities, and one of the Vatican’s most trenchant internal critics. Influenced by the Norwegian sociologist Johan Galtung, who introduced the idea of ‘structural violence’ to the secular world, Boff expanded on the concept of ‘structural sin’, the idea that sin can be a social or economic action, and not simply an individual one.
The Brazilian theologian Leonardo Boff, pictured in 2018. Courtesy Manuale d’Avila/Flickr
Around this time, repressive military dictatorships in the cone of South America were beginning to transition to civilian governments, and anti-communist fervour shifted its centre of gravity to the Central American nations. The religious studies professor R Andrew Chesnut says it wasn’t only the Vatican that was on the offensive against liberation theology: ‘They faced fierce national repression in countries such as Brazil, Argentina, El Salvador and Guatemala, where brutal military dictatorships saw them as communists.’ The ensuing dirty wars in Central America, with plenty of aid and comfort from the United States government and mercenaries, killed around 350,000 people in Guatemala, Nicaragua and El Salvador, and displaced millions more.
Most proponents of liberation theology explicitly condemned violence, while pointing out the differences between ‘structural sin’ that they saw as the institutionalised violence of inequality, and violent action against injustice. However, Ratzinger and other orthodox Catholics continued to mount objections to the movement, gathering evidence from Torres’s deeds and Boff’s words to declare the movement heretical.
Their attentions turned inward, they failed to see a far greater threat to their power and authority in another movement bubbling up from below, albeit from a very different Christian tradition. It was one that would go on to claim far more of the continent’s souls and move them even further away from the edicts of European cardinals.
Pentecostalism, a US-born branch of evangelical Protestantism centred on the Holy Spirit, had been gaining ground in the region throughout the 1970s. Two decades later, Pope John Paul II denounced Pentecostals as ‘ravenous wolves’ who were stealing the Catholic flock. Their attraction was prosperity theology, or the gospel of health and wealth, where true believers who give generously to their Church will receive back riches of mind, body, spirit and wallet many times over.
Emerging from the same social and economic conditions as Base Communities and liberation theology, the divergent paths of the two religious movements over the next four decades tell a much broader story about the secular world.
I f there was a family that liberation theology was born to serve, it was Edir Macedo’s. Born in the small country town of Rio das Flores in Brazil, to a Catholic mother who endured 33 pregnancies, including 16 miscarriages and 10 premature births, he was one of seven surviving children. The fourth child and second oldest son, Macedo’s genetically deformed hands made him the target of bullying, both at school and at home.
Like many Brazilians from the regions, the family migrated to the outskirts of a major city, in their case, Rio de Janeiro. When he was a teenager, Macedo’s elder sister Elcy developed chronic asthmatic bronchitis that was failed by the medical treatment available. The family sought help at a spiritist centre, a kind of folk homeopathy that brought together strands of African slave religions, Christianity and pseudoscience.
Having no luck, Elcy heard a radio broadcast from the Canadian Pentecostal preacher and faith healer Robert McAlister. Visiting his New Life Church in downtown Rio, her asthma disappeared and, within a year, the entire family became devotees. Macedo converted at 19, but when 11 years later, his daughter was born with a cleft palate, his devastation led him to a ‘revolt’ that he said was ‘not against God, but against hell.’ Macedo quit his job and took to street preaching in crime-soaked favelas, before forming his Universal Church of the Kingdom of God in 1977.
Long before McAlister had touched the Macedos, in 1911, the renegade Swedish Baptist Daniel Berg brought the Assembly of God to Brazil after witnessing the Pentecostal fire when he worked as a foundryman in the US. Evangelical missionaries came and went across the continent with little success, before Americans from the Foursquare Gospel arrived in the 1950s. Both were Pentecostal movements, a branch of the evangelical faith that emphasises the role of the Holy Spirit and a direct experience and personal interaction with God, and all of the blessings and miracles that come with it. While Charles Fox Parham could be credited with birthing the movement in Kansas in 1901, it was his adopted spiritual son, William J Seymour, a Louisianan son of freed slaves, who gave it life in 1906, with the Azusa Street Revival in what is now downtown Los Angeles.
Fearing the slippery slope towards communism, business leaders and Christian leaders found common cause
Unlike traditional Christian denominations, what became known as Pentecostalism cut away intermediaries and allowed believers a direct relationship not only with God, but with all of his promises. Specifically, the nine miracles of the Holy Spirit, including speaking in tongues, prophecy and faith healing. Though it faced stiff opposition from the Catholic Church and even some of the few Protestant Churches in the region, as early as 1916, the Pentecostal movement had penetrated eight countries in Latin America. The Assemblies of God grew by an average of 23 per cent per year between 1934 and 1964. Brazil alone boasted more than a million members of Pentecostal congregations by the early 1960s. The Pentecostal movement expanded to become a worldwide phenomenon in just over a century. Pentecostals and Charismatics numbered just 58 million in 1970; by 2020, that number was 635 million. What brought so many into the tent was the prosperity gospel .
Pentecostalism really got going in North America after the Second World War, coinciding with the US elevation to global superpower. This brought forward a flood of optimism; yet, for many leaders of both faith and industry, it was a time for vigilance. Business leaders were scarred by the economic safety net implemented by Franklin D Roosevelt’s New Deal. Christian leaders were terrified of the godless ‘reds’ sweeping Europe. In fearing the slippery slope towards communism, these interests found common cause. While they were busy merging the languages of business and Bible, the Pentecostals, often looked down on by their evangelical counterparts, were stoking new revivalist fires across the country. Out of the spirit of this patriotic postwar nation arose a new vision of Christianity.
Prosperity theology, often known as the gospel of health and wealth, emerged with the help of syndicated radio and cassette-tape technology. Kate Bowler, the author of Blessed: A History of the American Prosperity Gospel (2013), says: ‘Inverting the well-worn American mantra that things must be seen to be believed, their gospel rewards those who believe in order to see .’
Pentecostals embraced the resurgent idea of ‘mind power’, and added the miracles of healing and prosperity to what Bowler calls an ‘electrified view of faith’. If this way of ‘doing Jesus’ had just been plugged in, then the publication of Norman Vincent Peale’s The Power of Positive Thinking (1952) turned up the volume to max. The New York-based preacher, who was born Methodist then born-again Reformed, invented a simple formula – picturise, prayerise, and actualise – that self-help gurus have been peddling ever since, a precursor to the ‘mind, body, spirit’ trend in both Christian and secular thought in the 1960s and ’70s. Peale’s message was a huge hit with the faithful, spreading rapidly from coast to coast. His Marble Collegiate Church on Fifth Avenue was standing room only. One family was firmly planted in the front row with their young son: his name was Donald J Trump.
Bowler says the prosperity gospel is the result of the ‘American gospel of pragmatism, individualism and upward mobility’, where churchgoers ‘learned to use their everyday experiences as spiritual weights and measures’. Critics of the prosperity gospel rarely acknowledge that ‘when many people say “prosperity”, they mean survival,’ Bowler says. For many ordinary believers, it is the same quest for abundance that the liberation theologists expressed.
W hile the US was on the up, Brazil was in the midst of its own Great Migration. Between the 1930s and the ’80s, millions began moving to the more central cities from the poor north-east, displaced by ranchers who could make use of poor farming land for grazing. Slavery had nominally ended in 1888, but indentured servitude continued in some areas of the north-east into the 1990s. The cities offered rural northerners economic opportunities beyond tilling the fields for those who could only generously be described as their bosses. In 1960, São Paulo had a population of 4 million; today, the wider urban area has grown to five times that.
Pentecostalism gained traction among poor migrants from the countryside who moved to the outskirts of large cities looking for work, like the Macedos. That was followed by a reverse migration of sorts, as new converts took the religion back home to the impoverished rural communities where Base Communities had flourished. Early research on Pentecostal converts suggested that they were the poorest of the poor.
As gifts from the US to Latin America go, the prosperity gospel is up there with the military coup. The best and worst of this muscular US ethos trickled down the Atlantic, both by culture and by military might. From the 1950s, many middle-class Brazilians believed that US culture spoke to a glittering sense of progress that they ought to emulate.
Macedo understood what he aspired to be, but also where he came from. Universal churches began offering services between 5 am and midnight throughout the week, understanding that most working people struggled to attend in standard hours. More than that, the spiritual nourishment of going to a service before or after a long day of work was fortifying. As Pentecostal churches began to cram into the storefronts of the nation’s favelas, they quickly became familiar institutions that were markedly different from the distant Sunday cathedrals. Their pastors spoke in the vernacular of the streets, were often mixed-race and had little-to-no theological education; their sermons used everyday problems to understand the Bible.
The golden toilet seats of prosperity gospel entrepreneurs are not a deterrent but rather a sign of God’s gifts
After the end of military dictatorship and economic growth, by the 1980s and early ’90s, Brazil had the 11th largest economy in the world, but its social indicators were similar to poor African countries. As Mariz writes: ‘many already have rich spiritual lives, so they’re after something that can help them understand and cope with the changing world around them.’
The golden toilet seats and private jets of prosperity gospel entrepreneurs are not a deterrent but rather, for most true believers, a sign of God’s gifts. Derided as gullible, they would describe themselves as motivated. If Macedo rose up from below, why can’t I? Pentecostal preachers have always had a knack of turning established ideas on their head. Catholic priests came to resemble loathed bureaucrats; Pentecostal preachers looked a lot more like inspiring entrepreneurs. The average person seeking wealth is usually looking for it in terms of a promotion at work, helping to keep creditors at bay, or starting their own small business. During services, it’s not unusual to see worshippers hold aloft Bibles topped with debt notices or money.
As Macedo’s following grew, he used funds from his flock to buy a television network, Record, in 1989. Today, he is a self-made billionaire who has received everything he owns from believers – including his only son, Moysés, given to Macedo in the street as a baby by its birth mother. To this day, prosperity gospel’s trickle-up economics continue. Smaller churches practise what they preach and become satellites for established megachurches by ‘sowing’ a percentage of their church income in order to receive the blessings of those who are materially more successful, therefore spiritually more devout.
The threat to the Catholic Church was far greater than the one that liberation theology posed, and the Vatican was compelled to compromise. The Charismatic Catholic Renewal (CCR) arrived in Latin America in the early 1970s, around the same time as liberation theology was emerging and Pentecostalism was gaining steam. ‘Liberationists historically saw Catholic Charismatics as alienated bourgeois citizens with Right-wing political inclinations,’ Chesnut says. Largely imported from the US, ‘it found traction that liberation theology did not.’ In the CCR, believers could have their gospel of health and wealth, but keep their saints and retain connections to Europe and the Catholic hierarchy.
‘Among Protestants and Charismatic Catholics, the prosperity gospel has become hegemonic,’ Chesnut says. ‘It’s come from Latin America, but is now the driver of faith in Africa and other parts of the Global South too.’ Brazil has recently crossed the line where there are now more Pentecostals and Charismatics than traditional Catholics, undoing 500 years in 40, and it’s largely thanks to the prosperity gospel. For a long time, people assumed that it was a regional outlier. Just as they spoke Portuguese while everyone else spoke Spanish, they were embracing the Church of the Holy Spirit rather than Mary and the Saints. If recent trends are an indicator, the rest of Latin America may be on the cusp of this theological revolution, too.
The 1976 earthquake that left one-sixth of Guatemala homeless saw Californian missionaries arrive with the Pentecostal faith. Their missionary zeal and a bloody civil war, in part presided over by the world’s first Pentecostal leader, Efraín Ríos Montt, saw around 60 per cent of the country converted – making it proportionately the most Pentecostal nation in the world. Pope John Paul II’s ravenous wolves are howling at the doors of El Salvador, Chile and Argentina, but even in the staunch Catholic holdout of Mexico, the state of Chiapas recently tipped to majority Pentecostal. By no coincidence, it is also the country’s poorest and most Indigenous state.
I n 1984, only 16 years after the bishops in Medellin had first outlined a social justice under the banner of liberation, Cardinal Ratzinger was ready to issue last rites on the movement. The idea was simply too radical, and one that was ‘incompatible’ with established Church doctrine. To many inside and outside of the Holy See, it was nothing short of heresy – and needed to be punished accordingly.
Ratzinger summoned Boff to the Vatican to account for his accusations against Church authoritarianism, before Boff was given a silencing order, which the theologian Harvey Cox suggested was due to the ‘grass-roots religious energy’ Boff represented. Ratzinger continued to defend his stance, even after Boff left the Church altogether, suggesting that his censure had served as a warning to other like-minded theologians.
In August 1984, the Cardinal issued an extraordinary decree, Libertatis Nuntius , an instruction responding to the Latin American movement known as liberation theology, which called for a radical reorientation of the Church towards social and economic justice. Ratzinger, the future Pope Benedict XVI, might have been known as ‘God’s Rottweiler’, but when it came to teachings that questioned established hierarchies and beliefs, he was Pope John Paul II’s sottocapo . Together, the two fervent anti-communists executed a well-planned hit on a movement that orthodox clergy saw as Marxism in holy cloth.
Ratzinger warned that deviating from Papal authority risked ‘provoking, besides spiritual disaster, new miseries and new types of slavery.’ He believed that liberation theology subverted the meaning of truth and violence. A doctrine ‘incompatible with the Christian version of humanity’, it had, he said, an ‘ideological core borrowed from Marxism.’ To his mind, Marxists believe only in class and class struggle, and see society as inherently violent. That leaves no room for Christian ethics, such as good and evil. Unshackled from morality, Marxists are then compelled to participate in the struggle and to reverse the nature of domination by establishing their own.
Some argued that the ‘preference’ for the poor ran against the universal Christian message
Such was the hammer blow of Ratzinger’s decree that an early obituary of the movement in The New York Times showed that opposition to liberation theology was every bit as political as the movement itself. Written by Michael Novak, resident scholar in religion, philosophy and public policy at the American Enterprise Institute in Washington, DC, it claimed that the movement’s ‘Marxist thread’ was ‘just one of its many flaws. It is a robe of many vibrant colours.’
Novak objected to theologians who took liberation theology ‘seriously as a theology, rather than regarding it as merely a political vision.’ There was no reason to conflate the two movements, as he saw them: ‘the birth of social conscience in the Church’ and the secular dependency theory, the idea that the development and enrichment of some countries happens due to the exploitation of others. One of his more compelling arguments was that some corners of the traditionally anti-capitalist Latin American Church viewed their societies as capitalist. Most countries in the region, he argued, housed ‘precapitalist’ economies that were ‘disproportionately state-directed’.
Novak’s critique bears the liver spots of Ronald Reagan-era foreign and economic policy, believing that liberation theology’s ‘main enemy’ was the US, a ‘hostility’ that he said ‘embodies a kind of liberation theology’ in itself. In rejecting Western ideas of ‘distinctions between religion and politics, church and state, theological principles and partisan practice’, liberationists who wouldn’t take communion with Augusto Pinochet were postponing the Christian virtue of loving their enemy.
Spiritually, the critiques ran stronger. Some said the overtly political nature of liberation theology meant that it reduced salvation to action taken by humans, not God. By prioritising social action, it also undermined the principle of spreading the good news and saving individuals soul by soul. Some argued that the ‘preference’ for the poor ran against the universal Christian message. A Church that came from below, such as was found in Base Communities, ran against the hierarchical nature of the Church.
Perhaps most powerfully, liberation theology simply never strongly resonated with the people for whom it was supposed to serve. ‘In many ways, it was an overly intellectual construct,’ Chesnut says, a charge that the movement has never been able to overcome. At the same time, many of the churches ministered by liberation theology priests were departing from traditions, and ‘clearing their churches of the saints’, Chesnut says. For so many Latin American Catholics, the saints and the Virgin Mary were a significant part of their faith. In the end, ‘these liberationist parish churches came to resemble Protestant churches.’
The refined hands that crafted liberation theology were in stark contrast to the calloused palms that would ecstatically whoop and clap
Whether liberation theology would have been able to succeed without the Vatican’s intervention continues to be debated. It didn’t matter. Ratzinger appointing conservative bishops across Latin America helped to salt the earth and ensure the movement would be largely confined to small corners of academia. Fixated on combatting internal enemies, they failed to see what was happening out of their stained-glass windows. By 1992, the year when Boff left the priesthood before receiving a second silencing order, a Protestant church opened every two days in Rio de Janeiro. Soon, Macedo’s Universal Church of the Kingdom of God was opening two new churches each week. Reflecting on the seismic time, Ratzinger would blame the ‘widespread exodus’ of the faithful to Pentecostalism on ‘politicisation of the faith’ by liberation theology.
There were many reasons why people began converting, but doing it in reaction to the politics of a movement that struggled to move much beyond intellectual circles isn’t one of them. The refined hands that crafted liberation theology in darkened rooms were in stark contrast to the calloused palms that would ecstatically whoop and clap and promise the power of healing could be channelled through them. Too busy trying to defeat the insurgency from bookish Latin American priests, they failed to see the insurgency happening outside the cathedral walls.
Even then, it’s been argued that the Vatican managed to destroy the men, but not the ideas. As the US journalist John L Allen Jr points out, the ‘four elements of the liberation theology movement – the preferential option for the poor, structural sin, Base Communities and the “see, judge, act” method – have largely withstood the test of time.’ Chesnut argues that the current Brazilian president Lula and his Workers’ Party are the heirs to the Base Community movement that many of its figures had grown up in: ‘Lula and many others essentially migrated from the Church as they became politicised.’ Ironically, Pope Francis, once an opponent of liberation theology, may also be one of its few remaining proponents, even if he doesn’t expressly use those terms. In 2022, Ratzinger’s successor said that, sociologically, ‘I am a communist, and so too is Jesus.’ He is also rumoured to have personally consulted Boff on some of his recent encyclicals.
But the stark reality is that liberation theology, which wanted to build the kingdom of heaven on Latin American soil, has retreated further into academia, far from the mobilising force on the ground that it once promised. Whether it would have thrived without Ratzinger’s determination to stamp it out is a live question, but it’s one that may be best answered by the continuing rise of Pentecostalism and the prosperity gospel in the intervening 40 years. ‘When you’re poor, you often have acute and immediate afflictions, and you’re looking for immediate solutions to them,’ Chesnut says. ‘The problem with liberation theology was that it promised long-term structural solutions.’
The priests of liberation ‘opted for the poor, but the poor themselves opted for Pentecostalism,’ says Chesnut. But as the Spirit-led faith and its prosperity doctrine continues its unrelenting march through Latin America, it may just be that the very orthodox Catholic hierarchy in Europe that instigated the intellectual civil war in its own ranks is paying the greatest price."
"History is always political, and contest over it is a good thing | Aeon Essays",,https://aeon.co/essays/history-is-always-political-and-contest-over-it-is-a-good-thing,"At present, describing historians as political actors evokes bias, political manoeuvring and a lack of critical thinking. This description conjures up historians merely as political pundits, rummaging through history in search of evidence to support their own political goals and potentially falling into presentism. The past few decades have seen the rise of this hybrid profile, and while some have claimed that politicians need historians so that we can transform current political debates and use their expertise to help us project ourselves into the future, critical voices have warned that ‘rapid-fire’ superficial histories might serve political aims at the price of historical accuracy.
Therefore, defining J G A Pocock (1924-2023) both as a historian and a political actor stands in need of clarification since, arguably, he does not fit into a two-camp debate on the usefulness of history, but instead he shows how history inhabits us at a much deeper political level.
Originally published in 1975, Pocock’s book The Machiavellian Moment is an acclaimed masterpiece and one of the most influential 20th-century works for intellectual historians, political philosophers and political theorists. By 2025, it will have inspired scholars and public debates for 50 years. The Machiavellian Moment presents a fluid, non-linear and geographically diverse history of republicanism as a transatlantic political language that can travel among different periods and contexts, namely, from classical antiquity to Renaissance Florence, early modern England and colonial America.
Niccolò Machiavelli (detail, 1843) by Lorenzo Bartolini, Uffizi Galleries, Florence. Courtesy Wikipedia
The book generated academic and wider public controversies, since Pocock decentred the history of the foundation of American politics when he placed the American Revolution as only an episode of an Atlantic republican tradition. In other words, he traced the intellectual origins of the foundation of the United States as far back as the ancient Aristotelian ideal of citizenship and Florentine civic humanism. In doing so, he challenged, first, the understanding that the US Declaration of Independence was a pinnacle of modernity, the deliberate and singular foundation of a polity, and, second, the view that the debates surrounding America’s foundation were coined in a liberal vocabulary. In Pocock’s interpretation, these debates were neither fully liberal nor completely unprecedented in history.
This revisionist history was, additionally, written by a London-born New Zealand expatriate living in the US. He was a citizen of the British Commonwealth whose work revised the existing narratives about a former British colony reclaiming its political identity and cultural independence. In a way, this was a familiar story, since he experienced the contestation of political identities as an integral part of the history of the British peoples and conceived of British history as the history of several nations interacting with an imperial state.
D espite its significance, Pocock’s best-known book was not originally intended for the general reader, and it is famous for its evocative style and erudite character. On the contrary, he himself acknowledged that his interlocutors were highly specialised academics. The Machiavellian Moment , he later admitted, ‘was intended to be difficult’, written in a ‘complex and discursive style’ not meant to simplify the contradictions and complexities that were present in the story he was trying to tell. Given the breadth and depth of his work, it is no wonder that the substantive conclusions argued by Pocock remain relatively opaque or misunderstood.
The Machiavellian Moment is a study of the formation and transmission of classical republican ideals in the Western world. In doing so, it offers a sweeping view of the survival of the Aristotelian concept of the good life based on active citizenship and civic virtue, and the effort to avoid corruption and political instability. Three historical moments sequence this story: Florentine Renaissance, 17th-century England, and the American Revolutionary context. Niccolò Machiavelli and James Harrington are the primary drivers of this transformation and, in terms of the conceptual constellation around which republican language revolves, concepts like time, virtue, corruption and liberty are central. Pocock shows how Machiavelli, in line with Aristotle’s concept of active and virtuous citizenship, was particularly concerned with sustaining civic virtue in a moment of instability and decay in Florence, which precisely points to the expression ‘Machiavellian moment’ as the difficulty he faced in reconciling an ideal of citizenship with the uncertain and temporal character of republics.
Two other Machiavellian moments are later framed within a republican mindset, which according to Pocock shows the persistence and coherence of this tradition over time and space. On the one hand, Harrington’s goal in The Commonwealth of Oceana (1656) was the design of an ‘immortal’ English commonwealth – again trying to escape corruption and finitude – which authors inspired by Harrington later adopted in the 18th century. On the other hand, in founding the American republic, the Federalists were concerned with representative institutions, arguing that strong constitutional arrangements could save the republic from corruption. During that time, it was commerce and the rise of commercial society that posed the most significant threat, as wealth encouraged corruption. In Pocock’s account, their concern was preserving virtue and political stability, which required stopping members of society, including political representatives, from indulging in luxury, selfish passions and increasing their economic power at the expense of the public.
It was either Machiavelli or Locke who provided the philosophical underpinning for a new society
As what Pocock later called a ‘tunnel history’, The Machiavellian Moment revitalised the presence of republicanism in the history of political thought by mapping across centuries the efforts to maintain struggling republics. But it was the book’s final chapter – ‘The Americanization of Virtue’ – that had Pocock involved in most controversies. Pocock linked the American Revolution with classical republicanism as far back as Machiavelli via Harrington’s influence in Britain. He wanted to show how the language of classical republicanism was present in the nation-building efforts to guarantee popular virtue against the corruption and decay represented by commerce. Crucially, this meant that a modern US society retained early modern values, and the American Revolution, instead of marking a break with the old regime, was a chapter of European history. The Declaration and Constitution were thus not completely unprecedented, which in a way minimised their significance by partially dissolving them into longstanding political languages with roots in the old world.
George Washington (1795) by Giuseppe Ceracchi. Courtesy the Met Museum, New York
In contrast to Pocock’s views, Leo Strauss and his followers, for instance, held that there were only superficial connections between republicanism and the US founding, and that the exaggerated continuity of republicanism missed the influence and inaugural character of liberalism. They contended that Pocock, in highlighting the presence of republican pasts, had left little space for liberal ideas and liberal thinkers like John Locke . Louis Hartz’s The Liberal Tradition in America (1955), under the influence of C B Macpherson, had also pictured Locke as a honorary Founding Father, and liberalism, with its defence of individualism, commerce and limited government, as providing the philosophical basis for an emerging republic. The stage was set for a debate on antithetical positions: republicanism, rooted in the paradigm of civic humanism, reason and virtue, was contrasted with liberalism, defined by possessive individualism, emerging capitalism and private passions. The virtuous citizen/patriot stood opposite to the economic man, and it was either Machiavelli or Locke who provided the philosophical underpinning for a new society.
The idea that republicanism and liberalism were mutually exclusive political traditions and languages was a historiographical as well as a political commonplace and remains a widely accepted framework for understanding political discourse even to this day. The historian David Craig has noted that Pocock’s book helped popularise, as well as problematise, a clearcut division between liberalism and republicanism. Commenting on The Machiavellian Moment a few years after its publication, Pocock acknowledged that the book ‘consistently displays republicanism as being at odds with liberalism,’ although what he wanted to convey was the existence of a complex tension between republicanism and liberalism in the minds of America’s Founding Fathers.
Particularly with his last chapter, Pocock was successful at being provocative and fuelled a debate that resonated at historiographical, cultural and political levels. Time, politics and context were intricately intertwined categories. How people understand their pasts, and the history of how they write about their pasts, have political implications and determine (not entirely but crucially) the political experience at any given time. It is historians who undertake the fundamental tasks of writing and rewriting historical narratives and, in doing so, shape present political identities. Following Pocock’s approach, history is the most powerful element in the construction and destruction of self-knowledge of political societies, and one of the engines of a shared sense of community. Pocock’s words in this regard stand out: ‘what explains the past legitimates the present and moderates the impact of the past upon it’. In other words, Pocock’s critique of a liberal past also entailed a challenge to a liberal identity. It is in this sense that history fundamentally matters and historians are political actors.
F or an author who has devoted a great deal of attention to the political implications of societies’ historical imaginations, it makes sense to turn the spotlight on him and ask how The Machiavellian Moment was read politically in different contexts. This is an opportunity to contextualise Pocock’s text within a wider framework of reception and draw on some details of his professional biography – fittingly, as contextualism was a cornerstone of his own method.
The debates over the reception of The Machiavellian Moment have come to form a historiographical debate of their own. In the years following its publication, Pocock wrote several pieces addressing his numerous critics, both in the US and elsewhere. For sure, he wrote as a specialised historian, although the political implications could hardly be missed. One of them was the essay ‘ The Machiavellian Moment Revisited: A Study in History and Ideology’ (1981), in which the reader gets to dive into the ideologisation of his book. Pocock provides a justification of his own approach to republicanism among different historiographical (and ideological) schools. For him, the last part of the book presented two conceptions of liberty, the ‘active-participatory’ and the ‘negative-liberal’, which were both used in the political scenario of the 18th century and at the time in which he wrote the essay. It was the coexistence of republican and liberal ideas and the possibility of using the language of republicanism that some American historians later perceived as a threat to the liberal foundational constitutionalism, that is, as a problematic rewriting of the historical narratives underpinning political discourses. Scholars on the Left like Joyce Appleby, Isaac Kramnick or John P Diggins saw Pocock’s doubt that Locke, Montesquieu and David Hume provided the basis for the Federalist papers as doubt about America’s political character and cultural identity as derived from liberal values, that is, as an alleged departure from individualism, constitutional pluralism and commercial values.
His aim was to problematise the intellectual origins of the foundational myth as exclusively liberal
America, Pocock said, became a ‘very unclassical’ republic and that is why, he contends, a foundational myth served a political purpose and had the potential of speaking to contemporaries: it was a nation, he wrote , ‘founded in experiment’, in which a covenant created a bond among individuals. According to this so-called foundationalist myth, US history and society originated thanks to a conscious and deliberate act, or rather as an act that could be separated into two moments: the Declaration of Independence in 1776 and the ratification of the Constitution in 1788. The language of liberal rights, most notably articulated by Locke, served as the blueprint in this process. In this ‘foundational culture’, citizens judge the performance of the republic according to the observance of these principles. And therefore the existence of political corruption would not only mark a decadence, but also shake the very foundation of the (liberal) republic. In other words, liberalism became integral to the nation’s identity, and doubts about this correspondence amounted to attacks upon a cultural identity. What was at stake in situating either Locke or Machiavelli at the roots of US political history was Americans’ own understanding of who they were and are. In Pocock’s words in 2017, ‘in debating the fundamentals of their government, Americans debate who they essentially are.’
This debate was by no means confined to US academia. In this sense, Pocock was accused by Italian historians including Renzo Pecchioli of being an exponent of the ‘ ideologia americana ’. For instance, Cesare Vasoli reproached The Machiavellian Moment for being less a work of history than a work of ideology targeting the political roots of US culture. Pecchioli argued that Pocock’s interpretation pictured the US as the culmination of a republican tradition originating in Europe, and in doing so it positioned Florentine republicans as one link in the chain of a global republicanism. Reclaiming the singularity of European Renaissance republicanism, Pecchioli labelled Pocock’s history of republicanism as ‘neoliberal’, by which he meant that the global continuities described by Pocock masked instead a form of appropriating and undermining the significance of European traditions. Pocock was thus reduced to representing an ‘American liberal imperialism’ and, being unaware of his own ideological bias, he had established a liberal history of republicanism. Interestingly, the tentacles of this allegedly liberal expansionism even seized its political counterpart, republicanism. Pocock defended himself with his elegant and sharp rhetoric by showing that his Italian critics had misunderstood his conclusions, and that, quite on the contrary, the so-called republican thesis was not a strategy aimed at imposing an ideology of American liberalism onto the trajectory of European history. His aim was not to uphold American traditional liberal values, but precisely to problematise the intellectual origins of the foundational myth as exclusively liberal.
In sum, Pocock was paradoxically under attack for being too liberal, but also for not being liberal enough. Said differently, Pocock was criticised for being too American and for not being American enough – all the more intriguing for a New Zealander. In a way, these examples vividly illustrate the existence of a powerful but too simplistic correspondence between American identity and liberalism that Pocock’s Machiavellian Moment precisely challenged.
Pocock was never comfortable engaging in these debates, as he himself acknowledged. In the foreword to the French edition of The Machiavellian Moment , he wrote that the book had been ‘too successful for his own comfort’, creating a ‘vehement’ debate in a ‘confused and complicated scene’. Still, he was acutely aware that historical narratives are read differently in different contexts, since – as a historian of historiography himself – he devoted his career to situating ideas in the contexts to which they belong.
P ocock died in December 2023, a few months short of his 100th birthday. His death sparked well-deserved tributes across the world and heartfelt reflections on his legacy, with a number of academic events subsequently organised in his memory. Participating in some of them, I found that, when approaching his extensive body of work, a two-fold commonsense division emerged. On the one hand, one could look at his historical practice, that is, his studies on the history of legal and political thought and the history of historiography, where a number of monographs stand out: The Ancient Constitution and the Feudal Law: A Study of English Historical Thought in the Seventeenth Century (1957), The Machiavellian Moment: Florentine Political Thought and the Atlantic Republican Tradition (1975) and the six-volumes of Barbarism and Religion (1999-2015), his study of The History of the Decline and Fall of the Roman Empire (1776-1788) by Edward Gibbon.
On the other hand, Pocock was also celebrated for his theoretical and methodological contributions on how to study the history of political thought, mostly spread across journal articles and essays, and many of them collected in Politics, Language, and Time (1971), Virtue, Commerce, and History (1985) and The Discovery of Islands (2005). Among those who have praised Pocock’s methodological contributions, many have emphasised the significance of the notion of ‘political language’ as an idiom, rhetoric or specialised vocabulary (such as the language of ‘common law’, ‘civil jurisprudence’ or ‘classical republicanism’). Political debates can be conducted in a variety of languages (notice the plural form), since languages could coexist with each other, be adopted by different authors and travel between discursive contexts variously located in time and place. A somewhat less discussed theoretical point in Pocock’s work is his view of the intimate link between historiography and politics, which led him to believe that histories are political narratives that should be perpetually open to discussion.
Historians narrate what was and is to be admired or despised, imitated or avoided
In the two-fold division described above, his substantive monographs and his methodological writings are typically mapped separately within Pocock’s impressive production and often presented as independent from one another. This would suggest that, depending on the reader’s interest and focus, one could engage with Pocock as a historian of political thought while possibly remaining unaware of Pocock the theorist and political actor, and vice versa. However, an interesting path (among many) into Pocock’s oeuvre involves looking at how his insights on historiography illuminate his work as a historian of early modern republicanism and the political debates surrounding the reception of The Machiavellian Moment .
I take Pocock’s extensive reflections on the role of historians in the abstract as an invitation to consider his own writings in this light: he personified, according to his own formula, a historian who is also a political actor. In this picture , expert historians are prominent public figures who are far from comfortable seclusion in so-called ivory towers. Historians occupy a central, privileged position for crafting and re-crafting shared meanings and political identities. They narrate what was and is to be admired or despised, imitated or avoided.
Pocock’s views are related but not identical to those of his close associate Quentin Skinner. In defending the importance of contextualising intellectual history, Skinner had to counter accusations of antiquarianism and address critiques about the public irrelevance of historical knowledge. Building on this defence, Skinner has shown , for example, that political freedom has been historically linked to the absence of coercion, thus highlighting that these insights could be potentially helpful when navigating present-day politics. His strategy has been to underline that the past could be put to use in the present. In doing so, he nicely articulates a defence of the role of the historian as a public intellectual who brings together past, present and future, but his point remains susceptible to the pitfalls of political punditry.
While for Skinner history can be a political tool , Pocock’s claim is that history writing is, to some extent, political in itself . Pocock’s intricate relations between history and politics crucially merges the polity and the academia, that is, political debates and academic discourses are not simply in a constant dialogue, but rather they are forms of each other. In this sense, according to Pocock, historians are not potential partisans in disguise who use and abuse historical records for political purposes. History is not to be used for political intervention, but instead we get to inhabit the history that we believe. The downside is that the historian’s role carries an almost too heavy weight, as is apparent when examining the reception of The Machiavellian Moment .
I deas about the inseparability of history and politics run through many of Pocock’s theoretical essays. For instance, in ‘The Historian as Political Actor in Polity, Society and Academy’ (1996), republished in Political Thought and History (2008), Pocock radically asks: ‘what kind of political phenomenon is a history?’ and ‘what kinds of political reflection, or theory, may the various forms of historiography constitute?’ His replies emphasise the circularity of history writing as a political act and inevitably lead us to picture a degree of both contestation and consensus in public debates.
Contestation logically implies the existence of different narratives or positions on historical events. History has therefore shed its singular nature and embraced pluralism, which manifests in the existence of several possible ‘histories’ and ‘pasts’ – within the constraints of evidence. Pocock is far from falling into the relativist trap, by which, if anything goes, everything goes. In terms of what can be said about the past, he emphasises that histories are invented but also verified, which means they are both ‘discovered’ and ‘constructed’. In turn, the existence of multiple histories also leaves us with the possibility of upholding multiple political identities coexisting within a polity. In this regard, Pocock ends up giving competing stories about a polity a prominent place in his theory, as these stories constitute political identities, foster a sense of belonging and otherness.
He further elaborates on these ideas in the article ‘The Politics of Historiography’ (2005). The processes by which political societies ‘acquire pasts’ and ‘re-tell contested narratives’ in ‘endless’ and ‘multiple’ ways enhance citizens’ experiences. In fact, narratives form historical ‘myths’ that function to ‘uphold the continued existence’ of societies, that is, to bind them together and establish their autonomy and sovereignty. This is not to say that historians should be mere instruments of governmental propaganda (although they might have been), but rather that a degree of disagreement and pluralism is integral to both the historian’s craft and the citizen’s experience.
There was no history without politics, and no politics without a contested political identity
Pocock’s career and biography might provide some clues to further contextualise his approach. Although he moved to the US in 1966 and remained there until his death, Pocock grew up and spent most of his early life in New Zealand. He travelled between Britain and New Zealand while doing a PhD at the University of Cambridge, yet he retained his New Zealand citizenship and considered his home country to be instrumental in shaping what he called an ‘antipodean perception’ of history. The Machiavellian Moment , which he conceived during the years spanning his own transit between ‘the South Pacific Ocean’ and ‘the Mississippi Valley’, is fittingly also a history determined by the ‘voyaging’ of people and ideas. As he said in his Valedictory Lecture (1994), he had traced the ‘journeying’ of the Atlantic republican tradition. A bold statement follows, struck through by his own pen in the manuscript of this lecture, admitting that ‘only a mid-Pacific being … can truly develop a mid-Atlantic perspective’.
Being part of a family of settler descent, histories were naturally in motion, determined by ‘voyagings’ and generated by settlements and contacts. British history, which for him included the American Revolution, was a global phenomenon with locations in two hemispheres. And, as far as we accept that histories and political identities are inseparable, a sense of political contestation easily arises from a contested history. When thinking of history as the creation of autonomy, sovereignty and political identity, history writing sets the scene for a ‘contest for power’ in a postcolonial world. Both hegemonic and ‘subaltern’ (ie, subordinated) positions generated non-final histories that were never quite settled and were required to find ways to enter into a respectful dialogue. It is with these theoretical points in mind that I suggest framing the controversies generated by The Machiavellian Moment . The insistence among many American academics in reading Pocock’s work as a challenge to US history and identity was artificial from the perspective of a historian like Pocock who was ‘never quite at home’ and who had personally witnessed the struggles that configurate the political identities of the ‘British peoples’. For Pocock, there was no history without politics, and no politics without a contested political identity.
While attempting to read Pocock’s own career and contributions to the history of political thought as part of contemporary political debates, I am purposely avoiding simplistic definitions about his political sign in present-day terms. Many critics understood that The Machiavellian Moment was an attack of contemporary liberalism per se , but in fact his belief in the inherent contestability of histories accordingly required a robust pluralism and organic liberalism. Allowing different and sometimes opposite points of view was, he argued, a necessary condition for the historical profession to take place. To put it differently, the historical profession, as Pocock envisaged it, required a liberal political setting: ‘History is a field of study in which many explanations can, and must , exist together.’ (My emphasis.) In a sense, all history writing enables the creation of multiple worlds and facilitates a constant redefinition of the continuities and discontinuities that link past, present and future."
The rule of law and racial difference in the British Empire | Aeon Essays,,https://aeon.co/essays/the-rule-of-law-and-racial-difference-in-the-british-empire,"Law was central to the British colonial project to subjugate the colonised populations and maximise their exploitation. Convinced of its superiority, British forces sought to exchange their law for the maximum extraction of resources from the colonised territories. In The Dual Mandate in British Tropical Africa (1922), F D Lugard – the first governor general of Nigeria (previously governor of Hong Kong) – summed up the advantages of European colonialism as:
Lugard, here, expresses the European orthodoxy that colonised territories did not contain any Indigenous laws before the advent of colonialism. In its most extreme form, this erasure manifested as a claim of terra nullius – or nobody’s land – where the coloniser claimed that the Indigenous population lacked any form of political organisation or system of land rights at all. So, not only did the land not belong to any individual, but the absence of political organisation also freed the coloniser from the obligation of negotiating with any political leader. Europeans declared vast territories – and, in the case of Australia, a whole continent – terra nullius to facilitate colonisation. European claims of African ‘backwardness’ were used to justify the exclusion of Africans from political decision-making. In the 1884-85 Berlin Conference, for example, 13 European states (including Russia and the Ottoman Empire) and the United States met to divide among themselves territories in Africa, transforming the continent into a conceptual terra nullius . This allowed for any precolonial forms of law to be disregarded and to be replaced by colonial law that sought to protect British economic interests in the colonies.
In other colonies, such as India, where some form of precolonial law was recognised, by using a self-referential and Eurocentric definition of what constituted law, the British were able to systematically replace Indigenous laws. This was achieved by declaring them to be repugnant or by marginalising such laws to the personal sphere, ie, laws relating to marriage, succession and inheritance, and hence applicable only to the colonised community. Indigenous laws that Europeans allowed to continue were altered beyond recognition through colonial interventions.
T he rule of law was central both to the colonial legal enterprise and to the British imagination of itself as a colonial power. Today, the doctrine of the rule of law is closely associated with the works of the British jurist A V Dicey (1835-1922) who articulated the most popular modern idea of the rule of law at the end of the 19th century. The political theorist Judith Shklar in 1987 described Dicey’s work as ‘an unfortunate outburst of Anglo-Saxon parochialism’, in part because he identified the doctrine as being embedded within the English legal tradition and argued that the supremacy of law had been a characteristic of the English constitution ever since the Norman conquest. In his germinal Introduction to the Study of the Law of the Constitution (1889), Dicey noted three key features of the rule of law: firstly, the absence of arbitrary powers of the state; secondly, legal equality among people of all classes; and, lastly, that the general principles of constitutional law had developed as part of English common law, rather than being attributed to a written constitution.
Despite Dicey’s attempts to claim the doctrine of rule of law for the English legal tradition, its earliest formulation comes from ancient Greece. The ancient Greeks contrasted the rule of law positively to the rule of the despot and the tyrannical possibilities of unfettered or arbitrary rule. The doctrine has developed significantly over the past two centuries and can be divided into two main types: formal and substantive ideas of rule of law. Seeking to divest the rule of law from ideas of justice, the formal or thin version of the doctrine is best encapsulated by Joseph Raz in The Authority of Law: Essays on Law and Morality (1979):
The rule of law was used to give a gloss of moral legitimacy to the colonial enterprise
For formalists, the value of the doctrine lies in its ability to define the lawful authority in any jurisdiction; the constraints that it seeks to put on executive power, and its role in allowing individuals to plan their own lives in light of open, general, clear and reasonably stable rules of governance. On the other hand, modern substantive theories of rule of law associate the doctrine with various ideas of ‘good’, be it democratic government, or protection of human dignity and rights, or notions of liberty. In response to Raz, Tom Bingham noted in The Rule of Law (2010):
While they may define rule of law differently, both schools champion its importance both to the state and to the citizenry. I argue that, in the British Empire, the doctrine of the rule of law was similarly championed and used to give a gloss of moral legitimacy to the colonial enterprise. In doing so, it helped to hide the policies of racial and colonial difference that undergirded colonial law and enabled the extraction of resources from the colony to the metropole. Far from meeting the lofty goals of substantive ideas of rule of law, the exercise of legality in the colonies could not even fulfil the prosaic promises of the formalist conceptions of the doctrine.
T he British Empire established its earliest colonies in the 17th century in North America and expanded rapidly in the 19th century. At its peak in the early 20th century, it covered a quarter of the world and ruled over 450 million people. Dicey’s exposition of the rule of law was an imperial project and Dicey himself was a frequent participant in the British debates about the empire and its moral and legal responsibilities. He sometimes recognised that the rule of law when imposed by one society on another may itself be ‘arbitrary and oppressive’, however, the doctrine itself, as he understood it, was fundamentally sound. He attributed the discrepancy to the fact that certain civilisations were too ‘backward’ to appreciate its benefits. Despite these reservations, Dicey held a positive view of the British Empire and its commitment to rule of law, and noted that: ‘The one permanent, certain, indisputable effect of English government in the East has been the establishment of the rule of law.’
The British viewed their claim to have established the rule of law as a great achievement and an important benefit of the empire as it spread across the globe. It was central to Britain’s self-perception as a coloniser. Rule of law not only stood in direct opposition to the rule of the ‘oriental despot’, it also distinguished the British from other European colonisers such as the Spanish and the Belgians who were deemed to be brutal and inegalitarian. In practice, the doctrine was reserved for those seen as ‘civilised’ enough by the British imperial officers. When they wished not to extend the doctrine, imperial officials deemed certain communities too backward to merit its application. For instance, most colonised people were denied the right to a jury trial. Further, judges serving in the colonies, far from being independent, were appointed ‘at pleasure’ and were expected to be loyal to the colonial state, with their office being subject to executive removal. Any attempt by them to extend the rule of law to the colonised population against the perceived interests of the colonial regime led to the swift removal of the judge from office. In one such case, the Privy Council advised the removal of Joseph Beaumont as the chief justice of British Guiana in South America in 1868, on the grounds that he lacked ‘judicial temper’ and tended to embarrass the colonial government by criticising their practices against indentured labourers in the colony.
When from time to time attempts were made in the colonies to honour the principle of equality under the rule of law doctrine, these measures were limited. For instance, in the case regarding the institution of taxes in newly conquered Grenada, in Campbell v Hall (1774) almost echoing modern substantive – especially modern, rights based – ideas of rule of law, Lord Mansfield noted: ‘An Englishman in Ireland, Minorca, the Isle of Man, or the Plantations, has no privilege distinct from the natives while he continues there.’ Yet, the widespread practice of slavery across the empire, and other forms of colonial and racial difference belied Mansfield’s premise.
In The Nation and its Fragments: Colonial and Postcolonial Histories (1993), Partha Chatterjee posited that the rule of colonial difference underlies all colonial legal systems. That is, despite the supposed liberal ideology of the coloniser, the empire could operate only through a preservation of the superiority of the ruling group over the colonised population. This hierarchy between the coloniser and the colonised was not a byproduct of the system, but the very object that every colonial legal system was created to instil, maintain and justify.
Some were placed outside the ambit of the rule of law while still being subject to law’s coercion
A subject’s race and the so-called dichotomy between the ‘civilised’ and the ‘savage’ were central to the application of law in the colonies. These distinctions were based on ideas of intrinsic physical and biological difference between different populations, with the white Anglo-Saxon man placed at the apex of both the racial and cultural hierarchies. Those who were considered racially ‘inferior’ were also considered to be culturally ‘backward’, with each category serving to reinforce the other. This involved the linking of previously value-neutral physical attributes or cultural practices and assigning to them value-laden interpretations, either positive (as in the case of the ruling races) or negative (as in the case of colonised races).
A stark example of this idea of racial difference can be seen in Lord Sumner’s Privy Council judgment in Re Southern Rhodesia (1919):
Race, thus, played an important role in determining the types of rights that were made available to the colonised populations. Based on where they were assumed to be on the ‘scale of civilisation’, some communities came to be placed entirely outside the ambit of the rule of law while still being subject to law’s coercion. At their most stark, the racial inequalities upheld by the empire took the form of slavery that dehumanised the African origin population and remained legal until the Slavery Abolition Act of 1833 fully came into effect in 1838.
For instance, the slave code passed in Barbados, known as An Act for the Governing of Negroes 1688, explicitly noted that the slave population ‘are of Barbarous, Wild, and Savage Natures, and such as renders them wholly unqualified to be governed by the Laws, Customs, and Practices of our Nation’. This justified the creation of a dual legal system, wherein ‘slave crimes’ were to be tried in slave courts without the benefit of juries. Such laws not only created ‘status crimes’ – ie, crimes that could be committed only by enslaved people, such as being a runaway, abusing a planter/free person, possession of weapons – they also created a dual system of punishment wherein only enslaved people faced brutal punishments that sought to attack their bodily integrity, including flogging, branding, dismemberment and other bodily mutilations. As a result, the dominant experience of colonial law from the enslaved peoples’ point of view ‘was of terror and violence’.
Elsewhere in the world, the Indigenous people belonging to what were described as ‘savage tribes’ in Australia were seen to be inherently outside the law, as were those who were deemed to be ‘hereditary criminals’ under the Criminal Tribes Act 1871 in colonial India. And, in blatant disregard for the doctrine of rule of law, these communities were often collectively punished for any crime by an individual of the group.
By contrast, during the 19th century, white settler colonies came to enjoy some of the freedoms that were seen to be ideologically linked to the rule of law. Across the empire, the colonising population was aware of the privileges granted to them under colonial law and reluctant to see them reduced in any way. Using Chatterjee’s framework, Elizabeth Kolsky argues in her study Colonial Justice in British India (2010) that the notion of rule of law was constantly contradicted by the institutionalisation of racial difference under the law, as well as the overt partiality of legal personnel, including the police, judges and juries. This led to one of the biggest legal controversies in colonial India, when the Ilbert Bill of 1883 was proposed to allow Indian magistrates to preside over cases involving European British defendants. After sustained protest by the white population, the Bill was finally passed in 1884 after securing the compromise of ensuring that they could be tried only by European British majority juries. Of course, similar provisions were not made for the Indian population.
Similarly, in the context of South Africa, Martin Channock argues in The Making of South African Legal Culture 1902-1936 (2001) that the doctrine of rule of law developed in the country ‘primarily along the racial frontiers’ and was used to restrict the rights of African and Asian people in the jurisdiction. He uses the example of the Natives (Urban Areas) Act of 1923 to show how increasingly arbitrary and despotic powers were exercised by local municipalities to remove Black Africans from urban municipal areas, including a regulation that empowered the local superintendent to not only remove people from an area, but to order their huts to be destroyed if they did not comply within 24 hours. And yet, in Tutu and others v Municipality of Kimberley (1918-23) this regulation was found to be neither ultra vires nor unreasonable.
With regards to the Asian population, various economic laws that sought to deny them licenses to trade in South Africa reflected the racial unease of the colonisers. Channock outlines three main concerns: the colonisers worried that the proximity of white housewives to Asian traders in the absence of their husbands may lead to inappropriate contact; that Asian traders extending credit facilities to poor whites may erode racial hierarchies; and, similarly, white women working in Asian shops may lose their sense of ‘racial superiority’. Thus, it is evident that, despite any claims to the rule of law, racial difference was built into the very edifice of the South African colonial legal system and lay the foundations for racial segregation through Apartheid later.
D irect racial discrimination was also apparent in the punishment meted out for crimes. Across the British Empire, the most severe punishments were saved for violence committed by non-whites against the white population. And if the perpetrator was white, punishment for white-on-white violence was a lot more rigorous than the punishment for acts of violence committed by white men against the non-white population. In large part, the latter kind of violence was an intrinsic and normalised part of the colonial capitalist structure that allowed ‘masters’ to have the ‘right of correction’ to brutally beat, flog, mutilate or confine their workers as and when they saw fit. Invisibilised by its omnipresence, routine and indiscriminatory violence by the colonising race remains one of the British Empire’s most closely guarded secrets.
Further, the issue of equal punishment for the same crime for people of different races had always been contentious, and arguments against it focused both on the supposed mental and civilisational differences between the races and their physical or biological differences. For example, in 1844 the legislative member Herbert Maddock argued for shorter jail sentences to be awarded to Englishmen in India on the grounds that ‘the heat of a crowded building surrounded by high walls’ was not at all injurious to the health of the native population but would have a detrimental effect on white prisoners.
Racial discrimination under the law was further entrenched through indirect means by restricting the access of the non-white populations to both legal education and legal professions. For instance, in Tanganyika, in the absence of any local legal training being available, the colonial government required a British law degree to practise law in the territory, while at the same time following a policy of preventing Africans from receiving scholarships to study in Britain. Similar policies were followed by the British elsewhere in Africa, thus, effectively excluding the non-white population from entering the legal profession in large parts of the continent, which no doubt helped to stifle local resistance against colonial law and governance.
Despite the endemic failure of the doctrine of rule of law in the colony, the suitability of the rule and its application were never questioned. Instead, the failure was blamed on the corruption of local officials both white and non-white, or on the backwardness and criminality of the native population. Both this ‘corruption’ and ‘backwardness’ were then posited as reasons for colonial rule to continue until the local population was civilised and advanced enough to accept the mantle of rule of law by itself.
The rule of the colonial powers was anything but stable, open and clear governance
A few key reasons point towards the inevitable failure of any substantive notions of rule of law in the colonies. Firstly, the concept could not overcome its origins. Despite its universal claims, rule of law could not transcend its European social roots and, thus, mostly remained an oppressive imposition patchily enforced when it was of benefit to the coloniser. This links to a second issue: as David Killingray notes in ‘The Maintenance of Law and Order in British Colonial Africa’ (1986), the concept of rule of law remained incompatible with the continuing need of colonial law to oppress and exploit the colonised population. Due to its very nature, the colonial state needed to possess autocratic powers: ‘Government was usually by decree or proclamation, while a battery of laws and reserve powers were directed at the maintenance and preservation of the colonial order.’ Thirdly, racial discrimination within the colony further weakened the commitment to rule of law. Indeed, as we have seen, despite the rhetorical stance of legal equality, legal practice and conventions awarded distinct privileges to the white population and frequently tolerated, and even excused, white violence against the non-white population.
Setting aside the substantive ideas of rule of law, the evaluation of the formalist notion of the doctrine in the British Empire also points to the failure of rule of law in the colonial setting. Within formalist ideas of rule of law, the thinnest conception of the doctrine takes the form of rule by law. Rule by law is the idea that law is the means by which the state conducts its affairs and, thus, easily collapses into the notion of the ‘rule by the government’. Such a doctrine places minimal limitations on state power, save seeking to offer protection to citizens and communities by restricting unfettered or arbitrary rule by the executive. Yet, even formalist notions of rule of law were regularly undermined by the frequent suspension of civil law through the invocation of autocratic martial law under which the colonised people’s already limited freedoms were further restricted and the rule of the colonial powers was anything but stable, open and clear governance. Across the empire, the British frequently resorted to martial law from the 19th century onward, especially in response to popular movements such as the Demerara slave rebellion of 1823 (in modern Guyana), the Indian Uprising of 1857, and the Mau Mau Uprising in Kenya in the mid-20th century.
Despite the obvious flaws in the rule of law doctrine in the British Empire, the discourse encountered an unexpected twist in the 20th century. As the struggle against colonialism intensified in Asia and Africa, British officials’ lack of commitment to rule of law in the colony came to be branded by the anticolonialists as ‘un-British’ and condemned as the ‘lawless law’ of British rule. On one hand, the idea of rule of law was denounced as simply being a veil to cover the colonial and capitalist exploitation of the colonies; on the other hand, colonised people actively chose to use the concept as a means of legal and political protection, resistance, collaboration and subversion.
Even a scholar such as E P Thompson , a Marxist historian who was critical of law as a device that mediates and reinforces existing class relations, valorised the idea of rule of law in Whigs and Hunters (1975) and described the British contribution to it as ‘a cultural achievement of universal significance’. In fact, Thompson, like others, justified the ‘goodness’ inherent in rule of law by arguing that Indian freedom fighters, including M K Gandhi and Jawaharlal Nehru, had used the idea of rule of law in their quest for Indian independence. However, as critics of the doctrine highlight, it is important to remember that when colonised people couched their own demands for greater rights in the conceptual language of rule of law, they did so as a strategic move to gain legitimacy and visibility for their causes, and not necessarily as a commitment to the doctrine itself.
At the same time, the anticolonialists’ choice to use the rhetoric of rule of law in their own movements, even if it was a choice made for strategic reasons, points to the endurance of some of the ideals associated with the concept. Despite its status quo-ist nature, and complicity with liberal capitalist regimes, the doctrine has come to stand as shorthand for justice, equality and democracy, which were precisely the objectives that the anticolonial struggles of the 20th century sought to achieve. The enduring legacy of the doctrine to both colonial and anticolonial agendas continues in the 21st century, where the promotion of rule of law has devolved into a multi-billion-pound industry. While furthering neo-imperialist global structures, international developmental aid is routinely tied to rule-of-law commitments and is forced upon postcolonies in the Global South; at the same time, resistance movements in these countries seek to use the concept of rule of law to denounce global capitalist exploitation.
This essay draws from Kanika Sharma’s chapter ‘The Rule of Law and Racial Difference in the British Empire’ in the book Diverse Voices in Public Law (2023), edited by Se-shauna Wheatle and Elizabeth O’Loughlin."
What San Francisco carpooling tells us about anarchism | Aeon Essays,,https://aeon.co/essays/what-san-francisco-carpooling-tells-us-about-anarchism,"In the summer of 1999, with nowhere to go between my second and third year in college, I decided to stay with my sister who was living with a couple of roommates in an apartment in Berkeley, California. Soon, I was fortunate to land a summer internship at the San Francisco office of Amnesty International, then located just across the bay in the city’s downtown.
The internship was unpaid, and my student visa barred me from getting a paid job. To earn my keep – or, more modestly, my next lunch – I often went straight to the campus of the University of California, Berkeley after work to be a subject in psychology experiments. Labs would pay $15 in cash for filling out a survey, or $20 if they had to clamp down my fingers with electrical sensors before displaying images on a screen. I would have bought a ticket for the chance to see what labs at a vaunted university were doing behind secured doors, but instead they were paying me for it.
To get to San Francisco, I joined in the daily ritual of the commuting masses on BART (the Bay Area Rapid Transit system), the morning solemnity interspersed with outbursts of underground performances and conductor announcements. But a few weeks into my internship, I heard about an intriguing alternative to BART: the Berkeley carpool, a free ride.
Photo by Damien Maloney
This was before Google, so the only way to find out how carpooling worked was to show up. Here’s what I discovered: you walk to a designated spot – a quiet residential street in North Berkeley – during the regular morning commuting hour. There, you’d find a tidy line of people on the sidewalk, and alongside it a lineup of cars by the curb. There are no signs, no instructions. One by one, a person gets into a car at the front of the line and the car takes off; the line keeps moving as more people and cars replenish it at the back. Everything moves along swiftly. When it’s your turn, you get in the first car in line – no swapping spots, you get what you get – and greet the driver. There are two destination options, both in the heart of San Francisco, and you tell the driver which one you’re headed to.
Then it’s a 20-minute ride across the Bay Bridge, past Treasure Island and into San Francisco, with glimpses of the bay, the sky and the downtown. The genius of this system was that, while the rider got a free ride, the driver got reduced highway tolls and a faster ride into the city for taking in a passenger and using the express carpool lane during rush hour. It enabled a true win-win situation for the two parties, with an extra win for the environment, to the extent that it reduced the number of cars on the road. It worked beautifully.
There was an implicit norm, I soon learned: you did not make conversation with the driver, and the driver did not make conversation with you. Almost without exception, these rides were quiet ones, focused on getting from here to there. This served me just fine; I didn’t care much for small talk at 8 am, not before my morning coffee. Nevertheless, it was a constant source of wonderment that total strangers could reliably give me rides to work, only to be never seen again. There was a potential for a story to emerge if a pair decided to converse, but for me the rides were the story.
I’d play a mental game of luck: joining the line, I’d count the number of people in front of me, then count up to the car that would be my ride. Would this be my lucky day? The luckiest day – grant that I was 20 – was a ride in a sparkling Jaguar. The unluckiest I ever got was being covered in cat hair in a vehicle that was likely better off decommissioned, which was not so bad, all things considered.
Photo by Damien Maloney
A decade later as a graduate student, I read the Yale social scientist James C Scott’s The Art of Not Being Governed (2009) when it was first published, along with other works of anarchist history. Scott’s works revolutionised my thinking about authority and human agency. This was every bit the intention of Scott’s writing – to go against the grain of pervasive statism that underlies conventional notions of justice, good governance and social progress. In studying remote communities in Southeast Asia’s hinterlands, The Art of Not Being Governed offers analysis aimed at inverting our priors: where we might observe economic underdevelopment in the rural hills, Scott saw zones of relative autonomy from the state – a successful ‘evasion of subject status’. Where we might note high rates of illiteracy, held universally to be a social deficiency, Scott saw a deliberate safeguarding of oral traditions over generations – an act of ‘cultural refusal’. These communities’ rebuffing of written text was also strategic, as written histories risked exposing themselves to state authorities; illiteracy, then, was an ‘active choice’ made by the hill people in a state-centric world run by ‘text-based’ rulers.
Scott suggests we withhold our pity: while those living within the reaches of the state toiled under systems of taxation, conscription and servitude to state interests, the seemingly ‘uncivilised’ populations of the rural hills achieved a ‘purposeful statelessness’ that kept them free from all of the above. Scott’s scholarship does not ask us to buy these arguments wholesale so much as it pushes us to reconsider what the state stands for and how much agency our bases of knowledge afford to ordinary people as they navigate structures of power, whether in upland Southeast Asia or elsewhere. It asks us to do the hard work of continual questioning.
A few years later, as a professor of international relations, I included a segment on anarchism in one of my courses. This was the era of expansive state-building in Iraq and Afghanistan; soldiers and technocrats swooped in from abroad to install a new government designed by foreign officials, and sought to ram in the projects amid violent insurgencies jeopardising their plans. Though written in other contexts, Scott’s works served as a clarion call for a radically different approach. In Seeing Like a State: How Certain Schemes to Improve the Human Condition Have Failed (1998), Scott reminded us of the deep reserve of knowledge people held, organically, about their own communities, cultures and organisational systems, and urged healthy scepticism toward outsiders’ ability to prescribe solutions ‘from above’. When students invariably asked: ‘So what should the US do?’ in response to US-led foreign regime change run terribly amok, I used Scott to probe whether that was always the right question to ask – didn’t it preclude broader possibilities and voices? What alternatives can be envisioned? We discussed what Scott termed ‘high-modernist designs’ introduced by detached experts and their disregard for local knowledge, and the resulting mismatch between scientific ideals and reality. One could almost believe Scott was writing specifically about US foreign policy in the counterterrorism age, making his case against the imposition of grand, decontextualised, ahistorical social engineering projects that discounted those for whom the plans were ostensibly drawn – the people.
Everyday forms of nonviolent resistance can be carried out in many smaller but no less powerful ways
Anarchism has no single definition. Scott’s anarchism is not of the window-smashing anti-capitalist variety, nor is it a promotion of a Marxist utopia. It does not require people to live in communes and grow their own food, nor does it suggest an armed uprising to topple regimes. It is not even anti-statist per se, as Scott holds that the state ‘is the ground of both our freedoms and our unfreedoms’. Scott’s anarchism is one that allows me to embrace his scholarship while simultaneously, without intellectual incongruity, making a career of educating aspiring diplomats and civil servants and submitting my own child to public education, seen in other corners of anarchism as a bastion of statist propaganda.
Scott’s anarchism is, rather, a way of seeing the world, a ‘sensibility’ as he calls it, one that can be honed the way a bird watcher can train herself to hear the calls of a particular species, or a cook can learn to detect the early scent of a good char. It is not anti-statist but anti-oppression, pro-human and proactive. It is about noticing taken-for-granted social designs and erecting creative defences against powers and arrangements that chip away at one’s ability to exercise self-ownership. It is Scott following his ‘rational convictions’ and choosing to jaywalk when everyone else waited patiently for the light to turn at an intersection free of vehicle traffic. It is people’s feet forming the most direct and well-trodden path across a park’s lawn – known as ‘desire paths’ – in defiance of urban planners’ aesthetic inclinations. It is the city of Houston, where I live, declaring itself a ‘book sanctuary’ and ensuring its public libraries carried titles banned in schools, thus rendering state-imposed book bans de jure effected but de facto nullified in an unequivocal pushback against censorship.
The path of desire. Photo by Duncan Rawlinson/Flickr
Throughout history, communities have demonstrated a tremendous ability to think for themselves and, more than that, to resist, in an entirely pragmatic fashion, forces of domination and devise alternatives that help restore their agency. Scott’s anarchism is in this sense manifestly anti-authoritarian. There is a clear theme of subversion running through his scholarship. But what he brings to light, in particular in Weapons of the Weak: Everyday Forms of Peasant Resistance (1985), is people’s ability to carry out ‘petty acts of insubordination’ and acts of ‘quiet evasion’ against exploitative and repressive powers, the very opposite of violent revolts in the spectrum of resistance. Collective civil disobedience and mass protests are somewhat close cousins, but Scott insists that subtler, everyday forms of nonviolent resistance against domination can be found, and carried out, in many smaller but no less powerful ways. They may not always work, but when they do the result is a restoration of individuals’ sense of dignity.
Of the numerous examples of what it means to approach the world with an anarchist sensibility, I liked to use one that Scott offers in Two Cheers for Anarchism (2012): the contrast he draws between the Vietnam Veterans Memorial and the Iwo Jima Memorial, both located in the Washington, DC area. The Vietnam memorial is minimalist in its design: two black granite walls stretch across a lawn and meet to form a wide angle; they are engraved with the names – staggeringly numerous – of fallen US service members. The names are not listed alphabetically or by military rank, but chronologically in the order in which they fell. The memorial has no other adornments and makes no overt political claims. Rather, for Scott, ‘a great part of the memorial’s symbolic power is its capacity to honour the dead with an openness that allows all visitors to impress on it their own unique meanings, their own histories, their own memories.’ Those who lost a loved one will find the name they seek amid the multitude, then they might run their fingers over the engraved name, put a paper and pencil to it to make a rubbing, or leave mementos by the wall. The monument invites visitors to participate in meaning-making; it does not itself offer it. And, in so doing, it leaves room for as varied a view of the war and its legacies as is held by the public.
Vietnam Veterans Memorial. Photo by Howard Ignatius/Flickr
In contrast, the Iwo Jima Memorial (officially, the US Marine Corps War Memorial) is ‘manifestly heroic’, consisting of a colossal sculpture of a group of Marines raising the US flag atop Mount Suribachi on Iwo Jima in Japan in the Second World War. It captures a moment of military victory gained at a great cost in lives. For Scott, it is not surprising that the memorial is ‘monumental and explicit’ given the ‘virtual unanimity with which that war is viewed in the United States’. Yet, Scott suggests that the effect of the dramatic display is to foreclose the kind of reflection, participation and questioning the Vietnam memorial inspires; the Iwo Jima memorial is ‘symbolically self-sufficient’ if not ‘canned’. Visitors can ‘stand in awe’ and thus receive its message; they do not complete it.
Iwo Jima Memorial. Photo courtesy Wikipedia
This visually striking juxtaposition served well as an illustration of the anarchist sensibility. But as I taught it, I began to see that instruction of Scott’s ideas must itself hew to his anarchist principles to do his works any real justice. There was a fine line to tread between teaching Scott’s text and distilling the true spirit of his works, and intoning that Scott’s compelling commentary embodied the one true anarchist interpretation of the memorials, extolling one as artful while critiquing the other as patronising.
A true anarchist interpretation would grant that anyone who could appreciate the simple straight lines of the Vietnam memorial’s stone walls could also meaningfully reflect on the politics, triumphs and tragedy of the Battle of Iwo Jima despite – or perhaps because of – the ‘heroic’ display. Adopting an anarchist sensibility requires that one puts due faith in the ability of ordinary people to think for themselves, for their own betterment and the betterment of their communities. In his works, Scott – who died this summer, aged 87 – gives all the agency to people, turning our understanding of power upside down. People see through the machinations of authorities. They may not always push back given the risks, but they know they have a cache of capability to evade or thwart projects of manipulation and extraction.
To extend this line of thinking is to presume that ordinary people who come to view an evocative monument can question its overt message and will do so if curiosity leads. Given the battle in the Pacific War is perhaps the lesser known and the more distant in time, the vivid sculpture might in fact do more to inspire visitors to further learn about the war, and ultimately the horror of all wars, than might a less expressive one. Fundamentally, an anarchist sensibility would be open – the theme of this particular section of Scott’s book – to that possibility, and would thus give two cheers for the diversity of forms of public memorialisation that the nation’s capital offers.
In reading Scott’s text with students, I learned anew how teaching, too, can reflect the ethos of the Vietnam memorial as Scott saw it: egalitarian, open and participatory, with a preference not for parsimony over loud claims but critical thought and experience over intellectual hierarchies.
After all, if every war memorial looked like the Vietnam memorial, it would surely be a sight of authoritarian dystopia.
Had the carpool’s norms been codified, enforced with the threat of punishment for infractions?
And that’s when I recalled those summer mornings riding in strangers’ cars across the San Francisco Bay. I reflected, nearly two decades later, on the operational attributes of the Berkeley carpool. No one organised the carpool; it arose from within the community to meet people’s needs in the face of rush-hour traffic, highway tolls and commuter costs. There were no written rules; codes of conduct were implicit and shared through quotidian practice. A programme without formality, an establishment without walls – an institution of the most durable kind, as social science theories taught us. It worked, and all participants benefited.
I thought this illustration from our everyday lives would encourage students to seek out their own. Like looking up an old friend long out of touch, I searched online with some trepidation for information on the Berkeley carpool. Was it still around? Had some municipal office taken over its management and logistics? Had the norms been codified, enforced with the threat of punishment for infractions? Do these sorts of community projects, operating on nothing more than mutual trust and collective benefit, even exist anymore?
And there it was: the same system, intact after decades, its existence merely more visible in the digital age. ‘Casual carpool started 30+ years ago and has been community driven since its origin,’ the minimalist website states. ‘Everyone who participates in casual carpool benefits from using the system, therefore they have an incentive to treat other users respectfully and to promote safety in order to maintain the integrity of the system.’ All of its essential features remained unaltered. Still no rules, just some ‘etiquette’ guidelines: ‘Be friendly with other riders but know when they’re looking for a quiet ride … Don’t jump ahead of others, these lines move pretty quick.’
I read the final lines of its FAQs with a sense of redemption and reclamation: of a youthful memory, of a community’s capacity for problem-solving and self-organisation, and above all of the endurance of social trust amid the daily onslaught of headlines pointing towards its disintegration:
I half suspect the website’s text was written by a Bay Area college grad well versed in anarchist theory. And carpooling is no longer so arcane; similar programmes operate in other cities now. Regardless, I gave my own two cheers – for the unexpected convergence of a scholarly argument and its empirical manifestation, a critical reading and openness in a classroom leading, as ever, to discovery."
What is behind the explosion in talk about decolonisation? | Aeon Essays,,https://aeon.co/essays/what-is-behind-the-explosion-in-talk-about-decolonisation,"Decolonisation talk is everywhere. Scholars write books about decolonising elite universities. The government of India, a country that has been independent for 77 years, built a new parliament building in order to ‘remove all traces of the colonial era’. There are infographics on how to decolonise introductory psychology courses and guides on how businesses may decolonise their work places. Some Christians from regions that used to be colonies look to decolonise mission work through Biblical readings of Christ’s suffering. Why have expressions of decolonisation become so popular? And is there coherence to these many disparate uses of the term?
All these varied and even contradictory forms of decolonisation talk seek to draw upon the moral authority, impact and popular legitimacy of the 20th century’s great anticolonial liberation movements. And it is the gap between these movements’ promise of liberation and the actuality of continued power inequalities even after independence that has given the analytical and political space for such a wide, eclectic and contrasting array of individuals, groups and projects to wield the concept of ‘decolonisation’ to generate support for their endeavours. In the process, decolonisation talk has become more and more attenuated from the historical events of decolonisation.
The events of decolonisation involved colonised peoples, predominantly in Asia and Africa, rising up in the mid-20th century and overthrowing colonial systems of rule. National liberation movements that became postcolonial governments transformed the world order through the historical events of decolonisation. In 1945, for example, there were just 64 independent states, while today there are between 193 and 205, depending on who counts them. Before the Second World War, there were only three sovereign states with a Black head of state – Ethiopia, Haiti and Liberia.
Colonialism itself was uneven, complex and variegated. In practice, empires ruled by governing different communities differently, intensifying and maintaining often elaborate hierarchies of communities based on region, race, religion or ethnicity. For instance, colonial life looked very different in the French settler colonial cities of Algiers and Oran than it did in the Berber regions of Eastern Algeria. British colonial India was a patchwork of direct rule, princely states that were semi-autonomous regarding domestic policy, and excluded areas with a rather light imperial footprint, among other political configurations.
Other examples of colonial difference include the US insular island cases (1901), which determined that particular territories seized during the Spanish-American War would have unequal legal relationships with the continental United States: Puerto Rico, Guam and the Philippines were made into unincorporated territories where the US constitution did not fully apply, while Hawaii (and Alaska) were placed on the path to US statehood. Elsewhere, in East Africa, the German Empire recruited African soldiers who became a class of colonial intermediaries. The Japanese Empire included direct occupations in Manchuria and Korea with collaborations with anticolonial nationalists in Burma and Indonesia. Yet amid all the complexity, popular understandings of colonialism today have a clear iconography of Western conquest – of maps, pith helmets and boots bestriding non-Western continents.
I n contrast to the seeming clarity of colonialism (however much it elides that process’s own complications), current discussions of decolonisation can seem amorphous and slippery. This is not surprising since so many different people and groups are using the concept, at times at cross purposes. There are many forms of decolonisation talk drawn from the realms of culture, education, economics, politics, ideology, psychology, business, religion and more. They include postimperial (related to places that used to be empires or to institutions that used to facilitate empire) and postcolonial (related to places that used to be colonies, or to dependent power relationships created by colonialism) institutions and nations.
People across the political spectrum invoke decolonisation as an ideal and claim to represent its spirit. Some of these decolonisation discourses come from regions in what we now call the Global South, countries that were former colonies or have had continuously dependent economic relationships with postimperial countries and institutions. Calls for decolonisation coming from those regions seek more autonomy and freedom for these postcolonial countries, though they do not necessarily share the same political orientation. There are demands for economic decolonisation, as the New International Economic Order made in the 1970s. There are also claims for the need for cultural decolonisation, as the Indian government does today, which the anthropologist Alpa Shah considers a hijacking of the original concept.
Decolonisation talk also emanates from those affiliated with metropolitan or postimperial institutions, such as universities or corporations . Calls to decolonise curriculums, disciplines and university programmes seek to shift which ideas and communities should be the primary focus for these institutions’ prestige and resources. They highlight some of the seemingly surprising, even counterintuitive applications of decolonisation talk – that it can come from former centres and even agents of empire, such as universities that trained imperial civil servants. Even museums built upon collections amassed through colonial conquest promote their decolonising work.
During the summer of 1960, the United Nations seemed to recognise a new member state every week
We also hear people today use the term ‘decolonisation’ as a pejorative, in order to critique movements that seek to revise the contemporary status quo of international politics, such as those for Palestinian statehood. This is not an exhaustive set of cases. Readers will identify others. But we can see from the use of the term in economics, culture, education and political ideology that it’s a malleable concept and one a lot of people want to use and claim, in both celebration and critique. As the formal political independence moment of the mid-20th century has receded in time, decolonisation has grown less clearly tied to specific events.
Historical decolonisation, the 20th-century process where empires were broken up into independent states through a combination of warfare, protest and political negotiation – was the most significant global event since the Second World War. In the decades following 1945, more than 50 countries, primarily in Asia and on the African continent, gained their independence, mostly from European empires. The creation of newly independent states reached its apex in 1960, when 17 European colonies on the African continent gained independent statehood, such as the Republic of Congo, Nigeria, Madagascar and Somalia. During the summer of 1960, the United Nations seemed to recognise a new member state every week and national independence from colonial control became an international norm.
Countries had different paths to independence. India and Pakistan achieved self-rule (1947) through long, drawn-out, predominantly non-violent mass civil society protest movements. The Dutch were forced to accept Indonesia’s independence (1949) when they lost US financial recovery aid. Algeria (1962) won a war of national liberation. Zambia (1964) had a nationalist leader, Kenneth Kaunda, who held authority with fellow nationals as a legitimate anticolonial figure and also was recognised by US and UK mining companies as a reliable negotiating partner. Botswana became independent (1966) with its original capital located in a different country, as the UK government sought to quickly divest from an increasingly unpopular and financially unsustainable empire.
‘Decolonisation’ – the word for this transformation from a world of empires to that of postimperial and postcolonial states – was first a scholar’s or bureaucrat’s term, a description that gained popularity after the fact for the events it described. Outside of Frantz Fanon (himself a scholar) or some of the imperial civil servants that sought to constrain nationalist independence movements, few physically involved in the historic events of mid-20th-century decolonisation used the term itself at the time. Independence movements generally spoke of national liberation, which signalled the active purpose of their struggle. ‘Decolonisation’ has passivity embedded within. It does not point to who were the colonial subjects or who was the colonial ruler, who fought for independence or who fought to prevent it.
Since the word does not identify or signal who might be the specific actors responsible for ending (or perpetuating) colonial rule, it was a fundamentally small-c conservative label for revolutionary forms of politics – of regime change. The passive voice, the lack of a clear, active subject, has also helped to create analytical space for multiple and mutating meanings, which have made the term popular with governments and relatively powerful institutions today who can benefit from such strategic ambiguity.
A mid the moral legacy of national liberation movements’ pursuit of independence, the passivity embedded in the actual term, and the reappearance of decolonisation talk, it is worth remembering that, for many communities, the promise of such liberation remains largely unfulfilled. Not all peoples who sought national independence at the end of colonial rule received statehood. Kurds and Palestinians, Nagas and Tibetans, Catalans and West Papuans, among many others, have claimed independence without receiving the international recognition of statehood for their nationalist movement.
In addition, not all governments have fully enfranchised peoples claiming that they have been colonised and deserve national sovereignty: Uyghurs in China and Roma in Europe can attest to this. And for many postcolonial states, political sovereignty did not lead to economic empowerment in global systems of trade and resource extraction. Today, refugees and migrants regularly risk their lives in search of viable livelihoods, demonstrating the limits of the political decolonisation of the 20th century in fulfilling the goals of nationalist revolutionaries. Since the promise of decolonisation remains so clearly incomplete, the concept remains open to continuous interpretation.
Soon after the wave of political independence of former colonies in the 20th century, critics of neocolonial power relationships promoted the continuing need for economic decolonisation. As Margarita Fajardo has shown , Latin American international civil servants who founded and operated CEPAL – the UN’s Economic Commission for Latin America and the Caribbean – sought to broaden the project of political independence to include economic and social rights. Their endeavour, which began in the late-1940s, before national self-determination became an international norm, remains ongoing. Economic dependency theorists, cepalinos , advocates of the New International Economic Order and their heirs all called for decolonisation following political independence. They wanted to found a democratic global order of economically sovereign states with greater economic and social prosperity than that allowed by the postcolonial world order.
The Argentine economist and UN bureaucrat Raúl Prebisch emerged as a central figure in the movement for economic decolonisation. Prebisch’s leadership of CEPAL (1950-63) and then as founding secretary general of the UN Conference on Trade and Development (UNCTAD) supported the creation in 1964 of the group of 77 ‘developing countries’ (G-77) to ‘promote their collective economic interests and enhance their joint negotiating capacity on all major international economic issues.’ The G-77 was (and is, now made up of 134 countries) a group of Latin American, Asian, African and Middle Eastern countries that sought to reinforce each other’s political and economic sovereignty. They harmonised their views on global economic issues and worked for higher prices in global economic markets for the raw commodities their nations produced. The demand that greater economic equality – not just political independence – was part of decolonisation transformed what had been a set of movements to overthrow colonial rule into a mode of analysis employed by postcolonial states for challenging enduring hierarchies of economic power.
India assumed global standing as the exemplary case of peaceful, successful, anticolonial national liberation
Due to the power of a US-led global financial order, few of these economic decolonisation projects achieved their goals. However, their impact included the 1973 Oil Embargo where the Arab members of the Organization of Petroleum Exporting Countries (OPEC) used their control of a tight energy market to punish countries that had allied with Israel during the 1973 War, including the US. The embargo caused Western countries to become less reliant on Middle Eastern oil. In addition, the international economic boycott and divestment campaign against apartheid South Africa helped isolate the regime and push it towards negotiations by 1990. In this way, the economic projects of CEPAL, UNCTAD, the G-77 and those in their surrounding orbits shifted decolonisation from a historical process to a critical analysis of enduring economic inequality.
The Hindu nationalist government of Narendra Modi in India, which governs the largest postcolonial state in the world, has championed a very different decolonisation discourse, that of cultural decolonisation. India became independent in 1947, to a large extent through the peaceful popular mobilisation of the anticolonial leader M K Gandhi . After independence, India assumed global standing as the exemplary case of peaceful, successful, anticolonial national liberation in general perception. The government of Jawaharlal Nehru (1947-64) offered strong rhetorical support to decolonisation efforts across the world.
Nehru was a British-trained lawyer who traded his Western tailoring for traditional Indian clothes. In broad, simplistic strokes, Nehru came to symbolise a modern idiom of Indian politics – of the constitution, administration and secularism, meaning that the Indian state had no established religion. In contrast, Gandhi embodied a saintly idiom , focused on voluntary sacrifice, nonviolence even at the potential cost of life, attempting to reform politics by remaining at a distance from the functions of government. Modi himself has embraced a traditional idiom, connected to popular mobilisation along lines of religion and caste.
Modi is not only India’s head of government since 2014. He is the central figure of a large, organised, popular movement, the Bharatiya Janata Party (BJP) constructed around a form of politicised Hinduism ( Hindutva ) as well as the Hindi language (which is not the birth-language of most Indians) in national politics. The Modi regime has attempted to re-cast Nehru’s ‘secular modernity’ as a form of self-hating, colonial hangover. Modi has argued that independent India continues to require decolonisation from English-language education, as well as from the legacies of Muslim Mughal rule (1526-1857) which predated the British Raj. This call for cultural decolonisation combines the BJP’s ideological commitment to Hinduism and the Hindi language (the bridge language of the regions from which the party draws upon for its traditional strength) with Modi’s political charisma and skill.
The Modi government uses cultural decolonisation talk to combat Nehruvian secularism. They consider British imperial cultural remnants such as the English language within contemporary India and the presence of Islam as necessary targets for decolonisation. Since Nehru was a famous anticolonial nationalist leader during the events of historical decolonisation, it is ironic that the BJP has claimed that his legacy now requires cultural decolonisation.
The Modi regime’s use of decolonisation talk highlights the continuities between imperial cultural institutions and practices – of law, racism, language – in postcolonial states. Colonial governments prioritised European over vernacular languages, provided administrative and military jobs to particular communities defined by race or religion (or both), and enshrined these distinctions and illiberal forms of divide-and-rule into legal codes. Yet, with the exception of the anti-English-language policies, the Indian government does little to divest itself of the tools from the British Empire that they still find useful. For example, the Armed Forces (Special Powers) Act 1958 originated in a piece of British colonial law that shields the military from legal accountability in particular regions where it is deployed. The Indian government is not planning to decolonise itself of such a potent tool of government, despite its imperial origins. This logic of power is unsurprising. However, it remains seemingly incongruous for a dominant government, rather than dissidents or protestors, to wield decolonisation talk, a shift from the concept’s original meaning. While both economic decolonisation and cultural decolonisation are reappraisals of the results of historic decolonisation, the latter represents a change in who mobilises the discourse: from those who resist powerful established governments, to one of those governments itself. Therefore, it is a reconstitution of the history of decolonisation.
S tudents, scholars and activists within elite metropolitan and postimperial institutions also engage in decolonisation talk. An exemplary case is the Rhodes Must Fall movement, which originated as a student protest against a large bronze statue of Cecil Rhodes at the University of Cape Town in South Africa (a postimperial institution set up under empire) and spread globally, with particular resonance at the University of Oxford in the UK (a metropolitan institution located at the former empire’s centre).
Rhodes served as the prime minister of the Cape Colony in British South Africa, colonised Rhodesia (present-day Zimbabwe and Zambia), and donated the land on which the University of Cape Town was constructed, as well as the endowment for the prestigious Rhodes Scholarship at Oxford. In 2015, students at Cape Town demanded the removal of his statue on campus and called for the ‘decolonisation of education’. They launched a series of protests and an occupation of the university, which eventually removed the statue. Student protesters saw the university as ‘a microcosm of society’. What did it mean for contemporary South Africa that their university held ‘a landmark that bears this person’s name’? What can ‘decolonisation’ mean for those who attend a university built upon the legacy of one of the ultimate imperialists?
A new form of decolonisation talk, responding to the Gaza War, utilises ‘decolonisation’ as a pejorative
The Rhodes Must Fall protests spread nationally in South Africa and globally. They focused on emblematic images of colonialism and apartheid, which was the legal policy of racial segregation, discrimination and disenfranchisement in South Africa from 1948 to the early 1990s. The movement’s protests extended into calls for the decolonisation of education as a broad aspiration. They sought to shift whose histories curriculums celebrate or endorse – those who built empire, such as Rhodes, or those from historically disenfranchised communities? Valuable and important political ideals are by no means the unique property of Europeans and their descendants. The movement also focused its attention on which people should have access to the material resources of elite educational institutions such as scholarships, places at universities and employment opportunities. In this way, the call to decolonise education attempted to link the moral legitimacy of anticolonialism (and, in South Africa, the anti-apartheid movement) to conversations of decolonising curriculums, syllabuses and institutions.
The people – economists and civil servants, heads of government, and student protest movement members – who call for economic, cultural and educational decolonisation all claim the legacy of anticolonial national liberation movements and link their professional aspirations to that legacy. They draw upon the political legitimacy of these historic struggles against empire, even as they call for further transformations. In contrast, there is a relatively new form of decolonisation talk, responding to the Gaza War, that utilises ‘decolonisation’ as a pejorative. In a piece in The Atlantic in October 2023, the historian Simon Sebag Montefiore called decolonisation an ideology ‘taught in our universities as a theory of history’ that supports Palestinian self-determination at the expense of Israeli sovereignty. He argued that decolonisation ideology ‘dehumanises [the] entire nation’ of Israel. This condemnation and description of decolonisation ideology reacts to and reflects upon educational decolonisation, rather than the historic events of decolonisation.
All these discourses of economic, cultural, educational and ideological decolonisation are not compatible with each other. Economic decolonisation, for which Prebisch’s work and career were emblematic, critiqued the power of postimperial states to economically control ‘developing’ countries. In contrast, Modi’s call for cultural decolonisation has sought to strengthen the influence of his governmental authority within a postcolonial state (that was a symbol of historic decolonisation’s national liberation) by revising that same history. The Rhodes Must Fall protest movement and its global responses have focused on elite institutions that were at the heart of empire as they attempt to reconfigure who are the primary beneficiaries of those institutions’ resources, prestige and perceived legitimacy. In riposte, Sebag Montefiore’s characterisation of ‘decolonisation ideology’, ascribes a level of political influence and raw power to the ‘decolonisation of education’ that proponents of the movement have not attained.
A s the actual events of historical decolonisation grow more distant, forms of decolonisation talk increase. Decolonisation was once primarily a scholar’s term that effectively depolarised violent national liberation. Now it ascribes radicalism to projects in the realms of economics, culture, education and ideology – spheres whose purpose is not violent regime change.
Historical decolonisation was an international liberation project that reached the height of its political optimism in the 1960s and ’70s. It also provided an attractive source of inspiration and even analogy for movements that sought to rectify racism and other forms of injustice in the US and elsewhere. These connections dissolved as many postcolonial states were unable to provide peace and prosperity to their residents and citizens, the Black freedom movement grew less united, and deindustrialisation in so-called ‘developed’ countries (and the perception that it was caused by cheap imports and labour from predominantly postcolonial countries) eroded global solidarities.
The declining appeal of historical decolonisation led to its transformation into decolonisation talk. At the same time, it is the original promise (and the perceived viability of postcolonial states to deliver upon that promise) of its national liberatory potential that has made it a recurring source material to legitimise movements, even – or especially – for those whose aims are far removed from historic decolonisation’s regime change. While historic decolonisation is a continued reservoir of legitimacy for decolonisation talk, its inability to deliver liberation to many has created the space for so many discourses to flourish, even as they become increasingly distant from the history of decolonisation."
India is a postcolonial power. Its rule in Kashmir is colonial | Aeon Essays,,https://aeon.co/essays/india-is-a-postcolonial-power-its-rule-in-kashmir-is-colonial,"In April 1955, at a closed session of the Asian-African Conference in Bandung, Indonesia, India’s prime minister Jawaharlal Nehru spoke forcefully about the need for countries in Asia and Africa to refuse to join either of the two great powers – the United States and the Soviet Union – and to remain unaligned. Arguing that alignment with either power during the Cold War would degrade or humiliate those countries that had ‘come out of bondage into freedom’, Nehru maintained that the moral force of postcolonial nations should serve as a counter to the military force of the great powers. At one point, Nehru chided the Iraqi and Turkish delegates at the conference who had simultaneously spoken favourably about the Western bloc and the formation of NATO while lamenting the continued French colonisation of North Africa. Nehru said:
A few years later, in 1961, along with Josip Broz Tito of Yugoslavia, Sukarno of Indonesia, Kwame Nkrumah of Ghana and Gamal Abdel Nasser of Egypt, Nehru became one of the founders of the non-aligned movement. Having lifted the yoke of British colonialism, India presented itself as poised to take on the moral and political leadership of the decolonising world. This was perhaps to be expected, especially given that India was the largest and most populous country to become independent from European colonial rule. The story of India’s anticolonial struggle, too, had been mythologised by the nonviolent resistance offered by Indian figures such as Mahatma (‘great soul’) Gandhi. Nehru, too, was perceived as a charismatic and well-read leader who spoke for the people of Asia and Africa, and attempted to find what the scholar Ian Hall has called a ‘different way to conduct international relations’. The stature of both men played a critical role in establishing Indian dominance in the Third World order, and also in establishing ‘the idea of India’ as a secular liberal democracy that was built on the foundational idea of unity in diversity.
Indian prime minister Jawaharlal Nehru inspecting a guard of honour, in Srinagar during his visit to the city in November 1947. Courtesy Publicresourceorg/Flickr
Even as Nehru proclaimed the moral superiority of India for taking a stance against colonialism in all forms, he oversaw India’s colonial occupation of Kashmir. In Kashmir, Nehru said, ‘democracy and morality can wait’.
In the middle of the 20th century, a wave of anticolonial and national liberation movements gained independence from European powers, by exercising their right to self-determination. Nationalist leaders of the former colonies, however, remained committed to the ideals of the nation-state and its territorial sovereignty that derived from European modernity. Independence, it was widely accepted, came in the form of the nation-state, which outshone other forms of political organisation or possibilities. The borders of the nation-state became contested, as European powers often imposed boundaries that ill suited visions of what constituted the political community. This would have deleterious consequences for places where geography, demographics, history or political aspirations posed serious challenges to nationality. In turn, newly formed nation-states asserted their newfound sovereignty through violence and coercion, which had implications for Indigenous and stateless peoples within their borders whose parallel movements for self-determination were depicted as illegitimate to the sovereign nation-state order. Mona Bhan and Haley Duschinski call this process ‘Third World imperialism’.
Some anticolonial nationalists were real nationalists, that is, they saw claims of self-determination within their imagined community of a nation as ‘separatist’, ‘secessionist’, ‘ethnonationalist insurgencies’ or ‘terrorism’. Such framings, rife in Indian discourses on Kashmir, are ahistorical and dehumanising. When we move beyond seeing these regions from the perspective of the dominant nation-state, we come to see how they are places with their own histories, imaginaries and political aspirations – some of which may reinscribe the nation, while others seek to move beyond it through understandings of other forms of sovereignty.
In popular and even scholarly discourses, colonialism is often seen as happening ‘overseas’ – from Europe to somewhere in the Global South. Many people see colonialism as something that we are past temporally, despite acknowledgement of its ongoing legacies. Forms of colonialism within the Global South remain more difficult for many to see because many of these regions are geographically contiguous to one another and, thus, seen as having some form of cultural or racial unity that would form a nation. This results in what Goldie Osuri calls a ‘structural concealment of the relationship between postcolonial nation-states and their [own colonies],’ as well as the concealment of ‘the manner in which postcolonial nationalism is also an expansionist project.’ Contemporary colonies – like Kashmir, Western Sahara, Puerto Rico, Palestine, East Turkestan, among others – show the porous boundary between colonialism and postcolonialism, raising some difficult questions about the current global order.
T he Himalayan region of Kashmir, at the northernmost tip of the subcontinent, is surrounded by India, Pakistan, China and Afghanistan. Kashmir had long been a separate kingdom, at the confluence of Persian and Indic spheres – hard to simply mark into the Persianate or the Indic (themselves, as Mana Kia points out , somewhat amorphous descriptions). Starting in the 16th century, Kashmir came to be ruled as a province by the Mughal, Afghan, and Sikh empires. When the British ruled the subcontinent, they sold Kashmir to the Dogras, Hindu chiefs from the nearby region of Jammu, in the aftermath of the first Anglo-Sikh War in 1846. Under the Dogras, the newly constituted Jammu and Kashmir was one of the larger princely states within the broader ambit of British colonial rule. Its strategic significance in the north of the subcontinent was important for the British, especially during political competition with Russia for influence in Central Asia, known as the Great Game.
Unlike most princely states, Jammu and Kashmir was one of the few where the religious identity of its ruler was different from those of the majority of its subjects. The Dogras were Hindu, while more than three-quarters of the people in the state were Muslim. This perhaps would not have been so significant had the Dogras not effectively run what the historian Mridu Rai has called ‘a Hindu state’, whereby the rulers privileged the Hindu minority and excluded ‘Muslims in the contest for the symbolic, political and economic resources of the state’. Kashmiri Muslims faced immense repression. Most of them were peasants or artisans, forced to pay high taxes to the Dogra authorities. While an anticolonial movement against the British spread across British India, in Jammu and Kashmir, an anti-Dogra freedom movement gained traction in the 1930s and ’40s, only to be sidelined by the sweeping events across the subcontinent.
Hari Singh, the last Dogra ruler of Jammu and Kashmir, pictured in 1931. Courtesy Wikipedia
During the Partition of 1947, the territories that the British directly (British India) or indirectly (princely states) governed in the subcontinent became the two new nation-states of India and Pakistan. Independence, and partition, ended nearly two centuries of British colonial rule. Partition was far from inevitable. Leaders of the Muslim League, such as Muhammad Iqbal and Muhammad Ali Jinnah, discussed a large federation with largely self-governing autonomous provinces to address the concerns of communities, especially Muslims of the subcontinent, who feared Hindu domination in a democratic India. In 1947, when the British hastily drew the lines that established India and Pakistan, nearly 1 million people were killed and another 15 million displaced in the ensuing violence. However, the consolidation of India’s other territorial boundaries was not without incident. Junagadh, a princely state in what is today Gujarat, which had a Muslim ruler but a majority Hindu population, was annexed in February 1948; here, a plebiscite was held and an overwhelming majority voted for India. In September 1948, Nehru violently annexed the princely state of Hyderabad during what was called Operation Polo. Nehru crushed movements for self-determination in Northeastern India, in Nagaland and Manipur.
While Nehru viewed the UN as promoting world peace, he resisted a number of UN resolutions
In mid-1947, in the princely state of Jammu and Kashmir, Hari Singh, the last Dogra ruler, brutally crushed a local Muslim anti-Dogra rebellion. The rebels wanted Jammu and Kashmir to join Pakistan and were afraid that the Hindu ruler would opt for India. The height of the violence became known as the Jammu Massacre and lasted from October to November 1947. The Dogras, supported by Right-wing forces in India, including the RSS (Rashtriya Swayamsevak Sangh or the ‘National Volunteer Organisation’) ethnically cleansed Muslims from Jammu, changing the demographics of the region from a Muslim to a Hindu majority in a matter of weeks. After Pathan Muslims from northwest Pakistan joined their coreligionists in the rebellion against the Dogras and were threatening to take over Kashmir, Singh signed a contested Treaty of Accession with the Indian government. By the terms of the treaty, India sent its army into Kashmir in late October 1947. India and Pakistan subsequently went to war and, in January 1948, India took the Kashmir issue to the United Nations. The UN called for a plebiscite or referendum to be held in the region once hostilities ceased (with the options being India or Pakistan). In 1949, the UN brokered a ceasefire line, later renamed the Line of Control, that divided the region between the two countries.
At first, Nehru agreed to the plebiscite, confident that the people of the region would vote for India. Yet, as it became clear that a plebiscite would not go in India’s favour, his commitment to it waned. While he ostensibly viewed the UN as an important international body tasked with promoting world peace, Nehru resisted a number of UN resolutions. He declared that Pakistan had joined military alliances with the US which made the plebiscite moot. India used other justifications for its opposition to a referendum, asserting that Pakistan had not removed its army from Kashmir, which the UN had called for, and arguing that local elections to the Jammu and Kashmir Constituent Assembly served in lieu of the plebiscite and proved that Kashmiris had opted for India. Nehru maintained that these local elections made a plebiscite redundant. In fact, UN resolutions had called for both countries to remove their troops, but there was no agreement about the manner of troop removal, or their number, nor about the entity that would oversee the plebiscite.
In 1951, the US also stated that local elections in Kashmir were not a substitute for a plebiscite. Within the part of Kashmir that it controlled, the Indian government put client regimes in power that were in support of accession to India, promising them greater autonomy within the Indian union. This autonomy was enshrined in Article 370 of the Indian constitution, which gave the Jammu and Kashmir state ‘special status’ within the Indian Union. It ‘allowed’ the state its own constitution, flag and legislative assembly; in addition, the head of the state was called a prime minister, unlike Indian states where the head was a chief minister. India was supposed to be responsible for defence, foreign affairs and communication. While India argued that Kashmir’s client regimes and local political leaders were ‘democratically elected’, this was not the case. The first election in 1951 for the local assembly was rigged as the pro-accession National Conference ran unopposed in 73 out of 75 seats. Those who opposed Kashmir’s accession to India were not allowed to run. Pakistan resisting its troop removal from Kashmir was also based on the argument that a plebiscite could not take place under a local government that was effectively put in power by the Indian state as that would influence the outcome.
W ithin a few years, India moved beyond the restricted mandate of Article 370, and started to intervene in Kashmir’s internal affairs. Kashmir’s first prime minister and client politician, Sheikh Abdullah, offered some resistance. A 1953 coup replaced him with his deputy, Bakshi Ghulam Mohammad. The Indian government would replace him with the next prime minister, G M Sadiq. Meanwhile, Kashmiri resistance to Indian rule grew, as Kashmiris demanded the plebiscite recommended by the UN and agreed upon by India and Pakistan. In the 1960s, some organised political mobilisations began to speak of a third option – complete independence from both India and Pakistan. Eventually, in the late 1980s, a rigged election and the impact of international developments – including the first Intifada in Palestine and the Afghan defeat of the Soviet Union – sparked an armed rebellion against Indian rule, supported by Pakistan. India militarised Kashmir at this time, making it the most militarised region in the world. The 1990s were a harrowing period in Kashmir, with daily news of killings, massacres, enforced disappearances, sexual violence, torture, crackdowns and arrests. Protected by draconian laws like the Armed Forces Special Powers Act, the Indian army had (and still has) impunity in its control and governing of Kashmir. As Amnesty International reported in 1995, and the Office of the UN High Commissioner for Human Rights confirmed in 2018 and 2019 , there is a ‘consistent pattern of gross violations of human rights in Jammu and Kashmir’.
Kashmir is India’s colony. The exercise and expansion of Indian territorial sovereignty, especially in Kashmir, is a colonial exercise. The exercise of Indian power in Kashmir is coercive, lacks a democratic basis, denies a people self-determination, and is buttressed by an intermediary class of local elites or compradors. But it is also colonial because India’s rule in Kashmir relies on logics of more ‘classical’ forms of colonialism from Europe to the Global South: civilisational discourses, saviourism, mythologies, economic extraction and racialisation. As with all imperial or colonial forces, India has sought to rule over Kashmir through subjugating its people and trampling their rights.
Kashmir’s history is far more vibrant than that conceived of by exclusionary Indian nationalist history
India’s status as a leader of a global anticolonial order has made it difficult for the world to see Kashmiris as colonised. It has obscured the anticolonial struggle of Kashmiris against India. So, there has not been much support for Kashmir’s anticolonial struggle among various solidarity and anticolonial movements around the world. For decades, India insisted that the ‘Kashmir conflict’ was a territorial dispute to be solved between India and Pakistan; in recent years, it has denied that there even is a dispute or conflict in Kashmir. India instead maintains that Pakistan is interfering in India’s ‘internal affairs’. This claim completely erases the agency of Kashmiris who have been demanding their right to self-determination for more than seven decades.
Today, from Indian leaders on international forums to the BJP (Bharatiya Janata Party) IT Cell accounts on social media, you will hear that Kashmir is ‘an integral part of India’. The repetition is often supplemented by narratives of a 5,000-year-old Indian civilisation featuring a prominent role for Kashmir or claims that Kashmir simply belongs to Hindus. In reality, Kashmir’s history is far more vibrant than that conceived of by exclusionary Indian nationalist history; Kashmir defies easy civilisational binaries. Through the Silk Road , Kashmir was a pivotal part of East and Central Asia. Kashmiri traders and travellers journeyed from Srinagar to Samarkand, Bukhara, Kashgar and Tibet. Just as Kashmir was home to vibrant Sanskrit literature like the Rajatarangini , it was also home to Persian literature, like the Waqiat-i-Kashmir and the Baharistan-i-Shahi . Kashmir does not exclusively belong to any community – it has been home to Buddhists, Hindus, Muslims (including Sunnis and Shias) and Sikhs.
Many Indian scholars, too, replicate the notion that Kashmir is ‘integral’ to India. Viewing Kashmir’s history only from the prism of Indian nationalist frameworks, scholars like Sumit Ganguly and Sumantra Bose are unable to move beyond the need to situate Kashmir firmly within the Indian nation-state. Even the postcolonial scholar Partha Chatterjee, who, while critical of nationalism and a founder of the field of subaltern studies, conceptualises Kashmir entirely within an Indian constitutional or national framework. Mainly focusing on the events surrounding 1947 in Kashmir, as well as the decades after the armed rebellion of the 1980s, an earlier generation of Indian scholars tried to find answers to the ‘failures’ of Indian democracy to better accommodate Kashmir within its federal structure, refusing to acknowledge the denial of self-determination and imposition of a colonial occupation. More recently, the field of Critical Kashmir Studies has emerged to contest these statist framings, placing the study of Kashmir more firmly into anticolonial and anti-occupation epistemologies. Scholars of Critical Kashmir Studies examine how colonialism, settler-colonialism and occupation are all important aspects of India’s relationship with Kashmir, elements of which India has used to fortify its rule in Kashmir over time, and to manage Kashmiri resistance.
I n truth, Kashmir was made integral to India in the aftermath of Partition. Through Kashmir’s client regimes, as well as the type of state-building that occurred under those regimes, India was able to further legally, economically and politically integrate Kashmir into the Indian Union. In my book Colonizing Kashmir (2023), I examine the decade that Bakshi Ghulam Mohammad, the second prime minister of the state of Jammu and Kashmir, was in power, from 1953-63. As a client politician, he was tasked with confirming the state’s contested accession to India, but also with ensuring that Kashmiris realised that being under Indian rule would benefit them. The Indian government and Kashmir’s client regimes initially supposed that, if Kashmiris were to see the benefits of Indian rule, alternative political aspirations, such as independence or merging with Pakistan, could be kept at bay. As Nehru is reported to have told his predecessor, Sheikh Abdullah: ‘India would bind Kashmir in golden chains.’
I argue that Bakshi did this by utilising the politics of life, in which the Indian government and Kashmir’s client regimes propagated development, empowerment and progress to secure the wellbeing of Kashmir’s population and to normalise the occupation for multiple audiences. In an attempt to secure the livelihoods of Kashmiris, the politics of life entailed foregrounding the day-to-day concerns of employment, food, education and provision of basic services. At the same time, demands for self-determination were heavily repressed. Policies focused on land reform, building schools and increasing employment opportunities.
Bakshi was acutely invested in financially integrating Kashmir to India. He differed from Abdullah in seeing financial integration as important to development. Between 1953-1954, Bakshi renegotiated Kashmir’s financial relationship with the Indian government, placing certain fiscal demands on the Indian state with regards to grants and agricultural subsidies. The new arrangement also undermined Kashmir’s autonomy, ensuring that it would not be self-sufficient. In this way, Kashmir grew dependent on the Indian state, which gave the Indian government great leverage. Bakshi’s example is important to understand that colonial occupations are not a one-way process. They require native enablers, local collaborators who have agency in determining its contours.
Cultivation of Kashmir’s links with Hinduism was important to the early Indian colonial project
In the 1950s and ’60s, India also turned to film and tourism in order to further India’s colonial occupation of Kashmir, especially for Indian audiences. Dozens of Indian films, including most of the leading blockbusters like Kashmir Ki Kali (1964) or Jab Jab Phool Khile (1965), were made in Kashmir during this time, and middle- and upper-class Indian tourists flocked to Kashmir throughout the year for fun and adventure. Through their personal or cinematic experiences of Kashmir’s beautiful landscape – its rivers, lakes, forests and mountains – Kashmir became what Ananya Jahanara Kabir calls a ‘territory of desire’ for the Indian imaginary, consolidating colonial claims.
Kashmir was also a place of religious attachment for Indian Hindus, and cultivation of Kashmir’s links with Hinduism was important to the early Indian colonial project. Nehru and other Indian leaders would say that India’s secular ideals (as opposed to Pakistan’s religious ones) were proven superior through its only Muslim-majority state ‘choosing’ India. Despite exploiting Kashmir’s ‘secular credentials’ for international audiences, for domestic ones, India largely presented Kashmir as a Hindu place and the heart of Indian civilisation from ancient to present times. Muslim monuments, mosques, figures and histories were erased or toned down in tourism materials for Indian travellers. In the dozens of Indian films made in Kashmir during this time, it was rare to find a Muslim character, astounding given its Muslim-majority status.
Through educational institutions, school curricula and cultural reform, the Indian government and Kashmir’s client regimes have attempted to produce certain kinds of Kashmiris, in particular, good Kashmiri secular subjects. Yet, as a part of this secularism, historical and literary works have foregrounded Hindu geographies, imaginaries and histories, relying on British colonial and Brahmanical understandings of Kashmir’s history. For example, the ‘origin’ story of Kashmir (basically, how the region came to be) used in history curricula and tourism manuals relied on mythological Sanskrit texts like the Rajatarangini. It portrayed Hindus as indigenous or aboriginal to Kashmir, and Kashmir being a place of ancient Hindu learning. Muslims were depicted as ‘invaders’. Accounts of Kashmir’s past rely on Sanskrit texts (and also conflate mythology with history) while erasing other works in Persian that offer different narratives of history and belonging by drawing upon Kashmir’s significance for the Islamic world. In short, Indian nationalist history has relied on Orientalist and Brahmanical renderings of history to help enable anti-Muslim history. This has then furthered the idea that Kashmir is ‘integral’ to India.
Bakshi’s decade in power consolidated India’s colonial occupation of Kashmir, but it still did not result in emotionally integrating Kashmiris to the Indian union. The year 1963, when Bakshi was ousted from power, saw the flourishing of large movements for self-determination in Kashmir. After the Indian government massively rigged a local election in 1987, Kashmiris took up armed resistance. The Indian state resorted to killings, torture and disappearances. This does not mean that the decades prior were peaceful – state repression was high – but that various strategies were foregrounded in different moments, especially in response to Kashmiri resistance and international developments.
I n August 2019, India revoked Kashmir’s semi-autonomous status, fully annexing the region, and advancing its settler-colonial ambitions. The government revoked laws that had previously restricted land, property and employment rights to Kashmir’s permanent residents. These restrictions had been insisted upon by Kashmir’s earlier client regimes to protect the demographics of the Muslim-majority state. Jammu and Kashmir’s Muslims now fear demographic change and an accelerated settler-colonial agenda by which Indian (Hindus) can now buy land and property and settle in the region, undermining the movement for self-determination. Indian officials are already on the record calling for ‘Israeli-like’ settlements to be built in Kashmir for Hindus. The Modi government’s removal of Article 370 was based on a long-standing demand by Hindu nationalists who felt unhappy that the Indian state under the Indian National Congress was trying to appease Kashmir’s Muslims with promises of autonomy. This decision was immensely popular in India.
Today, India is again using the politics of life, or the idea that it is benefiting Kashmiris through development and better opportunity to justify the abrogation, while also using film and tourism to declare normalcy. In the current phase of Indian control, the Indian state has completely undermined civil society. All possible modes of dissent – from pro-freedom groups to journalism, academia and human rights organisations – have been clinically silenced . From internet shutdowns, to the arrests of journalists or human rights defenders, to the surveilling of social media sites and restricting movement by suspending passports, India has left no stone unturned to criminalise political speech and project normalcy to domestic or international audiences. A new description of ‘white-collar terrorist’ is given to anyone who contests Indian sovereignty, and anti-terror legislation is used against all forms of expression, including for example, against students cheering for the Pakistani cricket team, as happened last year. Because Kashmiri Muslims fear losing their livelihoods or property, many have been forced to resort to self-censorship.
The United Arab Emirates and Israel have signed agreements with the Indian government, ensuring foreign investment for Kashmir. India has long exploited Kashmir’s natural resources, including water. During the cold winter months, Kashmiris face electricity scarcity and loadshedding. Yet India sells Kashmir’s hydroelectric power to Rajasthan and other states. Kashmir could see escalating climate disaster; experts have long warned about its receding glaciers and other ecological fragilities, exacerbated by decades of military occupation. With the Indian government giving contracts to Indian companies to mine for minerals, Kashmir is further vulnerable as these companies do not adhere to environmental regulations, nor do they have knowledge of the local ecology. India’s contemporary colonisation is defined by surveillance technology, the arms trade, neoliberal resource extraction, criminalisation of all forms of dissent, and climate change.
Many countries around the world have their own Kashmirs, places they have subjugated either through overt forms of violence or through assimilating forms of control, and at times both.
Contemporary forms of colonialism exist across authoritarian and democratic governments. In the case of India, they exist in a country that claims to be the largest democracy in the world. The case of Kashmir not only challenges this claim but contests the idea of India altogether."
Why liberals fear-mongering about Trump should read Hobbes | Aeon Essays,,https://aeon.co/essays/why-liberals-fear-mongering-about-trump-should-read-hobbes,"Alongside equality, freedom and opportunity, fear has long played a powerful role in political discourse. In ordinary life, fear is often a fitting response to danger. If you encounter a snake while out on a hike, fear will lead you to back away and exercise caution. If the snake is poisonous, fear will have saved your life. By contrast, the fears that dominate political discourse are less concrete. We are told to fear elites, terrorists, religious zealots, godless atheists, sexists, feminists, Marxists and the enemies of democracy. Yet even as these purported poisons are less obviously lethal, political rhetoricians have long understood that making them salient is a powerful way to shape citizens’ motivations. As Donald Trump told Bob Woodward: real power is fear.
It is tempting to think that political fear is largely manufactured – a cynical ploy to manipulate the masses. Trump’s dark vision of the United States would seem to be a prime example of this. Yet, fear can be fitting in politics. Citizens face real dangers from failed political leadership, as lethal to our livelihood as snake bites.
Thomas Hobbes, the 17th-century political philosopher, understood fear. Hobbes was born in 1588 in the English town of Malmesbury, during the Anglo-Spanish war. As rumours of an impending Spanish attack circulated, he described his mother as ‘filled with such fear that she bore twins, me and together with me fear.’ Fear would follow Hobbes throughout his life. England in the 17th century was torn apart by religious and political factions, recurring plagues, misinformation, inflation and a changing labour market. Like in our current moment, pessimism and uncertainty ran rampant. As Jonathan Healey notes in his fantastic book on this period, The Blazing World (2023), the parallels between these historical periods are not hard to find: ‘We, too, are living through our own historical moment in which a media revolution, social fracturing and culture wars are redefining society and politics.’
Frontispiece of Leviathan (1651) by Thomas Hobbes, engraved by Abraham Bosse. Public domain
Many dismiss Hobbes as a curmudgeon whose argument for authoritarianism was guided by his view that people are naturally selfish and violent. In Leviathan (1651), his most influential book, he argues that, without a powerful executive in absolute control, we would lead lives that are ‘nasty, brutish, and short’. He also seems to suggest that, once that sovereign is established, we have no right to rebel against it since the alternative is invariably worse (though commentators disagree on whether this is a fair interpretation).
We have good reason to reject the view that even the most horrific authoritarian regimes are always better than the chaos brought about by rebellion. Stability is not the only political value. But we have lost sight of how important it is. And though we certainly should reject Hobbes’s most extreme authoritarian conclusions , there is much we can learn from understanding the motivations that led Hobbes to accept them, particularly in this current moment in which the appeal of authoritarians like Trump is ascendant.
O n the Hobbesian picture, fear is a fitting response to instability and insecurity. As Hobbes describes in one of the most influential passages in political philosophy, ‘wherein men live without other security, than what their own strength, and their own invention shall furnish them’, there is no point in hard work, because the results of it are uncertain. You might work hard to build a home or start a business, only for it to be taken from you by someone who finds a way to do so through strength or cleverness. And, as Hobbes argues, if the connection between our effort and the fruit of that effort is severed by uncertainty and instability, then so much of what we value loses its point. Without security there is ‘no Culture of the Earth; no Navigation, nor use of the commodities that may be imported by Sea; no commodious Building; no Instruments of moving, and removing such things as require much force; no Knowledge of the face of the Earth; no account of Time; no Arts; no Letters; no Society.’ Instead, Hobbes suggests, we live in a state of continual fear.
It is not hard to see why insecurity about the future diminishes our lives. A dental emergency or a stolen catalytic converter might wipe out your savings. Inflation can turn a budget teetering on the edge of affordability into a financial emergency. When you are living in precarity, planning seems futile. Inflation, a rental increase or a medical emergency can leave you feeling a fool, with your plans and little else to show. Security is the foundation for much of what makes our lives worth living.
Like in the tumultuous period in which Hobbes wrote, far too many people currently face various sources of insecurity and instability. The 2023 report on the economy by the American Academy of Arts and Sciences finds that many Americans cite financial uncertainty and precarity as a central concern. The Starbucks worker who has no idea when their shift will be, how many hours they will work, or whether they will be able to keep their job is in a state of insecurity that makes it hard to plan for the future. And even if you are feeling flush today, workers are increasingly working jobs without guarantees against being fired from one day to the next, or of being able to afford retirement. Almost half of private-sector employees in the US do not have the option of saving for retirement through work.
The US is the only developed nation in the world to have the phenomenon known as ‘medical bankruptcy’
Inflation is also a source of insecurity. When you cannot know whether you can afford tomorrow what you can afford today, you are not certain how far your salary will go, even if you feel sure you will remain employed. Your life feels increasingly tenuous when your expenses multiply from one month to the next. Historically, Americans have purchased homes to protect themselves against rising housing costs, increasing rents and eviction. However, for younger Americans, owning a home has become a dream rather than a plan.
Housing costs have become one of the principal complaints of citizens across the wealthiest countries. Gallup has found that, in OECD countries, half of respondents are dissatisfied with the availability of affordable housing. Only 10 per cent of US adults surveyed by a Wall Street Journal/NORC poll in July 2024 said homeownership was easy to achieve, though 89 per cent thought it essential to their future. Furthermore, half of all renters in the US spend more than 30 per cent on rent and are classified as ‘cost-burdened’, according to a recent report by the Joint Center for Housing Studies at Harvard University.
Healthcare costs are also a concern for many. The US is the only developed nation in the world to have a common phenomenon known as ‘medical bankruptcy’, and it is the leading cause of bankruptcy for Americans. In the United Kingdom, the National Health Service has suffered under decades of austerity economics. And, of course, the threat of climate change looms over all our lives. In many parts of the developing world, its devastating effects are already taking lives, destroying homes, and turning existence itself into a perilous proposition. After a brief pandemic blip, the safety net is in tatters in the US, and in the UK nearly a third of children live in poverty.
An economic system that values market efficiency over creating security and stability erodes the central planks of our lives – work, home and health.
Even if you are lucky enough not to experience these sources of insecurity yourself, it is rational to fear the possibility when you see it happening to those around you. What Hobbes understood is that instability and insecurity ripple through our social world undermining the lives even of those who haven’t been directly affected. If my neighbour’s insurance refuses to cover the damage his house sustained during an unprecedented storm and my friend’s insurance bill has wiped out her savings, my position starts to feel less secure. And when instability and insecurity take hold of the citizenry, the political project is in peril.
M any have interpreted Hobbes as a ‘law and order’ philosopher primarily concerned with political infighting and civil war. However, unpacking the historical context in which he was writing allows us to see that political instability was but one factor in a broader set of conditions to which Hobbes was responding. The English economy transitioned from feudalism to a market-based system during this period. The face of poverty changed from one of serfdom in the countryside, where at least one could count on room and board, to wage poverty in cities where homelessness and starvation were real threats. The very real precarity facing what we would now call the working class was a critical factor.
In Leviathan , Hobbes draws an extensive metaphor between the body politic and literal bodies to warn against the various diseases that can lead to the dissolution of the commonwealth. He writes that:
This inflammation – massive inequality of power and wealth – breeds the instability and insecurity that characterise Hobbes’s historical period and resonate so much with our own.
If enough people stop trusting that this system works, we are, as Hobbes would put it, in a state of ‘warre’
For Hobbes, much like the fear of a poisonous snake should lead us to tiptoe away from danger, the rational response to the fear of insecurity is to seek its opposite: stability and security. Without it, we lose the precondition that makes so much of what we value – education, culture, industry, community – possible. Hobbes argues that it is rational to sacrifice many of our freedoms to achieve such stability and security. Those freedoms are, after all, entirely pointless if we are in conditions where we cannot enjoy them.
Political society is meant to solve this problem by protecting us against uncertainty and insecurity so we can lead our lives looking forward, rather than in a heightened state of anxiety about how to make it through today. The problem we face is that there are many people for whom the system of government doesn’t offer protection from daily insecurity and instability. And if enough people stop trusting that this system works better for them than the alternative, we are, as Hobbes would put it, in a state of ‘warre’.
At a rally in Virginia in June 2024 after the disastrous first debate of the latest election season, Trump said: ‘As every American saw firsthand last night, this election is a choice between strength and weakness, competence and incompetence, peace and prosperity, or war or no war.’ Carefully crafting the choice as one between the security that comes from strength and competence, and the insecurity that comes from weakness and incompetence, Trump again reinforced his message. If you don’t choose me, your lives will get only more insecure and uncertain. And after he was the target of an assassination attempt, he reinforced this message by emerging, fist pumping in the air – a picture of strength in the face of chaos.
Trump’s proposed solution is authoritarianism (as he said in 2016: ‘I, alone, can fix it’). The problem with this solution is that it ends up trading one source of insecurity – our fractured political system – for another – the whims of an individual whose principal interest is his own power rather than the wellbeing of the body politic. But even if we reject this solution as flawed, we cannot dismiss the concerns that drive many to consider it. The democratic party has a new candidate now, but it isn’t clear whether Kamala Harris and Tim Walz’s policies will address the need for security and stability.
H obbes was wary of democracy precisely because he thought it would lead to instability and insecurity. Competing factions and groups would undermine the system’s stability by vying for power. Hobbes argued that stability is to be found in consolidating power into the sovereign and in the compliance of the governed. However, we cannot forget that for Hobbes, as for other social contract theorists, compliance is earned, not demanded.
Democratic liberal states have rejected authoritarianism as the solution to the problem of insecurity, preferring to emphasise the benefits of living in a state where our wellbeing is safeguarded by the enshrinement of our freedom into laws and institutions. Stability is meant to be the product of citizens’ acceptance of the shared values at the heart of liberalism. But does this compact guarantee the material security and stability that are preconditions for flourishing lives? For the millions who worry about whether they can afford their grocery bill, rent or medical expenses, the answer appears to be no.
Nostalgia’s power is most potent when no compelling, believable vision of a brighter future exists
The COVID-19 pandemic and its aftermath offered a glimpse of a solution. When things felt precarious and uncertain, the US government stepped in with eviction moratoriums, universal basic income, free vaccines and a child tax credit. (Let’s not forget that Trump made sure that those stimulus checks bore his name.) But a few short years later, we are back to business as usual, leaving millions on the edge of precarity.
This is not to deny that xenophobia, sexism and racism also churn through the current discontent with liberalism. Nostalgia for times past is a strong undercurrent of the appeal of Right-wing movements. Some want the return of the well-paid factory job with strong benefits, while others want a return to white male supremacy. But nostalgia’s power is most potent when no compelling, believable vision of a brighter future exists.
Those who fear Trump’s re-election and the rise of Right-wing political movements keep reminding us that democracy is on the line. But sowing fear and doubt only adds to the growing sense of insecurity and uncertainty that is already unravelling people’s trust in the liberal project. It plays right into the hands of the strategy that Trump is so adept at playing. For people to see the value in the current system, we need to do more than fear-monger about the alternative.
Hobbes is often interpreted as being narrowly focused on justifying a powerful state that could control our worst appetites so as to prevent us from killing each other. I have argued that if we take his concern for stability and security seriously, the solution requires a far more radical rethinking of liberal states as they currently exist. Material insecurity and political instability cannot be divorced. A liberal state that leaves so many feeling as if their lives are on the verge of being ‘nasty, brutish, and short’ falls short of solving the problem that political society is meant to solve. Freedom is meaningless if you cannot count on a stable connection between the work you put in today and a good life tomorrow. But this is precisely the connection that has become severed for so many. If political society is to enable flourishing lives, then we need a political and economic system that can provide that kind of stability. This requires more than mere rhetoric. If we fail, Leviathan is waiting in the wings."
"The myth of civic vs ethnic nationhood in Europe, east and west | Aeon Essays",,https://aeon.co/essays/the-myth-of-civic-vs-ethnic-nationhood-in-europe-east-and-west,"At around three o’clock, on a warm and beautiful summer day, the headwaiter approached Hans Kohn’s table. It was 1914, and Kohn was tucked away with a friend in the cool and quiet Café Radetzky in the Malá Strana area of Prague, preparing for his upcoming bar examination. Sunday strollers crowded the city’s streets and parks while people chatted over beer and coffee in the open air. All presumably an unwelcome distraction for the two young law students.
The waiter’s hand trembled as he handed them a special edition of a local newspaper. It announced that Archduke Franz Ferdinand, the heir to the Austro-Hungarian throne, had been assassinated in Sarajevo. The mood in the country would quickly move from bewilderment to belligerence, driving Austria-Hungary into a disastrous world war. Four years later, the country was gone from the map of Europe.
On that warm June day in 1914, few anticipated a war that would reshape the political map of Europe. When Kohn was mobilised into military service, he had expected to be home by Christmas. Instead, he ended up among the unlucky hundreds of thousands captured by Russian troops and cursed to spend the war deep in the Russian imperial interior.
Hans Kohn in the uniform of the Austro-Hungarian Empire in 1914. Courtesy the Leo Baeck Institute New York
By 1920, when Kohn had made it back to Europe, Prague was the capital of Czechoslovakia, one of several new nation-states that had sprouted up during his absence, based on the principle of national self-determination. These new nation-states replaced the vast multinational – or anational – empires that had previously blanketed central and eastern Europe: the others being the German, Russian, and Ottoman Empires.
Conflicts between various nationalist movements had plagued Habsburg-ruled Austria-Hungary in the decades before its collapse. Once the war was obviously lost, the path was open for them to take over from the delegitimised and eventually deposed Habsburg dynasty. An even more radical nationalism would flourish in the successor states of the various empires. Germany under the Nazi regime – the largest of all – elevated racialist nationalism into the organising principle of its eastward conquests, finding no shortage of local collaborators keen to settle scores with their supposed national enemies.
Countless millions would perish in the orgy of violence unleashed by their messianic nationalist dreams, including the majority of Prague’s Jewish community to which Kohn had once belonged. In the wake of the Second World War, 12 million Germans were sent westwards out of fear or retribution. All the Allied powers agreed that the surest guarantee of future stability for central and eastern European nation-states was to have as few ethnic minorities as possible. Kohn’s youth in fin de siècle Austria-Hungary had been spent in one of the most ethnically diverse countries in Europe. By the mid-20th century, its former territories hosted some of its most homogeneous.
F or Kohn, the real culprit for the downfall of his multinational homeland was not the war itself, but the force of nationalism that in its waning years exerted such a powerful sway over its people. He was hardly alone in this assessment. For at least three-quarters of a century, central and eastern Europe has served as the prime example of the pitfalls of nationalism. In particular, of the kind of ethnic nationalism that, we are often told, is characteristic of this non-Western world.
It was this particularly central and eastern European ethnic nationalism, this perspective goes, that was responsible for the collapse of the diverse and cosmopolitan Habsburg Monarchy. For the failure of the new democracies that took over from empires in 1918 – Germany, Austria, Poland, Czechoslovakia and others. And for the rise of authoritarian and fascist regimes in their place, the largest of which would go on to perpetrate the most horrific genocide in human history in the Holocaust.
According to this orthodox view, the essentially ethnic nature of central and eastern European nationalism contrasts starkly with that of the Western democracies of France, the United Kingdom and the United States. They are characterised as thoroughly civic nations, based not on supposedly primordial tribal identity, but on common citizenship and a democratic understanding of politics. In all three, the US, UK and France, their civic nationalism is a centuries-old tradition, dating back to their foundation as modern nations.
Kohn was the first historian to systematically seek out the roots of this divergence between the nature of nationhood in the Western democracies, and the central and eastern European countries in his weighty book The Idea of Nationalism (1944). Kohn’s book grew into a foundational work of ‘nationalism studies’ in the Anglophone world and has influenced generations of scholars and readers alike. He did not just see Western and non-Western nationalisms as different, but came to believe they were, in effect, totally different phenomena.
The assimilation of minorities into the dominant ethnicity in Western nation-states was celebrated as progress
Kohn argued that Western nationalisms were ‘based upon liberal middle-class concepts … pointing to a consummation in a democratic world society’, while central and eastern European nationalisms derived from ‘irrational and pre-enlightened concepts … tending towards exclusiveness’. The enlightened Western ones, he claimed, developed in France, the UK and the US, the primordial superstitious ones in Germany, before spreading across the rest of central and eastern Europe and, eventually, the world. The backwardness of all non-Western countries apparently made it all but predetermined that the latter would win out over the former.
While the distinction between these two kinds of nationhood was known to 19th-century thinkers, the notion that ethnic and civic aspects of nationhood were necessarily in conflict, or that one or the other was purely characteristic of a certain part of Europe, was not. Western ‘civic’ nation-states have always been built on the dominance of certain ethnic groups with their own language, traditions and myths of origin and distinctiveness. Indeed, the assimilation of minorities into the dominant ethnicity in Western nation-states was celebrated as progress.
Central and eastern European nationalists did not ‘reject’ the civic values of their Western counterparts but tried to follow them closely. They acknowledged civic rights for all that lived in a given nation-state but sought – like their Western counterparts – to eventually see all ethnic, linguistic or religious minorities assimilated into the general civic nation that was ultimately shaped by the dominant ‘state-forming’ ethnic group.
They were in awe of the assimilatory power of the English language and its culture in Britain or North America, and of the French equivalents ultimately defined in and around Paris. That German or Hungarian nationalists wanted to see Slavs or Jews shed their culture and become true Germans and Hungarians did not reflect some ‘irrational and pre-enlightened’ exclusivism. It simply reflected the reality of the Western nation-state.
Nevertheless, in nationalism studies, this geographic distinction between Western civic and non-Western (or ‘Eastern’) ethnic nationalism remains one of the most deeply engrained orthodoxies. The problem is it simply isn’t true. To understand how this misleading but influential view took shape, it is necessary to understand how the descent into ethnic extremism in early 20th-century central Europe shaped the enduring works of early theorists of nationalism. Many of whom – like Kohn himself – were ultimately shaped by its consequences, their work marked by a deep desire to discover where the histories of their homelands had ‘gone wrong’.
K ohn was born in 1891 in Austria-Hungary, perhaps the most bewildering state in modern European history. In fact, it wasn’t one state at all, but two. The half colloquially referred to as Austria consisted of three kingdoms, six duchies, two archduchies, a grand duchy, two margraviates, two princely counties and a free city, all with their own unique histories, identities, flags, forms of patriotism, celebrations of belonging and more. The other, Hungarian half itself had a kingdom within a kingdom in Croatia-Slavonia.
Profile portrait of Hans Kohn. Courtesy the Leo Baeck Institute , New York
What united all these political entities was the emperor-king Franz Joseph, who sat atop the Habsburg dynasty that had ruled most of these polities for centuries. With two brief interruptions, the Hapsburg family, from the 15th century to 1866, had acted as hereditary heads of the entirety of today’s Germany, first as Holy Roman Emperors and then as ‘heads of the presiding power’ of the German Confederation, itself consisting of 39 different German states.
This bewildering political tapestry did not make for simple nation-building on the Western model. France and Britain were centralised, but central Europe was decentralised. The former had dominant national languages, but the latter was extremely multilingual. The former had strong centres of political authority, the latter diffuse and overlapping ones. Kohn’s hometown of Prague was the historical capital of the Kingdom of Bohemia sandwiched between the Austrian duchies to the south and the rest of Germany to the north. It was a prime example of the kind of national complexities arising from this complicated Habsburg inheritance.
In the 18th and 19th centuries, Bohemian natives nurtured wildly different visions of their homeland’s place in a possible national state. Most German-speaking Bohemians envisioned it as a part of Germany or a German-dominated Austrian state. Nationally conscious Bohemian Slavs variously imagined it as part of a wider Slavic-Austrian state, a more narrowly ‘Czechoslovak’ one, a purely Czech one, or even a bilingual Czech-German nation-state.
Only after 1871 did the idea that civic borders should conform to ‘objective’ national ones based on ethnic criteria come to prominence
The problem faced by all nationalisms emerging out of central Europe before 1918 was that no ethnic nation was congruent with the state. Insofar as German or Czech-speaking nationalists in Bohemia, for example, saw their nation as the one truly representative of the kingdom, they would have to assimilate their rivals against their will. Or, as Kohn maintained, ‘redraw the political boundaries in conformity with ethnographic demands’, supposedly one of the tenets of ‘non-Western’ nationalisms.
Somewhat bizarrely considering nationalism captivated Europe only in the 19th century, Kohn concluded The Idea of Nationalism with the 18th, content that he had discovered the roots of the two nationalisms by then. Yet the historical record contains very few demands from 18th- or even 19th-century eastern and central Europeans for the redrawing of borders. The first example of ‘objective’ ethnographic measures being used as the basis for border changes in Europe was in the Franco-Prussian War in 1871, and there its goal is only the exchange of a few villages on the initiative of an entrepreneurial statistician.
Only in the decades after 1871 did this idea that civic borders should conform to ‘objective’ national ones based on ethnic criteria come to prominence. Importantly, it arose with the maturity of nationalist movements, not at their birth. For most of the 19th century, we find political or civic nations in central Europe seeking to assert their rights to manage their own affairs while opening up the boundaries of the nation to people of wildly diverse religious or linguistic backgrounds. In return, however, they asked for assimilation, that outsiders identify with the political community of the state and its leading ethnic group. Sometimes – as in Bohemia – competing claims arose about the question of which ethnic group had the right to be identified with the political nation.
Scholars usually date the emergence of modern nationalism to the 18th century. But it’s also true that the word ‘nation’ has been used in Europe for centuries. What changed is the modern claim that nations consist of the ‘masses’ and the modern nationalist assertion of the rights of those masses to statehood. That’s what we call ‘nationalism’.
But for a long time before modern nationalism, the word ‘nation’ frequently referred to political nations in premodern Europe. That is, the nation as a corporate group consisting of those whose rights and privileges marked them out as a distinct group in and above society. Whose privileges made them the group that ruled society, complete with their own language, customs, traditions and identity. This is why one of the great innovations of the French Revolution was the extension of nationhood through political emancipation to the broad masses of French society.
Kohn saw the romantic veneration of the common folk as the root of reactionary non-Western nationalism
The nobilities of the Habsburg-ruled kingdoms – of diverse ethnic and linguistic origins – had strong and well-developed conceptions of belonging to a common nation. So strong, in fact, that they resisted incorporation into the kind of centralised absolutist states characteristic of 18th-century Europe. The Habsburg Monarchy was nearly torn apart by the pressures of such policies pursued by Joseph II, who rescinded most of them on his deathbed in 1790.
In the kingdoms of Bohemia, Hungary and Croatia, nationalism was pioneered in the 19th century by patriotic nobles keen to assert their ‘state right’. That is, their political sovereignty as a corporate nation with the right to manage their own affairs. They were aided by small groups of middle-class publicists and scholars who, under the strong influence of German romanticism, sought to reform and cultivate vernacular languages native to the kingdoms, build narratives of historical continuity for the nations, and educate the broader masses in order to make them productive members of the nation at large. Kohn saw exactly this romantic veneration of the common folk as the root of reactionary non-Western nationalism.
He even claimed in The Idea of Nationalism that, after 1806, local central European elites proclaimed ‘the uniqueness of the folk … as an aggressive factor in the struggle against Western society and civilisation.’ An exaggeration of the role played by some nationalist publicists during the Napoleonic Wars, whose vitriolic anti-French views were more important to Wilhelmine or First World War-era German nationalists than 19th-century ones.
German liberal nationalists of the first half of the 19th century contrasted their envisioned nation-state not with Western society and civilisation, but with the reactionary princely confederation in which they lived. When national revolts broke out in Greece or Poland, German nationalists cheered their fellow Europeans fighting for freedom against despotic regimes. They largely recognised the liberal struggle as a cosmopolitan European one, not as a narrowly German one nor as one that existed in opposition to ‘the West’.
England and France as bastions of progress and civilisation presented models for admiration, emulation, and – occasionally – envy. In the 1830s and ’40s, Hungary’s generation of reform nobles who transformed the country’s social and political life were enamoured by England, as were many German intellectuals. England seemed to represent everything that their countries lacked in terms of national and political life, where the prosperity created by liberal social and political ideas allowed for the full flourishing of national life.
The most difficult question faced by liberal nationalists in ‘non-Western’ countries was not how to redraw borders to make nation and state congruous. It was rather how to reconcile the model provided by France, England or the US with their own circumstances. In other words, how to transform the civic nation from a narrow noble elite to a broader public of educated middle-class men from a confusing collection of linguistically and politically diverse states tied together by the House of Habsburg.
I n 1848, a series of revolutions broke out across Europe. Terrified at the sight of disgruntled masses in the streets, European monarchs made once-unthinkable concessions. They called democratically elected assemblies, drafted constitutions and ratified liberal laws. Though by 1850 the revolutions would be defeated – the assemblies closed, constitutions revoked and laws overturned – they had given the middle-class liberal nationalist public its first taste of politics.
The opening of the Frankfurt Parliament in Paulskirche in 1848. Note the portrait of Germania. Courtesy Wikipedia
In 1848, a German National Assembly formed in Frankfurt where revolutionaries produced the first draft constitution for a German nation-state. Hungary, meanwhile, adopted a raft of liberal legislation in spring 1848 that transformed it into a modern parliamentary state. The realisation of the right of ‘historic’ nations like Hungary and Germany (as well as Italy and Poland) to statehood would have meant a de facto partition of central Europe among these four nation-states. Revolutionaries across Europe celebrated the prospect, dismissing the objections of Czechs, Slovaks or Slovenes who would be subsumed in the German or Hungarian nation-states as the cries of ‘unhistoric’ nations or mere ‘fragments of peoples’.
None of these nationalist movements sought to withhold civic rights to members of ethnic minorities. Rather, they expected them to assimilate, as did those minorities who lived in prosperous, progressive Western states. As one deputy asked in the Frankfurt Parliament in 1848: ‘What would the French say if the Breton, Basque and old Ligurian fragments of peoples declared they no longer wanted to be French?’
Deference to the Western nation-state – where the supposedly most advanced ethnic group in the state had become the core of a democratic civic nation – was a common point made in prerevolutionary Hungary as well. The politician Ferenc Pulszky, who himself had travelled extensively in Britain, asked in the 1840s: ‘What do we Hungarians demand of the Slavs[?] … we demand nothing more than what the English ask of the Celtic inhabitants of Wales and high Scotland, nothing more than the French ask of Brittany and Alsace.’ Pulszky could not see why the civic model of nationhood could work for France but not Hungary.
Perhaps 40 per cent of the country spoke Hungarian – not enough to claim that only Hungarian be used in public life
The Western nation-state was a model for some, but a warning for others. From the enslavement of people of African descent and the displacement of native Americans in the US to the suppression of minority languages and dialects in France to the disenfranchisement of Catholics in the UK and the gradual elimination of Celtic languages, Western nationalisms were predicated on the homogenising force of a dominant national group that gave no quarter to national minorities in public life. Unsurprisingly, representatives of national minorities in German states and in Hungary near-universally refused to accept subordinate status in someone else’s nation-state. Or to recognise that the nation-state belonged to groups that were themselves minorities.
Though Bohemia’s elites overwhelmingly spoke German, the majority of Bohemians were Czech speakers. Hungary, meanwhile, was perhaps the most linguistically diverse country in Europe. German dominated its cities, Hungarian the nobility, and Latin served as the official language until 1844. Yet the masses spoke an array of Slavic, Romance, Germanic and Hungarian vernaculars. Only perhaps 40 per cent of the country spoke Hungarian (even by the 1880 census, it was only 46.5 per cent), a plurality but not enough to claim that only Hungarian could be used in public life.
Given the examples set by Western countries, it makes sense that Hungarian nationalists presumed that the predominant national vernacular would be the linguistic rallying point for national development. France, the US and the UK were all home to enormous ethnic, linguistic and racial minorities. But these minorities were either assimilated to the dominant nationality or excluded from national life altogether.
Conversely, there was little reason to expect that nationally conscious Czechs should have accepted that their state was fundamentally German. Or that Hungary’s numerous nationally conscious minorities – Croats with their own subordinate kingdom, Slovaks concentrated heavily in the north, or Romanians who formed majorities in large swathes of Transylvania – should have accepted the conflation of a Magyar ethnic nation with a Hungarian state. In either case, this had little to do with any kind of distinct idea of nationalism but was caused by inherent contradictions in the model of the idealised Western nation-state in a central European context.
Despite such conflicts and contradictions, central European nationalists did not reject the civic nation. The final draft of the revolutionary constitution produced by the Frankfurt Parliament declared in the most straightforward civic terms: ‘The German people consists of the citizens of the states that form the German Empire.’ In 1868, a year after the creation of Austria-Hungary out of a unitary Austrian Empire, the new Hungarian government wrote into the constitution that there would be only a single Hungarian nation. Multiple ‘nationalities’ were also recognised, but there was only a single civic nation. Anyone could be a member, but they had to speak, dress and effectively become a Magyar.
It was an outrage to minority nationalists, but surely no less of an outrage than the national development of Western nation-states. The French Ministry of Public Instruction found in 1863 that at least a quarter of the country spoke no French at all. For the millions of Occitans in the south – whose Romance tongue was at least related to standard French – to Celtic Bretons in the northwest, rebellious Corsicans and totally unique Basques in the southwest, ‘becoming French’ entailed assimilation into a language and culture that was not quite theirs. The congruence of the French ethnic and civic nations was not a result of pure ideas, but of decades – centuries even – of nation-building.
I n the latter half of the 19th century, international statisticians were faced with a seemingly intractable question: how could a nation be measured objectively? From the 1850s, a series of conferences had brought together statisticians from across Europe and North America in an attempt to harmonise how countries collected statistics around the world. In 1872, they endorsed the notion that ‘mother tongue’ could determine the boundaries of nationalities. But this was not a universally recognised measure. The following year, the newly founded International Commission on Statistics tasked three Austro-Hungarians with tackling the problem head-on.
The Austro-Hungarians couldn’t agree. One put forward the civic idea of conscious self-identification, another the ethnic idea that it was ‘racial’, and the third simply argued it was a complicated mix that was difficult to measure universally. The tensions between civic and ethnic nationalisms were on full display, but they had little to do with geography. Their range of opinions came at a time in the late 19th century in which ethnic nationalism was increasingly influential across the continent, inspired by the rise of racialist thinking, eugenics and social Darwinism.
By the turn of the 20th century, young radicals across Europe put forth ethnic conceptions of nationhood in which Jews especially were singled out as being a foreign element supposedly unable to be a member of the political community of the nation. But the appearance of antisemitism did not signal a quick triumph among nationalisms with long traditions of Jewish assimilation. In the years up to 1918, the majority of German Jews insisted that they were simply ‘German citizens of the Jewish faith’. Jews in Hungary were no less assimilated. In Austria and Bohemia, most Jews were German-speakers who similarly identified themselves as German.
Kohn was one such German-speaking Jewish Bohemian. He embraced Zionism prior to the First World War, at a time when it was a tiny movement among the elite, and assimilated, Jewish communities of central Europe. After stints in Paris and London, he ended up in Palestine in the mid-1920s hoping to live his Zionist beliefs. After a wave of violent riots broke out there in 1929, Kohn grew disillusioned. The same ‘spirit of extreme nationalism among [Austria-Hungary’s] peoples’ that had made the ‘building of a peaceful multiethnic state’ he saw manifesting itself in Palestine too. He opted to abandon Palestine for the US, where he settled in 1933.
Citizenship cannot force people to feel part of a civic nation
Unlike in the empire of his youth, or the Jewish state of his dreams, in the US Kohn thought he had found a country in which nationalism as a progressive and tolerant force had produced a truly just and liberal society. The contrast between this US reality and the exclusivist ambitions of German, Zionist or Czech nationalists deeply moved the former lawyer-cum-historian. Nationalism, he concluded, was not the problem, but only a certain ‘type’ of non-Western nationalism.
But nationalisms – both Western and non-Western – contain a complicated mixture of civic and ethnic factors, excluding some while offering others the chance for inclusion through assimilation. The state acts as the most powerful force for both. That’s why historians of nationalism like to say states make nations, not the other way around. Nationalism is, by definition, exclusivist insofar as it excludes those who do not think of themselves as a part of the nation. Citizenship cannot force people to feel part of a civic nation, just as citizenship does not stop some from trying to exclude others from their ethnic nation.
Today, it feels vaguely accurate to say that countries like the US, the UK or France base their national identity on the ‘civic’ nationhood of common citizenship. Poland, Hungary, Czechia or even Russia, on the other hand, appear wedded to a more ethnic idea of nationhood rooted in a common language, traditions and myths of origin. It would be an error to read the world of 2024 into the past, as much as it would be an error to read the world of 1944 into the past. An error to assume that today’s ethnic homogeneity in central and eastern European countries, as well as the inclusive nationhoods of Western democracies, are nothing but the consummation of eternal and essential truths rather than the result of contingent historical events. Unfortunately, this was precisely Kohn’s error.
In many ways, Kohn’s The Idea of Nationalism is really a book about a prototype of the Sonderweg (‘special path’) thesis, seeking to explain where German history had ‘gone wrong’ to such an extent that it led to Nazism. Why it did not follow the supposedly inclusive path of other Western countries like France, UK and the US but instead went down a fascist path that culminated in the catastrophe of the Second World War and the Holocaust.
Yet the US of the 1930s that Kohn was so enamoured with was a country whose extremely restrictive immigration policies sought to retain ‘Anglo-Saxon’ ethnic dominance, to which end much of the country mandated racial segregation until the 1960s. With few exceptions, over the past three centuries, the building of all modern nation-states required one ethnic group dominating and assimilating others.
Looking for the ‘roots’ of central and eastern Europe’s lagging behind the West in modernisation, and also at the horror of Nazism, led Kohn to make anachronistic claims about the long-term ethnic continuity and nature of their nations. Civic and ethnic nationalisms were never two distinct courses of historical development taken by different nations, but in fact two different aspects of the development of almost all modern nation-states. The tension between them unfolded across the 19th and 20th centuries as nationalism spread across Europe and the world."
"On the varna system, Gandhi’s plan to empower the workers | Aeon Essays",,https://aeon.co/essays/on-the-varna-system-gandhis-plan-to-empower-the-workers,"Political theorists often argue that citizens need to have certain capabilities for their political projects to be successful. Ancient and medieval political theorists, like Plato or Aquinas, often demand that people receive advanced spiritual and civic education as a prerequisite for participating in rule. This training is intricate. It takes time, and it can be expensive. Pre-industrial economic systems do not generate a very big surplus. In highly stratified ancient republics, citizenship was often reserved for the rich and powerful.
Modern liberals, like Adam Smith or Benjamin Constant, tend to take a different approach – they argue that most people already have the qualities that are necessary for citizenship. If they don’t have them, they can gain them by participating in markets and in civil society organisations, without need for careful planning. It helps that modern liberals envision a more limited role for their citizens – they need enough civic education to be able to vote for representatives, but they are not expected to make important everyday political decisions.
Gandhi was a different sort of thinker. He wanted ordinary people to make difficult moral and political judgments themselves. Instead of lowering the bar for citizenship or excluding the poor and the weak from citizenship, Gandhi argues that it is possible to dramatically improve the capabilities of ordinary people.
Gandhi spinning yarn in the late 1940s. Photo Wikipedia
To do this, he called for the reconstruction of the varna system, in which young people adopt the professions of their parents. In its original form, the system consists of four varnas. There are the Brahmins, who serve as scholars, priests or teachers. There are the Kshatriyas, who serve as rulers, administrators or warriors. There are the Vaishyas, who serve as farmers or merchants. Finally, there are the Shudras, who serve as artisans, labourers or servants. The members of all four varnas are householders, in the sense that it is permissible for people occupying any of the four varnas to produce children. One’s varna is determined by one’s parents’ varna.
T he varnas are often ranked so that the Brahmins enjoy the highest status, followed by the Kshatriyas, the Vaishyas and the Shudras. But Gandhi rejected ranking the varnas in this way. For him, the varna system becomes a caste system when the varnas become hierarchical status markers. In his view, all four varnas are meant to be equal, and people in all four varnas are meant to be able to engage in spiritual development – not just the Brahmins.
There are some Indians outside the varna system. The Dalits – or untouchables – are considered to be without a varna. For Gandhi, the category of Dalit is itself an offence against the varna system, insofar as it is a category that presupposes a hierarchical ranking and excludes some people from spiritual realisation. There are also some Indians who are not householders, but have instead committed themselves to ascetic lifestyles. After some number of lifetimes at the householder level, a Hindu practitioner is said to advance into a new ashrama or stage of life. While Brahmins serve as spiritual teachers, they remain householders, and so have not yet transitioned to asceticism. A person who wishes to become an ascetic must not have any dependents. This does not necessarily mean that the ascetic can never have had a spouse or children, provided that when the ascetic embraces asceticism, appropriate provisions have been made. Once asceticism is embraced, commitments to celibacy and childlessness necessarily follow, lest any new dependents be acquired. Taken together, the whole varna system is called varnashrama , referring together both to the four kinds of householders and the four stages of life.
He believed the system could and should raise everyone to the same level of spiritual and political education
Why would Gandhi wish to revive this system, a system that – by his own admission – develops very easily in an undemocratic direction, into a system of hierarchical caste ? When childhood is about preparing to compete in the job market and adulthood is consumed with worry about money, there’s no time for spiritual growth. But if children learn how to make a living at home, from their parents, Gandhi argues, they ‘need not even go to a school to learn it’. This leaves the mind ‘free for spiritual pursuits’. It allows the education system to focus on character development, on art and philosophy. By freeing Indians from the need to find their own way to earn a living, Gandhi hoped to give them the time necessary to become great souls.
Gandhi’s envisioned reform of the varna system faced obstacles. For one, the varna system and the caste system are often confused, even by Indians. Many people think that some professions are higher status than others. If profession is hereditary and different professions become associated with different levels of social status, this can result in a system of status hierarchy, in which some families occupy higher positions and others are subordinated. For Gandhi, caste hierarchy was a corruption of the varna system. Gandhi was a committed egalitarian – he believed the system could and should be used to raise everyone to the same level of spiritual and political education. However, caste perverted varna in the opposite direction, creating rigid, impenetrable social and political barriers between families.
T he varna system was plagued by caste hierarchy, but that was just the beginning of its problems. By the early 20th century, many of the traditional professions were no longer performed. Gandhi, for instance, had given up the profession of his parents to become a lawyer. When he made the decision to go to England for a legal education, he was kicked out of Sabarmati Ashram. Jawaharlal Nehru, India’s first prime minister, was born a Brahmin. But Nehru took no interest in reading spiritual works. Instead, he went to law school.
Gandhi became convinced that it was a great evil for Indians to abandon the hereditary professions. Indians must not go to law school. If they do, this would give rise to a class of trained professionals, a group of bureaucrats, who would dominate India. These bureaucrats would run India the same way the British had run India, and under them ordinary Indians would remain incapable of participating in political decision-making.
In 1915, when Gandhi returned to India from South Africa, he argued that Indians who had adopted the Western professions – like law, medicine and engineering – should give them up. They should instead take up traditional Indian crafts. Gandhi himself gave up the law and took up the spinning wheel, making khadi – a kind of traditional Indian cloth. In the caste system, the manual crafts occupied the lowest position. High-caste Indians were prohibited from engaging in manual work on pain of expulsion from their caste. By encouraging Indians to take up the manual crafts, Gandhi subverted the caste system. But he also hoped to lay the groundwork for recovering varna.
If all Indians could learn the traditional crafts – and if all Indians consistently refrained from purchasing industrially produced goods – the crafts would ensure the livelihoods of all Indians. Future generations could simply learn the traditional crafts at home, from their parents, allowing them to pursue spiritual growth and participate directly in politics.
The manual crafts weren’t just a protest against the British but key to universal self-realisation in India
So, at first, the schools would need to teach the crafts – to ensure they were known to everyone, and to violate caste prohibitions on manual labour. But once the crafts were widely known and the caste prohibitions were no more, the crafts could be learned at home, and the schools could be turned to their true purpose – preparing young people to rule themselves. Gandhi called this self-rule ‘swaraj’.
Why the emphasis on crafts? For Gandhi, only the traditional crafts were universally available to Indians, even under British rule. Training Indians as farmers would not work as long as ownership of farmland remained concentrated. Indian farmworkers would be made to work long hours as agricultural labourers unless and until the land could be redistributed, and that could happen only after the departure of the British. Gandhi believed it was necessary to prepare for swaraj immediately, and the crafts presented themselves with practical and political appeal.
It would be possible to revive the crafts only if Indians made a point to exclusively purchase products made by traditional methods. For the crafts to survive in the long term, Indians would have to continue the anticolonial protest against manufactured goods even after independence. For Gandhi, the manual crafts weren’t just a protest against the British – they remained central to producing conditions for universal self-realisation in India.
As the Second World War drew to a close, Gandhi grew concerned that Indian independence would come too early, before this was properly grasped by the other independence leaders. His friend Nehru disagreed with him about the traditional crafts. In a letter to Nehru, Gandhi argued that by performing a ‘quota’ of manual labour, the people could ‘rest content’ with their ‘real needs’, freeing them up for spiritual learning. Nehru countered that traditional villages were ‘backward intellectually and culturally’, and that an economy based on primitive technology would be isolated and uncompetitive.
Gandhi Day paraders in Delhi, July 1922. Photo by Topical Press Agency/Getty
For Gandhi, Nehru had missed the point. As long as Indians could produce all the necessaries of life through the traditional crafts and they refrained from purchasing industrial goods, there was no need to make the economy competitive. What good is it to make the economy competitive, if that means that most people will have to spend all their time struggling to earn a living? What kind of life is that? How are people who live that way meant to find the time for politics and spirituality? Such a country would be riven with violence and exploitation. From Gandhi’s point of view, it would be hardly any different from British India.
After this exchange of letters in 1945, Gandhi became increasingly focused on preserving the traditional crafts, especially spinning cloth on the traditional spinning wheel. He emphasised the spinning wheel ever more heavily, so much so that, even to this day, the wheel lies at the centre of the Indian flag.
A fter the Second World War ended in 1945, independence was imminent. With very little time left to win the argument, Gandhi became suspicious of the other Indian independence leaders. In late 1945, Gandhi accused them of wanting ‘to destroy khadi’. In 1946, he emphasised that the introduction of the industrial spinning mill is so corrosive to his political project that if a ‘tyrant wants to destroy the spinning-wheel itself … we should ourselves perish with the spinning-wheel and not live to witness its destruction.’ He insists that spinning is the only way ‘to achieve swaraj for the poorest of the poor and the weakest of the weak’.
In Gandhi’s final years, he grew more and more focused on khadi. His writings in 1946 and 1947 refer to this cloth hundreds of times. He worries about uncertified khadi dealers, its commercialisation, the use of fabrics and materials to circumvent khadi rules. He argues that it is necessary to create a ‘yarn bank’ to ensure that khadi workers always have access to the materials. Spinning will work as a vehicle for swaraj only if the spinners understand the role it plays. He writes: ‘[I]f workers themselves lack faith then the claim for khadi will fall to the ground.’
The workers are to desist from adopting the mill because they know it is the thin end of the wedge, that to abandon the wheel for the mill is to start the process of colonialism all over again. If the workers do not understand that, then they will allow the wheel to be taken from them. Without the wheel, the varna system cannot be recovered, and any swaraj the workers obtain will be empty. In July 1946, a critic accuses Gandhi of forcing the villagers to spin. Gandhi replies that Indian villagers ‘gave up khadi because they were tempted by mill-cloth’. He compares mill-cloth to a poisonous drug, suggesting he is freeing the villagers from a kind of addiction. He denies that he is violating their rights – if mill-cloth is not available and the villagers do not make their own cloth, they ‘have the right to shiver in the cold and remain naked’.
Commit to this new education, and Gandhi was confident that ‘in five years India will be a leading country in Asia’
In July 1946, Gandhi writes that towns existed before the arrival of the British. Things were ‘bad enough then’ but now ‘they are much worse’ because the towns have become cities devoted to enriching both ‘Indian millionaires’ and ‘British masters’. Khadi is to ‘undo the great mischief’. That mischief is not just the British, but the spiritual situation that, for Gandhi, allowed the British to colonise India. This is a view Gandhi maintained throughout his life. In 1908, he argued that the British were able to establish themselves in India only because the Indians assisted them. He writes that ‘in order to become rich all at once’ the Indians welcomed the British ‘with open arms’.
In the autumn of 1946, Gandhi was still hoping that Nehru understood – or could be made to understand – the importance of khadi. Gandhi says: ‘We shall have full freedom only when our uncrowned king Pandit Jawaharlal Nehru and his colleagues in the Interim Government devote themselves to the service of the poor as people expect them to do.’ He quotes Nehru as having called khadi the ‘livery of our freedom’.
In May 1947, Gandhi pleads for government workers to ‘forget their quarrels and disputes over ideologies and start learning and teaching spinning, khadi work and village industries’. If they commit to this new kind of education, Gandhi expresses confidence that ‘in five years India will be a leading country in Asia’.
But, over the course of 1947, it became increasingly clear that Gandhi was not going to win the argument. In June, he bemoaned the situation, calling the other independence leaders ‘selfish’. In November, Gandhi writes that if the village industries are neglected in an independent India, ‘we will be acting like a man who remembers God in sorrow and forgets Him when He showers [us in] happiness.’ Later that month, he confesses that ‘talk of khadi and village industries does not interest people any more.’ ‘I know that khadi and all allied activities have slackened because we have achieved swaraj,’ Gandhi writes, ‘India will get what is ordained for her. What can we do?’
I n the days and weeks leading up to his death in January 1948, Gandhi began suggesting a new political system designed to empower the villages – the Panchayati Raj. Representative democracy could not be relied upon to integrate the economy and religion into a system that unites the need to survive with the need to spiritually thrive. But, before his alternative political system could be elaborated, much less implemented, Gandhi was assassinated. Just a few weeks earlier, in December 1947, Gandhi had lamented that ‘the main implication of khadi’ was not grasped by the independence movement. He said he had ‘no doubt’ that khadi is ‘more important than ever if we are to have freedom’ for ‘the masses of the villagers of India’. ‘Through khadi,’ Gandhi struggled ‘to establish supremacy of man’ over machine. He strove for equality of all men and women, and he strove ‘to attain subservience of capital under labour in place of the insolent triumph of capital over labour’.
So, Gandhi saw varna as the way to discipline capital so that it served life. But his vision for the role of the varna system was always quixotic. Indians, including Nehru, embraced economic modernisation. As Nehru put it, he felt there was ‘no reason’ why millions of Indians should not have ‘comfortable up-to-date homes where they can lead a cultured existence’. This was to be achieved with electricity, trade, modern transportation and heavy industry, not with a return to traditional village crafts. Gandhi left open the possibility that, if Indians felt it good and necessary, then they could add new professions beyond the traditional crafts. He recognised that political decision-making is difficult and requires capacities and specialties that are not easily cultivated in people. Even deeply religious people who are sincerely committed to the truth often disagree with one another, and for Gandhi this was baked into the human condition.
In 1930, Gandhi had written that, while all faiths ‘constitute a revelation of Truth’, they are all ‘imperfect and liable to error’. He suggested that this stems from the fact that, while ‘the soul is one’, the ‘bodies which she animates are many’. Since we cannot ‘reduce the number of bodies’, faith in the unity will ‘partake of human imperfection’. Embodied human beings will put their faith ‘into such language as they can command’, and their words are interpreted by other imperfect beings. Everyone will think themselves right, but ‘it is not impossible that everyone is wrong’. This produces a need for tolerance – not an ‘indifference towards one’s own faith’, but a ‘purer love for it’.
Gandhi tasks the poor with preventing the varna system from ossifying into one of caste
In the spirit of this view, Gandhi often described himself as one who ‘experiments’ with truth. Satyagraha , nonviolent civil resistance , rests on the idea that all of us, even those with spiritual education, can be mistaken. Other people should confront us in those situations – carefully, and nonviolently.
For Indians to have true swaraj, they must have the education necessary not merely to understand the reasoning behind Gandhi’s economic model, but to participate themselves in reforming that model based on their own understanding of truth. They must be able to think for themselves about whether all Indians should perform the manual crafts. They must be able to develop views about which professions are necessary and which are unnecessary. Gandhi’s desire to empower Indian citizens to rule themselves led him to allow India’s citizens freedom to work in additional professions, provided they practise them out of love rather than greed.
That proposal came with risks of its own. If one varna contains both those who depend exclusively on the traditional crafts and those who perform additional professions, this could lead to hierarchy within it. This is especially likely if those who perform additional professions are able to derive additional income from those professions. At points, Gandhi suggests that those who earn additional income from additional professions could serve as ‘trustees’, retaining some control over the wealth they gain from their additional professions, provided that they use this wealth to benefit others. This would leave some economic and political inequalities intact. Over time, it could lead to the reemergence of caste.
Gandhi ultimately tasks the poor with preventing the varna system from ossifying into one of caste. To perform this role, they must acquire the advanced civic education necessary to engage in satyagraha, and that in turn is possible only insofar as they are able to earn a living through the crafts. This was an enormous responsibility to place upon the shoulders of ordinary workers. The varna system can resist lapsing into a system of caste only when it is possible for the workers to consistently become spiritually learned and to remain spiritually learned across time. For Gandhi, it is only when the poor gain knowledge that they ‘become strong’ and ‘learn how to free themselves’. Nothing less will do, because the varna system is too fragile to maintain itself by lesser means.
Those who view Gandhi merely as a critic of violence, hierarchical caste and untouchability miss what is meant here by freedom and equality. This is about securing for every Indian the economic prerequisites for spiritual growth. For Gandhi, it is only in a world where everyone practices the crafts – and everyone can learn them at home from their parents – that there will be time enough for every person to develop their own spiritual praxis. In such a system, there is clearly observance of hereditary occupation, and therefore of varna.
Gandhi failed to establish this system, and no alternative system has arisen to perform the same function. The poor are still compelled to trade away their time in the struggle for survival, while the rich waste the time they take from the poor. But Gandhi tried to solve this problem, and many of us do not even try.
This piece contains excerpts from ‘The Varna System in Gandhi’s Theory of Civic Education’, first published in the journal Economic and Political Weekly in May 2024."
Why planetary problems need a new approach to politics | Aeon Essays,,https://aeon.co/essays/why-planetary-problems-need-a-new-approach-to-politics,"‘Everybody knows that pestilences have a way of recurring in the world,’ Tedros Adhanom Ghebreyesus declared to the World Health Assembly on 29 November 2021, quoting Albert Camus’s The Plague . ‘Outbreaks, epidemics and pandemics are a fact of nature,’ Tedros, the director-general of the World Health Organization since 2017, continued in his own words. ‘But that does not mean we are helpless to prevent them, prepare for them or mitigate their impact.’ Exuding confidence, he proclaimed: ‘We are not prisoners of fate or nature.’
The topic of this special session of the WHA – only the second one convened since the WHO was founded in 1948 – was to establish international negotiations to reach a global agreement on ‘pandemic prevention, preparedness and response’. The delegates passed a resolution directing negotiators to begin work on a pandemic treaty to be ready to present for approval by the 77th WHA in May 2024. But, days before the assembly meeting was due in Geneva, word leaked that the Intergovernmental Negotiating Body had failed to meet the deadline. There would be no pandemic agreement.
It wasn’t for lack of trying. The diplomats, working 12-hour days, understood the importance of their task. Having just suffered through the COVID-19 pandemic, the stakes were – and are – exceedingly clear. ‘ COVID-19 has exposed and exacerbated fundamental weaknesses in the global architecture for pandemic preparedness and response,’ Tedros explained. The only way forward after so much suffering, he urged, was ‘to find common ground … against common threats,’ to recognise ‘that we have no future but a common future.’ As the co-chair of the negotiations Roland Driece put it, reaching a global agreement was necessary ‘for the sake of humanity’.
D espite a broad consensus that everyone would be better off were we globally prepared, negotiations still stalled. The major sticking points appear in Article 12 of the draft treaty , ‘Pathogen Access and Benefit-Sharing System’. Under this arrangement, countries would be required to rapidly share information about emerging pathogens, including samples and genetic sequences. But the Global South justifiably fears that their costly efforts at monitoring and information-sharing will be used to create tests, vaccines and therapeutics that get hoarded by the Global North. Negotiators from lower-income countries insist that the treaty includes guarantees for equitable access to any pharmaceutical developments, something that wealthier countries are hesitant to accept. ‘We don’t want to see Western countries coming to collect pathogens, going with pathogens, making medicines, making vaccines, without sending back to us these benefits,’ Jean Kaseya, the director-general of the Africa Centers for Disease Control and Prevention, told The New York Times .
Beyond political disputes over finance mechanisms, the equitable distribution of vaccines and treatments, and intellectual property rights, the reason for the failure to reach a global pandemic agreement boils down to the core conceptual feature of the contemporary international system: state sovereignty. Though the draft treaty is adamant in its respect for national sovereignty – it both reaffirms ‘the principle of the sovereignty of States in addressing public health matters’ and recognises ‘the sovereign right of States over their biological resources’ – nation-states have baulked at granting new authority to the WHO. Republicans in the United States Senate have demanded that the US President Joe Biden’s administration oppose the pandemic treaty, claiming it would ‘constitute intolerable infringements upon US sovereignty’. The United Kingdom government, likewise, has said it will support the treaty only if it ‘respects national sovereignty’.
In politics, there is no ‘world’; only states. For pathogens, there are no ‘states’; there is only the world
These concerns about sovereignty get to the molten core of the problem with this pandemic treaty, or really any pandemic treaty – indeed the entire multilateral system. The WHO, like every other arm of the United Nations, isn’t accountable to the world or even to world health but to the nation-states that are its members. As a result, things that would be good for ‘the world’ – like a global strategy to fight the next pandemic – often crash into firm convictions about the national interest as well as the hard-won, jealously guarded principle of national autonomy.
Tedros may believe that ‘the world still needs a pandemic treaty,’ and that it’s his mission ‘to present the world with a generational pandemic agreement,’ but he will again and again face the same problem: in politics, there is no ‘world’; only states. Compounding the problem is the fact that for pathogens, there are no ‘states’; there is only the world.
This basic mismatch between the scale of the problem and the scale of possible solutions is a source of many of today’s failures of global governance. Nation-states and the global governance institutions they have formed simply aren’t fit for the task of managing things such as viruses, greenhouse gases and biodiversity, which aren’t bound by political borders, but only by the Earth system. As a result, the diplomats may still come to agree on a pandemic treaty – they’ve committed to keep working – but, so long as the structure of the international system continues to treat sovereignty as sacrosanct, they will never be able to effectively govern this or other planetary-scale phenomena.
I n our quest for control over nature’s slings and arrows, we humans have dammed rivers and made war on microbes, turbocharged grain production and ventured into outer space. We’ve domesticated animals into companions, labour and food, and figured out how to turn the fossilised remains of ancient lifeforms into energy. We’ve constructed homes and cities, razed forests and grasslands, built berms and seawalls, all to keep the elements at bay and improve our own lives. As we did all this, we took account only of human needs and desires – or rather, of some humans’ needs and desires – and ran roughshod over everything else. What’s good for fungi, flora or fauna remains irrelevant, if not deliberately negated. From a certain point of view – one held mainly by the wealthy and powerful – it seems as if Man has conquered Nature, or at any rate is justified in trying.
These pretensions of mastery have cultural as well as technological origins. Culturally, we in the West, at least, have inherited a tradition of human exceptionalism rooted in the idea that human beings, uniquely, are made in God’s image and, as the Bible says, are meant to ‘have dominion … over all the earth’. Over millennia, human civilisations have developed the tools to enact that dominion – to use nature solely as our ‘instruments’, as Aristotle put it. Technologies, from the control of fire to writing to the internal combustion engine to CRISPR, have given humans immense power over other species and Earth itself. But too often our self-image produced by the interactions of our culture and our technologies has led to the belief that this power is unbound and that we have succeeded in taming nature.
There is no possibility of human thriving unless the ecosystems that we are part of thrive
An emerging scientific consensus, however, makes clear that not only have we not tamed nature, we can’t tame nature, for the simple reason that we are part of nature. Human beings are inextricably part of the biosphere, part of Earth. These insights emerge from rigorous scientific study, not mystical reflection, and reveal our place within the biogeochemical churn of this planet. A vast and expanding infrastructure of sensors across, above and below Earth, and the networks of software and hardware that process and interpret the mountains of data the sensors produce, have demonstrated, with an accuracy and precision unmatched by previous generations, that humans are embedded in this planet’s system of systems.
What this new and growing planetary sapience is revealing is systematic wreckage. Scientists have determined that human actions (really, some humans’ actions) have pushed Earth past the ‘safe operating space for humanity’ for six of nine ‘planetary boundaries’, including climate change, biosphere integrity and freshwater change. We now understand not only the damage that we are doing to planetary systems but the damage that we are doing to ourselves as elements of those systems. The Earth sustains us, not the other way around. There is no possibility of human thriving unless the ecosystems that we are part of thrive.
The realisation of our planetary condition may insult our narcissistic self-regard, but it also yields a positive possibility: that human flourishing is possible only in the context of multispecies flourishing on a habitable planet. The aim of habitability is meant to diverge from the now-dominant concept of sustainability. While the concept of sustainability treats nature both as distinct from humans and as existing for humans’ responsibly managed instrumental use, the concept of habitability understands humans as embedded in and reliant on the more-than-human natural world. Stripped of sustainability’s anthropocentrism, habitability focuses on fostering the conditions that allow complex life in general – including, but not only, humans – to live well. This vision of multispecies flourishing is at once generous and selfish. Expanding the circle of concern to include the multispecies menagerie is certainly more beneficent than current politics typically allows, but it is also absolutely about ensuring the survival of our species. What’s bad for them is, ultimately, bad for us. These goals – thriving ecosystems in a stable biosphere supporting human lives and nonhuman life – must be our new lodestar.
The central question of our time is: how can we achieve this?
T he term that scholars and policymakers initially proposed to make sense of this new knowledge is ‘global’. It is now common knowledge that Earth is experiencing global climate change, we just lived through a global pandemic, global biodiversity is at risk of its sixth mass extinction event, and this is an era of global economic, political and cultural interconnections. Yet this familiar language of the global papers over an important distinction. The word globe as it’s used in discussions of globalisation, observed the historian Dipesh Chakrabarty in 2019, ‘is not the same as the word globe in the expression global warming’. The globe of globalisation is a fundamentally human concept and category: it frames Earth from a human point of view. This globe is constructed for and by human intentions and concerns. Globalisation, the process of worldwide integration predicated on this perspective, is about the movement of people and their stuff, ideas, capital, data, and more.
The globe of global warming is a different object altogether. This concept and category – which we will now call the ‘planetary’ – frames Earth without adopting a human point of view. From the planetary, as opposed to the global, perspective, what stands out is the interlinked systems of life, matter and energy. This concept forces us to take on objects and processes that are much vaster and much smaller than we can easily comprehend, as well as timeframes far outside lived human experience. Trying to make sense of the ‘intangible modes of being’ captured by the concept of the planetary, as the anthropologist Lisa Messeri writes in Placing Outer Space (2016), is a struggle, but we have no choice. The globe of global climate change – the planet – impacts humans and is impacted by humans, but it existed before our species evolved and will be here long after our extinction.
In approaching problems such as climate change as global – that is, in a fundamental way, human – we have made a categorical mistake. For one, it suggests the goal for our action should be sustainability – an anthropocentric, global concept – rather than habitability – a multispecies, planetary concept. Moreover, the framing of problems as global suggests that they can be addressed with the tools we have at hand: modern political ideas and the architecture of global governance that has emerged since the Second World War. But planetary problems cannot. This helps to explain why decades of attempts to manage planetary problems with global institutions have failed.
The UN answers not to humanity nor the world, but the nations that united to join it
The failure to halt greenhouse gas emissions – the cause of planetary climate change – is a prime example. In June 1992, at the Earth Summit held in Rio de Janeiro, the representatives of 154 nation-states signed the United Nations Framework Convention on Climate Change (UNFCCC), committing to ‘prevent dangerous anthropogenic interference with the climate system’. The international agreement was hailed as a landmark step in global environmental governance, but the very text of the treaty reveals the source of its own impotence. Alongside its plea for ‘the widest possible cooperation by all countries’ toward avoiding the ‘adverse effects’ of climate change, the treaty reaffirms nation-states’ ‘sovereign right to exploit their own resources’, including, of course, fossil fuel resources. ‘The principle of sovereignty of States,’ the UNFCCC declares, is the bedrock of any ‘international cooperation to address climate change.’
The UNFCCC, which remains the primary global body tasked with curbing climate change, doesn’t respond to the atmosphere, nor to the planet it envelops. Like the WHO, it responds instead, and only, to its member states. The member states, meanwhile, respond to their human citizens (at least, ideally). No part of this chain of authority is concerned with the planet’s climate as a whole. In this, the UNFCCC is no different than any of the other institutions of global governance. The international system is built upon the foundation of the sovereign nation-state. The UN and its many parts and agencies – from UNICEF to the Universal Postal Union – answer not to humanity nor the world, but the nations that united to join it.
Though it is better than not to have international forums to foster dialogue and cooperation among nation-states, the contemporary global governance architecture does not overcome the territorially and politically fragmented structure of the nation-state system. In fact, global governance projects and reinforces nation-state politics at a worldwide scale. International politics isn’t ‘carried out for the sake of world interests,’ remarks the political philosopher Zhao Tingyang in All Under Heaven (2021), ‘but only for national interests on a world scale.’
Managing world-scale, or planetary, problems, however, requires acting for ‘world interests’. Thus planetary problems require solutions at the planetary scale. The scale of these problems is incommensurate with our current institutional capacity to govern them. Managing problems at the scale the planet, therefore, requires creating governance institutions at the scale of the planet.
T his doesn’t mean, however, that we’d be best served by a world government. To the contrary – the nature of planetary problems makes a single world state ill-suited for the task at hand. While fundamental characteristics of planetary phenomena operate at the scale of Earth, the consequences of these phenomena that we most care about occur at a local level.
Climate change, for instance, is caused by the emission of greenhouse gases into the atmosphere by specific tailpipes driving on specific roads, specific power plants operating in a specific territory, and so on. But once those rooted, place-based carbon compounds drift into the atmosphere, they become an undifferentiated part of the atmosphere’s chemical makeup. It is the overall concentration of greenhouse gases in the atmosphere that changes the climate. Ultimately, the reason climate change concerns us, however, isn’t because of its average global effects, but because of how a changing climate manifests in specific places. What matters is how rising temperatures, increased aridity or flooding impact regions, communities and households.
No single political form is adequate for the multiscalar nature of planetary problems
From a policy perspective, this is the fundamental structure of all planetary problems: they transpire across immense, unhuman geographies and timescales but their consequences play out in particular ways in particular places (shaped by the intersection of geographic, topographic, ecological, social, economic and political conditions, and more). Take, for example, the COVID-19 pandemic. The pandemic – which emerged from the dynamic relationship between human beings and viruses that has shaped our species since it first evolved – was driven by the movement of SARS-CoV-2 from body to body, a process that respects neither the borders between nation-states nor the boundaries between species – other animals, including cats and civets, are susceptible to the virus. As a result, the disease spread to every corner of the planet.
The concern with the contagious disease, however, was how it affected – and, though now oft ignored, continues to affect – communities, families and individuals. The abstract vastness of a planet-scale pandemic mattered to most of us when it shuttered beloved restaurants, kept families apart, and infected friends, family or us. This interplay between scales is a critical feature for the governance of planetary problems, from stratospheric ozone depletion, atmospheric aerosol loading and space junk, to growing antibiotic resistance, biodiversity loss and anthropogenic genetic disruptions, to upended biogeochemical cycles.
What makes planetary problems so difficult to govern is that we need structures that can act at both the planetary scale and the hyperlocal scale. Nation-states are not fit for purpose. They can band together to form international organisations and they can delegate authority to subnational units (provinces, states, cities, etc), but as a political form, the nation-state is focused on that national scale. Issues that operate ‘above’ or ‘below’ the nation are peripheral to the state’s primary concerns.
The nation-state’s ill-suitedness is a major concern since it is the primary political institution today. But, in fact, no single political form is adequate for the multiscalar nature of planetary problems. What we need is plural forms of governance that can operate at all the scales necessary to tackle the problem.
M ultilevel governance is already the norm around the world. Policy decisions and implementation take place at multiple levels of government and other public authorities, from neighbourhood councils up through city governments to national capitals and international organisations. But there are two crippling flaws with the existing multilevel governance architecture for the globe. First, some of the necessary scales lack governance institutions. In particular, the current system is missing planetary governance institutions, institutions that are tasked with and capable of managing planetary challenges. Second, most smaller-scale, subnational governance institutions don’t have the authority or resources necessary to address local challenges in a way that satisfies and responds to constituent desires.
Both flaws in the current system stem from the same cause: national sovereignty. While governance responsibilities are distributed among many levels, ultimate authority at present sits with only one institution, the nation-state. As a result, global governance institutions and local governments are subordinate to sovereign nation-states. Nation-states can – and sometimes do – delegate authority to international and subnational institutions, but that authority is subject to a limit: it cannot interfere with whatever the nation-state deems to be its sovereignty. The result of this constraint is that local-scale issues often aren’t governed robustly, and planetary-scale issues rarely are.
Adequately governing these many scales requires two significant changes to the worldwide architecture of governance: introducing new scales of institutions and transforming how governance authority is distributed through the system.
To simplify matters, let’s consider three primary scales of governance: local, national and planetary. Each is designed to manage the appropriately scaled issues and challenges, and together they operate as a system. Our basic vision is a structure made up of well-resourced, high-functioning institutions at all scales, from the planetary to the local, capable of governing at all scales, from the planetary to the local.
A planetary health institution would act against infectious diseases at all scales, from local to planetary
The widest scale, the planet itself, requires the widest-scale institution: planetary institutions. These, in our vision, are the minimum viable body for the management of planetary issues. We contend that each planetary problem requires its own planetary institution to govern it. As a result, a planetary institution would have defined and restricted authority at the planetary scale over a specific planetary phenomenon.
Planetary institutions, therefore, are not world government. A world state would be a single, general-purpose governance institution with broad authority over the whole planet. What we envision is multiple, functionally specific governance institutions with narrow authority over particular issues. At the same time, however, planetary institutions are not contemporary global governance. Global governance institutions today operate as multilateral associations of sovereign nation-states, which ultimately represent the interests of their member states. Unlike the WHO and UNFCCC, planetary institutions should be more directly accountable to the interests of the planet as a whole.
An example of an institution that could actually properly manage aspects of ‘world health’ on behalf of the entire world might be called the Planetary Pandemic Agency. To be effective, this planetary health institution would need the capabilities and authority to act against infectious diseases anywhere on the planet. This requires monitoring of outbreaks and enforcement of preventative measures at all scales, from local to planetary – authorities that the WHO lacks. Such an agency, moreover, must have a planetary approach to health in the sense that it understands human health as interconnected with the health of animals, ecosystems and the Earth system. So, it must be planetary not only in terms of scale but in terms of a holistic vision: that protecting our health requires protecting the planetary whole. (To its credit, the draft pandemic treaty promotes ‘a One Health approach … recognising the interconnection between the health of people, animals and the environment.’) Rather than focusing on isolated toxicities and pathogens, a planetary health institution that lives up to its mandate must keep front of mind that infectious diseases emerge from the place of humans in biogeochemical and ecological systems.
The middle scale should be governed by nation-states tasked with managing the issues fit for their scale. Nation-states thus still have a role under our vision, but that role is much reduced from the present. Nestled in a broader multiscalar governance framework, nation-states will in fact likely be better equipped to succeed at the tasks and functions for which they are appropriate, namely, distributing and redistributing economic gains and losses. Economic governance – which is a political, not a technical, activity – has historically worked best at the national scale, where political institutions can facilitate collective life between the immense abstractness of the planetary and the place-based familiarity of the local.
We must redesign the entire architecture of how and where governance decisions are made
Local governance institutions, finally, should be empowered to develop and implement robust responses to local problems and demands. They should have the resources and authority necessary to pursue policies that are appropriate to local social, political, climatic and ecological conditions, as well as to adapt agilely as those conditions change. This would represent a sea change from the operations of most local governments today. It requires well-equipped local institutions capable of managing the shared challenges of their residents. One proposal for building the capacity of local institutions is to strengthen the formal and informal ties between subnational governments. That is, to build upon the success of city-to-city networks, such as the C40 Cities Climate Leadership Group (a network of almost 100 mayors of global cities committed to climate action), and establish new or augment existing transnational networks for exchange and cooperation among local governments.
Building and supporting governance institutions at all scales, from the smallest face-to-face communities to the entire Earth, provides the foundation for adequate governance at all scales. It addresses the critique made by Elinor Ostrom, a Nobel laureate in economics, of the widespread assumption among policymakers ‘that only the global scale is relevant for policies related to global public goods’. Her pathbreaking work demonstrated that that effective management of large-scale problems requires work by large-scale, medium-scale and small-scale bodies. This is what our proposed architecture sets out to provide. It offers a vision for one worldwide governance system, but not one with a unitary world governance led from one centre of power. Power, in our architecture, is dispersed among the units that need it to tackle specific problems.
Our takeaway from the revelation of humankind’s planetary condition is twofold. We need to establish new governance institutions at the scale of the planet that are able to manage phenomena at the scale of the planet. But that isn’t the only implication. We must redesign the entire architecture of how and where governance decisions are made. Dealing with planetary challenges requires both the possibility of planet-wide action and action at all other appropriate scales throughout the system. The complexity of life on this planet means that there is no one-size-fits-all institution. Rather, we must create institutional structures that foster flexibility, with multiple institutions for multiple scales, individually and collectively crafting effective governance for diverse populations seeking to thrive on one interconnected planet.
H ow can we organise such a complex system of governance? How should we decide which authorities should be allocated where? Our answer builds on the centuries-old principle of subsidiarity . The principle of subsidiarity states that in a multilayered governance system, larger-scale institutions shouldn’t intervene in a decision or task unless and until a smaller-scale institution cannot do it themselves. In other words, the authority to make decisions should be made at the smallest scale capable of functionally governing the issue at hand.
Subsidiarity is in direct opposition to the status quo principle for the allocation of authority, state sovereignty, which gives all authority to nation-states. To be sure, sovereign states can then decide to delegate certain authorities, if they wish, to international organisations, subnational governments or private actors, but the international system today puts nation-states in the driver’s seat. Every issue and function, regardless of whether states are well-suited to manage them, go to nation-states by default. Climate change, to take a pressing and archetypical planetary problem, is governed, in the end, by states. Even the 2015 Paris Agreement, the most important global climate accord, makes clear that the action comes from nation-states: ‘Parties shall pursue domestic mitigation measures, with the aim of achieving the objectives of such contributions,’ the diplomats wrote, leaving goal-setting and enforcement to each state.
By contrast, subsidiarity understands that while states are good for some things, they aren’t good for everything. States should have authority over the issues that fit them, but authority over other issues should move to institutions at other scales with a better fit. At the centre of the principle of subsidiarity is the message that in a diverse world there cannot be just one right answer.
Applying subsidiarity with our dawning recognition of our planetary condition generates a new principle for the allocation of authority: planetary subsidiarity. Planetary subsidiarity is the principle that we offer for allocating authority over an issue to the smallest-scale institution that can govern the issue effectively to promote habitability and multispecies flourishing. The principle provides a tool for assessing how to simultaneously address planetary challenges, such as pandemics and biodiversity, while at the same time maximising local empowerment.
Local officials should have authority over the how , not the how much
How might this principle apply in practice? Consider again the case of climate change. The first thing to acknowledge is that climate change is a quintessential planetary issue. Greenhouse gas emissions that take place anywhere have an impact everywhere. It doesn’t matter if carbon is burned in central Los Angeles or rural Laos, once it enters the atmosphere it has consequences for the entire Earth system. As a result, the smallest-scale jurisdiction that can effectively mandate climate mitigation must encompass the whole planet. Yet that doesn’t mean that a planetary institution tasked with governing carbon emissions would take charge of the entire process. Instead, a planetary climate governance institution would take only high-level decisions – about, say, the maximum permissible carbon budget for the planet each year – and then turn over the implementation to smaller-scale institutions. The planetary institution, in other words, makes only decisions that must be made at the planetary scale in order to be effective.
Nation-states would receive the planetary mandates on greenhouse gas reductions that must be met and then develop national policies for achieving them. Given the distributional consequences of these decisions across sectors and regions, the nation-state – which is the only political institution in history that has succeeded in meaningful economic redistribution – is best positioned to act. National politics, we believe, is the best site for hashing out questions like: should certain sectors or regions be compensated for losses? Or, who should pay for these changes?
After nation-states distribute the costs and benefits of climate mitigation across their society and economy, it should be up to local-scale institutions – regions, provinces, states, municipalities, villages, neighbourhoods, and the like – to determine the details of implementation. This is because local institutions are best placed to respond to local concerns, place-based affordances and constraints, and political, cultural, climatic and ecological conditions. It should not be up to localities, with particular economic interests or political preferences, to decide whether to reduce greenhouse gas emissions or by how much, but they should determine how to meet those reductions. Local officials – preferably working in networks with others facing similar challenges – should have authority over the how , not the how much . (Though this applies only to minimums; we encourage implementers to exceed their mandated reductions.)
Subsidiarity helps us to determine which of these institutional scales should have what authority over climate mitigation. It is a tool for aligning scales, functions and authority in an appropriate manner for promoting habitability and multispecies flourishing.
S hifting our conceptual toolkit from the global to the planetary will take time and great effort. But it is nothing compared with what it will take to transform our political system from one founded on the sovereign nation-state to one rooted in planetary subsidiarity. It would represent a revolution in the governance of the world – and we do not have a map for how to get there. Change must come the way it always comes, through new ideas and political struggle. Beyond that truism, however, we do not pretend to see a path for such a radical transformation of the basic structures of politics and governance.
In this, we find ourselves in good company. Even ideas that eventually succeeded in transforming systems of governance often took many decades and even centuries to be adopted. The idea behind the League of Nations (established in 1920) and the UN (established in 1945) lies with Immanuel Kant’s notion, from Perpetual Peace (1795), that ‘The law of nations shall be founded on a federation of free states.’ Forty years later, in his poem ‘Locksley Hall’ (1835), Alfred, Lord Tennyson could dream of ‘the Parliament of man, the Federation of the world’ where ‘the common sense of most shall hold a fretful realm in awe, / And the kindly earth shall slumber, lapt in universal law.’ But it took the cataclysm of the First and the Second World Wars to move this idea from the minds of philosophers and pages of poets to actual political institutions.
Crises, like world wars, are often the midwife for institutional change. Major changes to governance structures typically occur during or in the aftermath of disasters that push the existing institutional order to or past its breaking point. It’s a tragedy of politics that these changes generally come too late – that the crisis itself is what makes ‘impossible’ proposals finally seem not just reasonable but necessary. The science-fiction novel The Ministry for the Future (2020) by Kim Stanley Robinson offers one scenario where a devastating heatwave killing tens of millions of people leads to the establishment of a creative new governance structure. It isn’t difficult to imagine additional calamities for this planet.
We can’t predict what the galvanising catastrophe might be that brings about new systems of governance. We must focus our efforts instead on defining a clear perspective on what planetary governance could and should be. Holding such a vision in our minds may make it more possible to take advantage of the crisis that will all but inevitably arrive given the inadequacy of the current system. As we enter a period of not just geopolitical but geophysical uncertainty, calibrating our North Star – our vision of where we want to head – will be more important than ever."
What the Ju/’hoansi can tell us about group decision-making | Aeon Essays,,https://aeon.co/essays/what-the-ju-hoansi-can-tell-us-about-group-decision-making,"The Dilemma of the Deserted Husband unfolded in the late 1950s amid a band of G/wi hunter-gatherers, a subgroup of Ju/’hoansi (often known as !Kung San), dwelling in the Kalahari Desert of Southern Africa. According to the South African-born anthropologist and Bushman Survey Officer George Silberbauer, a woman named N!onag//ei had left her husband, /wikhwema, for his best friend. Few were surprised. After all, /wikhwema was a temperamental and pompous man, and a bit of a joke. In contrast, the new husband, /amg//ao, held unconventional charm. He was ‘a virtuoso dancer, a consistently successful hunter and … rumoured to be a bit of a demon as a lover.’
Deserted G/wi spouses usually move on within a few months. But not /wikhwema. Mourning the dual losses of his friend and wife, he complained endlessly. Before long, his incessant whining became a burden on everyone. After more than a year, people were at their wits’ end. Complicating matters, given his role as an ‘owner’ of the band’s territory, /wikhwema could not be expected to relocate elsewhere. The band had no choice but to stick together.
Eventually, a ‘lateral thinker’ proposed a novel solution. Why not permit a polyandrous marriage? This unconventional suggestion meant that N!onag//ei could have both /amg//ao and /wikhwema as husbands, a departure from the monogamous norms of G/wi society. After much deliberation, the innovation was accepted. The new couple’s marriage was salvaged, as was /wikhwema’s pride, and the band was relieved of his whining.
The Dilemma of the Deserted Husband was not solved by the unilateral decision of a single leader. Nor did people raise their hands in a majority vote. Instead, it was the product of long deliberation. For months, there were discussions, disagreements and compromises. The goal of the process was consensus, to find a solution that everyone could live with, even if it was imperfect (the new throuple ‘did not exactly live happily ever after’, according to Silberbauer).
For the vast majority of human history, people made group decisions through consensus. It is perhaps the most conspicuous feature of political life among recent hunter-gatherer societies, from the Ju/’hoansi to the Aboriginal peoples of Australia to the Indigenous societies of the early Americas. As an anthropologist, I have observed consensus-based decision-making myself among hunter-gatherers in the rainforests of Malaysia.
Though the small-world life of hunter-gatherers may seem far removed from our own digitalised and global world, the problems of group life have remained fundamentally the same for hundreds of thousands of years. In the face of conflict and polarisation, ancient human groups needed processes that yielded good outcomes. What can we learn from a political form shaped by hundreds of thousands of years of trial and error? By examining how hunter-gatherers achieve consensus, perhaps we can develop better strategies to solve the problems we face today.
H uman prehistory was littered with poor group decisions. Whether it was an ill-timed raid or the wrong choice of watering hole, some of our would-be hunter-gatherer ancestors vanished without a trace. We know this because, among hunter-gatherers today, group decisions are matters of existential importance. As an anthropologist, I have been trying to understand how exactly hunter-gatherer groups succeed – and how they fail.
In the annals of group failure, groupthink is the most common culprit. The phrase was coined by the journalist William H Whyte Jr in 1952, but it is generally associated with the Yale psychologist Irving Janis, who argued that the pressures of social conformity can doom group performance. People may be bullied by their superiors or feel that they risk ostracism from their peers. Important information is left unspoken. By failing to weigh their options judiciously, groups consider only a fraction of the space of possible solutions. Scholars have called this an information cascade. As evidence of groupthink, they often point to famous debacles such as the Bay of Pigs, the Challenger disaster, and the 2003 invasion of Iraq.
At the other end of the spectrum, groups can fail by fragmentation. This possibility is hardly considered in Western studies of social psychology, but it looms large for hunter-gatherers. Through the centrifugal forces of disagreement, the group splinters, with subgroups or individuals going their own way, thereby forfeiting the benefits of cooperation that come from living in a bigger group. As we saw in the Dilemma of the Deserted Husband, hunter-gatherer bands recognise the benefits of staying together.
The Ju/’hoansi are careful not to entrust key decisions to single individuals or small sub-groups
A few years after Silberbauer observed the Dilemma of the Deserted Husband, the young Harvard anthropologist Megan Biesele travelled to the Kalahari to begin her PhD research. It was 1970, and Biesele was there to study ritual and folklore among a band of Ju/’hoansi in the Dobe area, an inhospitable expanse of sand and bushland seasonally flooded by rain, not far from where Silberbauer worked among the G/wi. As she became proficient in the Ju/’hoan language, Biesele observed many group discussions and decisions, taking special notice of the Ju/’hoansi’s consensus-based decision-making. Together, Biesele’s and Silberbauer’s observations show us how the Ju/’hoansi keep groupthink and fragmentation in check, navigating between what Silberbauer calls ‘ the Scylla of excessive interdependence and the Charybdis of fragmenting anarchy’.
The Ju/’hoansi are careful not to entrust key decisions to single individuals or small sub-groups. Leadership is temporary and knowledge-based, shifting even within a single conversation. Leaders refrain from stating their opinions early in the conversation, which could bias the opinions of others who have yet to speak. The role of a leader in group decisions is to guide deliberation, state the group’s mood, and help finalise a decision. Leaders are respected, but they cannot coerce others. Biesele refers to this as ‘sapiential authority’.
With the goal of consensus, the group itself is the decision-maker. Decisions typically start as grassroots affairs between neighbours and friends. Only later does the community gather together for a formal meeting. During deliberation, everyone – man or woman, old or young – is encouraged to state their opinion about important matters. In the egalitarian culture of the Ju/’hoansi, people do their own thing and therefore have their own unique experiences and ways of representing problems that may be relevant to a group decision.
The Ju/’hoansi are not culturally diverse, but their permissiveness of individual differences means their groups are functionally diverse. The social norm of widespread participation ensures the free and open exchange of information, reducing the likelihood of an information cascade. Biesele documented a principle that, if each person’s opinion was not heard, trouble would follow. Repressed opinions, it was said, could cause sickness.
F or the Ju/’hoansi, there is little connection between individuals and the ideas they promulgate. As Silberbauer notes in Politics and History in Band Societies (1982): ‘It often happens that the suggestion finally adopted is one which was initially voiced by somebody who has taken no further part in the proceedings, leaving it to others to take up, and “push” his or her proposal.’ No one remembers the lateral thinker who solved the Dilemma of the Deserted Husband. An idea is like a bloody antelope carcass: once in the public square, it is more or less public property. To attribute an idea to a person would contravene the egalitarian nature of the band.
Deliberation also means disagreement. Claims are sceptically evaluated based on evidence, according to the anthropologists Melvin Konner and Nicholas Blurton Jones, who investigated Ju/’hoansi knowledge of animal behaviour in the 1970s. The Ju/’hoansi are careful to distinguish between first-hand knowledge and hearsay or speculation. There was a norm that discouraged rampant speculation: when someone said that children could be killed by fires, an old man said that people should only speak when they have seen things happen. One man was laughed at for his gullibility when he said that he had heard that elephants would bury their babies up to their necks.
One day, while tagging along with two trackers, Biesele heard constant feedback as the two busily corrected each other about where animal tracks were leading. In this dialectic, they responded to evidence and explained their logic. Neither took anything personally. In an email in 2022, Biesele told me: ‘Trackers’ conversations are fully cooperative and open to both new ideas and to corrections by other trackers, specifically to ensure the best-reasoned outcomes. So democracy and science are closely allied in the people’s minds, and closely govern how decisions are made.’
Unlike in modern politics, group decisions are not something to be won or lost
The Ju/’hoansi keep their cool, recognising that anger and heated feelings can lead to impulsive decisions and misunderstandings. According to Silberbauer, ‘the band is reluctant to come to decision under the sway of strong feelings: if discussion becomes too angry or excited, debate is temporarily adjourned by the withdrawal of the attention to the calmer participants until things cool down.’ Confrontation is avoided through a variety of subtle stratagems: pretending to cook, or urgently attending to a thorn in one’s foot. When things get too heated, people disengage, signalling a lack of sympathy for the outburst. The fate of the Ju/’hoansi contrarian is neither exile nor execution. It is to be ignored.
This isn’t to say that debate never occurs. Silberbauer observed ‘a bit of cut and thrust between orators’, however he found that point-scoring ultimately played little role in the ultimate decision. In a highly interdependent band, this makes sense because one’s fate is largely tied to that of other bandmates. As a result, unlike in modern politics, group decisions are not something to be won or lost. Attentive of this, the Ju/’hoansi avoid the mistake of equating rhetorical flourish with truth. The idea of sparring orators dealing knockout blows would be anathema to the Ju/’hoansi. A knockout blow is self-defeating, like punching oneself in the face.
When it comes to finalising a course of action, the Ju/’hoansi are sceptical of voting. In small groups, Biesele has found, the Ju/’hoansi see the act of voting as polarising. As the anthropologist David Graeber explained in Possibilities (2007):
Instead, discussion continues until a consensus is reached. Everyone has to agree on the course of action because it legitimates the decision as belonging to the group. It is not merely the actual result of the decision that counts, but the process itself. Everyone must attend to what Silberbauer calls the social balance-sheet. The social balance-sheet is no less than the promise of future cooperation, perhaps the most important thing in the life of a hunter-gatherer.
Consensus is also about the creation of shared meaning. The Ju/’hoansi, according to Silberbauer, are not only exchanging facts about reality but also values, objectives and ‘logical and causal relationships between items of information’. To decide well, the band must think together.
Perhaps the best illustration of this process of cognitive convergence comes from Kenneth Liberman, who worked among Aboriginal populations of the western Australian desert. Each day starts with the Morning Discourse, in which people take turns voicing concerns, thoughts, ideas. Each comment builds on the previous. The state of affairs of the group becomes publicly available. Nothing is directed toward individuals, only the group. ‘The favoured strategy here is to depersonalise one’s remarks and tone of voice as much as possible,’ wrote Liberman in 1985. ‘The effect is something like acting as if someone else is doing the talking.’ Rather than each person expressing views as an individual, it is almost as if the group is talking through each individual. The Morning Discourse shapes the consensus, when ‘all think in the same way with the same head, not in different ways.’ Sometimes hunter-gatherers don’t even bother to articulate the decision, so clear is the consensus and the subsequent course of action.
T he Ju/’hoansi style of decision-making finds echoes in the works of great thinkers. Cicero believed that conversation should be inclusive, allowing everyone a turn to speak. It should also be free of passion and gossip about people not present, and easy-going. John Dewey felt deliberation was critical to a vibrant democracy. Jurgen Habermas advocates for widespread participation of all individuals and for groups to seek ‘rational consensus’. He believes that people should pose rational arguments that are ‘in the best interest’ of all participants, thereby limiting the chances of fragmentation and inequality. These arguments are theoretical but, for the Ju/’hoansi, this philosophy is everyday life.
Understanding the Ju/’hoansi mode of communication from a modern perspective requires investigating the nature of dialogue. The word ‘dialogue’ is derived from the Greek ‘ dia ’ (through) and ‘ logos ’ (word or meaning), and is often translated as ‘a flow of meaning’. According to William Isaacs, who teaches workshops on dialogue-based approaches to communication, dialogue is ‘a shared inquiry, a way of thinking and reflecting together. It is not something you do to another person. It is something you do with people … Dialogue is a living experience of inquiry within and between people.’ Contrast this with debate, the root of which comes from the Old French word debatre – ‘to fight’.
When done correctly, dialogue could result in an extraordinary form of human cooperation
One of the most famous advocates of a dialogue-based approach to conversation was the American-born British theoretical physicist David Bohm. Alongside his seminal contributions to quantum theory, Bohm maintained interest in problems such as consciousness and creativity. In On Dialogue , a book he wrote just a few years before his death in 1992, Bohm tackled the problem of communication breakdown in society that seems similar to our current predicament of polarisation: ‘within any single nation, different social classes and economic and political groups are caught in a similar pattern of inability to understand each other.’
To Bohm, dialogue helps to establish a shared common understanding between individuals. It would mean abandoning assumptions, not clinging so tightly to our opinions, and doing a lot of listening. Yet he knew that dialogue was difficult. It required guardrails and open-mindedness. But when done correctly, dialogue could result in an extraordinary form of human cooperation with unparalleled creativity: ‘nobody is trying to win … we are not playing a game against each other, but with each other. In a dialogue, everybody wins.’
Channelling his physicist’s intuition, Bohm analogised the alignment of group sentiment with a laser: in contrast to normal light, which is scattered all about, a laser aligns light beams in the same direction, producing coherence. Bohm believed that ‘thinking together’ had the same effect. There is some empirical support for this idea. Recently, researchers at Dartmouth College in New Hampshire asked business students to watch movies and discuss their opinions. Looking at scans of brain activity before and after the discussion period, the researchers discovered that brain activity aligned after the discussion. As with the Morning Discourse, the students were thinking together.
Bohm believed that dialogue to be an ancient mode of communication, arguing that communities with coherent meaning probably existed ‘in some groups in the primitive Stone Age conditions’. Indeed, the parallels between the Ju/’hoansi mode of communication and dialogue-based methods are hard to miss. I imagine if there were a Ju/’hoansi philosopher, her description of her society’s decision-making process might read something like Bohm’s On Dialogue .
J ust because hunter-gatherers do something does not make it necessarily good. But according to social psychologists, the features of Ju/’hoansi decision-making are the very ones that make for high collective intelligence. Consider a 2020 article from the Harvard Business Review outlining the best practices that optimise good decision-making in small groups: heterogeneous groups are better than homogeneous ones; dissent is crucial; people should arrive at their opinions independently; people should feel free to speak their minds; individuals should share collective responsibility.
Taken together, there is robust evidence that the Ju/’hoansi are able to avoid levels of polarisation like we see in our current political moment. This is achieved not necessarily through individual virtue but rather with cultural guardrails and prolonged deliberation. The Ju/’hoansi are well aware that their social norms around deliberation improve the quality of their decisions. As Biesele told me, Ju/’hoansi informants would say things like: ‘It’s necessary to draw on the strengths of each person, to minimise the chances that decisions will be made on the basis of the weakness of one or a few persons.’ In contrast, even though discussion generally improves group reasoning performance, people in Western society are poor at recognising this fact.
One Ju/’hoan said: ‘We never wanted to represent our communities: that was a white people’s idea’
The Ju/’hoansi’s political self-consciousness has informed their responses to jarring changes in their lives. Silberbauer conducted his work on the Ju/’hoansi political process from 1958-66. By the late 1960s, mining, cattle and development had dispossessed them of their traditional territories. The autonomous life they had lived for aeons was effectively over. Their houses, mobility, diet and society would be irrevocably changed. So too would their politics. The ‘close-knit, self-sufficient organisation of band society and the completeness of members’ control of its political processes are gone,’ Silberbauer wrote. ‘The “ethnographic present” is now the past.’
Yet Biesele has found that old habits die hard. As founder and director of the nonprofit Kalahari Peoples Fund, she has devoted her career to documenting and aiding the Ju/’hoansi’s transition to modernity. She recorded and translated meetings of the Ju/’hoan people’s organisation that would go on to become the first internationally recognised Conservancy in the new nations of Botswana and Namibia. Biesele has written eloquently about how the Ju/’hoansi have been resistant to give up their old ways of making decisions through consensus. Challenges arose when individuals or small groups were designated as representatives to act as a connection to the government. ‘This was a very foreign idea,’ Biesele said, ‘but the people could see the need for interacting in this way with the new administrations, so they debated how they could possibly do it successfully.’ This task was undertaken with considerable hesitation. One Ju/’hoan said: ‘We never wanted to represent our communities: that was a white people’s idea in the first place.’ As Biesele documents, the Ju/’hoansi have favoured cooperative institutions that tap into their deep history as decision-makers.
M any anthropologists and archaeologists believe that humans lived in nomadic egalitarian bands for much of our species’ history. If this is true, then the Ju/’hoansi and other hunter-gatherers tell us something important about what politics in the Palaeolithic might have looked like. Amid the crackle and pop of a Pleistocene campfire, under the anonymity of darkness, our ancestors began to think as one. In that moment, we became political animals, the first and only species in the history of the world to grasp how its own collective intelligence could be made and unmade.
Just like any hunter-gatherer today, our ancestors would have been self-conscious political actors. They would have realised the importance of the process to the result. And they would have actively maintained political structures that maximised their collective intelligence. Groups that failed to do so would have perished.
Recognising the self-conscious political agency of hunter-gatherers challenges 20th-century perspectives that ‘primitive’ cultures exhibited a uniformity of belief and personality. Describing Aboriginal Australians in 1915, the French sociologist Émile Durkheim wrote:
Nothing could be further from the truth. As Liberman observed in Australia, there is just as much eccentricity and variation in a band of hunter-gatherers as there is among ourselves. This is a critical part of the recipe for high collective intelligence.
We should be gentle with ourselves about the magnitude of our challenge because hunter-gatherers have it easier
Our ancestors would have seen no necessary contradiction between seeking consensus (and compromise) and seeking truth. Yet this is a commonly held view among social scientists who focus solely on a narrow slice of human history – the present; for example, the psychologist Irving Janis, who believed that tight-knit groups were especially prone to groupthink. Or, more recently, the political scientist Jason Brennan, who wrote : ‘Human beings are wired not to seek truth and justice but to seek consensus … They cower before uniform opinion.’ On the contrary, the Ju/’hoansi boast an impressively fertile ecology of conversation that derives, ultimately, from the distinctive combination of high levels of interdependence and egalitarian social norms. With a unified approach to a common goal, along with norms that encourage free and open expression and diverse viewpoints, it is in everyone’s interest to seek the truth.
All of this calls into question our own preoccupation with debate as a form of truth-seeking. In the sphere of communication, prominent book titles include Win Every Argument: The Art of Debating, Persuading, and Public Speaking (2023), Good Arguments: How Debate Teaches Us to Listen and Be Heard (2022), How to Argue and Win Every Time (1995), and The Art of Being Right (1831). Undoubtedly, debate can be useful for presenting alternative viewpoints and hashing out logical inconsistencies. But it often results in little more than hardened views and hurt feelings. Debate is a tool designed to convince, not to solve collective problems. ‘I never yet saw an instance of one of two disputants convincing the other by argument,’ wrote Thomas Jefferson in 1808.
It may seem intractable to scale up insights from the Ju/’hoansi to modern problems of anonymous digital ecosystems and nation-states. Undoubtedly, the Ju/’hoansi style of deliberation is best suited to face-to-face interaction. Lost in the digital world are the subtle cues and gestures that help us to gauge others’ feelings, and to communicate our displeasure. We should be gentle with ourselves about the magnitude of our challenge because, in a sense, hunter-gatherers have it easier: they share similar values and conceptions of the world, their world is smaller, their range of choice narrower and less abstract than ours. Their decisions are more concrete and immediate.
On the other hand, some of our most important decisions still occur in small face-to-face groups, whether it’s in the Oval Office, the corporate boardroom, or the family dinner table. The Ju/’hoansi show us how the best outcomes can be achieved in these groups. Success comes from material interdependence, common purpose and shared meaning. Also critical are the conversational guardrails that enable us to truly think in, and as, groups. This is ancient knowledge that any of us can put into action now: don’t get heated, detach ideas from ego, put yourself in others’ shoes, listen. And always speak your mind."
We need to pull the plug on the idea of indigeneity in India | Aeon Essays,,https://aeon.co/essays/we-need-to-pull-the-plug-on-the-idea-of-indigeneity-in-india,"In early May 2023, a video surfaced on social media of a mob of young men parading two naked women. With the women’s faces, bosoms and genitalia blurred, the boys could be heard chastising them: ‘Your men raped our women, now we will rape you.’ This happened in Manipur, a state in northeast India. ‘We’ denoted the Meiteis, the dominant tribe in the state, and ‘you’ the Kukis, their hill neighbours.
In December 2019, on the plains of neighbouring Assam, a huge civil-society movement against the Citizenship Amendment Act (CAA) was taking shape. The CAA is a controversial law that grants fast-track citizenship to Hindu, Sikh, Buddhist, Jain, Parsi and Christian – but not Muslim – immigrants from Pakistan, Afghanistan and Bangladesh. People across religions and classes took to the streets, raising slogans that ultimately boiled down to a common sentiment : ‘We are the sons of the soil, and they must be deported.’ Here, ‘we’ represented Assamese speakers, and ‘they’ referred to the Bangladeshi immigrants against whom the Assamese have been rallying since the mid-1970s.
The Assamese and Meitei nativists both drew on a common impulse to prefigure themselves as indigenous while reviling another as a threat to the purity of their indigeneity. However, against the claims of the Meiteis and the Assamese, the Kukis and the Bengali-speaking immigrants also maintain avowals of indigeneity.
Assam and Manipur are bordering states in the northeastern stretch of India. Located at the confluence of South Asia and Southeast Asia, the region serves as the country’s gateway to Tibet, Bangladesh and Myanmar. Since May 2023, the Meiteis and the Kukis have been engaged in an ethnic strife that has killed at least 175 people, injured 1,000 more, destroyed more than 4,000 houses, and left close to 70,000 displaced. Though the anti-CAA movement in Assam took far fewer lives, mostly resulting in the death of Assamese protesters due to police brutality, it brought civic and economic activities in the state to a standstill for close to two months. As much as the political conflicts in Manipur and Assam differ in important ways, both also feature groups with overlapping, at times even mutually exclusive, claims over land, nativity and nationhood that rest on the mantle of indigeneity. From caste-Hindu communities to tribes living in the contiguous highlands between India and Myanmar, multiple groups identify as indigenous. Yet none of them can fully satisfy the definition of indigenous peoples that has emerged in international law and Indian jurisprudence.
W hat international law has to say about indigeneity has repercussions outside courtrooms and global organisations. Since the 1960s, there has been a flow of ideas from decolonial struggles on the margins to influential legal institutions, defining who is indigenous and who is not. These definitions have then travelled back to the grassroots in vernacularised idioms. The intellectual itinerary of ‘indigeneity’ has been a circuit back and forth between the UN and its allied institutions, on the one hand, and societies in the Americas and Oceania that have experienced a wholesale replacement of their native populations, or settler colonialism, on the other. India, meanwhile, is mired in its own squabbles over tribality, caste and religion. Indian contests over forms of belonging do not quite accord with the efforts underway in the West to internationalise indigeneity.
Until about 50 years ago, indigenous peoples were completely absent from global institutions. Their legal personality arose in contrast to the enduring effects of the European conquest and the genocides in the Americas and Australia. It was only in 1971 that the UN finally admitted in ECOSOC Resolution 1589 (L) that ‘indigenous populations often encounter racial prejudice and discrimination’. Special Rapporteur José Martínez Cobo was then tasked with compiling a detailed report on the status of indigenous groups around the world. The prevalent belief at that point in time favoured the integration of these disenfranchised communities within their parent states. Cobo’s 25-chapter report , published over 12 years, offered a definition of indigeneity that still remains the dominant legal standard.
Indigeneity has become just another ambivalent and opportunity-costed postcolonial identity
To put it succinctly, Cobo constructed two objective markers of indigeneity: precolonial continuity and territorial rootedness. The first seeks to carve ‘historical continuity with pre-invasion and pre-colonial societies’. The second captures the intimate relationship that the indigenous share with their lands to differentiate them from ‘other sectors of the society’. In law and international covenants, these two criteria have become a hard-and-fast matrix to distinguish communities as indigenous and vest them with certain cultural and material rights. To this objective roster, the International Labour Organization’s handbook Understanding the Indigenous and Tribal Peoples Convention (1989) added a subjective element, allowing groups to self-identify as indigenous. We now have an awkward juridical position where the capacity to call oneself indigenous exists, but, in order to translate the entitlement into concrete rights, communities must rely on courts and international institutions that still encourage Cobo’s formula. Notably, Cobo’s objective criteria, along with subjective self-identification, loosely compose the putative understanding of indigeneity in the dialogue between international law and the struggles of the marginalised.
Over the past half century, the progress of indigenous rights has been so significant that communities today can harbour aspirations in international law to transcend the nation-states responsible for their historical enslavement. When municipal constitutionalism fails, the language of human rights provides an alternative framework for activism and action at multinational platforms – the Inter-American Court of Human Rights, for example. Creation stories, traditions of Dreaming, and other mythological or religious methods of land ownership can now be translated into the grammar of modern law to converse with the state and the civil society in the only voice they recognise. In India, on the contrary, competition over such strategic benefits has concocted a systemic trend of killing and maiming over who is more indigenous and to the exclusion of whom. Amid the country’s extraordinary diversity, indigeneity has become just another ambivalent and opportunity-costed postcolonial identity. Any liberatory promise of freedom contained within the concept has been obscured by violent recurrences of one Assam after another Manipur. In India, there really is no determinative way to spell out who is indigenous.
L et’s consider the insistence on precolonial continuity in Cobo’s definition, which has also become a commonsensical notion. The Meiteis in Manipur, who populate the plains, portray themselves as more indigenous since they have purportedly been living there for longer than the Kukis in the nearby hills. The Meiteis are largely Hindu, hence their sense of being there concomitantly fuses the Hindu Right’s impossible assertion of timeless sovereignty over India. The Kukis are predominantly Christian, and their conversion in the past 200 years is derided by the Meiteis as a break from their indigenous or tribal faith. The Kukis in turn dismiss Meitei indigeneity on the grounds that their Hindu affiliation should curtail any minority status. The recent violence in Manipur, after all, started after a High Court judgment recognising the Meiteis as a Scheduled Tribe, which would have given them affirmative action measures of a similar kind to that enjoyed by the Kukis. Tribality, or indigeneity, in India is not a trait to be determined once and for all, but is a dynamic assemblage of affinities and differences with respect to the caste-Hindu mainstream. Different communities can shift registers to indigeneity to proliferate a seemingly historical, anthropological or legal claim of belonging. But these truths, Manipur teaches us, are usually circumscribed by violent political or fundamentalist clashes.
The problem in Assam stems from the Assamese-speaking majority fearing that they will be rendered a minority in their own home if immigrants from Bangladesh are naturalised. Two facts cannot be denied. First, immigration is a reality in Assam. It began in the 19th century as a colonial policy of transporting labourers to cultivate inhospitable sandbars in the Brahmaputra basin, and was aggravated in the aftermath of the 1971 Bangladesh liberation war. Second, unlike the Hindu Right’s paranoia of Muslim domination, demographic overhauls are not uncommon to the Northeast. Between 1881 and 2011, the tribal population in the neighbouring state of Tripura dropped from 63.77 per cent to 31.78 per cent, turning them into a minority against Bengali immigrants. The Assamese thus have long been afraid of becoming homeless in their own home.
So, who is indigenous to Assam? In the 15th and 16th centuries, the caste-Hindu population was consolidated by a priestly class that came from mainland India. The Ahoms – the people who unified Assam and ruled the area until the British dethroned them in 1826 – themselves arrived here in the 13th century from South China. The tribes, likewise, can trace unhindered presence in the state only from the recent past, and have much older cultural ties across the Southeast Asian highlands. In protests, these communities all identify as Assamese speakers although some speak it only as a second language. They celebrate themselves as indigenous in distinction to Bengali immigrants. Most of these groups can show some magnitude of cultural uninterruptedness, truncated simply by an act of migration that clearly did not have colonial motives. But they also exhibit a xenophobia that frequently erupts as riots and the internment of Bangladeshi immigrants in premises harrowingly akin to concentration camps. To speak of precolonial continuity in this situation does not lead to any ascertainment of indigeneity; for the question of varying degrees and dynamics of continuity within the category of Assamese speakers cannot be resolved through Cobo’s criteria. The Assamese themselves do not care for greater clarity in their movements for belonging and self-determination. As a result, we have another instance where indigeneity in India is linked to exclusion and violence.
The contours of that indigenous personality are contested, so territory affords little help in identification
Cobo’s idea of territorial rootedness has produced another share of dangers. Territory in indigenous struggles entails more than land ownership. It often becomes a material venue that hosts fights against resource extraction. To talk of territory is to inaugurate dialogue between various indigenous peoples and between the indigenous and their settler neighbours about self-determination, co-managing resources, and apportioning legal entitlements. In India as well, marginalised communities have highlighted their profound associations with where they live to combat the militant-managerial state’s hunger for natural riches that lie beneath their sacred hills and rivers. But when these justified claims are redirected against rival groups for control over scarce resources, indigeneity becomes a morbid weapon.
The Kukis contend that the Meiteis’ preponderance in the Imphal Valley has enabled a monopoly over developmental initiatives. The Meiteis, on the other hand, allege that the Kukis threaten the territorial integrity of India. By accommodating ethnically similar immigrants from Myanmar in their villages, the Kukis supposedly have managed to boost their population to the brink of a demographic replacement. Their territorial relationship to an imagined ethnic terrain spread across the international border is taken by the Meiteis and Hindu nationalists as a token of disrespect to India’s sovereignty. Territory, by definition, is a political construction of geography. This is as valid elsewhere as it is in India. But in Australia and the Americas, movements around territorial control are oriented towards reclaiming the authority that European colonialism had invaded under the guise of terra nullius or barring the state from further expropriating these lands. In the settler-colonial societies of these continents, we can see coherence, in principle, in a reflexive distinction between indigenous and non-indigenous populations. In India, the contours of that indigenous personality itself are murky and contested, so territory affords little help in identification. It becomes another battleground.
In Assam, inclusion in the National Register of Citizens (NRC) required proving ancestry or residence in the state before 24 March 1971 through any one of 14 possible documents, among which were property records and electoral rolls. Many immigrants who had arrived recently managed to forge documents. Many who had been living in the state for generations lacked the necessary papers, and the state declared them foreigners. The government then planned to regularise the possession of numerous sandbars, the main home of immigrants, into ownership based on the new NRC. This project of who gets to rightfully call certain lands their own and who must be imprisoned prompted groups to manufacture nativity through a legal device to win property awards.
Unlike the Aboriginal peoples in Australia, very few communities in South Asia can backdate where they currently live to originary creation stories. For most, their communitarian folk identity is constitutive of constant movements. One can move upwards from Assam towards the Himalayas and would chance upon the Lisu people in Arunachal. Although they had migrated from modern-day Myanmar and China, they learned to call their current villages their undisturbed home until the state evicted them from their forests to create a national park. Or one can travel south to Mizoram’s border with Myanmar’s Chin State. Here the Bru people have been living under the threat of constant pogroms at the hands of the majority Mizo population. But the Mizo were quite content to open the borders for their ethnic brethren from Myanmar following the 2021 coup. The Mizo, the Bru and the Lisu are all tribal communities. Movement is at the heart of their history and selfhood. Any attempt to freeze this indeterminacy under the guise of indigeneity will only incite such groups to fight among themselves for supremacy over the territories where they now find themselves trapped.
I t might be possible to abjure Western notions of indigeneity drawn from their brush with settler colonialism and devise the concept anew for India. But doing so is not easy. Anthropologically, there remain doubts about how to objectively distinguish tribal formations. For the sake of argument, we can agree that those constitutionally designated as Scheduled Tribes are properly indigenous, irrespective of the heterogeneity within this classification itself. The difficulty arises when some tribal groups actually tend to fulfil Cobo’s criteria and still choose to represent themselves differently.
The word ‘Adivasi’ means original inhabitants. Some forest tribes use this self-referent as a more perspicacious name for their cultural uniqueness. Many present-day Adivasis trace their ancestry back by four millennia to the decline of the Indus Valley Civilisation. Their association with their traditional forests, too, goes back to antiquity. Alas, we may have found the Indian indigenous. The Gond, the Oraon, the Santhals can well seek membership in the international cast. Adivasi is not an official term regulated by the state, thus all the other tribes in the Northeast may also join the bandwagon and begin to so denote themselves.
At present, however, these northeastern communities prefer their tribal identities over being hailed as Adivasi. Regardless of anthropological and historical uncertainty, both terms invoke similar attitudes, though ‘Adivasi’ foregrounds the entitlement of being original inhabitants, whereas ‘tribality’ underlines differentiation from the caste-Hindu, capitalist mainstream. Wanting to be indigenous as tribals is a political refusal to subscribe to Adivasi marginalisation. Still more, the Adivasis themselves hold on to their unique identification. It is not that they have never claimed to be indigenous. Nor is it the case that the concept has not been gaining popular appeal and theoretical currency of late. Rather, indigeneity for the Adivasis is a semantic gambit devoid of the conceptual baggage it carries in international law and popularly in the West. In their dealings with the state, civil society and mainstream populations, they present themselves by the chosen name of their organised solidarity – original inhabitants, quite indigenous, but à la an Adivasi, not an indigenous, people.
The Mizo, the Kukis, the Meiteis and others harness the term to help hide their wanton records
The Adivasis in the Chota Nagpur Plateau have grown tired of the Indian legal system’s prolonged failure to stop their lands from being mined and their forests from being cleared. In response, they conceived of a movement called Pathalgadi. They engraved their constitutional rights on gigantic stone tablets that are usually epitaphs to dead ancestors. The move signified the national civic legal order as dead to them because the communities wanted to show that they could represent their rights better than the Indian state. The solidarity they were striving to build was across Adivasi cosmologies and not the globalised index of indigeneity. For them, Adivasi was the politically loaded term, while indigeneity, when used intermittently, was just an everyday shorthand to translate the belonging they were fighting for into a language intelligible to all.
Even those who seem to fit the American and Australian template of indigeneity in India have an alternative way – an Adivasi way – to define themselves. This is in addition to the Mizo, the Kukis, the Meiteis and others who harness the term to help hide their wanton records. The UN Declaration on the Rights of Indigenous Peoples gives us an alternative to the deadlock of whether indigeneity or being indigenous in India is a strictly legal matter. It advocates the subjective right of groups to self-identify, which opens up the possibility of fostering the term as a political comparative to link select organised movements around the world. The prospects of these struggles rely on agreement as to who is indigenous. In India, the established criteria in international law and the dominant scholarship make the matter of indigeneity itself a source of confusion, ending up as a racialised justification for violence.
Be it in Assam or Manipur, or of the Mizo or the Pathalgadi Adivasis, their struggles are all geared towards self-determination. Conversely, their travails stem from the intrusion, apathy and high-handedness of the state. Why should they not fight for complete sovereignty, for freedom from the state that has failed them? One, international law denies sovereignty to indigenous self-determination. In fact, The Indigenous and Tribal Peoples Convention of 1989 stipulates that ‘the term peoples’, when read with indigenous, ‘shall not be construed as having any implications as regards the rights which may attach to the term under international law’. Second, flirting with secessionism is a risky move, especially if the enemy now is not the courtroom or another community but the formal armed forces. Very few groups, therefore, exhibit consensus in desiring freedom at such high costs. And secessionism is not a guarantee against violence, however justified its cause might be. There is no assurance that partitioning Manipur into two separate states for the Kukis and the Meteis will ensure a peaceful population transfer or that xenophobia will wane out in an independent Assam. But a newfound zeal for full sovereignty in these places will disentangle indigeneity from what the people are actually struggling for – freedom from the disorder spawned by the Indian state.
Without any theoretical cohesion about what the term ‘indigenous’ denotes, it will continue to be an empty signifier that India has borrowed from elsewhere. The only substance behind it has proven to be violence. No doubt anthropologists, political theorists and critical legal scholars have also criticised the rigid Western design of indigeneity and cautioned against adopting it as a universally cogent concept. Many have also regretted the violence inflicted in its name. But in India these pathologies have become the rule. Since, as we saw, the fundamental method of designating indigeneity does not work here, there is no medium except violence to settle competing claims – neither a legal doctrine, nor any scope for public reasoning. This, in the end, brings us to two conclusions. First, no matter how vernacularised, localised or situated indigeneity gets as we move towards the grassroots, the persistence of a Western agent – be it in international law, academia or activism – means that the spectre of objective definitions will return every so often as conflicts even in places where they are supposed to work. Yet without transnational networks and some international supervision, there is no assurance that indigenous peoples anywhere will finally be free from pain and suffering in their home countries. Perhaps all experiences of oppression need not be articulated in the lexis of indigeneity. There are other political methods, like the Adivasis’, to envision a shared feeling of belonging somewhere before colonisers took over. Indigeneity may be useful for others, but we can pull the plug on it in India."
Who needs AI text-generation when there’s Erasmus of Rotterdam | Aeon Essays,,https://aeon.co/essays/who-needs-ai-text-generation-when-theres-erasmus-of-rotterdam,"The Renaissance scholar and educator Erasmus of Rotterdam opens his polemical treatise The Ciceronian (1528) by describing the utterly dysfunctional writing process of a character named Nosoponus. The Ciceronian is structured as a dialogue, with two mature writers, Bulephorus and Hypologus, trying to talk Nosoponus out of his paralysing obsession with stylistic perfection. Nosoponus explains that it would take him weeks of fruitless writing and rewriting to produce a casual letter in which he asks a friend to return some borrowed books. He says that writing requires such intense concentration that he can do it only at night, when no one else is awake to distract him, and even then his perfectionism is so intense that a single sentence becomes a full night’s work. Nosoponus goes over what he’s written again and again, but remains so dissatisfied with the quality of his language that eventually he just gives up.
Portrait of Erasmus of Rotterdam Writing (1523) by Hans Holbein. Courtesy the Kunstmuseum Basel
Nosoponus’s problem might resonate. Who has not spent too long going over the wording of a simple email, at some point or another? Today there is an easy fix: we have large language models (LLMs) to write our letters for us, helpfully proffering suggestions as to what we might say, and how we might phrase it. When I input Nosoponus’s intended request into GPT-4, it generated the following almost instantly:
But there was a solution in the 16th century, too. A humanist education on the Erasmian model could train its students to produce letters of any length, on any topic – quickly, easily and eloquently. The French humanist François Rabelais, a contemporary of Erasmus, appears to have understood these compositional techniques as automating the creating of text in a way that, retrospectively, looks a lot like how LLMs function. If we want to understand LLMs, and what they are and aren’t capable of, we can look at earlier versions of the same technology – like Erasmian humanism. We can also read authors like Rabelais, who is already thinking about automatic text-generation along these lines, as someone who appreciates the effectiveness of Erasmian generative technology, but at the same time sees it as vitiating the social force of language and, ultimately, ruining language as a tool for moral and political life.
Rabelais was a monk, doctor, personal secretary and spy, but today he is mostly remembered for his five-part literary fiction about Gargantua and Pantagruel, a family of (literal) aristocratic giants who navigate life in 16th-century France while being much larger than everyone else. His second book in the series, Gargantua (1534), is centrally concerned with the advantages and costs of a humanist education. Those of his characters who receive humanist training produce distinctive and sophisticated language, but when they speak or write they are always primarily communicating what a fantastic education they’ve had, rather than whatever they actually have to say. When we read a sentence such as: ‘No more just cause for pain can arise among men than if, from where they should in all fairness expect gracious benevolence, they receive travail and injury’ (my own translation of Rabelais) – or take its modern chatbot equivalent: ‘It is profoundly disheartening when those entrusted with our care or respect instead subject us to mistreatment’ (GPT-4) – we are more likely to be impressed by the linguistic capacities of the writer or the language model that generated it, than to reflect on the idea that it’s bad when someone who should treat you well treats you poorly.
Erasmus and Nosoponus are both writing in Latin, which, like any humanist, they learned by laboriously imitating the great Latin writers of antiquity (despite the fundamental difficulties involved in trying to match their native fluency and eloquence in a second language). Renaissance intellectuals argued fiercely over whose Latin they ought to imitate, although they actually disagreed very little. Cicero was the chosen scribe; his standing as the best prose stylist of antiquity was uncontested. The main debate, therefore, was between the Ciceronians, who believed in imitating their hero to the exclusion of all other models, and the eclectics, who believed in imitating Cicero along with other models. Erasmus was an eclectic. Over the course of The Ciceronian , his authorial stand-in Bulephorus gradually wins over Nosoponus, the titular Ciceronian, to the eclectic side. To understand the arguments he makes, and the stakes involved in Nosoponus’s conversion, you need to understand how humanist stylistic imitation worked.
H umanists read and reread the classical authors whom they wanted to sound like – not unlike how an LLM trains on a corpus. They internalised the qualities of their models’ prose as much as possible, and also kept a sort of verbal rainy-day fund in something called a commonplace book, which is to say that, as they read, they transcribed a stockpile of salient words, metaphors, turns of phrase and clichés – usually organised thematically – that they would then draw upon while writing. An eclectic writer would gather the high points of many different authors and genres, but a Ciceronian would copy only from Cicero. Writing with a commonplace book, then, would enable an eclectic to draw on the greater range of his reading to make his own prose more versatile, while a Ciceronian’s style could become only more homogeneously Ciceronian. Bulephorus points out that Cicero himself read widely among his own contemporaries: if Cicero were alive today, he argues, he would produce texts tailored to the modern world, and therefore quite unlike anything in the surviving Ciceronian corpus. Cicero, in other words, would be an eclectic.
But The Ciceronian makes it clear that Ciceronians and eclectics do not just differ in the content of their commonplace books, but in how they use them. For eclectics, they are a kind of safety net: if Bulephorus can’t think of the right word, he can just consult his book, much as we might consult a thesaurus, saving himself all the time he would otherwise have spent trying to come up with it on his own. For a strict Ciceronian, however, a commonplace book is more like a security checkpoint. Nosoponus generates language without help, but feels compelled to laboriously verify the Ciceronian origin of every single word or construction that he wants to use in a commonplace book, which has, in effect, become an exhaustive concordance of the Ciceronian corpus. This insistence on maintaining conscious, executive control of the process of writing makes compositional fluidity impossible for him, to the point where he refuses to conjugate his verbs without first ascertaining whether Cicero used whichever particular inflected form he needs. No wonder a single sentence is a full night’s work.
The letter and speech sound good, but they are rather long and dull: there is something generic about them
Erasmus’s goal as a teacher was to train his students in just the sort of fluency that Nosoponus lacks. He did this by giving his students fictional or historical scenarios that required delicate rhetorical handling – in much the same way that we might prompt an LLM today – and having them compose letters or speeches ‘in character’. They might write a love-letter from Paris to Helen, for example, or they might write as Agamemnon, summoning his allies to make war on Troy, or else pleading with Menelaus to forget Helen and avoid the ghastly human cost of such a war. They could write as Menelaus vituperating Paris for stealing his wife, or perhaps forgiving Helen for leaving him. (Erasmus gives a slew of examples, including many of these, in his pedagogical manual On the Writing of Letters .)
Portrait of François Rabelais by Okänd. Courtesy the National Museum, Stockholm
When Rabelais’s humanist characters finally get the chance to put their training to practical use – attempting to avert a war with their neighbour, Picrochole – the texts they produce read as Erasmian schoolroom exercises. The giant Grandgousier sends his son Gargantua a letter summoning him back to defend the family land from his war-mongering neighbour Picrochole. At the same time, Grandgousier’s ambassador Ulrich Gallet also gives an anti-war oration urging Picrochole to desist from his aggression. Rabelais gives the text of both the letter and the speech in full. They sound good, but they are rather long and rather dull: there is something generic about them. They could just as easily have been produced by Renaissance schoolboys prompted with these scenarios, or by LLMs simulating them.
They also don’t work . Grandgousier and Gallet may decry the horrors of war, insisting on the moral obligation to avoid it, but both the speech and the letter actually accelerate the violence rather than preventing it. They fail to make peace because they are generic.
T he automation of language, whether by Erasmians or by LLMs, depends on rejecting novelty: both work in identical fashion by decomposing apparently new situations and topics into familiar elements, so that those situations can be addressed with language that is already associated with those elements in the training corpus. What this means for Grandgousier and Gallet is that the humanist mindset that enables them to speak so well also makes them approach the conflict with a certain arrogance – with the assumption that they can anticipate anything the other side might conceivably have to say on the basis of what they have already read.
Indeed, the irony of their predicament is that Rabelais’s humanist characters harp on the necessity of establishing a dialogue with Picrochole, but prevent it happening with the very language in which they express the importance of hearing his perspective. This is language not as an olive branch but as a shield.
Grandgousier writes to Gargantua saying: ‘I have amiably sent to [Picrochole] multiple times, to understand in what, by whom, and in what way he felt he had been done wrong, but I have had no response from him but wilful defiance.’ But this isn’t true. Grandgousier has sent his ambassador to Picrochole only once, not multiple times – and recently enough that Gallet could not possibly have reached him already, let alone returned with a response.
The reason ‘unicorn’ and ‘raccoon’ appear in different contexts is that raccoons actually exist
Maybe Grandgousier is lying because he has no real interest in averting the war, and the letter sets up a paper trail to establish the geopolitical moral high ground. But what if we don’t think about the false claims he makes as lies, but as something closer to the so-called ‘hallucinations’ of large language models?
An LLM responds to a prompt with text that it ‘considers’ likely or appropriate in the context established by that prompt. Because it is so sensitive to context, it will tell me about unicorns if I ask for a fairy tale, but not if I ask about North American wildlife – but that does not mean that it has an internal representation of reality or of what it means for something to be real (although, of course, when prompted it can generate text describing such a representation). It does not ‘understand’ that the reason words like ‘unicorn’ and ‘raccoon’ appear in different sorts of contexts is that raccoons actually exist. This is fine as long as we ask LLMs for information that is common and consistent – like the number of teaspoons in a tablespoon. But when we ask for something obscure, or highly nuanced, they may just make up something that looks like a plausible answer – such as a fake legal precedent (where LLM performance is no better than random guesswork), or an NBA line-up with players from the wrong team.
Something similar might be going on with Grandgousier, who might inhabit the role of a good Christian-humanist prince in the same way that an LLM inhabits the persona its user assigns it, as a cluster of discursive and rhetorical features. After all, he is navigating a scenario straight out of Erasmian moral instruction: we know that he ‘should’ stop at nothing to avoid the war because we have read Erasmus’s extensive anti-war writings. And, indeed, his letter says everything that we, or he, might generically expect to hear from a virtuous prince in his situation (or from an Erasmian schoolboy imagining himself as one). And so, while Grandgousier is engaged in the highly trained and semi-automatic process of composition, he might write untruths out of a sort of discursive reflex – operating not contrary to but without reference to the truth – again like an LLM, which only approximates reality by saying the sorts of things that, based on its training, it expects its users will expect it to say.
Recall Erasmus’s argument against Ciceronianism, which is that no single author can offer the generic and linguistic resources for every possible discursive situation. But the difference between Erasmus’s eclecticism and the strict Ciceronianism he lampoons is one of degree rather than kind. On both models, the humanist can accommodate novelty only insofar as the tools he draws from a predefined corpus allow him to. But because the eclectic corpus is larger and more diverse than the Ciceronian one, its limitations are more subtle – and LLMs are more subtle still, since they are trained on corpora drastically larger than either. And yet Rabelais’s humanists display the very same solipsism as LLM users today, who, in thinking ‘with’ LLMs, merely refract their own thoughts through a body of other people’s words.
There is nothing wrong with this, but it is not communication.
B oth the letter and the diplomatic speech are by definition communicative genres: they exist to produce an immediate effect on the particular other to whom they are addressed. The letters Erasmus’s students write are, by contrast, pedagogical exercises, which only the instructor ever reads. Gallet’s diplomatic mission fails because he treats it as an exercise. Although he is sent to get Picrochole’s side of the story, he merely simulates an attempt to do so through a series of rhetorical questions: ‘So what frenzy moves you now … ? Where is faith? Where is law? Where is rationality? Where is humanity? Where is fear of God?’ Somewhat ironically, he later reprimands Picrochole for failing to mitigate conflict through dialogue, as he is himself failing to do: ‘If some wrong had perhaps been done by us … you ought first to have enquired after the truth of it, and next rebuked us for it.’
This brings us to the another consequence of the solipsism of autonomous language, which is the degeneration of what the British philosopher J L Austin in the 1950s called its illocutionary force – that is, what it means for someone to have said something, as opposed to the (locutionary) meaning of what they actually say. The illocutionary function of an LLM’s language is compromised, most proximately, because there is no singular, social agent speaking: an LLM can’t get married, make a bet, or christen a ship (to use Austin’s examples). But even when one person uses an LLM to generate language to send to another person, there is a loss of social ownership over the language in question – which, much like an ambassador, is sent to address someone on someone else’s behalf.
Gallet sets his discourse in motion without considering its pragmatic effect upon its audience
Gallet’s embassy fails because he is blind to the pragmatic dimension of his language. His speech is wildly aggressive and insulting. He describes Grandgousier’s prior alliance with Picrochole as a ‘sacred friendship’; tells Picrochole that his military aggression demonstrates ‘that nothing is holy or sacred to those who have emancipated themselves from god and from reason, in order to follow their perverse inclinations’; says that his conduct has been ‘so far beyond the bounds of reason, so repulsive to common feeling, that it can barely be comprehended by human understanding.’ And yet, if we consider Gallet’s embassy as a social action – which is clearly how the deeply offended Picrochole takes it – it unambiguously signals a desire for war, even though its denotative content is pacifist. We can take this tonal mismatch it in two different ways. It could be a devious attempt on Gallet’s part to escalate the war while retaining the optics of victimhood. But then Gallet may also simply be failing to consider the illocutionary force of his speech, which he treats as a generic rhetorical exercise on the theme of the evils of war, rather than as communicative diplomatic action.
There is a simple ethical lesson to be drawn from all of this. If we wish to encounter – and make peace with the ‘other’ – we must avoid the example of Gallet, who sets his discourse in motion without considering its pragmatic effect upon its audience. We should, instead, imitate Picrochole himself, who greets Gallet not with a speech but with a question: ‘What have you got to say?’
In structuring the outbreak of the war around a series of humanist failures to ask, and to listen, Rabelais at once characterises those failures as moral, and locates them in the cognitive structures that delimit Erasmian rhetorical training. The accusation is this: that Erasmus gives his students a technology for producing language as an end in itself, but he doesn’t teach them how to communicate. Rabelais shows us that, when the production of discourse is automated, it becomes strictly monologic and loses its illocutionary social power. This sort of autonomous language is just like an ambassador: it speaks for us, but it cannot speak as us."
Acting is an ancient tool of connection we can all play with | Aeon Essays,,https://aeon.co/essays/acting-is-an-ancient-tool-of-connection-we-can-all-play-with,"It’s a paradox of our age that we have never been more connected, nor ever more alone. We live in a time of unparalleled interaction, communicating instantly with people across the world, with intimate insight into other people’s lives. Yet we feel increasingly disconnected from ourselves, each other and the world. In the midst of so much connectivity, we are living through an epidemic of loneliness and social isolation.
‘Loneliness’ and ‘social isolation’ are different. Loneliness is a subjective, self-reported measure of inner experience, while social isolation is quantifiable: a measure of someone’s participation in collective activity. Their effects, though, correlate, with loneliness generally considered more dangerous to health. Both are manifestations of a deeper malaise: disconnectedness.
Research suggests disconnectedness is growing. Writing in The Harvard Gazette in 2023, Tyler VanderWeele, professor of epidemiology at the Harvard School of Public Health, outlined his recent findings:
A report by the US Surgeon General in 2023 had an eye-catching statistic: lacking social connection is as dangerous as smoking 15 cigarettes a day. The report listed specific health areas affected by disconnectedness, including cardiovascular disease, hypertension, diabetes, anxiety/depression, suppressed immune response and impaired cognitive function (including increased likelihood of dementia). Disconnectedness can literally be lethal.
Technology appears to help us tackle this affliction. For many, it was an essential lifeline during the isolation of COVID-19. Yet, not only is it failing to reverse the trend towards disconnectedness, it may be worsening it. This might, in part, be explained by the way our primary mode of online interaction – social media – presents edited, air-brushed representations of the lives of others, which provokes unfavourable comparison with our own messy and unedited experience. And the hours we spend satisfying our addiction to the instant gratification of online communication is time we might, in earlier generations, have spent in real-word interaction.
Nonetheless, the paradox of connected disconnectedness points to something deeper. It suggests that connecting via digital devices lacks qualities essential to mental and physical health. Despite, or perhaps because of, our growing reliance on external technology to facilitate connection, we are losing human technologies of connectedness – the technologies that enabled us to evolve across millennia. How can we better connect with one another without relying upon digital technology?
We possess proven ways to establish and sustain vibrant connection. They are ancient and intrinsically human, and remain available to us all. They are found in the lineage of a technique that today we call ‘acting’.
T he core of an actor’s work is to create and sustain connection, to build a shared experience with her or his audience. I call these technologies of connection ‘What Actors Know’. You do not have to be an actor to use What Actors Know, any more than you need to be a philosopher to explore ideas, or a chef to cook an evening meal. It is wisdom that emerged from within each of us, and enabled our evolution into social animals.
We will start our exploration of What Actors Know a long way from the world of showbiz or the emotional intensity of the rehearsal room. Performance happens when one self meets another self so, to understand What Actors Know, we need to explore what we mean by ‘self’.
A common perspective on ‘self’ is that we are each essentially isolated from one another. Our selves are seen as discrete and clearly distinct. This hyper-individualistic paradigm leads to what the activist and writer Charles Eisenstein calls ‘The Story of Separation’: the separate self in a world of others .
But this kind of individualism has not been the dominant perspective for most of the long sweep of human evolution. From within their Indigenous wisdom systems, the Native American scholars Darcia Narváez and Wahinkpe Topa write in Restoring the Kinship Worldview (2022) that ‘All things in the world live and share [a] mysterious web of interconnected energy.’ In Sand Talk (2019), the Australian Aboriginal writer Tyson Yunkaporta similarly argues for ‘your true status as a single node in a cooperative network. [You] retain your autonomy while simultaneously being profoundly interdependent and connected.’ Such ideas are also emerging in contemporary science: for instance, in Being You (2021), the British neuroscientist Anil Seth holds that ‘our inner universe is part of, and not apart from, the rest of nature.’
Those best able to access and communicate edge-of-the-mind states became the original shamans
What Actors Know names the skills actors use to connect and communicate. It is a contemporary articulation of the ancient, fundamentally human capabilities that drove our evolution as social creatures. It draws on this sense of self as interconnected. The holders of this knowledge were not always known as actors. In different contexts we name them storytellers, seers, priests, healers, shamans . Though their function has changed over time, and in different contexts, their underlying knowledge and techniques persist.
Like contemporary actors, they connected and communicated.
Within every society, there have emerged individuals to explain and interpret the visible and invisible world to their communities. In their book Inside the Neolithic Mind (2005), David Lewis-Williams and David Pearce explore the origins of human community by analysing the art and architecture of Neolithic settlements. They suggest that early shamans/priests created stories that bound their communities together by drawing on the universal, embodied, human experience of sleeping, dreaming and hypnogogia (the transition between waking and sleep). Those best able to access, articulate and communicate edge-of-the-mind states became the original seers, priests, shamans, storytellers.
Those ur-actors articulated dimly perceived shared realities (for we all have dreams) in ways that connected with their listeners. They described travelling from one dimension to another, merging with animals and plants, conversing with the dead, meeting gods. They imagined into existence cosmologies and religions. They bonded community. Echoes of their stories remain in the carvings and architectures their communities left behind.
Shamans wove together tapestries of connectedness in which their audiences could imagine their place. To paraphrase the theatre director Peter Brook, they made the invisible visible. The stories they told resonated because they articulated half-remembered experiences of their listeners. As Lewis-Williams and Pearce write:
These early shamans developed skills to hold the attention of their listeners long enough to share their visions, visions that emerged from their ability to look inward and make public what they found there. These techniques still guide the actor. Actors still look inward to their unique humanity and, simultaneously, outward to their audience and to the world. They employ techniques to develop fully integrated, interconnected human-ness.
A n actor’s first and most important job is to connect. Without connection, there is no communication. The connections an actor makes are multidirectional and dynamic. She connects inwards, to her imagination, body, senses, breath, blockages, fears, memory and every other aspect of ‘inner self’. Simultaneously, she connects outwards with the people she is performing with – real or imaginary – and with her audience.
Her connections are dynamic. She does not issue information for others to absorb. Nor is her inner world an unruly classroom waiting to be controlled by reason and will. She both changes and is changed by what she connects with, internally and externally. She simultaneously shapes and is shaped by her imagination or memories, and by how others respond to her. She develops, but is also defined by, her physical (in)capacities. She sends quanta of connection to co-creators and audience, and is changed by what comes back: an unusual inflection of voice, a shuffling audience member, an unexpected laugh. If she ‘over-acts’ or in some other way performs poorly, her audience responds with ridicule, discomfort, bewilderment or boredom. They break the circuit of connection she has initiated.
A musician may not ‘know’ the harmonic theory beneath the music she makes, but she still works within it
Acting is based in dynamic, multidirectional connectedness. I refer to this connectedness as ‘interconnection’. What Actors Know is a technology of interconnection.
I first encountered What Actors Know, the wisdom of live performance, working in live theatre. I started in the Western tradition, and gradually expanded my work across multiple art forms and cultures. I performed, directed and taught extensively on almost every continent, with actors, physical performers, improvisers, clowns, musicians, puppeteers, circus artists, singers and many who resist categorisation. I became especially interested in physicality, mind-body integration, presence and ensemble. In all my work, including corporate training or community development, I noticed universal, transdisciplinary techniques and capabilities: a common sub-structure for the effective creation of connection.
It is not a sub-structure actors are necessarily aware of. A musician may not ‘know’ the harmonic theory beneath the music she makes, but she still works within it. Similarly, when an actor creates and shares a performance, she draws on What Actors Know, even if she does not name it as such.
There are seven elements of What Actors Know. Each supports the actor’s fundamental role: to connect. Each, if we apply them to our own lives, helps us connect better both inwardly and outwardly, fundamentally counteracting both loneliness and social isolation.
T hough each of the seven elements is important, if there is a foundation on which everything else is built, it is this: an actor develops the capacity to be present with both her internal landscape and the world around her. This is the first element of What Actors Know.
You are sitting in a coffee shop, talking with a friend. Suddenly, he is no longer listening. Nothing has changed. He continues to nod, smile and ‘uh-huh’ at appropriate moments – but you know he is thinking of something else. In evolutionary terms, this signals danger. Perhaps he is aware of something you are not, or he has lost interest in you, and you are at risk of exclusion from the community your friend is part of. Humans are hardwired to recognise and respond to presence and to its absence.
Presence attracts attention. Breaking presence distracts. This is the foundation of an actor’s work: she must be present with the people (or camera) she is performing to. That is how she holds the attention of her audience. She connects, then sustains connection. Both require sustained presence. Without presence, performance is lifeless: a display of technical capacity, a demonstration rather than a vital experience.
The presence an actor develops involves quietening the mind, eradicating distraction, developing mindful awareness of inner and outer impulses, and a capacity to react spontaneously and appropriately. It is a two-way ebb and flow of stimulus and response. She simultaneously guides the journey of her performance and is affected by other performers and her audience. Unless she is present, she can neither notice nor respond to what is unfolding. She disconnects.
Effective and engaging communication – verbal or otherwise – requires dynamic shaping
Enhancing our ability to be more fully present is profoundly healthful. This is a subject covered widely in self-help, spiritual and other literatures. Jill Bolte Taylor, writing in My Stroke of Insight (2006) about her journey of recovery from a stroke, describes presence as integral to calming an anxious mind: ‘Step one to experiencing inner peace is the willingness to be present in the right here, right now.’ In All About Love (1999), bell hooks quotes the Buddhist teacher Thich Nhat Hanh on the power of being present with the world outside of the mind: ‘Our appointment with life is in the present moment. The place of our appointment is right here, in this very place.’
But, once established, how does an actor maintain connection? Integral to this is the second element of What Actors Know. Actors must develop skills to shape time, energy and space. Audiences disconnect from monotony. Speech without variations in tone and rhythm is deadly. It plods along, devoid of inflection, pause, dynamic or a satisfying sense of completion. We struggle to attend to someone who speaks like a poorly trained automaton.
To avoid this, the performer starts, develops and completes gestures, phrases and movements. She intensifies, sustains and relaxes focus. She constructs her performance by introducing new detail, tone and content, or deepening and developing what already exists. She learns rhythms of creating, sustaining and ending. Effective and engaging communication – verbal or otherwise – requires dynamic shaping. If you watch Denzel Washington, Ian McKellen and Patrick Stewart each performing the same ‘Tomorrow and tomorrow’ soliloquy from Macbeth , you’ll see clearly how their capacity to shape time, energy and space radically alters the text’s meaning:
Actors risk public failure, rejection and shame, encountering the (primitive) fear of exclusion from community. A stage is, intrinsically, a hostile environment. As John T Cacioppo and William Patrick write in their book Loneliness (2008): ‘[T]he lonely person too often assumes the worst, tightens up, and goes into the psychological equivalent of a protective crouch.’ Stage-fright is strikingly similar to loneliness, based in our evolutionarily adaptive need to belong. In an article in The Guardian in 2020, the psychotherapist Linda Brennan anonymously quotes well-known actors describing their experience of stage-fright. ‘I become hyper-reactive,’ one said, ‘and any sound, movement or comment can make me jump – or scream.’ Another spoke of ‘feelings of shame and humiliation … like they were going to die.’
Acquiring skills to communicate counteracts the urge to sink into a ‘protective crouch’. It enables us to develop agency. We take control of the multi-directional flow of communication. We replace fear with empowerment: counteracting our feeling of impotence. We un-crouch.
M aintaining the essential ‘liveness’ of performance requires actors to train the third element of What Actors Know: how to be flexible and adaptable. Faced with risk, we tend to seek control, to have a detailed plan. Neither reality nor the external world are under our control, though. Life is dynamic. As Heraclitus pointed out 25 centuries ago, you cannot step into the same river twice . It is not the same river, nor are you the same person. For, as he also wrote: ‘Everything flows.’
When we stay rigidly wedded to a plan, we are trying to inhabit a reality that does not exist. An actor who rehearses a performance, then reproduces it regardless of the size of audience or auditorium, unaffected by her fellow performers, is neither dynamic nor live.
Many years ago, I watched a performance in a student showcase. It involved two lovers sitting on a park bench under a streetlight, singing a duet. Unfortunately, the bench had not been screwed together properly. As one lover leant towards the other, the bench tilted and let out a loud squeak, then, as they leant back the other way, so did the bench, its squeak a counterpoint to their duet. It was brilliantly funny – or at least it would have been had the actors adapted to what the audience could see and hear. Unfortunately, they carried on regardless, sticking to what, in rehearsal, they had planned. Though we tried not to, the audience laughed. Connection between their intention and our reception broke. They were performing as if in a ‘controlled’ reality, one that does not actually exist.
A skilled communicator treats mistakes as opportunities, using whatever happens to enhance her audience’s experience: a misremembered line, a sudden question, a restless audience-member, a misplaced prop. She might adapt significantly, changing the entire direction of a performance. She might adapt subtly, shifting details of energy or tone.
Some actors may be self-centred, but the craft of acting is a balance between self and others
You have probably endured meetings or lessons where the speaker drones through a script, ignoring what is evident to all: that people have lost interest, do not understand or have urgent questions. It is a deadly experience. Balancing intention and flexibility, the actor, and the rest of us, can use connection to create conversation, not to lecture.
Connection is essential to forming community. The fourth element of What Actors Know is this: actors balance individuality and the collective. Despite popular perception of a profession filled with rampant egotists, acting is collaborative: communal both in creation and outcome. Some actors may be self-centred and self-absorbed, but the craft of acting is based in a balance between self and others, knowing when to take centre-stage and when to leave space for others.
An actor negotiates the interplay of individual and collective expression. She recognises when she is the centre of an unfolding moment, and develops the confidence and discipline to deliver fearlessly. She also recognises when someone else is the focus, and finds how best to support them. She empowers and encourages others, without sacrificing personal agency. She shines, and enables others to shine.
A performance during which one player monopolises attention, disregarding everyone else, soon becomes unwatchable. Performance, even virtuoso solo performance, is seldom simply showing off. In the wider world, the person who constantly has to have the last word, or steal the punchline of a story, or turn every conversation back onto their own brilliance, however interesting and intelligent they might be, is ultimately a bore. We may initially be attracted to their wit, but soon tire of their egotism.
Performing develops empowered humility. When someone refuses to leave centre stage or, equally importantly, refuses to occupy centre stage, their performance dies. There is no space on stage for tyrants or passengers. Interconnection requires exchange, not domination. This perspective, integral to understanding the workings of ensemble, offers a powerful model of community: each person has agency, none is isolated. All are in service, both of their own needs, and of the collective intention of the group of which they are a part.
D eveloping from the notion of the healthy integration of individual and collective, we encounter the fifth element of What Actors Know. Creative processes need participants to move beyond binary thinking. Actors do not operate from an attitude of me-or-you, but of me-with-you: not self-or-other, but self-with-others.
There’s a much-quoted line by the poet Rumi: ‘Beyond notions of right-doing and wrong-doing, there’s a field. I’ll meet you there.’ It is in this field that an actor’s work begins. Binary thinking is predicated on either/or. It assumes separation. It is the thinking that underpins the Story of Separation, predicating disconnection as the default state in which we exist.
At a technical level, notions of right/wrong are sometimes useful: text and movements need to be memorised and reproduced correctly. But, beyond the technical level, binary thinking hinders. There is no right nor wrong way to play Hamlet. There is no correct or incorrect way to dance. Creativity works on spectrums of choice – more/less appropriate, more/less conventional, more/less accessible. An actor embraces spectrum thinking: working with choice rather than certainty. Spectrum thinking recognises that between apparent opposites lie many possibilities. From within, an actor creates sometimes competing choices.
When we bring binary thinking to a relationship, we replace awareness of what actually exists with insistence on what we think ought to exist. A teacher might want respectful silence or excited interaction. If it is not there, she might behave as if something is ‘wrong’. This impedes her ability to respond to what is actually happening in the room at that moment. It hinders authentic connection.
Playing fuels creativity. Creativity leads to discovery, and discovery leads to learning
Putting aside binary thinking helps us connect. It reminds us to treat each moment as a unique opportunity to create a unique bond. Each unexpected response or ‘inappropriate behaviour’ becomes a chance to reorientate connection. We meet people as they are, and invite them to meet us as we are. We navigate diverse environments in a spirit of generosity and discovery.
The sixth element of What Actors Know is a truism, but essential nonetheless: actors play. ‘To play’ is to try something without risk of significant consequences. Playing fuels creativity. Creativity leads to discovery, and discovery leads to learning. Play is the intrinsic mammalian learning mechanism . Rehearsal is where actors play, discovering things without being watched by a critical public.
An actor does not stop playing when she performs. She remains a ‘player’, ‘playing’ a role. Playfulness, or lightness-of-touch, is integral to the unspoken contract between actor and audience. Connection fractures without it. If an audience believes an actor is really hurt, really being punched, really dying, other human impulses such as concern, revulsion or a desire to help take over. However serious the content of a performance, we assume an actor is gaining enjoyment, gratification or reward.
How playful an actor shows herself to be depends on the material she is performing. How playful you choose to be in a meeting or conversation also relies on context. Always, though, an element of lightness, or detachment, nourishes the flow of energy back and forth between you and others. This is not easy when you feel crushed by a sense of responsibility, or fear of failure. Fearless play is a rigorous self-discipline.
One of my main jobs training performers is to encourage genuine playfulness, inviting participants to lay aside notions of shame and binaries of right/wrong. When things go in unexpected directions, fearlessly we can use those moments to create genuine, open and vulnerable connectedness, inwardly and with the world.
Ultimately, an actor must trust the process. This is the seventh and final element of What Actors Know. An actor has no product. She structures a creative process which she then goes through in front of her audience. She communicates via the connections she establishes and sustains. Each connection is a process: an unfolding sequence of moments.
The elements of What Actors Know enable this process. They show us how to connect through presence, sculpting communication, being adaptable, balancing self with others, moving beyond binary thinking and developing a playful lightness-of-touch.
These are not acting skills. They are human skills. They draw on intrinsic human competencies, including imagination , attention, self-awareness, empathy , body/mind integration and neuroplasticity . They enable us to transcend isolation and, interconnecting, to establish community. They have shaped our evolution, and can shape our progress towards our shared future.
R eviewing the millennia of human evolution we’ve emerged from, Cacioppo and Patrick write of the adaptive evolutionary imperative of interconnectedness:
Since prehistory, the tellers of stories, those who learned to connect, sustain connection and communicate powerfully, have built the bridges between isolated individuals and, in doing so, created social beings. They constructed the ‘social brain’. What might it look like today, in our increasingly disconnected world, to fully embrace What Actors Know? We could each start by paying attention to the areas I’ve outlined. You can notice and resist distraction – enhancing presence – put aside the judgmental nature of binary thinking, embrace playfulness and flexibility, pay attention to sculpting the dynamics of each moment, balance personal agency and the empowerment of others, and trust the unfolding nature of lived experience.
Each of us could employ the skills of performance to connect during meetings, in classrooms, at political events
Opportunities to do this emerge not only from personal choice. They can also be supported by public and organisational policy. Schools, businesses, health services, governments and community organisations might recognise interconnection as integral to health, and structure themselves accordingly. Performance – both to watch and participate in – could, as it once was, become central to education, community, politics, aged care, healthcare: as essential as good food, fresh air and clean water. We could embed lifelong opportunities for people to remember, develop, value and celebrate their intrinsic human communicative capabilities, rather than increasingly outsourcing them to technology.
Re-centring performance does not only mean enhancing cultural provision, though that is one strategy. It means each of us employing the deep-level skills of performance to connect and communicate during meetings, in classrooms, at political events, in discussion groups. Doing so, we enhance our capacity, internally and externally, to integrate, interconnect and be more fully human.
Could we recommit, unconditionally, to revitalising our interconnected humanity? I’m not suggesting a romantic return to a past that never existed. I am suggesting that we move forward humanly. Might we accept what our ancestors, from neolithic shamans to contemporary actors, discovered? That we become complete through connection, inward and outward. That we all interconnect.
Utopian? Perhaps. Maybe we need some fresh utopian stories to live by if we are to counteract the isolated, lonely, dehumanised dystopia threatening to engulf us all."
"Is AI our salvation, our undoing, or just more of the same? | Aeon Essays",,https://aeon.co/essays/is-ai-our-salvation-our-undoing-or-just-more-of-the-same,"Here’s the quandary when it comes to AI: have we found our way to salvation, a portal to an era of convenience and luxury heretofore unknown? Or have we met our undoing, a dystopia that will decimate society as we know it? These contradictions are at least partly due to another – somewhat latent – contradiction. We are fascinated by AI’s outputs (the what ) at a superficial level but are often disenchanted if we dig a bit deeper, or otherwise try to understand AI’s process (the how ). This quandary has never been so apparent as in these times of generative AI. We are enamoured by the excellent form of outputs produced by large language models (LLMs) such as ChatGPT while being worried about the biased and unrealistic narratives they churn out. Similarly, we find AI art very appealing, while being concerned by the lack of deeper meaning, not to mention concerns about plagiarising the geniuses of yesteryear.
That worries are most pronounced in the sphere of generative AI, which urges us to engage directly with the tech, is hardly a coincidence. Human-to-human conversations are layered with multiple levels and types of meanings. Even a simple question such as ‘Shall we have a coffee?’ has several implicit meanings relating to shared information about the time of the day, a latent intent to have a relaxed conversation, guesses about drink preferences, availability of nearby shops, and so on and so forth. If we see an artwork titled ‘1970s Vietnam’, we probably expect that the artist is intending to convey something about life in that country during end-war and postwar times – a lot goes unsaid while interacting with humans and human outputs. In contrast, LLMs confront us with human-like responses that lack any deeper meaning. The dissonance between human-like presentation and machine-like ethos is at the heart of the AI quandary, too.
Yet it would be wrong to think that AI’s obsession with superficial imitation is recent. The imitation paradigm has been entrenched in the core of AI right from the start of the discipline. To unpack and understand how contemporary culture came to applaud an imitation-focused technology, we must go back to the very early days of AI’s history and trace its evolution over the decades.
A lan Turing (1912-54), widely regarded as the father of AI , is credited with developing the foundational thoughts of the discipline. While AI has evolved quite dramatically over the 70 years since Turing died, one aspect of his legacy stands firmly at the heart of contemporary AI deliberations. This is the Turing test, a conceptual test that asks whether a technology can pass off its output as human.
Imagine a technology engaged in an e-chat with a human – if the technology can fool the interlocutor to believe that they are chatting with a human, it has won the Turing test. The chat-based interface that today’s LLMs use has led to the resurgence of interest in the Turing test within popular culture. Also, the Turing test is so embedded within the contemporary AI scholarly community as the ultimate test of intelligence that it may even be scandalous to say that it is only tangential to judging intelligence. Yet that’s exactly what Turing had intended in his seminal paper that first introduced the test.
Turing very evidently did not consider the imitation game a test of intelligence
It is noteworthy that Turing had called it the ‘imitation game’. Only later was it christened the ‘Turing test’ by the AI community. We don’t need to go beyond the first paragraph of Turing’s paper ‘Computing Machinery and Intelligence’ (1950) to understand the divergence between the ‘imitation game’ and judgment of whether a machine is intelligent. In the opening paragraph of this paper, Turing asks us to consider the question ‘Can machines think?’ and he admits how stumped he is.
He picks himself up after some rambling and closes the first paragraph of the paper by saying definitively: ‘I shall replace the question by another, which is closely related to it and is expressed in relatively unambiguous words.’ He then goes on to describe the imitation game, which he calls the ‘new form of the problem’. In other words, Turing is forthwith in making the point that the ‘imitation game’ is not the answer to the question ‘Can machines think?’ but is instead the form of the replaced question.
Photo by Heinz-Peter Bader/Getty
The AI community has – unfortunately, to say the least – apparently (mis)understood the imitation game as the mechanism to answer the question of whether machines are intelligent (or whether they can think or exercise intelligence). The christening of the ‘imitation game’ as the ‘Turing test’ has arguably provided an aura of authority to the test, and perhaps entrenched a reluctance in generations of AI researchers to examine it critically, given the huge following that Turing enjoys in the computing community. As recently as 2023, leaders of several nations gathered in the United Kingdom, at Bletchley Park – once Turing’s workplace – to deliberate on AI safety. In that context, the fact that Turing very evidently did not consider the imitation game a test of intelligence should offer some comfort – and courage – to engage with it critically.
A gainst the backdrop of Turing’s formulation of the imitation game in the early 1950s in the UK, interest was snowballing on the other side of the Atlantic around the idea of thinking machines. John McCarthy, then a young mathematics assistant professor at Dartmouth College in New Hampshire, procured funding to organise an eight-week workshop to be held during the summer of 1956. This would later be known as the ‘founding event’ of AI, and records suggest that the first substantive use of the term ‘artificial intelligence’ is in McCarthy’s funding proposal for the workshop, submitted to the Rockefeller Foundation.
For a moment, forget about ‘artificial intelligence’ as it stands now, and consider the question: what disciplines would naturally be involved in the pursuit of developing intelligent machines? It may seem natural to think that such a quest should be centred on disciplines involved with understanding and characterising intelligence as we know it – the cognitive sciences, philosophy, neuroscience, and so on. Other disciplines could act as vehicles of implementation, but the overall effort would need to be underpinned by know-how from disciplines that deal with the mind. Indeed, it was not a coincidence that Turing chose to publish his seminal paper in Mind , a journal of philosophy with substantive overlaps in the cognitive sciences. The Dartmouth workshop was notably funded by the Biological and Medical Research division of the Rockefeller Foundation, reflecting that the above speculations may not be off the mark. Yet McCarthy’s workshop was radically different in structure.
Mathematical researchers no longer had to feel alone when talking about thinking machines as computing
The Dartmouth workshop was dominated by mathematicians and engineers, including a substantive participation from technology companies such as IBM; there was little presence of scholars from other disciplines. A biographical history comprising notes by Ray Solomonoff, a workshop participant, and compiled by his wife Grace Solomonoff, provides ample evidence that the ‘artificial intelligence’ project was actively shunted along the engineering direction and away from the neuro-cognitive-philosophical direction. In particular, Solomonoff’s notes record one of the core organisers, Marvin Minsky, who would later become a key figure in AI, opining thus in a letter in the run-up to the workshop:
It could be that other participants shared Minsky’s view of philosophical and language matters as time-wasting trivialities, but not voiced them as explicitly (or as bluntly).
In a description of discussions leading up to the workshop, the historian of science Ronald Kline shows how the event, initially conceptualised with significant space for pursuits like brain modelling, gradually gravitated towards a mathematical modelling project. The main scientific outcome from the project, as noted in both Solomonoff’s and Kline’s accounts, was to establish mathematical symbol manipulation – what would later be known as symbolic AI – as the pathway through which AI would progress. This is evident when one observes that, two years later, in a 1958 conference titled ‘Mechanisation of Thought Processes’ (a name that would lead any reader to think of it as a neuro-cognitive-philosophical symposium), many participants of the Dartmouth workshop would present papers on mathematical modelling.
The titles of the workshop papers range from ‘heuristic programming’ to ‘conditional probability computer’. With the benefit of hindsight, one may judge that the Dartmouth workshop bolstered the development of thinking machines as an endeavour located in engineering and mathematical sciences, rather than one to be led by ideas from disciplines that seek to understand intelligence as we know it. With the weight of the Dartmouth scholars behind them, mathematical researchers no longer had to feel alone, apologetic or defensive when talking about thinking machines as computing – the sidelining of social sciences in the development of synthetic intelligence had been mainstreamed.
Y et the question remains: how could a group of intelligent people be convinced that the pursuit of ‘artificial intelligence’ shouldn’t waste time on philosophy, language and, of course, other aspects such as cognition and neuroscience? We can only speculate, again with the benefit of hindsight, that this was somehow fallout from a parochial interpretation of the Turing test, an interpretation enabled by developments in Western thought over four to five centuries. If you are one who believes that ‘thinking’ or ‘intelligence’ is possible only within an embodied and living organism, it would be absurd to ask ‘Can machines think?’ as Turing did, in his seminal paper.
Thus, even envisioning synthetic intelligence as a thing requires one to believe that intelligence or thinking can exist outside an embodied, living organism. René Descartes, the 17th-century philosopher who is embedded in contemporary popular culture through the pervasively recognised quote ‘I think, therefore, I am’, posited that the seat of thought in the human body is the mind, and that the body cannot think. This idea, called Cartesian mind-body dualism, establishes a hierarchy between the mind (the thinking part) and the body (the non-thinking part) – marking a step towards localising intelligence within the living organism.
The high-level project of synthetic intelligence has no natural metric of success
Not long after Descartes’s passing, on the other side of the English Channel, another tall philosopher, Thomas Hobbes, would write in his magnum opus Leviathan (1651) that ‘reason … is nothing but reckoning’. Reckoning is to be interpreted as involving mathematical operations such as addition and subtraction. Descartes and Hobbes had their substantive disagreements – yet their ideas synergise well; one localises the thinking in the mind, and the other reductively characterises thinking as computation. The power of the synergy is evident in the thoughts of Gottfried Leibniz, a philosopher who was probably acquainted with Descartes’s dualism and Hobbes’s materialism as a young adult, and who took the reductionist view of human thought further still. ‘When there are disputes among persons,’ he wrote in 1685, ‘we can simply say, “Let us calculate,” and without further ado, see who is right.’ For Leibniz, everything can be reduced to computation. It is against this backdrop of Western thought that Turing would – three centuries later – ask the question ‘Can machines think?’ It is notable that such ideas are not without detractors – embodied cognition has seen a revival lately, but still remains in the fringes.
While centuries of such philosophical substrate provide a fertile ground for imagining synthetic intelligence as computation, a mathematical or engineering project towards developing synthetic intelligence cannot take off without ways of quantifying success. Most scientific and engineering quests come with natural measures of success. The measure of success in developing aircraft is to see how well it can fly – how long, how high, how stably – all amenable to quantitative measurements. However, the high-level project of synthetic intelligence has no natural metric of success. This is where the ‘imitation game’ provided a much-needed take-off point; it asserted that success in developing artificial intelligence can simply be measured by whether it can generate intelligent-looking outputs that could pass off as a human’s.
Analogous to how Descartes’s ideas suggested that we need not bother about the body while exploring thinking, and in a similar reductive spirit, the structure of the ‘imitation game’ suggested that artificial intelligence need not bother about the process (the how ) and could just focus on the output (the what ). This dictum has arguably shaped artificial intelligence ever since; if a technology can imitate humans well, it is ‘intelligent’.
H aving established that imitation is sufficient for intelligence, the AI community has a natural next target. The Turing test says that the human should be tricked by the technology into believing she is interacting with another human to prove intelligence, but that criterion is abstract, qualitative and subjective. Some humans may be more adept than others in picking up subtle signs of a machine, much like some contemporary factcheckers have the knack of spotting that little evidence of inauthenticity in deep fakes. The AI community must find reliable pathways towards developing technological imitations that could be generally regarded as intelligent by humans – in simple terms, there needs to be a generalisable structure adequate to reliably feigning intelligence. This is evident in McCarthy’s own words in 1983, when he characterises AI as the ‘science and engineering of making computers solve problems and behave in ways generally considered to be intelligent’ – of the two things, the former is not novel, the latter is. We will look at two dominant pathways across the 1960s to the ’80s that powered the quest for AI’s advancement through designing imitation technology.
In the 1960s, Joseph Weizenbaum developed a simple chatbot within the domain of Rogerian psychotherapy, where the idea is to encourage the patient to think through their condition themselves. The chatbot, named ELIZA, would use simple transformation rules, often just to put the onus back on the human; while very unlike LLMs in its internals, the emergence of LLMs has led to narratives comparing and contrasting the two.
‘When part of a mechanism is concealed from observation, the behaviour of the machine seems remarkable’
An example transformation, from Weizenbaum’s own paper about the system, involves responding to ‘I am (X)’ by simply making the chatbot ask ‘How long have you been (X)?’ Notwithstanding the simple internal processing, ELIZA’s users, to Weizenbaum’s amusement, often mistook it to be human.
‘Some subjects have been very hard to convince that Eliza (with its present script) is not human,’ wrote Weizenbaum (italics original), in an article published in 1966 in Communications of the ACM , among the foremost journals in computing.
This resonates with a general observation, which may sound prophetic in hindsight, made by Ross Ashby at the Dartmouth conference: ‘When part of a mechanism is concealed from observation, the behaviour of the machine seems remarkable.’
T oday, the ELIZA effect is used to refer to the category of mistake in which symbol manipulation is mistaken for cognitive capability. Several years later, the cognitive scientist Douglas Hofstadter would call the ELIZA effect ‘ineradicable’, suggesting that a gullibility intrinsic to humans could be adequate for AI’s goals. The ELIZA effect – or the adequacy of opaque symbol manipulation to sound intelligent to human users – would turbocharge AI for the next two or three decades to come.
The wave of symbolic AI led to the development of several AI systems – often called ‘expert systems’ – that were powered by symbol manipulation rulesets of varying sizes and complexity. One of the major successes was a system developed at Stanford University in the 1970s called MYCIN, powered by around 600 rules and designed to recommend antibiotics (many of which end in -mycin, hence the name). One of AI’s main 20th-century successes, the victory by IBM’s Deep Blue chess-playing computer over the reigning (human) world champion in 1997, was also based on the success of rule-based symbolic AI.
While opaque symbolic AI has been widespread, there has been a second high-level mechanism that was found to be useful to create a pretence of intelligence. As a step towards understanding that, consider a simple thermometer or a pressure gauge – these are intended to measure temperature and pressure. They obviously have nothing to do with ‘intelligence’ per se.
But let’s now connect a simple decision mechanism to the thermometer: if the temperature goes above a preset threshold, it switches on the air conditioner (and vice versa). These little regulating mechanisms, often called thermostats, are pervasive in today’s electronic devices, whether it be ovens, water heaters, air conditioners, and even used within computers to prevent overheating. Cybernetics, the field involving feedback-based devices such as thermostats and their more complex cousins, was widely regarded as a pathway towards machine intelligence. Grace Solomonoff records ‘cybernetics’ as a potential name considered by McCarthy for the Dartmouth workshop (in lieu of the eventual ‘artificial intelligence’); the other being ‘automata theory’. The key point here is that the sense-then-respond mechanism of self-regulation employed within the likes of a thermostat could seem like some form of intelligence. We can only speculate on the reasons why we might think so; perhaps it’s because we consider sensing to be very intrinsically connected with being human (the loss of sensory capacity – even simply the loss of taste, which most of us experienced during COVID-19 – can be very impoverishing), or because the body maintains homoeostasis, among the most complex versions of life-sustaining self-regulation.
McCarthy could be seen talking about a thermostat’s beliefs, and even extending the logic to automated tellers
Yet we’re not likely to confuse simple thermostats for thinking machines, are we? Well, as long as we don’t think as McCarthy did. More than two decades after the Dartmouth workshop, its pioneering organiser would go on to write in the article ‘Ascribing Mental Qualities to Machines’ (1979) that thermostats had beliefs .
He writes: ‘When the thermostat believes the room is too cold or too hot, it sends a message saying so to the furnace . ’ At parts in the article, McCarthy seems to recognise that there would naturally be critics who would ‘regard attribution of beliefs to machines as mere intellectual sloppiness’, but he goes on to say that ‘we maintain … that the ascription is legitimate . ’
McCarthy admits that thermostats don’t have deeper forms of belief such as introspective beliefs, viz ‘it doesn’t believe that it believes the room is too hot’– a great concession indeed! In academia, some provocative pieces tend to be written just out of enthusiasm and convenience, especially when caught off guard. A reader who has seen bouts of unwarranted enthusiasm leading to articles may find it reasonable to urge that McCarthy’s piece shouldn’t be over-read – perhaps, it was just a one-off argumentation.
Yet, historical records tell us that’s not the case; four years later, McCarthy would write the piece ‘The Little Thoughts of Thinking Machines’ (1983). In that paper, he could be seen talking about a thermostat’s beliefs, and even extending the logic to automated tellers, which was probably starting to become an amusing piece of automation around that time. He writes: ‘The automatic teller is another example. It has beliefs like, “There’s enough money in the account,” and “I don’t give out that much money”.’
Today, the sense-then-respond mechanism powers robots extensively, with humanoid robots dominating the depiction of artificial intelligence in popular imagery, as can be seen with a quick Google image search . The usage of the adjective smart to refer to AI systems could be seen as correlated with an abundance of sense-then-response mechanisms: smart wearables involve sensors deployed at a person-level, smart homes are homes with several interconnected sensors all over the home, and smart cities are cities with abundant sensor-based surveillance. The new wave of sensor-driven AI, often referred to as ‘internet of things’, is powered by sensors.
Opaque symbolic AI and sensor-driven cybernetics are useful pathways to design systems that behave in ways generally considered to be intelligent, but we still ought to expend the effort to design these systems. Does the design requirement pose hurdles? This question leads us to the next epoch in AI scholarship.
A rapidly expanding remit of AI started experiencing some strong headwinds in certain tasks, around the 1980s. This is best captured by Hans Moravec’s book Mind Children (1988) in what has come to be known as Moravec’s paradox:
The AI that had started to excel at checkers and chess through symbolic methods wasn’t able to make progress in distinguishing handwritten characters or identifying human faces. Such tasks are what may fall within a category of innately human (or, well, animal) activities – something we do instantly and instinctively but can’t explain how. Most of us can instantly recognise emotions from people’s faces with a high degree of accuracy – but won’t be enthusiastic about taking up a project to build a set of rules to recognise emotions from people’s images. This relates to what is now known as Polanyi’s paradox: ‘We can know more than we can tell’ – we rely on tacit knowledge that often can’t be verbally expressed, let alone be encoded as a program. The AI bandwagon has hit a brick wall.
A rather blunt (and intentionally provocative) analogy might serve well here, to understand how AI scholarship wiggled out of this conundrum. In school, each of us had to attempt and pass exams to illustrate our understanding of the topic and achievement of learning outcomes. Yet some students are too lazy to undertake the hard work; they simply copy from the answer sheets of their neighbours in the exam hall.
We call this cheating or, in milder and more sophisticated terms, academic malpractice. To complete the analogy, our protagonist is the Turing test, and the AI scholarship is not lazy, but has run out of ways to expand to address tasks that we do based on tacit knowledge. It is simply incompetent. If the reader would forgive the insinuating tone, I observe here that AI took the same pathway as the lazy student: copying from others – in this case, from us humans.
Crude models are lazy learners; deep learning models are eager learners
To really see this copying paradigm, consider a simple task, that of identifying faces in images. For humans, it is an easy perception task. We see an image and instantly recognise the face within it, if any – we almost can’t not do this task each time we see a picture (try it). Blinking an eye would take more time.
If you entrust an AI engineer to do this today, they wouldn’t think twice about adopting a data-driven or machine-learning methodology to undertake it. It starts by gathering a number of images and having human annotators label them – does each contain a face or not? This leads to a collection of two stacks of images; one with faces, another without. The labelled images would be used to train the machines, and that is how those machines learn to make the match.
This labelled dataset of images is called the training data. The more sophisticated the machine learning model, the more images and rules and operations it would use to decide whether another picture in front of it contains a face or not. But the fundamental paradigm is that of copying from labelled data mediated by a statistical model, where the statistical model could be as simple as a similarity, or it could be a very complex and carefully curated set of ‘parameters’ (as in deep learning models, which are more fashionable in current times).
Crude models are lazy learners because they don’t consult the training data until called upon to make a decision, whereas deep learning models are eager learners because they distil the training data into statistical models upfront, so that decisions can be made fast.
While there is enormous complexity and variety in the types of tasks and decision-making models, the fundamental principle remains the same: similar data objects are useful for similar purposes . If machine learning had a church, the facade could sport the dictum (in Latin, as they do for churches): Similia objectum, similia proposita. If you’re curious what this means, please consult a data-driven AI that’s specialised for language translation.
T he availability of LLMs since the release of ChatGPT in late 2022 heralded a global wave of AI euphoria that continues to this day. It was often perceived in popular culture as a watershed moment, which indeed it could be, on a social level, since AI never before pervaded public imagination as it does now. Yet, at a technical level, LLMs have machine learning at their core and are technologically generating a newer form of imitation – an imitation of data ; this contrasts with the conventional paradigm involving imitation of human decisions on data.
Through LLMs, imitation has taken a newer and more generalised form – it is presented as an all-knowing person, always available to be consulted on anything under the sun. Yet, it follows the same familiar copying pathway that is entrenched at the core of machine learning. As the prominent AI researcher Emily Bender and fellow AI ethicists would argue , these are ‘stochastic parrots’; while parrots that simply repeat what they hear are impressive in their own right, randomised – or stochastic – query-dependent and selective reproductions of training data have been discovered as a paradigm to create a pretence of agency and, thus, of intelligence. The reader may remember that the heuristics of opaque symbol manipulation and sensor-driven cybernetics had their heydays in the 1960s and ’70s – now, it is the turn of randomised data copying.
It is evident that biases and hallucinations are features, not bugs
The much-celebrated value of LLMs is in generating impeccable outputs: pleasing and well-written text. One may wonder how LLMs generate well-formed text when so much of the text on the web is not of such good quality, and may even imagine that to be an intrinsic merit of the technology. This is where it becomes interesting to understand how LLMs piggyback on various forms of human input. It has been noted that the most popular commercial LLM, ChatGPT, employed thousands of low-paid annotators in Kenya to grade the quality of human text and, in particular, to exclude ones regarded as toxic. Thus, the observed higher quality of LLM text is also an artefact and output of the imitation paradigm rooted in the core of AI.
Once you understand this, it’s easier to understand why LLMs could produce substantially biased outputs, including those on gender and racial lines, as noted in recent research . The randomised data-copying paradigm involves mixing and matching patterns from different parts of training data – these create narratives that don’t gel well, and consequently yield embarrassingly absurd and illogical text, often referred to as ‘hallucinations’. Understanding LLMs as imitation on steroids, it is evident that biases and hallucinations are features, not bugs. Today, the success of the LLM has spilled over to other kinds of data to herald the advent of generative AI that encompasses image and video generation, all of which are infested with issues of bias and hallucinations, as may be expected.
L et’s take an adversarial position to the narrative so far. Artificial intelligence, as it stands today, may be designed to produce imitations to feign intelligence. Yet if it does the job, why obsess ourselves with nitpicking?
This is where things get a bit complicated, but very interesting. Consider a radiologist trained in diagnosing diseases from X-rays. Their decision is abundantly informed by their knowledge of human biology. We can get many such expert radiologists to label X-rays with the diagnosis. Once there are enough X-ray diagnosis pairs, these can be channelled into a data-driven AI, which can then be used to diagnose new X-rays. All good. The scene is set for some radiologists to receive redundancy letters.
Years pass.
As luck would have it, the world is hit by COVID-27, a respiratory pandemic of epic proportions, like its predecessor. The AI knows nothing of COVID-27, and thus, can’t diagnose the disease. Having pushed many radiologists into other sectors, we no longer have enough experts to diagnose. The AI knows nothing about human biology, and its ‘knowledge’ can’t be repurposed for COVID-27 – but there is an abundance of X-rays labelled for COVID-27, encompassing all its variants, to retrain the statistical model.
The same AI that pushed radiologists out of their jobs is now in need of those very same folks to ‘teach’ it to imitate decisions about COVID-27. Even if no COVID-27 comes, viruses mutate, diseases change, the world never remains static. The AI model is always at risk of becoming stale. Thus, a continuous supply of human-labelled data is the lifeblood of data-driven AI, if it is to remain relevant to changing times. This intricate dependency on data is a latent aspect of AI, which we often underestimate at risk of our own eventual peril.
The statistical models of AI codify our biases and reproduce them with a veneer of computational objectivity
Replace radiology with policing, marking university assessments, hiring, and even making decisions on environmental factors such as weather prediction, or genAI applications such as video generation and automated essay writing, and the high-level logic remains the same. The paradigm of AI – interestingly characterised by the popular AI critic Cathy O’Neil in Weapons of Math Destruction (2016) as ‘project[ing] the past into the future’ – simply doesn’t work for fields that change or evolve. At this juncture, we may do well to remember Heraclitus , the Greek philosopher who lived 25 centuries back – he would quip that ‘change is the only constant’.
As the historian Yuval Noah Harari would say , belief that AI knows all, that it is truly intelligent and has come to save us, promotes the ideology of ‘dataism’, which is the idea of assigning supreme value to information flows. Further, given that human labelling – especially in social decision-making such as policing and hiring – is biased and ridden with stereotypes of myriad shades (sexism, racism, ageism and others), the statistical models of AI codify these biases and reproduce them with a veneer of computational objectivity. Elucidating the nature of finer relationships between the paradigm of imitation and AI’s bias problem is a story for another day.
If imitations are so problematic, what are they good for? Towards understanding this, we may take a leaf out of Karl Marx’s scholarship on the critique of the political economy of capital, capital understood as the underpinning ethos of the exploitative economic system that we understand as capitalism. Marx says that capital is concerned with the utilities of objects only insofar as they have the general form of a commodity and can be traded in markets to further monetary motives. In simple terms, towards advancing profits, efforts to improve the presentation – through myriad ways such as packaging, advertising and others – would be much more important than efforts to improve the functionality (or use-value) of the commodity.
The subordination of content to presentation is thus, unfortunately, the trend in a capitalist world. Extending Marx’s argument to AI, the imitation paradigm embedded within AI is adequate for capital . Grounded on this understanding, the interpretation of the imitation game – err, the Turing test – as a holy grail of AI is hand-in-glove with the economic system of capitalism. From this vantage point, it is not difficult to see why AI has synergised well with the markets, and why AI has evolved as a discipline dominated by big market players such as Silicon Valley’s tech giants. This market affinity of AI was illustrated in a paper that showed how AI research has been increasingly corporatised, especially when the imitation paradigm took off with the emergence of deep learning.
The wave of generative AI has set off immense public discourse on the emergence of real artificial general intelligence. However, understanding AI as an imitation helps us see through this euphoria. To use an overly simplistic but instructive analogy, kids may see agency in imitation apps like My Talking Tom – yet, it is obvious that a Talking Tom will not become a real talking cat, no matter how hard the kid tries. The market may give us sophisticated and intelligent-looking imitations, but these improvements are structurally incapable of taking the qualitative leap from imitation to real intelligence. As Hubert Dreyfus wrote in What Computers Can’t Do (1972), ‘the first man to climb a tree could claim tangible progress toward reaching the moon’ – yet, actually reaching the Moon requires qualitatively different methods than tree-climbing. If we are to solve real problems and make durable technological progress, we may need much more than an obsession with imitations."
What the Ju/’hoansi can tell us about group decision-making | Aeon Essays,,https://aeon.co/essays/what-the-ju-hoansi-can-tell-us-about-group-decision-making,"The Dilemma of the Deserted Husband unfolded in the late 1950s amid a band of G/wi hunter-gatherers, a subgroup of Ju/’hoansi (often known as !Kung San), dwelling in the Kalahari Desert of Southern Africa. According to the South African-born anthropologist and Bushman Survey Officer George Silberbauer, a woman named N!onag//ei had left her husband, /wikhwema, for his best friend. Few were surprised. After all, /wikhwema was a temperamental and pompous man, and a bit of a joke. In contrast, the new husband, /amg//ao, held unconventional charm. He was ‘a virtuoso dancer, a consistently successful hunter and … rumoured to be a bit of a demon as a lover.’
Deserted G/wi spouses usually move on within a few months. But not /wikhwema. Mourning the dual losses of his friend and wife, he complained endlessly. Before long, his incessant whining became a burden on everyone. After more than a year, people were at their wits’ end. Complicating matters, given his role as an ‘owner’ of the band’s territory, /wikhwema could not be expected to relocate elsewhere. The band had no choice but to stick together.
Eventually, a ‘lateral thinker’ proposed a novel solution. Why not permit a polyandrous marriage? This unconventional suggestion meant that N!onag//ei could have both /amg//ao and /wikhwema as husbands, a departure from the monogamous norms of G/wi society. After much deliberation, the innovation was accepted. The new couple’s marriage was salvaged, as was /wikhwema’s pride, and the band was relieved of his whining.
The Dilemma of the Deserted Husband was not solved by the unilateral decision of a single leader. Nor did people raise their hands in a majority vote. Instead, it was the product of long deliberation. For months, there were discussions, disagreements and compromises. The goal of the process was consensus, to find a solution that everyone could live with, even if it was imperfect (the new throuple ‘did not exactly live happily ever after’, according to Silberbauer).
For the vast majority of human history, people made group decisions through consensus. It is perhaps the most conspicuous feature of political life among recent hunter-gatherer societies, from the Ju/’hoansi to the Aboriginal peoples of Australia to the Indigenous societies of the early Americas. As an anthropologist, I have observed consensus-based decision-making myself among hunter-gatherers in the rainforests of Malaysia.
Though the small-world life of hunter-gatherers may seem far removed from our own digitalised and global world, the problems of group life have remained fundamentally the same for hundreds of thousands of years. In the face of conflict and polarisation, ancient human groups needed processes that yielded good outcomes. What can we learn from a political form shaped by hundreds of thousands of years of trial and error? By examining how hunter-gatherers achieve consensus, perhaps we can develop better strategies to solve the problems we face today.
H uman prehistory was littered with poor group decisions. Whether it was an ill-timed raid or the wrong choice of watering hole, some of our would-be hunter-gatherer ancestors vanished without a trace. We know this because, among hunter-gatherers today, group decisions are matters of existential importance. As an anthropologist, I have been trying to understand how exactly hunter-gatherer groups succeed – and how they fail.
In the annals of group failure, groupthink is the most common culprit. The phrase was coined by the journalist William H Whyte Jr in 1952, but it is generally associated with the Yale psychologist Irving Janis, who argued that the pressures of social conformity can doom group performance. People may be bullied by their superiors or feel that they risk ostracism from their peers. Important information is left unspoken. By failing to weigh their options judiciously, groups consider only a fraction of the space of possible solutions. Scholars have called this an information cascade. As evidence of groupthink, they often point to famous debacles such as the Bay of Pigs, the Challenger disaster, and the 2003 invasion of Iraq.
At the other end of the spectrum, groups can fail by fragmentation. This possibility is hardly considered in Western studies of social psychology, but it looms large for hunter-gatherers. Through the centrifugal forces of disagreement, the group splinters, with subgroups or individuals going their own way, thereby forfeiting the benefits of cooperation that come from living in a bigger group. As we saw in the Dilemma of the Deserted Husband, hunter-gatherer bands recognise the benefits of staying together.
The Ju/’hoansi are careful not to entrust key decisions to single individuals or small sub-groups
A few years after Silberbauer observed the Dilemma of the Deserted Husband, the young Harvard anthropologist Megan Biesele travelled to the Kalahari to begin her PhD research. It was 1970, and Biesele was there to study ritual and folklore among a band of Ju/’hoansi in the Dobe area, an inhospitable expanse of sand and bushland seasonally flooded by rain, not far from where Silberbauer worked among the G/wi. As she became proficient in the Ju/’hoan language, Biesele observed many group discussions and decisions, taking special notice of the Ju/’hoansi’s consensus-based decision-making. Together, Biesele’s and Silberbauer’s observations show us how the Ju/’hoansi keep groupthink and fragmentation in check, navigating between what Silberbauer calls ‘ the Scylla of excessive interdependence and the Charybdis of fragmenting anarchy’.
The Ju/’hoansi are careful not to entrust key decisions to single individuals or small sub-groups. Leadership is temporary and knowledge-based, shifting even within a single conversation. Leaders refrain from stating their opinions early in the conversation, which could bias the opinions of others who have yet to speak. The role of a leader in group decisions is to guide deliberation, state the group’s mood, and help finalise a decision. Leaders are respected, but they cannot coerce others. Biesele refers to this as ‘sapiential authority’.
With the goal of consensus, the group itself is the decision-maker. Decisions typically start as grassroots affairs between neighbours and friends. Only later does the community gather together for a formal meeting. During deliberation, everyone – man or woman, old or young – is encouraged to state their opinion about important matters. In the egalitarian culture of the Ju/’hoansi, people do their own thing and therefore have their own unique experiences and ways of representing problems that may be relevant to a group decision.
The Ju/’hoansi are not culturally diverse, but their permissiveness of individual differences means their groups are functionally diverse. The social norm of widespread participation ensures the free and open exchange of information, reducing the likelihood of an information cascade. Biesele documented a principle that, if each person’s opinion was not heard, trouble would follow. Repressed opinions, it was said, could cause sickness.
F or the Ju/’hoansi, there is little connection between individuals and the ideas they promulgate. As Silberbauer notes in Politics and History in Band Societies (1982): ‘It often happens that the suggestion finally adopted is one which was initially voiced by somebody who has taken no further part in the proceedings, leaving it to others to take up, and “push” his or her proposal.’ No one remembers the lateral thinker who solved the Dilemma of the Deserted Husband. An idea is like a bloody antelope carcass: once in the public square, it is more or less public property. To attribute an idea to a person would contravene the egalitarian nature of the band.
Deliberation also means disagreement. Claims are sceptically evaluated based on evidence, according to the anthropologists Melvin Konner and Nicholas Blurton Jones, who investigated Ju/’hoansi knowledge of animal behaviour in the 1970s. The Ju/’hoansi are careful to distinguish between first-hand knowledge and hearsay or speculation. There was a norm that discouraged rampant speculation: when someone said that children could be killed by fires, an old man said that people should only speak when they have seen things happen. One man was laughed at for his gullibility when he said that he had heard that elephants would bury their babies up to their necks.
One day, while tagging along with two trackers, Biesele heard constant feedback as the two busily corrected each other about where animal tracks were leading. In this dialectic, they responded to evidence and explained their logic. Neither took anything personally. In an email in 2022, Biesele told me: ‘Trackers’ conversations are fully cooperative and open to both new ideas and to corrections by other trackers, specifically to ensure the best-reasoned outcomes. So democracy and science are closely allied in the people’s minds, and closely govern how decisions are made.’
Unlike in modern politics, group decisions are not something to be won or lost
The Ju/’hoansi keep their cool, recognising that anger and heated feelings can lead to impulsive decisions and misunderstandings. According to Silberbauer, ‘the band is reluctant to come to decision under the sway of strong feelings: if discussion becomes too angry or excited, debate is temporarily adjourned by the withdrawal of the attention to the calmer participants until things cool down.’ Confrontation is avoided through a variety of subtle stratagems: pretending to cook, or urgently attending to a thorn in one’s foot. When things get too heated, people disengage, signalling a lack of sympathy for the outburst. The fate of the Ju/’hoansi contrarian is neither exile nor execution. It is to be ignored.
This isn’t to say that debate never occurs. Silberbauer observed ‘a bit of cut and thrust between orators’, however he found that point-scoring ultimately played little role in the ultimate decision. In a highly interdependent band, this makes sense because one’s fate is largely tied to that of other bandmates. As a result, unlike in modern politics, group decisions are not something to be won or lost. Attentive of this, the Ju/’hoansi avoid the mistake of equating rhetorical flourish with truth. The idea of sparring orators dealing knockout blows would be anathema to the Ju/’hoansi. A knockout blow is self-defeating, like punching oneself in the face.
When it comes to finalising a course of action, the Ju/’hoansi are sceptical of voting. In small groups, Biesele has found, the Ju/’hoansi see the act of voting as polarising. As the anthropologist David Graeber explained in Possibilities (2007):
Instead, discussion continues until a consensus is reached. Everyone has to agree on the course of action because it legitimates the decision as belonging to the group. It is not merely the actual result of the decision that counts, but the process itself. Everyone must attend to what Silberbauer calls the social balance-sheet. The social balance-sheet is no less than the promise of future cooperation, perhaps the most important thing in the life of a hunter-gatherer.
Consensus is also about the creation of shared meaning. The Ju/’hoansi, according to Silberbauer, are not only exchanging facts about reality but also values, objectives and ‘logical and causal relationships between items of information’. To decide well, the band must think together.
Perhaps the best illustration of this process of cognitive convergence comes from Kenneth Liberman, who worked among Aboriginal populations of the western Australian desert. Each day starts with the Morning Discourse, in which people take turns voicing concerns, thoughts, ideas. Each comment builds on the previous. The state of affairs of the group becomes publicly available. Nothing is directed toward individuals, only the group. ‘The favoured strategy here is to depersonalise one’s remarks and tone of voice as much as possible,’ wrote Liberman in 1985. ‘The effect is something like acting as if someone else is doing the talking.’ Rather than each person expressing views as an individual, it is almost as if the group is talking through each individual. The Morning Discourse shapes the consensus, when ‘all think in the same way with the same head, not in different ways.’ Sometimes hunter-gatherers don’t even bother to articulate the decision, so clear is the consensus and the subsequent course of action.
T he Ju/’hoansi style of decision-making finds echoes in the works of great thinkers. Cicero believed that conversation should be inclusive, allowing everyone a turn to speak. It should also be free of passion and gossip about people not present, and easy-going. John Dewey felt deliberation was critical to a vibrant democracy. Jurgen Habermas advocates for widespread participation of all individuals and for groups to seek ‘rational consensus’. He believes that people should pose rational arguments that are ‘in the best interest’ of all participants, thereby limiting the chances of fragmentation and inequality. These arguments are theoretical but, for the Ju/’hoansi, this philosophy is everyday life.
Understanding the Ju/’hoansi mode of communication from a modern perspective requires investigating the nature of dialogue. The word ‘dialogue’ is derived from the Greek ‘ dia ’ (through) and ‘ logos ’ (word or meaning), and is often translated as ‘a flow of meaning’. According to William Isaacs, who teaches workshops on dialogue-based approaches to communication, dialogue is ‘a shared inquiry, a way of thinking and reflecting together. It is not something you do to another person. It is something you do with people … Dialogue is a living experience of inquiry within and between people.’ Contrast this with debate, the root of which comes from the Old French word debatre – ‘to fight’.
When done correctly, dialogue could result in an extraordinary form of human cooperation
One of the most famous advocates of a dialogue-based approach to conversation was the American-born British theoretical physicist David Bohm. Alongside his seminal contributions to quantum theory, Bohm maintained interest in problems such as consciousness and creativity. In On Dialogue , a book he wrote just a few years before his death in 1992, Bohm tackled the problem of communication breakdown in society that seems similar to our current predicament of polarisation: ‘within any single nation, different social classes and economic and political groups are caught in a similar pattern of inability to understand each other.’
To Bohm, dialogue helps to establish a shared common understanding between individuals. It would mean abandoning assumptions, not clinging so tightly to our opinions, and doing a lot of listening. Yet he knew that dialogue was difficult. It required guardrails and open-mindedness. But when done correctly, dialogue could result in an extraordinary form of human cooperation with unparalleled creativity: ‘nobody is trying to win … we are not playing a game against each other, but with each other. In a dialogue, everybody wins.’
Channelling his physicist’s intuition, Bohm analogised the alignment of group sentiment with a laser: in contrast to normal light, which is scattered all about, a laser aligns light beams in the same direction, producing coherence. Bohm believed that ‘thinking together’ had the same effect. There is some empirical support for this idea. Recently, researchers at Dartmouth College in New Hampshire asked business students to watch movies and discuss their opinions. Looking at scans of brain activity before and after the discussion period, the researchers discovered that brain activity aligned after the discussion. As with the Morning Discourse, the students were thinking together.
Bohm believed that dialogue to be an ancient mode of communication, arguing that communities with coherent meaning probably existed ‘in some groups in the primitive Stone Age conditions’. Indeed, the parallels between the Ju/’hoansi mode of communication and dialogue-based methods are hard to miss. I imagine if there were a Ju/’hoansi philosopher, her description of her society’s decision-making process might read something like Bohm’s On Dialogue .
J ust because hunter-gatherers do something does not make it necessarily good. But according to social psychologists, the features of Ju/’hoansi decision-making are the very ones that make for high collective intelligence. Consider a 2020 article from the Harvard Business Review outlining the best practices that optimise good decision-making in small groups: heterogeneous groups are better than homogeneous ones; dissent is crucial; people should arrive at their opinions independently; people should feel free to speak their minds; individuals should share collective responsibility.
Taken together, there is robust evidence that the Ju/’hoansi are able to avoid levels of polarisation like we see in our current political moment. This is achieved not necessarily through individual virtue but rather with cultural guardrails and prolonged deliberation. The Ju/’hoansi are well aware that their social norms around deliberation improve the quality of their decisions. As Biesele told me, Ju/’hoansi informants would say things like: ‘It’s necessary to draw on the strengths of each person, to minimise the chances that decisions will be made on the basis of the weakness of one or a few persons.’ In contrast, even though discussion generally improves group reasoning performance, people in Western society are poor at recognising this fact.
One Ju/’hoan said: ‘We never wanted to represent our communities: that was a white people’s idea’
The Ju/’hoansi’s political self-consciousness has informed their responses to jarring changes in their lives. Silberbauer conducted his work on the Ju/’hoansi political process from 1958-66. By the late 1960s, mining, cattle and development had dispossessed them of their traditional territories. The autonomous life they had lived for aeons was effectively over. Their houses, mobility, diet and society would be irrevocably changed. So too would their politics. The ‘close-knit, self-sufficient organisation of band society and the completeness of members’ control of its political processes are gone,’ Silberbauer wrote. ‘The “ethnographic present” is now the past.’
Yet Biesele has found that old habits die hard. As founder and director of the nonprofit Kalahari Peoples Fund, she has devoted her career to documenting and aiding the Ju/’hoansi’s transition to modernity. She recorded and translated meetings of the Ju/’hoan people’s organisation that would go on to become the first internationally recognised Conservancy in the new nations of Botswana and Namibia. Biesele has written eloquently about how the Ju/’hoansi have been resistant to give up their old ways of making decisions through consensus. Challenges arose when individuals or small groups were designated as representatives to act as a connection to the government. ‘This was a very foreign idea,’ Biesele said, ‘but the people could see the need for interacting in this way with the new administrations, so they debated how they could possibly do it successfully.’ This task was undertaken with considerable hesitation. One Ju/’hoan said: ‘We never wanted to represent our communities: that was a white people’s idea in the first place.’ As Biesele documents, the Ju/’hoansi have favoured cooperative institutions that tap into their deep history as decision-makers.
M any anthropologists and archaeologists believe that humans lived in nomadic egalitarian bands for much of our species’ history. If this is true, then the Ju/’hoansi and other hunter-gatherers tell us something important about what politics in the Palaeolithic might have looked like. Amid the crackle and pop of a Pleistocene campfire, under the anonymity of darkness, our ancestors began to think as one. In that moment, we became political animals, the first and only species in the history of the world to grasp how its own collective intelligence could be made and unmade.
Just like any hunter-gatherer today, our ancestors would have been self-conscious political actors. They would have realised the importance of the process to the result. And they would have actively maintained political structures that maximised their collective intelligence. Groups that failed to do so would have perished.
Recognising the self-conscious political agency of hunter-gatherers challenges 20th-century perspectives that ‘primitive’ cultures exhibited a uniformity of belief and personality. Describing Aboriginal Australians in 1915, the French sociologist Émile Durkheim wrote:
Nothing could be further from the truth. As Liberman observed in Australia, there is just as much eccentricity and variation in a band of hunter-gatherers as there is among ourselves. This is a critical part of the recipe for high collective intelligence.
We should be gentle with ourselves about the magnitude of our challenge because hunter-gatherers have it easier
Our ancestors would have seen no necessary contradiction between seeking consensus (and compromise) and seeking truth. Yet this is a commonly held view among social scientists who focus solely on a narrow slice of human history – the present; for example, the psychologist Irving Janis, who believed that tight-knit groups were especially prone to groupthink. Or, more recently, the political scientist Jason Brennan, who wrote : ‘Human beings are wired not to seek truth and justice but to seek consensus … They cower before uniform opinion.’ On the contrary, the Ju/’hoansi boast an impressively fertile ecology of conversation that derives, ultimately, from the distinctive combination of high levels of interdependence and egalitarian social norms. With a unified approach to a common goal, along with norms that encourage free and open expression and diverse viewpoints, it is in everyone’s interest to seek the truth.
All of this calls into question our own preoccupation with debate as a form of truth-seeking. In the sphere of communication, prominent book titles include Win Every Argument: The Art of Debating, Persuading, and Public Speaking (2023), Good Arguments: How Debate Teaches Us to Listen and Be Heard (2022), How to Argue and Win Every Time (1995), and The Art of Being Right (1831). Undoubtedly, debate can be useful for presenting alternative viewpoints and hashing out logical inconsistencies. But it often results in little more than hardened views and hurt feelings. Debate is a tool designed to convince, not to solve collective problems. ‘I never yet saw an instance of one of two disputants convincing the other by argument,’ wrote Thomas Jefferson in 1808.
It may seem intractable to scale up insights from the Ju/’hoansi to modern problems of anonymous digital ecosystems and nation-states. Undoubtedly, the Ju/’hoansi style of deliberation is best suited to face-to-face interaction. Lost in the digital world are the subtle cues and gestures that help us to gauge others’ feelings, and to communicate our displeasure. We should be gentle with ourselves about the magnitude of our challenge because, in a sense, hunter-gatherers have it easier: they share similar values and conceptions of the world, their world is smaller, their range of choice narrower and less abstract than ours. Their decisions are more concrete and immediate.
On the other hand, some of our most important decisions still occur in small face-to-face groups, whether it’s in the Oval Office, the corporate boardroom, or the family dinner table. The Ju/’hoansi show us how the best outcomes can be achieved in these groups. Success comes from material interdependence, common purpose and shared meaning. Also critical are the conversational guardrails that enable us to truly think in, and as, groups. This is ancient knowledge that any of us can put into action now: don’t get heated, detach ideas from ego, put yourself in others’ shoes, listen. And always speak your mind."
The birth of our system for describing web content | Aeon Essays,,https://aeon.co/essays/the-birth-of-our-system-for-describing-web-content,"One weekend in March 1995 , a group of librarians and web technologists found themselves in Dublin, Ohio, arguing over what single label should be used to designate a person responsible for the intellectual content of any file that could be found on the world wide web. Many were in favour of using something generic and all-inclusive, such as ‘responsible agent’, but others argued for the label of ‘author’ as the most fundamental and intuitive way to describe the individual creating a document or work. The group then had to decide what to do about the roles of non-authors who also contributed to any given work, like editors and illustrators, without unnecessarily expanding the list. New labels were proposed, and the conversation started over.
The group was participating in a workshop hosted by the OCLC (then the Online Computer Library Center) and the National Center for Supercomputing Applications (NCSA) in an attempt to create a concise but comprehensive set of tags that could be added to every document, from text files to images and maps, that had been uploaded to the web. Arguments about these hypothetical tags raged over the course of the next few days and continued long into the night, often based on wildly different assumptions about the future of the internet. By Saturday afternoon, the workshop co-organiser Stu Weibel was in despair of ever being able to reach any kind of consensus. Yet by the end of the long weekend, the eclectic crowd had created a radical system for describing and discovering online content that still directly powers web search today, and which paved the way for how all content is labelled and discovered on the open web .
Representatives of libraries and publishers and also of the W3C (World Wide Web Consortium) at the 1997 Canberra conference. Courtesy Thomas Baker/ Dublin Core .
Two recent developments had added importance, and pressure, to the 1995 conference: the recent launch of Mosaic, the first widely available web browser; and the subsequent rapid pace of content being uploaded online. Mosaic, which became available to the public in 1993, had a graphical point-and-click interface that anyone could use. Mosaic removed the need to write your own interface to explore the web, so suddenly anyone could go online. Public use of the web leapt dramatically. Mosaic also allowed users to upload their own text documents, images and video online, leading to a spike in content scattered across the web. Meanwhile, a large and growing set of scholarly materials had been moving online throughout the early 1990s, and university librarians were among the first people flagging how difficult these files were to find.
B y 1995, there were half a million unique content-rich pages, text files and other ‘document-like objects’, as the workshop participants called them, on the web, but no good way to search for them without already knowing where and what they were. Early web-search tools, such as Archie and Gopher, could query only for the titles of the files themselves or their locations, meaning that you had to know either the exact name of the file you were looking for, or else where it was located (its full URL, or Uniform Resource Locator). So, for example, if you wanted to find a copy of an essay that you thought someone had posted online, you couldn’t just search for the author’s name and the title of the essay. This barrier made many, if not most, online documents essentially inaccessible to most people. As Weibel wrote in his report for the 1995 workshop: ‘the whereabouts and status of this [online] material is often passed on by word of mouth among members of a given community.’
To make these files truly discoverable for users, some kind of tagging system with top-level information, such as author and subject, was needed: in other words, metadata. In this context, metadata can be thought of as a short set of labels associated with the document that allow you to both find it and know what it is without opening it .
To develop a system that worked, they needed input from ‘the freaks, the geeks, and the ones with sensible shoes’
Librarians have been creating bibliographic metadata for thousands of years. For example, at the Library of Alexandria, each papyrus scroll had a small tag attached to it with information about the title, author and subject of the scroll, so that readers didn’t need to unroll it to know what it was. Librarians could also use these tags to properly re-shelve the scrolls in pots or shelves.
Now the web needed the same thing.
The workshop to solve this problem began as a hallway conversation in 1994 at the second International World Wide Web Conference in Chicago. Weibel, who worked in the research group at the library consortium OCLC, was standing around drinking coffee in the hallway with five or six people, including his boss, Terry Noreault, and Eric Miller, his colleague on the OCLC research team. As Weibel remembers: ‘We were talking about how nice it would be if there were easier ways to find the 500,000 individually addressable objects [documents] on the web … I looked at my boss, and he just nodded and agreed to organise a workshop for it.’
The workshop was quickly co-organised by Weibel and Miller, who wanted to be able to take the results to the next web conference in Germany the following spring. In order to develop a system that worked, they knew they needed input from three different groups of people: encoding and markup experts in specialised disciplines, who could help ensure that metadata was effectively associated with the online files; computer scientists; and librarians, or, as multiple people who attended the first workshop told me with deep affection, ‘the freaks, the geeks, and the ones with sensible shoes’.
Some 52 people showed up to the workshop in Dublin, Ohio. The variety of attendees, and their perspectives on how documents on the web should be organised, was striking. As Priscilla Caplan, a librarian who attended the conference, wrote at the time: ‘There were the IETF [Internet Engineering Task Force] guys, astonishingly young and looking as if they were missing a fraternity party to be there. There were TEI [Text Encoding Initiative] people … geospatial metadata people … publishers and software developers and researchers.’ All had very different goals, but ‘nearly everyone agreed that there was a tremendous need for some standard’.
I n 1995, most librarians were using MARC (MAchine-Readable Cataloguing) to create metadata for their library catalogues. MARC records are complex, extremely long, and require deep expertise to create. These kinds of elaborate descriptions could never work at scale for the entire web. Automated approaches weren’t on the table back then, and it soon became clear to all attendees, even those who had showed up thinking that they might be tweaking an existing system, that the metadata standard for the web would have to be something entirely new: simple enough for anyone to label their own documents as they posted them online, but still meaningful and specific enough for other people and machines to find and index them. A brand-new, simple and succinct metadata system would mean that, for the half a million existing items online, and the millions and billions more that everyone knew were coming, there would need to be one agreed-upon way of adding the metadata tags, with the same kinds of information in the tags themselves.
Creating these labels involved figuring out not just what would be needed to find files that were online now, but also what might be needed later as web content continued to snowball. There was no formalised voting or veto process to come up with the system; each piece of metadata was created through consensus, compromise and, occasionally, real fights. Much of the argument, in fact, concerned the nature of the future no one could truly predict in full.
For example, many attendees didn’t anticipate that automated search engines were coming, though some of the more technical people saw them on the horizon and were pushing requirements for improved geolocated discovery. As Miller says: ‘I remember introducing the [geolocation] coverage element and getting a lot of blowback. I made the point that coverage is going to be local as well as global, like: Find a restaurant near me . We were trying to push the envelope so that we would be ready when other technologies advanced and other services became available.’ Other attendees saw geospatial data as something put in to assuage a person or community, and they weren’t sure it made sense given the need to keep the system lean.
In the beginning, the disagreements seemed insurmountable, and Miller felt disheartened: ‘The first night we thought: This is gonna fail miserably . At first nobody saw eye to eye or trusted each other enough yet to let each other in and try to figure out the art of the possible.’ But as concessions and then agreements were made, people began to feel energised by the creation of a new system, even if imperfect; one piece at a time, their system could bring the content of the web within reach for everyone. As Caplan remembers: ‘By the second day, there was a lot of drinking and all-night working groups. We were running on adrenaline and energy. By the last day, we realised we were making history.’
Dublin Core was revolutionary in its creation of a very new middle ground
The result of all the arguments was ‘Dublin Core’ (DC) metadata, the first metadata standard for describing content on the web. The final short group of DC tags, or metadata ‘elements’, was drawn from a longer list that had been developed, iterated, analysed, argued over, and eventually cut down to a list of 13. In his workshop report, Weibel provided an example of the elements, using the University of Virginia Library’s record of Maya Angelou’s poem ‘On the Pulse of Morning’, transcribed by the library from Angelou’s performance at Bill Clinton’s inauguration:
Fiercely truncated compared to a library catalogue record (MARC records have 999 fields) and simple enough that anyone could create them, DC was revolutionary in its creation of a very new middle ground: a record ‘more informative than an index entry but is less complete than a formal cataloguing record’, as Weibel wrote in his 1995 report. DC tags could be created manually and easily by anyone, not just a librarian, allowing for more documents to be described in a standardised way so that automated tools could index them comprehensively. The ease and simplicity of DC tags, while still being specific enough to be meaningful, were key to their success. As Miller explains, DC ‘makes the simple things simple, and the complex things possible.’
Today, DC looks very familiar and even obvious, in part because it has so deeply influenced the way that metadata is embedded into webpages. Metadata tags, or ‘metatags’, are now a fundamental infrastructure for the open web, where they usually take the form of HTML (HyperText Markup Language), the most used system for displaying content in a web browser. HTML metatags label pages for crawling and indexing by search engines like Google and other web-scale search services. For example, the metatag ” dc.Author ” content= ” Maya Angelou ” /> is flagged and parsed by web indexers to mean that the author of the content on the webpage is Maya Angelou. The information embedded in metatags is used for matching queries to search engine result pages; much of SEO (search engine optimisation) work is just adding comprehensive, detailed metatags.
T he original DC metatags are still being used globally today, and they also directly influenced many others, from the Web 2.0 metatags for social media posts to generic and specific tags for various types of content-rich HTML pages on the web. For example, the Poetry Foundation’s electronic version of ‘On the Pulse of Morning’ contains multiple sets of metadata embedded into the HTML source code: from standard DC metatags, like < ’ dcterms.Title ’ > ; to tags for Twitter/X , like the < ’ twitter:image ’ > tag used to add an image when the poem is shared on the platform; and the Open Graph tag < ’ og:see_also ’ > that Facebook/Meta uses to point users to related content.
DC has directly influenced and shaped the past 30 years of finding things on the web, from RSS feeds to the data models underlying the knowledge graphs and panels that make up the information cards on the front page of Google search results. Miller, who moved from DC to the World Wide Web Consortium (W3C), the group that oversees standards, protocols and languages for the entire web, told me: ‘You can connect any web standard back to certain DC characteristics, lessons learned, or principles … A lot of the stuff that came out of the global standards was directly to solve the industry standards that DC defined.’
The summer after the 1995 workshop, the librarian Lorcan Dempsey, then located at the University of Bath and leading the UK Office for Library and Information Networking (UKOLN), offered to help spread the standard. Dempsey helped to run future workshops, initiating with Weibel the annual conference series known today as the Dublin Core Metadata Initiative (DCMI), which still meets to make tweaks and pose arguments about what changes will be needed for the future. But the core set of metatags has remained remarkably stable. As Weibel told me: ‘After two and a half days, we had reached general consensus about what the elements should be, as well as their characteristics. People are still arguing at Dublin Core conferences, but we did a pretty good job of figuring out the basic skeleton.’
Uptake was quick for the brand-new standard. In 1997, Weibel was at a small workshop in Bonn, Germany, where he learned that a team in Germany, led by Roland Schwänzl at Osnabrück University, had added DC tags for their content pages, representing the first real-world use of the metatags. That moment was critical for Weibel as the point when he saw that this would actually work for people: ‘It marked the first time that I realised the impact of this navel-gazing that we were doing: other people without an intellectual stake went and built systems based on it.’ Once the standards were encoded, the ‘navel-gazing’ had become legacy code, and DC was real.
If there was magic about Dublin Core, it was the social process, not technology
Everyone I interviewed who attended the first few workshops still seems surprised that they managed to reach consensus at all after just a few days, and they all assert that this agreement was as remarkable a product as the list of metadata elements itself. The structure of productive disagreement, and sharing different visions of where things were going, was part of the success. As Weibel describes: ‘The real product, in a way, was consensus. It almost didn’t matter what we ended up with. It wasn’t rocket science – there was no magic about it … we simply had shared problems that needed to be addressed, and it worked.’ If there was magic about DC, it was the social process, not technology, set against the backdrop of profound optimism at the possibility of a new world.
What made the conference difficult was also what made the standards work: the diversity of those involved. Caplan describes the workshop as at times reminding her of the bar scene in Star Trek: Deep Space Nine : ‘dozens of alien species milling about and talking slightly different English-like languages.’ The consensus-driven success of DC, led by a diverse, non-commercial group from wildly different fields, would, frankly, be almost impossible today; even the concept of bringing together a heterogeneous web community to solve web-sized problems with neutral, open standards now seems quaint. DC also encouraged broad use of metadata to improve description and organisation of digital resources, leading to a more connected and discoverable web ecosystem. That ecosystem is disappearing rapidly in the platformised, hyper-corporate space of the current web .
Much of DC’s success had to do with the lack of online commercial activity at the time. Only a few years previous to 1995, the National Science Foundation Network had updated their acceptable use policy to allow commercial traffic. As Weibel describes: ‘Nobody had the blinding light of startups in their eyes … The lack of commerce on the web was in part what allowed there to be an open standard, with real neutrality. Of course, that didn’t last long. It’s hard to imagine now. Everyone wants to start a company.’
The future of the web is firmly moving in the opposite direction of DC values around transparent approaches for information discovery. Large commercial tech platforms work hard to keep users in their walled gardens by utilising in-app black-box algorithms rather than linking out to external locations. Generative AI tools are incentivised to replace exploration of the open web, and almost none of the current text-based generative AI tools cite their sources.
These changes mark what is often described as the end of the open web, where corporations centralise services and invisibly control what you see when you try to find something, with business models that are ever more incentivised to move away from interoperable web standards. The historical period of the open web, which arguably began with Mosaic 30 years ago, is already receding, and perhaps has already ended; the story of DC bookmarks the historical moment of when this era was new, and when the web was a source of almost pure optimism.
The year ‘1995 was an amazing time, just between two worlds,’ Caplan remembers. ‘There was a strong sense that a new information environment was emerging, something like a new dawn,’ Dempsey adds. That environment is now gone, with DC tags acting as a still-functional artefact. As one DC participant told me: ‘That environment, which was just emerging, has disappeared, along with that sense of a movement combined with a standard – what we’re left with is the standard.’"
How changing the metaphors we use can change the way we think | Aeon Essays,,https://aeon.co/essays/how-changing-the-metaphors-we-use-can-change-the-way-we-think,"If Ralph Waldo Emerson was right that ‘language is fossil poetry’, then metaphors undoubtedly represent a significant portion of these linguistic remnants. A particularly well-preserved linguistic fossil example is found in the satirical TV show Veep : after successfully giving an interview designed to divert the public’s attention from an embarrassing diplomatic crisis, the US vice-president – portrayed by the outstanding Julia Louis-Dreyfus – comments to her staff: ‘I spewed out so much bullshit, I’m gonna need a mint.’
When used properly, metaphors enhance speech. But correctly dosing the metaphorical spice in the dish of language is no easy task. They ‘must not be far-fetched, or they will be difficult to grasp, nor obvious, or they will have no effect’, as Aristotle already noted nearly 2,500 years ago. For this reason, artists – those skilled enhancers of experience – are generally thought to be the expert users of metaphors, poets and writers in particular.
Unfortunately, it is likely this association with the arts that has given metaphors a second-class reputation among many thinkers. Philosophers, for example, have historically considered it an improper use of language. A version of this thought still holds significant clout in many scientific circles: if what we care about is the precise content of a sentence (as we often do in science) then metaphors are only a distraction. Analogously, if what we care about is determining how nutritious a meal is, its presentation on the plate should make no difference to this judgment – it might even bias us.
B y the second half of the 20th century, some academics (especially those of a psychological disposition) began turning this thought upside down: metaphors slowly went from being seen as improper-but-inevitable tools of language to essential infrastructure of our conceptual system.
Leading the way were the linguist George Lakoff and the philosopher Mark Johnson. In their influential book , Metaphors We Live By (1980), they assert that ‘most of our ordinary conceptual system is metaphorical in nature’. What they mean by this is that our conceptual system is like a pyramid, with the most concrete elements at the base. Some candidates for these foundational concrete (or ‘literal’) concepts are those of the physical objects we encounter in our every day, like the concepts of rocks and trees. These concrete concepts then ground the metaphorical construction of more abstract concepts further up the pyramid.
Lakoff and Johnson start from the observation that we tend to talk of abstract concepts as we do of literal ones. For instance, we tend to speak of ideas – an abstract concept that we cannot directly observe – with the same language that we use when we speak about plants – a literal concept with numerous observable characteristics. We might say of an interesting idea that ‘it is fruitful’, that someone ‘planted the seed’ of an idea in our heads, and that a bad idea has ‘died on the vine’.
The goal of an argument under the ‘dance’ framing would not be to ‘win’ it but to produce a pleasing final product
It is not just that we speak this way: Lakoff and Johnson take us to really understand and make inferences about the (abstract) concept of an idea from our more tangible understanding of the (concrete) concept of a plant. They conclude that we have the conceptual metaphor IDEAS ARE PLANTS in mind. (Following convention, I will capitalise the conceptual metaphor, wherein the abstract concept comes first and is structured by the second.)
Lakoff and Johnson further illustrate this with the following example. In English, the abstract concept of an argument is typically metaphorically structured through the more concrete concept of a war: we say that we ‘win’ or ‘lose’ arguments; if we think the other party to be uttering nonsense, we say that their claims are ‘indefensible’; and we may perceive ‘weak lines’ in their argument. These terms come from our understanding of war, a concept we are disconcertingly familiar with.
The novelty of Lakoff and Johnson’s proposal is not in noticing the ubiquity of metaphorical language but in emphasising that metaphors go beyond casual speech: ‘many of the things we do in arguing are partially structured by the concept of war.’ To see this, they suggest another conceptual metaphor, ARGUMENT IS A DANCE. Dancing is decisively a more cooperative enterprise than war – the goal of an argument under this framing would not be to ‘win’ it but to produce a pleasing final product or performance that both parties enjoy. The dynamics of how we’d think about an argument under such a framing would be very different. This highlights the role of metaphors in creating reality rather than simply helping to represent it.
M etaphors thus seem to provide the foundation of how we conceptualise abstract concepts (and, therefore, much of the world). A single metaphor, though, only partly structures complex concepts – typically, more are used. Take the concept of romantic love. A widespread conceptual metaphor in a variety of languages is ROMANTIC LOVE IS A JOURNEY. It is common to say that a relationship is ‘at a crossroads’ when an important decision must be made, or that people ‘go their separate ways’ when they split. (Charles Baudelaire’s 1857 poem ‘Invitation au voyage’ is a notable play on this conceptual metaphor, where the speaker invites a woman to both a metaphorical and a literal journey.) Again, these metaphorical conceptualisations greatly affect how we act in a relationship: without the notion of a crossroads in my relationship, I probably would not have considered the need for a serious conversation with my partner about our state.
But love, so important for human life, is partially structured by innumerably many other metaphors. Another common one – perhaps fossilised by Ovid’s poem with the same title – is ROMANTIC LOVE IS WAR. It is common to read that one party ‘conquers’ the other or is ‘gaining ground’ with an initially reluctant partner, and that one’s hand can be ‘won’ for marriage. (Already with this example, we see that pervasive metaphorical framings can have not-so-subtle misogynistic undertones.)
To the eternal question ‘What is love?’, conceptual metaphor theory has an answer: the bundle of metaphors that are used to conceptualise it. LOVE IS A JOURNEY and LOVE IS WAR are two instances of this bundle that highlight and create different aspects of the concept of love.
Any speaker knows that the language we use matters, and that there is a complex feedback between the language we speak and the thoughts we think. Empirical studies support this intuition: having different conceptual metaphors in mind, people will tend to make different decisions in the same context (a reasonable indicator that they harbour different concepts).
Metaphors influence opinions, including how people view climate change or the police
In one such study , two groups were shown a report on the rising crime rate in a city. One group received a report that opened with the statement ‘Crime is a virus ravaging the city,’ while the other group received a report that started with ‘Crime is a beast ravaging the city.’ The two groups were thus primed to metaphorically structure the concept of crime with two distinct concepts: virus or beast. They were then asked about which measures they would implement to solve the crime problem. Those who were primed to have the conceptual metaphor CRIME IS A BEAST were much more likely to recommend punitive measures, such as increasing the police force and putting criminals in jail (just as one would, presumably, put a beast in a cage). Those who were primed to entertain CRIME IS A VIRUS tended to suggest measures that are associated with epidemiology: to contain the problem, to identify the cause and treat it, and to implement social reforms. Remarkably, the participants were not aware of the effect these metaphorical framings had on their choices. When asked why they chose the solutions they did, respondents ‘generally identified the crime statistics, which were the same for both groups, and not the metaphor, as the most influential aspect of the report.’
Crime is not an outlier: studies with similar setups strongly suggest that the choice of conceptual metaphors significantly influences the opinions and decisions of individuals in a variety of settings. Among others, these include how people view the threat of climate change, their attitudes towards the police, and their financial decision-making.
The significance of metaphors and analogical thinking is even more pronounced in children. Spearheaded by work by the cognitive scientists Dedre Gentner and Keith Holyoak, the study of analogical reasoning is now a flourishing research programme. There is considerable evidence of the importance of the use of analogy in the development of children; studies suggest that relational thinking – essential for making analogies – predicts children’s test scores and reasoning skills. Though many of these studies have yet to be replicated, metaphors seem to literally shape the brain.
It is also not an exaggeration to say that metaphors scaffold science, that conceptual system of organising knowledge. In Polarity and Analogy (1966), a fascinating study of the use of analogies and metaphors in ancient Greek science, the historian Sir Geoffrey Lloyd makes a compelling case for the importance of analogies in guiding early scientific thought. For example, Lloyd highlights how analogies with political organisations shaped views about the cosmos. A typical ancient Greek approach to explain the Universe involved postulating fundamental substances and then explaining how these interact (Empedocles famously proposed that the four fundamental substances are fire, air, water, and earth). To help determine the relations between the substances, these ancient scientists would invoke analogies with their political systems. One prominent conceptual metaphor used was the COSMOS IS A MONARCHY, where a single substance has supreme power over the others. This language is still used in modern-day physics when we hear that the laws of the Universe govern our world. Another prevalent conceptual metaphor was the COSMOS IS A DEMOCRACY; this framing, which appeared only after democracy was established in Athens, holds that the fundamental substances are in equal rank and function with a sort of contract among themselves.
This use of political metaphors is not just stylistic. Lloyd writes that ‘time and again in the Presocratics and Plato, the nature of cosmological factors, or the relationships between them, are understood in terms of a concrete social or political situation’. From the point of view of conceptual metaphor theory, this makes sense: to understand a new, abstract and invisible concept (the fundamental substances of the Universe), it is only natural that these thinkers analogised it to phenomena they had direct experience with (their political organisation).
Metaphors and analogies are not mere artefacts of ancient science but also vital instruments of the contemporary scientific orchestra. They help formulate and frame theories: political metaphors, not unlike those used by the ancient Greeks, are frequent in modern biology, which is rife with the language of ‘regulators’ – invoking the regulatory bodies now present in modern governments. These metaphors highlight the checks and balances that exist within complex biological systems, paralleling the way government regulators maintain order in their respective domains. Military metaphors are also common: the immune system is repeatedly framed as an army that protects the body from ‘invading’ pathogens. Metabolic pathways are also often analogised to freeways, equipped with ‘bypasses’, and sometimes experiencing ‘roadblocks’ or ‘traffic’, as noted by the philosopher Lauren Ross.
Analogies are also central for generating new hypotheses (what we might call scientific creativity). A notable example is that of Charles Darwin’s idea of natural selection, which he came to by drawing an analogy with the selective practices of farmers. Roughly, the analogy could be cashed out as follows: nature selects organisms for fitness in a similar way that farmers select the best crops for taste, disease resistance and other attributes.
G iven the nature of our metaphorical minds, it is worth asking: are our conceptual metaphors apt? We owe it to ourselves and others to reflect on the appropriateness of the metaphors we employ to frame the world. These choices – conscious or not – can be constructive or disastrous.
Consider the metaphorical discourse between doctors and patients in cancer care. These conversations shape how the patients judge their own experience and so, inevitably, impact their wellbeing. War metaphors are ubiquitous , which says a lot about our culture. Cancer care , unsurprisingly, is no different: patients are often said to be ‘fighting a battle’ with cancer and are judged on their ‘fighting spirit’. Research, however, suggests that this conceptual metaphor causes real harm to some patients. For example , the Stanford palliative care doctor Vyjeyanthi Periyakoil found that ‘opting to refuse futile or harmful treatment options now becomes equivalent to a cowardly retreat from the “battleground” that may be seen as a shameful act by the patient’. In other words, a patient who is already preoccupied with dying from the disease may feel the additional – unnecessary and cruel – shame for not continuing to ‘fight’.
An oncologist review article urges nurses and doctors to rethink the usefulness of this militaristic metaphor. The alternative proposed is to use the conceptual metaphor CANCER IS A JOURNEY to frame the patient experience. Reconceptualising it in this way leads to different thoughts: cancer is not a battle to be conquered, but an individual and unique path to navigate; the experience with the disease is not something that ends (as war typically does) but an ongoing neverending process (with periodic hospital visits to monitor any recurrence).
Any suggested conceptual re-engineering needs to be tested to see if it actually works better than the previous framing. This seems to be the case for the journey metaphor: patients who reframed their cancer experience in this way had a more positive outlook, generally increased wellbeing and reported spiritual growth. (I suspect that a similar mindset switch would do a lot of good for people suffering from mental health and chronic diseases, since these are even less obviously distinct entities that need to be ‘fought’, but rather experiences patients have to live with, often for the rest of their lives.)
The war metaphor is also known to increase racist sentiments, something we’ve seen during the pandemic
Being clear at both linguistic ends – patient and doctor, and more generally non-expert and expert – on what metaphors are used to conceptualise illness is critical: two interlocutors speaking about what they think is the same concept, but each framing that concept with a different metaphor, is a recipe for miscommunication. And miscommunication can be painful, especially when one party is experiencing a disease that profoundly consumes every aspect of their being.
We should also question current metaphorical framing of complex societal challenges – writing in The New York Times in 2010, the economist Paul Krugman warns that ‘bad metaphors make for bad policy’. The COVID-19 pandemic is a case in point: the long-standing practice to employ war metaphors to speak about pandemics was a trend observed with the coronavirus outbreak as well. Common phrases included ‘nurses in the trenches’, healthcare workers as a ‘first line of defence’, and politicians announcing that the nation is at ‘war’ against an invisible enemy.
At first examination, war metaphors might seem to convey the gravity of the situation and mobilise people for action. But it is important in such cases to consider the unintended consequences that come with a choice of metaphorical framing. War, for example, generally requires intense nationwide mobilisation for action, whereas plagues require the majority of the population to stay home and do nothing. The war metaphor is also known to increase racist sentiments, something we’ve seen during the COVID-19 pandemic.
As an alternative, some linguists have suggested that a more fitting metaphor would be to reconceptualise it as the PANDEMIC IS A FIRE, since this emphasises the urgency and destructiveness of the health crisis, while avoiding some of the drawbacks of the war metaphor. This is not to say that it is wrong or unethical to have in mind the PANDEMIC IS A WAR – it could be that the war framing is in fact the best to mobilise people and motivate them to stay home during pandemic emergencies. The point is, rather, that knowing its potential problems should prompt us to use the metaphor with extra precautions.
It should be clear that the power a choice of metaphor(s) has in structuring our thoughts makes the tool vulnerable to be hijacked by grifters and politicians to advance their own agenda. To take but one example, in 2017 Donald Trump used a version of Aesop’s fable of The Farmer and the Snake to metaphorically frame immigrants in a negative light. The fable recounts a farmer who, on her way home, finds a freezing and ill snake. Taking pity on the creature, the woman brings it home and keeps it warm. On her way back from work the next day, she sees that the snake is healthy again. Consumed by joy, she gives the snake a hug. The snake, in turn, fatally bites her. The farmer asks the snake why it would do such a thing; feeling no remorse, the snake says: ‘You knew damn well I was a snake before you took me in.’ By reading out this story in a speech, Trump primed the audience to conceptualise that IMMIGRANTS ARE SNAKES, and the UNITED STATES IS A WOMAN. The philosopher Katharina Stevens makes a convincing case that Trump used this fable to lend support to the belief that immigrants are a national security threat (just as the snake is a threat to the woman).
Metaphors can also perpetuate a language of dehumanisation that paves the conceptual road for the worst kinds of human atrocities. During the Rwandan genocide, the country’s main radio station played a key role in framing how its Hutu majority saw the Tutsi minority: they repeatedly used metaphors to dehumanise the Tutsis – a well-known example is of analogising Tutsis to cockroaches. When such a metaphor is so internalised that it structures the concept people have of such a group, it follows almost immediately that they will want to get rid of them (just as they would of actual cockroaches). That is what happened. The particularly frightening power of conceptual metaphors is not that a group is seen unfavourably and then , to emphasise this point of view, referred to by dehumanising metaphors. Rather, it is that the metaphorical construction used to frame a particular group in the first place is a reason why the other group sees them that way. Lakoff was right when he warned that ‘Metaphors can kill.’
S uppose we notice that we harbour concepts whose metaphorical foundation causes harm. Can we really reconstruct the concept with a different metaphorical foundation? Lakoff and Johnson think so – I hope they are right, even if doing so is no easy task.
The first step is to notice the metaphor; this is not always obvious. One way of reconstructing part of the history of feminist thought is to say that the thinkers spotted the pernicious metaphor of framing women as objects in the conceptual structure of the patriarchal society around them. Among those who pointed out the pervasive conceptual metaphor WOMEN ARE OBJECTS was the feminist Andrea Dworkin, who wrote that ‘objectification occurs when a human being … is made less than human, turned into a thing or commodity’. Though in contemporary discourse there is an acknowledgment that this conceptualisation is widespread (consciously or not), at the time of writing Woman Hating (1974), Dworkin explicitly emphasises the need to make people aware of it.
Once the conceptual metaphor is explicitly spelled out, the next step is to argue why it is undesirable and in need of change. With objectification, many ethical problems arise; significantly, the autonomy of the woman is reduced, which enables unbalanced power dynamics. This is a considerable harm in need of imperative remedy. To fight back, feminist writers have searched for the cause of this metaphorical conceptualisation and sought – and continue to seek – to dismantle it. (Dworkin and her fellow feminist Catharine MacKinnon take pornography to be a primary cause, though this has been challenged by other thinkers.)
The most important first step is to be aware that a concept we have is constructed metaphorically
The deeper a metaphor is rooted in the collective psyche, the harder it is to replace. But, even when ingrained, small changes can sometimes have important effects. One such minor change was done by The Guardian : in 2019, they changed their style guide to advise authors to use the term climate ‘crisis’ or ‘emergency’ instead of climate ‘change’. The editor-in-chief, Katharine Viner, justified this by noting that the current language sounded ‘rather passive and gentle when what scientists are talking about is a catastrophe for humanity’. This sort of change in language can slowly alter how readers understand the gravity of the climate situation.
Much research still needs to be done. How can we know whether a conceptual metaphor is doing what we want of it? What features do good alternative conceptual metaphors have in common? How can we successfully dismantle the foundational metaphor of a concept? Some harmful metaphors will be harder to free ourselves from than others; however, the most important first step is to be aware that a concept we have is constructed metaphorically. Finding these should, in many cases, be rather easy: after all, as the philosopher Nelson Goodman observes, ‘metaphor permeates all discourse, ordinary and special, and we should have a hard time finding a purely literal paragraph anywhere.’
Metaphors are (metaphorically) woven into the fabric of our language and thought, shaping how we grasp and articulate abstract concepts. We should therefore feel free to prudently explore alternative metaphors and judge whether they perform better. A collective effort to notice and change the metaphors we use has enormous potential to reduce individual and societal harm."
